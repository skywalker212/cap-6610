{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d971c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc0d9158",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2034411",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "077ffd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "batch_size = 128\n",
    "nz = 100\n",
    "lr = 0.001\n",
    "b1 = 0.5\n",
    "b2 = 0.9999\n",
    "epochs = 100\n",
    "log_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17d43c6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x109f786b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf207795",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dirname = os.path.dirname('.')\n",
    "data_directory = os.path.join(curr_dirname, \"data\")\n",
    "gan_results_directory_name = \"results/gan\"\n",
    "os.makedirs(gan_results_directory_name, exist_ok=True)\n",
    "gan_results_directory = os.path.join(curr_dirname, gan_results_directory_name)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        data_directory, train=True, download=False, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5],[0.5])])\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "babee2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88e98723",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # create a reusable block with #in_feat input features and #out_feat output features\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(nz, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            nn.Linear(512, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # create a reusable block with #in_feat input features and #out_feat output features\n",
    "        def block(in_feat, out_feat):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(int(np.prod(img_shape)), 512),\n",
    "            *block(512, 256),\n",
    "            *block(256, 128),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        out = self.model(img_flat)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66126659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "adversarial_loss = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71452237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize generator and discriminator\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "580ba82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizers\n",
    "optimizer_generator = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cb9fed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ea17137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tDiscriminator Loss: 0.693879\tGenerator Loss: 0.683723\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tDiscriminator Loss: 0.534340\tGenerator Loss: 0.717528\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tDiscriminator Loss: 1.110346\tGenerator Loss: 0.120707\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tDiscriminator Loss: 0.917732\tGenerator Loss: 0.222989\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tDiscriminator Loss: 0.505935\tGenerator Loss: 0.729454\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tDiscriminator Loss: 0.473992\tGenerator Loss: 0.787526\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tDiscriminator Loss: 0.660993\tGenerator Loss: 1.191703\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tDiscriminator Loss: 0.784841\tGenerator Loss: 0.369021\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tDiscriminator Loss: 0.719712\tGenerator Loss: 0.389931\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tDiscriminator Loss: 0.666876\tGenerator Loss: 0.610794\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tDiscriminator Loss: 0.594827\tGenerator Loss: 0.685070\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tDiscriminator Loss: 0.588460\tGenerator Loss: 1.875387\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tDiscriminator Loss: 0.621758\tGenerator Loss: 0.618418\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tDiscriminator Loss: 0.293191\tGenerator Loss: 1.438888\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tDiscriminator Loss: 0.595956\tGenerator Loss: 0.500568\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tDiscriminator Loss: 0.723201\tGenerator Loss: 0.363034\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tDiscriminator Loss: 0.533988\tGenerator Loss: 1.146230\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tDiscriminator Loss: 0.656779\tGenerator Loss: 0.618197\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tDiscriminator Loss: 0.577822\tGenerator Loss: 0.648496\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tDiscriminator Loss: 0.568271\tGenerator Loss: 0.712036\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tDiscriminator Loss: 0.472651\tGenerator Loss: 1.065974\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tDiscriminator Loss: 0.578004\tGenerator Loss: 0.661489\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tDiscriminator Loss: 0.517496\tGenerator Loss: 1.652653\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tDiscriminator Loss: 0.416319\tGenerator Loss: 1.125486\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tDiscriminator Loss: 0.561831\tGenerator Loss: 1.118916\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tDiscriminator Loss: 0.553633\tGenerator Loss: 0.861547\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tDiscriminator Loss: 0.875001\tGenerator Loss: 0.326741\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tDiscriminator Loss: 0.587065\tGenerator Loss: 0.771611\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tDiscriminator Loss: 0.597590\tGenerator Loss: 0.999647\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tDiscriminator Loss: 0.635083\tGenerator Loss: 0.889355\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tDiscriminator Loss: 1.050508\tGenerator Loss: 0.169578\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tDiscriminator Loss: 0.702921\tGenerator Loss: 0.442459\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tDiscriminator Loss: 0.542417\tGenerator Loss: 0.941878\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tDiscriminator Loss: 0.579936\tGenerator Loss: 1.133669\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tDiscriminator Loss: 0.980374\tGenerator Loss: 0.195542\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tDiscriminator Loss: 0.562608\tGenerator Loss: 0.653594\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tDiscriminator Loss: 0.892580\tGenerator Loss: 0.231760\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tDiscriminator Loss: 0.492827\tGenerator Loss: 1.035132\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tDiscriminator Loss: 0.648316\tGenerator Loss: 0.652503\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tDiscriminator Loss: 0.604448\tGenerator Loss: 0.777824\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tDiscriminator Loss: 0.598464\tGenerator Loss: 0.747508\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tDiscriminator Loss: 0.559800\tGenerator Loss: 1.095971\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tDiscriminator Loss: 0.488974\tGenerator Loss: 0.795441\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tDiscriminator Loss: 0.631234\tGenerator Loss: 0.554468\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tDiscriminator Loss: 0.483536\tGenerator Loss: 1.521441\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tDiscriminator Loss: 0.501325\tGenerator Loss: 0.964530\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tDiscriminator Loss: 0.580586\tGenerator Loss: 0.952316\n",
      "Train Epoch: 2 [0/60000 (0%)]\tDiscriminator Loss: 0.516788\tGenerator Loss: 1.086032\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tDiscriminator Loss: 0.552333\tGenerator Loss: 1.061672\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tDiscriminator Loss: 0.719413\tGenerator Loss: 0.387720\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tDiscriminator Loss: 0.592642\tGenerator Loss: 1.287630\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tDiscriminator Loss: 0.558750\tGenerator Loss: 0.854981\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tDiscriminator Loss: 0.542455\tGenerator Loss: 1.303660\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tDiscriminator Loss: 0.625467\tGenerator Loss: 1.038631\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tDiscriminator Loss: 0.523552\tGenerator Loss: 1.078716\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tDiscriminator Loss: 0.531688\tGenerator Loss: 1.152357\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tDiscriminator Loss: 0.643004\tGenerator Loss: 1.885158\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tDiscriminator Loss: 0.590528\tGenerator Loss: 0.624009\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tDiscriminator Loss: 0.560217\tGenerator Loss: 0.838968\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tDiscriminator Loss: 0.764551\tGenerator Loss: 0.334300\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tDiscriminator Loss: 0.495907\tGenerator Loss: 1.604870\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tDiscriminator Loss: 0.416555\tGenerator Loss: 1.112167\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tDiscriminator Loss: 0.567495\tGenerator Loss: 0.995715\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tDiscriminator Loss: 0.625036\tGenerator Loss: 0.502751\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tDiscriminator Loss: 0.473798\tGenerator Loss: 1.359895\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tDiscriminator Loss: 0.668064\tGenerator Loss: 0.461723\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tDiscriminator Loss: 0.537770\tGenerator Loss: 0.771710\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tDiscriminator Loss: 0.508590\tGenerator Loss: 1.778202\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tDiscriminator Loss: 0.613426\tGenerator Loss: 0.572747\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tDiscriminator Loss: 0.646574\tGenerator Loss: 1.614416\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tDiscriminator Loss: 0.679230\tGenerator Loss: 0.435052\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tDiscriminator Loss: 0.709315\tGenerator Loss: 0.383714\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tDiscriminator Loss: 0.516998\tGenerator Loss: 1.174961\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tDiscriminator Loss: 0.645979\tGenerator Loss: 0.847976\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tDiscriminator Loss: 0.525625\tGenerator Loss: 0.889357\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tDiscriminator Loss: 0.463991\tGenerator Loss: 1.290324\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tDiscriminator Loss: 0.565188\tGenerator Loss: 1.063507\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tDiscriminator Loss: 0.540389\tGenerator Loss: 1.077371\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tDiscriminator Loss: 0.568253\tGenerator Loss: 0.627344\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tDiscriminator Loss: 0.565593\tGenerator Loss: 0.716603\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tDiscriminator Loss: 0.426401\tGenerator Loss: 1.318626\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tDiscriminator Loss: 0.550745\tGenerator Loss: 0.998971\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tDiscriminator Loss: 0.854256\tGenerator Loss: 0.296722\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tDiscriminator Loss: 0.553939\tGenerator Loss: 1.102939\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tDiscriminator Loss: 0.545573\tGenerator Loss: 0.717969\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tDiscriminator Loss: 0.486018\tGenerator Loss: 0.991673\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tDiscriminator Loss: 0.563296\tGenerator Loss: 1.741072\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tDiscriminator Loss: 0.592524\tGenerator Loss: 0.871216\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tDiscriminator Loss: 0.487734\tGenerator Loss: 1.352458\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tDiscriminator Loss: 0.648025\tGenerator Loss: 0.621308\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tDiscriminator Loss: 0.574397\tGenerator Loss: 0.871733\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tDiscriminator Loss: 0.465724\tGenerator Loss: 1.111651\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tDiscriminator Loss: 0.650717\tGenerator Loss: 0.606929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [58880/60000 (98%)]\tDiscriminator Loss: 0.510121\tGenerator Loss: 1.106806\n",
      "Train Epoch: 3 [0/60000 (0%)]\tDiscriminator Loss: 0.463550\tGenerator Loss: 1.164997\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tDiscriminator Loss: 0.544598\tGenerator Loss: 1.212850\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tDiscriminator Loss: 0.455523\tGenerator Loss: 1.116555\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tDiscriminator Loss: 0.585417\tGenerator Loss: 0.910444\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tDiscriminator Loss: 0.498483\tGenerator Loss: 1.047812\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tDiscriminator Loss: 0.579635\tGenerator Loss: 0.803064\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tDiscriminator Loss: 0.497153\tGenerator Loss: 1.114142\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tDiscriminator Loss: 0.514507\tGenerator Loss: 1.501490\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tDiscriminator Loss: 0.605591\tGenerator Loss: 0.656162\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tDiscriminator Loss: 0.482046\tGenerator Loss: 1.530283\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tDiscriminator Loss: 0.596950\tGenerator Loss: 0.989653\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tDiscriminator Loss: 0.489816\tGenerator Loss: 1.291976\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tDiscriminator Loss: 0.622397\tGenerator Loss: 0.691400\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tDiscriminator Loss: 0.489981\tGenerator Loss: 1.377427\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tDiscriminator Loss: 0.634141\tGenerator Loss: 0.712652\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tDiscriminator Loss: 0.502726\tGenerator Loss: 1.167661\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tDiscriminator Loss: 1.007330\tGenerator Loss: 0.178968\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tDiscriminator Loss: 0.516891\tGenerator Loss: 1.253868\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tDiscriminator Loss: 0.537928\tGenerator Loss: 1.444199\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tDiscriminator Loss: 0.613624\tGenerator Loss: 2.202220\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tDiscriminator Loss: 0.592120\tGenerator Loss: 0.545083\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tDiscriminator Loss: 0.781899\tGenerator Loss: 0.304223\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tDiscriminator Loss: 1.243987\tGenerator Loss: 0.097694\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tDiscriminator Loss: 0.535450\tGenerator Loss: 1.280034\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tDiscriminator Loss: 0.566543\tGenerator Loss: 1.556264\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tDiscriminator Loss: 0.465731\tGenerator Loss: 1.178181\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tDiscriminator Loss: 0.565044\tGenerator Loss: 1.365164\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tDiscriminator Loss: 1.301325\tGenerator Loss: 0.090977\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tDiscriminator Loss: 0.478733\tGenerator Loss: 1.312541\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tDiscriminator Loss: 0.486312\tGenerator Loss: 1.177659\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tDiscriminator Loss: 0.533227\tGenerator Loss: 1.877569\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tDiscriminator Loss: 0.737395\tGenerator Loss: 0.344070\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tDiscriminator Loss: 0.609173\tGenerator Loss: 1.937098\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tDiscriminator Loss: 0.538725\tGenerator Loss: 0.888432\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tDiscriminator Loss: 0.598526\tGenerator Loss: 0.718394\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tDiscriminator Loss: 0.456904\tGenerator Loss: 1.229348\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tDiscriminator Loss: 0.619719\tGenerator Loss: 1.000580\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tDiscriminator Loss: 0.559931\tGenerator Loss: 1.026078\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tDiscriminator Loss: 1.020623\tGenerator Loss: 0.176797\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tDiscriminator Loss: 0.528799\tGenerator Loss: 0.847627\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tDiscriminator Loss: 0.461895\tGenerator Loss: 1.046993\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tDiscriminator Loss: 0.611330\tGenerator Loss: 0.559873\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tDiscriminator Loss: 0.616941\tGenerator Loss: 0.812447\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tDiscriminator Loss: 0.518587\tGenerator Loss: 1.079052\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tDiscriminator Loss: 1.114975\tGenerator Loss: 0.172981\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tDiscriminator Loss: 0.502689\tGenerator Loss: 1.122377\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tDiscriminator Loss: 0.546900\tGenerator Loss: 0.658029\n",
      "Train Epoch: 4 [0/60000 (0%)]\tDiscriminator Loss: 0.520261\tGenerator Loss: 1.937087\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tDiscriminator Loss: 0.541598\tGenerator Loss: 2.324641\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tDiscriminator Loss: 0.474567\tGenerator Loss: 1.681987\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tDiscriminator Loss: 0.766614\tGenerator Loss: 3.369439\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tDiscriminator Loss: 0.532391\tGenerator Loss: 1.031858\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tDiscriminator Loss: 0.503427\tGenerator Loss: 1.411539\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tDiscriminator Loss: 0.446121\tGenerator Loss: 1.501458\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tDiscriminator Loss: 0.564489\tGenerator Loss: 0.872807\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tDiscriminator Loss: 0.808335\tGenerator Loss: 2.898985\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tDiscriminator Loss: 0.512305\tGenerator Loss: 1.204630\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tDiscriminator Loss: 0.409291\tGenerator Loss: 1.035988\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tDiscriminator Loss: 0.461315\tGenerator Loss: 1.487365\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tDiscriminator Loss: 0.640009\tGenerator Loss: 0.562220\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tDiscriminator Loss: 0.499446\tGenerator Loss: 1.022970\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tDiscriminator Loss: 0.528899\tGenerator Loss: 2.692851\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tDiscriminator Loss: 0.574889\tGenerator Loss: 0.788465\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tDiscriminator Loss: 0.501740\tGenerator Loss: 2.390087\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tDiscriminator Loss: 0.485260\tGenerator Loss: 1.111705\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tDiscriminator Loss: 0.610494\tGenerator Loss: 0.590142\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tDiscriminator Loss: 0.497859\tGenerator Loss: 1.206430\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tDiscriminator Loss: 0.511624\tGenerator Loss: 1.412349\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tDiscriminator Loss: 0.416393\tGenerator Loss: 1.507137\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tDiscriminator Loss: 0.551883\tGenerator Loss: 0.605383\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tDiscriminator Loss: 0.535944\tGenerator Loss: 0.803914\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tDiscriminator Loss: 0.499723\tGenerator Loss: 1.114692\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tDiscriminator Loss: 0.437385\tGenerator Loss: 1.832387\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tDiscriminator Loss: 0.518499\tGenerator Loss: 2.512901\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tDiscriminator Loss: 0.451946\tGenerator Loss: 1.276607\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tDiscriminator Loss: 0.488869\tGenerator Loss: 1.607831\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tDiscriminator Loss: 0.392093\tGenerator Loss: 1.497206\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tDiscriminator Loss: 0.461954\tGenerator Loss: 1.158004\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tDiscriminator Loss: 0.441380\tGenerator Loss: 1.038435\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tDiscriminator Loss: 0.416394\tGenerator Loss: 1.166383\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tDiscriminator Loss: 0.453819\tGenerator Loss: 1.552766\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tDiscriminator Loss: 0.737459\tGenerator Loss: 0.369205\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tDiscriminator Loss: 0.377006\tGenerator Loss: 1.922982\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tDiscriminator Loss: 0.434868\tGenerator Loss: 1.364586\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tDiscriminator Loss: 0.442686\tGenerator Loss: 1.370904\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tDiscriminator Loss: 0.415258\tGenerator Loss: 1.396578\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tDiscriminator Loss: 0.506948\tGenerator Loss: 0.944015\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tDiscriminator Loss: 0.451403\tGenerator Loss: 1.197679\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tDiscriminator Loss: 0.385931\tGenerator Loss: 1.482157\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tDiscriminator Loss: 0.416238\tGenerator Loss: 1.192460\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tDiscriminator Loss: 0.658893\tGenerator Loss: 0.476428\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tDiscriminator Loss: 0.428234\tGenerator Loss: 1.412122\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tDiscriminator Loss: 0.613354\tGenerator Loss: 0.611621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [58880/60000 (98%)]\tDiscriminator Loss: 0.477148\tGenerator Loss: 0.979776\n",
      "Train Epoch: 5 [0/60000 (0%)]\tDiscriminator Loss: 0.480630\tGenerator Loss: 1.397159\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tDiscriminator Loss: 0.456333\tGenerator Loss: 1.778709\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tDiscriminator Loss: 0.540558\tGenerator Loss: 0.742781\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tDiscriminator Loss: 0.499744\tGenerator Loss: 1.399389\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tDiscriminator Loss: 0.558147\tGenerator Loss: 1.086745\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tDiscriminator Loss: 0.461846\tGenerator Loss: 1.930776\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tDiscriminator Loss: 0.638113\tGenerator Loss: 0.567514\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tDiscriminator Loss: 1.024662\tGenerator Loss: 0.180709\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tDiscriminator Loss: 0.490994\tGenerator Loss: 1.403164\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tDiscriminator Loss: 0.518836\tGenerator Loss: 1.505994\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tDiscriminator Loss: 0.470338\tGenerator Loss: 1.177005\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tDiscriminator Loss: 0.535139\tGenerator Loss: 1.488720\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tDiscriminator Loss: 0.425874\tGenerator Loss: 1.208608\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tDiscriminator Loss: 0.448290\tGenerator Loss: 1.074543\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tDiscriminator Loss: 0.552749\tGenerator Loss: 0.879315\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tDiscriminator Loss: 0.501677\tGenerator Loss: 1.739459\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tDiscriminator Loss: 0.465394\tGenerator Loss: 1.119784\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tDiscriminator Loss: 0.620931\tGenerator Loss: 0.588067\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tDiscriminator Loss: 0.489268\tGenerator Loss: 1.001400\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tDiscriminator Loss: 0.670702\tGenerator Loss: 0.446919\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tDiscriminator Loss: 0.461400\tGenerator Loss: 1.495157\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tDiscriminator Loss: 0.465895\tGenerator Loss: 1.178356\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tDiscriminator Loss: 1.179262\tGenerator Loss: 0.130647\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tDiscriminator Loss: 0.528612\tGenerator Loss: 0.994767\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tDiscriminator Loss: 0.686809\tGenerator Loss: 0.405497\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tDiscriminator Loss: 0.641200\tGenerator Loss: 0.617755\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tDiscriminator Loss: 0.897547\tGenerator Loss: 0.283647\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tDiscriminator Loss: 0.491071\tGenerator Loss: 1.039564\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tDiscriminator Loss: 0.528542\tGenerator Loss: 1.011050\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tDiscriminator Loss: 0.432817\tGenerator Loss: 1.474893\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tDiscriminator Loss: 0.577965\tGenerator Loss: 0.836236\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tDiscriminator Loss: 0.568507\tGenerator Loss: 1.053817\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tDiscriminator Loss: 0.512721\tGenerator Loss: 0.781651\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tDiscriminator Loss: 0.559065\tGenerator Loss: 1.458375\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tDiscriminator Loss: 0.541242\tGenerator Loss: 1.284995\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tDiscriminator Loss: 0.454385\tGenerator Loss: 1.637524\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tDiscriminator Loss: 0.533525\tGenerator Loss: 1.124046\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tDiscriminator Loss: 0.517155\tGenerator Loss: 1.129788\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tDiscriminator Loss: 0.455998\tGenerator Loss: 1.717770\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tDiscriminator Loss: 0.563732\tGenerator Loss: 0.923150\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tDiscriminator Loss: 0.557375\tGenerator Loss: 0.864855\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tDiscriminator Loss: 0.593058\tGenerator Loss: 0.551024\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tDiscriminator Loss: 0.512497\tGenerator Loss: 1.536974\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tDiscriminator Loss: 0.534311\tGenerator Loss: 0.951904\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tDiscriminator Loss: 0.521437\tGenerator Loss: 1.557766\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tDiscriminator Loss: 0.555572\tGenerator Loss: 1.030792\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tDiscriminator Loss: 0.523543\tGenerator Loss: 0.904758\n",
      "Train Epoch: 6 [0/60000 (0%)]\tDiscriminator Loss: 0.504083\tGenerator Loss: 1.391757\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tDiscriminator Loss: 0.485980\tGenerator Loss: 1.090516\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tDiscriminator Loss: 0.570976\tGenerator Loss: 2.181825\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tDiscriminator Loss: 0.557932\tGenerator Loss: 1.308325\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tDiscriminator Loss: 0.505700\tGenerator Loss: 0.842714\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tDiscriminator Loss: 0.559075\tGenerator Loss: 1.087632\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tDiscriminator Loss: 0.512273\tGenerator Loss: 1.156689\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tDiscriminator Loss: 0.524091\tGenerator Loss: 1.020889\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tDiscriminator Loss: 0.452549\tGenerator Loss: 1.458843\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tDiscriminator Loss: 0.573270\tGenerator Loss: 0.928594\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tDiscriminator Loss: 0.544738\tGenerator Loss: 0.810483\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tDiscriminator Loss: 0.526941\tGenerator Loss: 1.771922\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tDiscriminator Loss: 0.665987\tGenerator Loss: 1.899992\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tDiscriminator Loss: 0.517776\tGenerator Loss: 1.074109\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tDiscriminator Loss: 0.605868\tGenerator Loss: 0.617281\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tDiscriminator Loss: 0.527996\tGenerator Loss: 1.690351\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tDiscriminator Loss: 0.524653\tGenerator Loss: 0.999607\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tDiscriminator Loss: 0.514907\tGenerator Loss: 1.026640\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tDiscriminator Loss: 0.551235\tGenerator Loss: 1.730853\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tDiscriminator Loss: 0.552907\tGenerator Loss: 1.609809\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tDiscriminator Loss: 0.522052\tGenerator Loss: 1.840323\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tDiscriminator Loss: 0.483323\tGenerator Loss: 1.268664\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tDiscriminator Loss: 0.496251\tGenerator Loss: 1.249509\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tDiscriminator Loss: 0.545499\tGenerator Loss: 1.049151\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tDiscriminator Loss: 0.599048\tGenerator Loss: 0.604027\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tDiscriminator Loss: 0.514209\tGenerator Loss: 1.030203\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tDiscriminator Loss: 0.489138\tGenerator Loss: 1.363881\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tDiscriminator Loss: 0.578262\tGenerator Loss: 0.688860\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tDiscriminator Loss: 0.475768\tGenerator Loss: 1.134206\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tDiscriminator Loss: 0.462104\tGenerator Loss: 1.118806\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tDiscriminator Loss: 0.474924\tGenerator Loss: 1.191108\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tDiscriminator Loss: 0.511892\tGenerator Loss: 1.188743\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tDiscriminator Loss: 0.599289\tGenerator Loss: 0.659343\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tDiscriminator Loss: 0.469227\tGenerator Loss: 1.190130\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tDiscriminator Loss: 0.477289\tGenerator Loss: 1.184519\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tDiscriminator Loss: 0.539896\tGenerator Loss: 1.168962\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tDiscriminator Loss: 0.541800\tGenerator Loss: 0.752494\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tDiscriminator Loss: 0.503846\tGenerator Loss: 1.122864\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tDiscriminator Loss: 0.537970\tGenerator Loss: 1.059679\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tDiscriminator Loss: 0.518375\tGenerator Loss: 1.535801\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tDiscriminator Loss: 0.535543\tGenerator Loss: 1.126268\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tDiscriminator Loss: 0.512940\tGenerator Loss: 0.847036\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tDiscriminator Loss: 0.467663\tGenerator Loss: 1.409335\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tDiscriminator Loss: 0.533997\tGenerator Loss: 0.873560\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tDiscriminator Loss: 0.474316\tGenerator Loss: 1.067798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [57600/60000 (96%)]\tDiscriminator Loss: 0.515455\tGenerator Loss: 1.204023\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tDiscriminator Loss: 0.704905\tGenerator Loss: 0.441759\n",
      "Train Epoch: 7 [0/60000 (0%)]\tDiscriminator Loss: 0.552777\tGenerator Loss: 0.973334\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tDiscriminator Loss: 0.938139\tGenerator Loss: 0.244869\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tDiscriminator Loss: 0.496359\tGenerator Loss: 1.229056\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tDiscriminator Loss: 0.520606\tGenerator Loss: 1.837012\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tDiscriminator Loss: 0.457654\tGenerator Loss: 1.242459\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tDiscriminator Loss: 0.515843\tGenerator Loss: 1.476061\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tDiscriminator Loss: 0.497124\tGenerator Loss: 0.998231\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tDiscriminator Loss: 0.495939\tGenerator Loss: 1.851988\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tDiscriminator Loss: 0.573274\tGenerator Loss: 0.641585\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tDiscriminator Loss: 0.628502\tGenerator Loss: 0.704684\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tDiscriminator Loss: 0.630657\tGenerator Loss: 0.605458\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tDiscriminator Loss: 0.589166\tGenerator Loss: 0.783616\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tDiscriminator Loss: 0.522060\tGenerator Loss: 1.405204\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tDiscriminator Loss: 0.468998\tGenerator Loss: 1.252868\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tDiscriminator Loss: 0.511027\tGenerator Loss: 1.494102\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tDiscriminator Loss: 0.532115\tGenerator Loss: 0.852849\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tDiscriminator Loss: 0.502527\tGenerator Loss: 1.363460\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tDiscriminator Loss: 0.530403\tGenerator Loss: 0.924617\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tDiscriminator Loss: 0.518435\tGenerator Loss: 1.173168\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tDiscriminator Loss: 0.604951\tGenerator Loss: 0.685313\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tDiscriminator Loss: 0.492088\tGenerator Loss: 1.057260\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tDiscriminator Loss: 0.468213\tGenerator Loss: 0.981624\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tDiscriminator Loss: 0.517013\tGenerator Loss: 1.169522\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tDiscriminator Loss: 0.476439\tGenerator Loss: 1.134528\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tDiscriminator Loss: 0.528196\tGenerator Loss: 1.087509\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tDiscriminator Loss: 0.494056\tGenerator Loss: 1.024092\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tDiscriminator Loss: 0.505212\tGenerator Loss: 1.337202\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tDiscriminator Loss: 0.466080\tGenerator Loss: 1.134540\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tDiscriminator Loss: 0.546467\tGenerator Loss: 1.812489\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tDiscriminator Loss: 0.791125\tGenerator Loss: 2.055584\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tDiscriminator Loss: 0.514883\tGenerator Loss: 0.962882\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tDiscriminator Loss: 0.584380\tGenerator Loss: 0.660109\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tDiscriminator Loss: 0.511911\tGenerator Loss: 1.257512\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tDiscriminator Loss: 0.541960\tGenerator Loss: 1.187416\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tDiscriminator Loss: 0.476769\tGenerator Loss: 0.840060\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tDiscriminator Loss: 0.471185\tGenerator Loss: 1.053675\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tDiscriminator Loss: 0.457570\tGenerator Loss: 1.241449\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tDiscriminator Loss: 0.513957\tGenerator Loss: 1.175171\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tDiscriminator Loss: 0.461279\tGenerator Loss: 1.304411\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tDiscriminator Loss: 0.546860\tGenerator Loss: 0.969941\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tDiscriminator Loss: 0.503984\tGenerator Loss: 0.954568\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tDiscriminator Loss: 0.469525\tGenerator Loss: 1.222346\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tDiscriminator Loss: 0.442219\tGenerator Loss: 1.251239\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tDiscriminator Loss: 0.492975\tGenerator Loss: 1.406711\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tDiscriminator Loss: 0.579940\tGenerator Loss: 0.736081\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tDiscriminator Loss: 0.517876\tGenerator Loss: 1.032824\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tDiscriminator Loss: 0.471034\tGenerator Loss: 1.233350\n",
      "Train Epoch: 8 [0/60000 (0%)]\tDiscriminator Loss: 0.533499\tGenerator Loss: 1.328236\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tDiscriminator Loss: 0.475230\tGenerator Loss: 1.336656\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tDiscriminator Loss: 0.496769\tGenerator Loss: 1.938618\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tDiscriminator Loss: 0.592798\tGenerator Loss: 1.671448\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tDiscriminator Loss: 0.482591\tGenerator Loss: 1.252623\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tDiscriminator Loss: 0.511086\tGenerator Loss: 1.593541\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tDiscriminator Loss: 0.539475\tGenerator Loss: 0.732849\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tDiscriminator Loss: 0.540765\tGenerator Loss: 1.159195\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tDiscriminator Loss: 0.616400\tGenerator Loss: 1.992282\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tDiscriminator Loss: 0.579982\tGenerator Loss: 0.859686\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tDiscriminator Loss: 0.669794\tGenerator Loss: 0.564421\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tDiscriminator Loss: 0.460401\tGenerator Loss: 1.533775\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tDiscriminator Loss: 0.530885\tGenerator Loss: 1.239319\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tDiscriminator Loss: 0.505040\tGenerator Loss: 1.222654\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tDiscriminator Loss: 0.541050\tGenerator Loss: 1.211017\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tDiscriminator Loss: 0.563581\tGenerator Loss: 0.995819\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tDiscriminator Loss: 0.436972\tGenerator Loss: 1.155457\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tDiscriminator Loss: 0.531251\tGenerator Loss: 0.993794\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tDiscriminator Loss: 0.658550\tGenerator Loss: 0.559144\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tDiscriminator Loss: 0.477280\tGenerator Loss: 1.074748\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tDiscriminator Loss: 0.485390\tGenerator Loss: 1.234188\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tDiscriminator Loss: 0.532526\tGenerator Loss: 1.387994\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tDiscriminator Loss: 0.629978\tGenerator Loss: 1.630867\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tDiscriminator Loss: 0.622994\tGenerator Loss: 0.524911\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tDiscriminator Loss: 0.622341\tGenerator Loss: 1.748749\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tDiscriminator Loss: 0.465600\tGenerator Loss: 1.302359\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tDiscriminator Loss: 0.483193\tGenerator Loss: 1.278037\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tDiscriminator Loss: 0.487934\tGenerator Loss: 1.161816\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tDiscriminator Loss: 0.513264\tGenerator Loss: 0.931735\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tDiscriminator Loss: 0.598309\tGenerator Loss: 0.842004\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tDiscriminator Loss: 0.534324\tGenerator Loss: 0.975958\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tDiscriminator Loss: 0.554527\tGenerator Loss: 1.854789\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tDiscriminator Loss: 0.533534\tGenerator Loss: 1.058244\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tDiscriminator Loss: 0.508608\tGenerator Loss: 1.061713\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tDiscriminator Loss: 0.482487\tGenerator Loss: 1.098284\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tDiscriminator Loss: 0.540484\tGenerator Loss: 1.063356\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tDiscriminator Loss: 0.571846\tGenerator Loss: 1.652001\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tDiscriminator Loss: 0.521837\tGenerator Loss: 1.069377\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tDiscriminator Loss: 0.480892\tGenerator Loss: 1.393004\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tDiscriminator Loss: 0.570159\tGenerator Loss: 1.652112\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tDiscriminator Loss: 0.526606\tGenerator Loss: 1.329918\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tDiscriminator Loss: 0.548710\tGenerator Loss: 1.155542\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tDiscriminator Loss: 0.498134\tGenerator Loss: 1.292536\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tDiscriminator Loss: 0.523406\tGenerator Loss: 1.033512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [56320/60000 (94%)]\tDiscriminator Loss: 0.461857\tGenerator Loss: 1.551403\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tDiscriminator Loss: 0.431798\tGenerator Loss: 1.353019\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tDiscriminator Loss: 0.470911\tGenerator Loss: 1.075820\n",
      "Train Epoch: 9 [0/60000 (0%)]\tDiscriminator Loss: 0.488416\tGenerator Loss: 1.554834\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tDiscriminator Loss: 0.509393\tGenerator Loss: 1.194408\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tDiscriminator Loss: 0.513162\tGenerator Loss: 1.058622\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tDiscriminator Loss: 0.708313\tGenerator Loss: 1.938932\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tDiscriminator Loss: 0.505819\tGenerator Loss: 1.143261\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tDiscriminator Loss: 0.489313\tGenerator Loss: 1.299106\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tDiscriminator Loss: 0.588168\tGenerator Loss: 1.148970\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tDiscriminator Loss: 0.497937\tGenerator Loss: 1.189614\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tDiscriminator Loss: 0.624410\tGenerator Loss: 1.809256\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tDiscriminator Loss: 0.515332\tGenerator Loss: 1.064094\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tDiscriminator Loss: 0.508383\tGenerator Loss: 1.360267\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tDiscriminator Loss: 0.755596\tGenerator Loss: 2.204247\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tDiscriminator Loss: 0.513540\tGenerator Loss: 1.153113\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tDiscriminator Loss: 0.486184\tGenerator Loss: 1.253245\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tDiscriminator Loss: 0.458660\tGenerator Loss: 1.453647\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tDiscriminator Loss: 0.535860\tGenerator Loss: 1.063296\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tDiscriminator Loss: 0.564897\tGenerator Loss: 1.673423\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tDiscriminator Loss: 0.495922\tGenerator Loss: 1.040872\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tDiscriminator Loss: 0.507524\tGenerator Loss: 1.116551\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tDiscriminator Loss: 0.498448\tGenerator Loss: 1.553117\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tDiscriminator Loss: 0.454488\tGenerator Loss: 1.710028\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tDiscriminator Loss: 0.544742\tGenerator Loss: 1.429347\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tDiscriminator Loss: 0.603807\tGenerator Loss: 1.408597\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tDiscriminator Loss: 0.519843\tGenerator Loss: 0.759594\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tDiscriminator Loss: 0.530667\tGenerator Loss: 1.535929\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tDiscriminator Loss: 0.507865\tGenerator Loss: 1.380874\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tDiscriminator Loss: 0.459925\tGenerator Loss: 1.174877\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tDiscriminator Loss: 0.531245\tGenerator Loss: 1.326832\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tDiscriminator Loss: 0.468699\tGenerator Loss: 1.200028\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tDiscriminator Loss: 0.598888\tGenerator Loss: 0.636757\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tDiscriminator Loss: 0.516741\tGenerator Loss: 1.497036\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tDiscriminator Loss: 0.535012\tGenerator Loss: 0.976365\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tDiscriminator Loss: 0.529028\tGenerator Loss: 1.300408\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tDiscriminator Loss: 0.498214\tGenerator Loss: 1.861551\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tDiscriminator Loss: 0.572247\tGenerator Loss: 2.303811\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tDiscriminator Loss: 0.515954\tGenerator Loss: 1.393191\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tDiscriminator Loss: 0.503696\tGenerator Loss: 0.972209\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tDiscriminator Loss: 0.532654\tGenerator Loss: 1.129589\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tDiscriminator Loss: 0.595408\tGenerator Loss: 0.689450\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tDiscriminator Loss: 0.557005\tGenerator Loss: 1.126404\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tDiscriminator Loss: 0.548784\tGenerator Loss: 0.747338\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tDiscriminator Loss: 0.457008\tGenerator Loss: 1.251369\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tDiscriminator Loss: 0.491891\tGenerator Loss: 1.200255\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tDiscriminator Loss: 0.632845\tGenerator Loss: 0.649512\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tDiscriminator Loss: 0.592950\tGenerator Loss: 0.753083\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tDiscriminator Loss: 0.558241\tGenerator Loss: 0.896979\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tDiscriminator Loss: 0.563865\tGenerator Loss: 2.088594\n",
      "Train Epoch: 10 [0/60000 (0%)]\tDiscriminator Loss: 0.607000\tGenerator Loss: 1.118803\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tDiscriminator Loss: 0.652561\tGenerator Loss: 0.657848\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tDiscriminator Loss: 0.508461\tGenerator Loss: 1.089045\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tDiscriminator Loss: 0.640986\tGenerator Loss: 1.507373\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tDiscriminator Loss: 0.498665\tGenerator Loss: 1.129848\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tDiscriminator Loss: 0.534778\tGenerator Loss: 1.634500\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tDiscriminator Loss: 0.477020\tGenerator Loss: 1.240739\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tDiscriminator Loss: 0.519696\tGenerator Loss: 0.975578\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tDiscriminator Loss: 0.575325\tGenerator Loss: 1.353679\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tDiscriminator Loss: 0.768612\tGenerator Loss: 0.467916\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tDiscriminator Loss: 0.559670\tGenerator Loss: 1.286627\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tDiscriminator Loss: 0.557415\tGenerator Loss: 1.401543\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tDiscriminator Loss: 0.526433\tGenerator Loss: 1.212158\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tDiscriminator Loss: 0.518902\tGenerator Loss: 1.371226\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tDiscriminator Loss: 0.547700\tGenerator Loss: 1.123477\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tDiscriminator Loss: 0.637004\tGenerator Loss: 1.179209\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tDiscriminator Loss: 0.510750\tGenerator Loss: 1.315220\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tDiscriminator Loss: 0.558416\tGenerator Loss: 0.723624\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tDiscriminator Loss: 0.507581\tGenerator Loss: 1.266163\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tDiscriminator Loss: 0.515315\tGenerator Loss: 1.311207\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tDiscriminator Loss: 0.502259\tGenerator Loss: 0.857953\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tDiscriminator Loss: 0.494252\tGenerator Loss: 1.308875\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tDiscriminator Loss: 0.446967\tGenerator Loss: 1.578834\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tDiscriminator Loss: 0.501599\tGenerator Loss: 1.141251\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tDiscriminator Loss: 0.506266\tGenerator Loss: 1.413986\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tDiscriminator Loss: 0.559257\tGenerator Loss: 1.368425\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tDiscriminator Loss: 0.591616\tGenerator Loss: 1.174497\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tDiscriminator Loss: 0.575917\tGenerator Loss: 0.893231\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tDiscriminator Loss: 0.619102\tGenerator Loss: 1.095448\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tDiscriminator Loss: 0.522990\tGenerator Loss: 1.659165\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tDiscriminator Loss: 0.531429\tGenerator Loss: 1.716436\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tDiscriminator Loss: 0.443141\tGenerator Loss: 1.598167\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tDiscriminator Loss: 0.530152\tGenerator Loss: 1.254405\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tDiscriminator Loss: 0.571098\tGenerator Loss: 1.728042\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tDiscriminator Loss: 0.575630\tGenerator Loss: 0.855356\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tDiscriminator Loss: 0.470098\tGenerator Loss: 1.262835\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tDiscriminator Loss: 0.454685\tGenerator Loss: 1.317366\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tDiscriminator Loss: 0.438853\tGenerator Loss: 1.373598\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tDiscriminator Loss: 0.481403\tGenerator Loss: 1.217800\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tDiscriminator Loss: 0.603012\tGenerator Loss: 0.790210\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tDiscriminator Loss: 0.516149\tGenerator Loss: 1.115809\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tDiscriminator Loss: 0.510547\tGenerator Loss: 1.481824\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tDiscriminator Loss: 0.479142\tGenerator Loss: 1.607749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [55040/60000 (92%)]\tDiscriminator Loss: 0.443550\tGenerator Loss: 1.375591\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tDiscriminator Loss: 0.751814\tGenerator Loss: 0.372475\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tDiscriminator Loss: 0.568447\tGenerator Loss: 1.074395\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tDiscriminator Loss: 0.520015\tGenerator Loss: 1.488251\n",
      "Train Epoch: 11 [0/60000 (0%)]\tDiscriminator Loss: 0.514506\tGenerator Loss: 1.218628\n",
      "Train Epoch: 11 [1280/60000 (2%)]\tDiscriminator Loss: 0.495114\tGenerator Loss: 1.131895\n",
      "Train Epoch: 11 [2560/60000 (4%)]\tDiscriminator Loss: 0.515664\tGenerator Loss: 0.851708\n",
      "Train Epoch: 11 [3840/60000 (6%)]\tDiscriminator Loss: 0.486352\tGenerator Loss: 1.145424\n",
      "Train Epoch: 11 [5120/60000 (9%)]\tDiscriminator Loss: 0.562440\tGenerator Loss: 0.823293\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tDiscriminator Loss: 0.488319\tGenerator Loss: 1.013823\n",
      "Train Epoch: 11 [7680/60000 (13%)]\tDiscriminator Loss: 0.603416\tGenerator Loss: 1.521021\n",
      "Train Epoch: 11 [8960/60000 (15%)]\tDiscriminator Loss: 0.521933\tGenerator Loss: 1.179245\n",
      "Train Epoch: 11 [10240/60000 (17%)]\tDiscriminator Loss: 0.517239\tGenerator Loss: 1.341312\n",
      "Train Epoch: 11 [11520/60000 (19%)]\tDiscriminator Loss: 0.552965\tGenerator Loss: 1.100520\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tDiscriminator Loss: 0.467451\tGenerator Loss: 1.200068\n",
      "Train Epoch: 11 [14080/60000 (23%)]\tDiscriminator Loss: 0.528753\tGenerator Loss: 1.297536\n",
      "Train Epoch: 11 [15360/60000 (26%)]\tDiscriminator Loss: 0.490284\tGenerator Loss: 1.454033\n",
      "Train Epoch: 11 [16640/60000 (28%)]\tDiscriminator Loss: 0.551767\tGenerator Loss: 1.204966\n",
      "Train Epoch: 11 [17920/60000 (30%)]\tDiscriminator Loss: 0.532541\tGenerator Loss: 1.059630\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tDiscriminator Loss: 0.563573\tGenerator Loss: 1.340338\n",
      "Train Epoch: 11 [20480/60000 (34%)]\tDiscriminator Loss: 0.530412\tGenerator Loss: 1.270863\n",
      "Train Epoch: 11 [21760/60000 (36%)]\tDiscriminator Loss: 0.534201\tGenerator Loss: 1.424049\n",
      "Train Epoch: 11 [23040/60000 (38%)]\tDiscriminator Loss: 0.521274\tGenerator Loss: 0.952302\n",
      "Train Epoch: 11 [24320/60000 (41%)]\tDiscriminator Loss: 0.549881\tGenerator Loss: 0.807015\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tDiscriminator Loss: 0.541478\tGenerator Loss: 1.381674\n",
      "Train Epoch: 11 [26880/60000 (45%)]\tDiscriminator Loss: 0.516280\tGenerator Loss: 1.358968\n",
      "Train Epoch: 11 [28160/60000 (47%)]\tDiscriminator Loss: 0.509402\tGenerator Loss: 0.960926\n",
      "Train Epoch: 11 [29440/60000 (49%)]\tDiscriminator Loss: 0.557896\tGenerator Loss: 1.107999\n",
      "Train Epoch: 11 [30720/60000 (51%)]\tDiscriminator Loss: 0.530339\tGenerator Loss: 1.438418\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tDiscriminator Loss: 0.547052\tGenerator Loss: 0.879335\n",
      "Train Epoch: 11 [33280/60000 (55%)]\tDiscriminator Loss: 0.538313\tGenerator Loss: 1.782390\n",
      "Train Epoch: 11 [34560/60000 (58%)]\tDiscriminator Loss: 0.521002\tGenerator Loss: 1.290870\n",
      "Train Epoch: 11 [35840/60000 (60%)]\tDiscriminator Loss: 0.561322\tGenerator Loss: 0.800880\n",
      "Train Epoch: 11 [37120/60000 (62%)]\tDiscriminator Loss: 0.475836\tGenerator Loss: 1.501919\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tDiscriminator Loss: 0.448969\tGenerator Loss: 1.146892\n",
      "Train Epoch: 11 [39680/60000 (66%)]\tDiscriminator Loss: 0.536874\tGenerator Loss: 1.199184\n",
      "Train Epoch: 11 [40960/60000 (68%)]\tDiscriminator Loss: 0.511867\tGenerator Loss: 1.240499\n",
      "Train Epoch: 11 [42240/60000 (70%)]\tDiscriminator Loss: 0.531216\tGenerator Loss: 1.540682\n",
      "Train Epoch: 11 [43520/60000 (72%)]\tDiscriminator Loss: 0.516448\tGenerator Loss: 1.699310\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tDiscriminator Loss: 0.518023\tGenerator Loss: 1.502914\n",
      "Train Epoch: 11 [46080/60000 (77%)]\tDiscriminator Loss: 0.576083\tGenerator Loss: 1.734577\n",
      "Train Epoch: 11 [47360/60000 (79%)]\tDiscriminator Loss: 0.600929\tGenerator Loss: 1.059123\n",
      "Train Epoch: 11 [48640/60000 (81%)]\tDiscriminator Loss: 0.524064\tGenerator Loss: 0.985014\n",
      "Train Epoch: 11 [49920/60000 (83%)]\tDiscriminator Loss: 0.534824\tGenerator Loss: 1.767872\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tDiscriminator Loss: 0.609824\tGenerator Loss: 1.075744\n",
      "Train Epoch: 11 [52480/60000 (87%)]\tDiscriminator Loss: 0.579614\tGenerator Loss: 0.807294\n",
      "Train Epoch: 11 [53760/60000 (90%)]\tDiscriminator Loss: 0.529503\tGenerator Loss: 1.305320\n",
      "Train Epoch: 11 [55040/60000 (92%)]\tDiscriminator Loss: 0.445868\tGenerator Loss: 1.414354\n",
      "Train Epoch: 11 [56320/60000 (94%)]\tDiscriminator Loss: 0.555541\tGenerator Loss: 1.016110\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tDiscriminator Loss: 0.519365\tGenerator Loss: 0.990267\n",
      "Train Epoch: 11 [58880/60000 (98%)]\tDiscriminator Loss: 0.516846\tGenerator Loss: 1.297018\n",
      "Train Epoch: 12 [0/60000 (0%)]\tDiscriminator Loss: 0.576848\tGenerator Loss: 0.997336\n",
      "Train Epoch: 12 [1280/60000 (2%)]\tDiscriminator Loss: 0.589577\tGenerator Loss: 1.201390\n",
      "Train Epoch: 12 [2560/60000 (4%)]\tDiscriminator Loss: 0.571911\tGenerator Loss: 0.882394\n",
      "Train Epoch: 12 [3840/60000 (6%)]\tDiscriminator Loss: 0.489325\tGenerator Loss: 1.029840\n",
      "Train Epoch: 12 [5120/60000 (9%)]\tDiscriminator Loss: 0.544574\tGenerator Loss: 1.073376\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tDiscriminator Loss: 0.466934\tGenerator Loss: 1.596287\n",
      "Train Epoch: 12 [7680/60000 (13%)]\tDiscriminator Loss: 0.519900\tGenerator Loss: 1.133757\n",
      "Train Epoch: 12 [8960/60000 (15%)]\tDiscriminator Loss: 0.479140\tGenerator Loss: 1.515954\n",
      "Train Epoch: 12 [10240/60000 (17%)]\tDiscriminator Loss: 0.504317\tGenerator Loss: 1.444514\n",
      "Train Epoch: 12 [11520/60000 (19%)]\tDiscriminator Loss: 0.571828\tGenerator Loss: 0.905477\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tDiscriminator Loss: 0.622741\tGenerator Loss: 1.033632\n",
      "Train Epoch: 12 [14080/60000 (23%)]\tDiscriminator Loss: 0.592489\tGenerator Loss: 1.727443\n",
      "Train Epoch: 12 [15360/60000 (26%)]\tDiscriminator Loss: 0.501008\tGenerator Loss: 1.149255\n",
      "Train Epoch: 12 [16640/60000 (28%)]\tDiscriminator Loss: 0.544493\tGenerator Loss: 1.335084\n",
      "Train Epoch: 12 [17920/60000 (30%)]\tDiscriminator Loss: 0.488638\tGenerator Loss: 1.239865\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tDiscriminator Loss: 0.459937\tGenerator Loss: 1.302791\n",
      "Train Epoch: 12 [20480/60000 (34%)]\tDiscriminator Loss: 0.505094\tGenerator Loss: 1.270076\n",
      "Train Epoch: 12 [21760/60000 (36%)]\tDiscriminator Loss: 0.470524\tGenerator Loss: 1.198615\n",
      "Train Epoch: 12 [23040/60000 (38%)]\tDiscriminator Loss: 0.512037\tGenerator Loss: 1.303357\n",
      "Train Epoch: 12 [24320/60000 (41%)]\tDiscriminator Loss: 0.550077\tGenerator Loss: 1.617891\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tDiscriminator Loss: 0.574806\tGenerator Loss: 1.018358\n",
      "Train Epoch: 12 [26880/60000 (45%)]\tDiscriminator Loss: 0.583112\tGenerator Loss: 1.167573\n",
      "Train Epoch: 12 [28160/60000 (47%)]\tDiscriminator Loss: 0.555580\tGenerator Loss: 1.193598\n",
      "Train Epoch: 12 [29440/60000 (49%)]\tDiscriminator Loss: 0.585548\tGenerator Loss: 0.846507\n",
      "Train Epoch: 12 [30720/60000 (51%)]\tDiscriminator Loss: 0.490539\tGenerator Loss: 0.987562\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tDiscriminator Loss: 0.489799\tGenerator Loss: 1.557451\n",
      "Train Epoch: 12 [33280/60000 (55%)]\tDiscriminator Loss: 0.501116\tGenerator Loss: 1.531697\n",
      "Train Epoch: 12 [34560/60000 (58%)]\tDiscriminator Loss: 0.634622\tGenerator Loss: 0.605773\n",
      "Train Epoch: 12 [35840/60000 (60%)]\tDiscriminator Loss: 0.581786\tGenerator Loss: 0.951813\n",
      "Train Epoch: 12 [37120/60000 (62%)]\tDiscriminator Loss: 0.534540\tGenerator Loss: 0.886523\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tDiscriminator Loss: 0.560820\tGenerator Loss: 1.102725\n",
      "Train Epoch: 12 [39680/60000 (66%)]\tDiscriminator Loss: 0.587880\tGenerator Loss: 0.838820\n",
      "Train Epoch: 12 [40960/60000 (68%)]\tDiscriminator Loss: 0.545538\tGenerator Loss: 0.718600\n",
      "Train Epoch: 12 [42240/60000 (70%)]\tDiscriminator Loss: 0.556864\tGenerator Loss: 1.915056\n",
      "Train Epoch: 12 [43520/60000 (72%)]\tDiscriminator Loss: 0.447106\tGenerator Loss: 1.436136\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tDiscriminator Loss: 0.535676\tGenerator Loss: 1.398533\n",
      "Train Epoch: 12 [46080/60000 (77%)]\tDiscriminator Loss: 0.470558\tGenerator Loss: 1.136035\n",
      "Train Epoch: 12 [47360/60000 (79%)]\tDiscriminator Loss: 0.592777\tGenerator Loss: 1.403818\n",
      "Train Epoch: 12 [48640/60000 (81%)]\tDiscriminator Loss: 0.504372\tGenerator Loss: 1.251440\n",
      "Train Epoch: 12 [49920/60000 (83%)]\tDiscriminator Loss: 0.496802\tGenerator Loss: 1.414348\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tDiscriminator Loss: 0.555818\tGenerator Loss: 0.872168\n",
      "Train Epoch: 12 [52480/60000 (87%)]\tDiscriminator Loss: 0.528539\tGenerator Loss: 1.162517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12 [53760/60000 (90%)]\tDiscriminator Loss: 0.536258\tGenerator Loss: 0.910032\n",
      "Train Epoch: 12 [55040/60000 (92%)]\tDiscriminator Loss: 0.532132\tGenerator Loss: 1.673212\n",
      "Train Epoch: 12 [56320/60000 (94%)]\tDiscriminator Loss: 0.516697\tGenerator Loss: 1.163503\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tDiscriminator Loss: 0.518871\tGenerator Loss: 1.099064\n",
      "Train Epoch: 12 [58880/60000 (98%)]\tDiscriminator Loss: 0.500931\tGenerator Loss: 1.341020\n",
      "Train Epoch: 13 [0/60000 (0%)]\tDiscriminator Loss: 0.503349\tGenerator Loss: 1.385181\n",
      "Train Epoch: 13 [1280/60000 (2%)]\tDiscriminator Loss: 0.598766\tGenerator Loss: 2.048275\n",
      "Train Epoch: 13 [2560/60000 (4%)]\tDiscriminator Loss: 0.501199\tGenerator Loss: 1.377681\n",
      "Train Epoch: 13 [3840/60000 (6%)]\tDiscriminator Loss: 0.461507\tGenerator Loss: 1.409968\n",
      "Train Epoch: 13 [5120/60000 (9%)]\tDiscriminator Loss: 0.566352\tGenerator Loss: 1.404617\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tDiscriminator Loss: 0.629493\tGenerator Loss: 0.897924\n",
      "Train Epoch: 13 [7680/60000 (13%)]\tDiscriminator Loss: 0.593618\tGenerator Loss: 1.448855\n",
      "Train Epoch: 13 [8960/60000 (15%)]\tDiscriminator Loss: 0.597106\tGenerator Loss: 1.435581\n",
      "Train Epoch: 13 [10240/60000 (17%)]\tDiscriminator Loss: 0.599441\tGenerator Loss: 0.869905\n",
      "Train Epoch: 13 [11520/60000 (19%)]\tDiscriminator Loss: 0.568238\tGenerator Loss: 0.971127\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tDiscriminator Loss: 0.566684\tGenerator Loss: 1.526331\n",
      "Train Epoch: 13 [14080/60000 (23%)]\tDiscriminator Loss: 0.544440\tGenerator Loss: 1.266332\n",
      "Train Epoch: 13 [15360/60000 (26%)]\tDiscriminator Loss: 0.523122\tGenerator Loss: 1.361323\n",
      "Train Epoch: 13 [16640/60000 (28%)]\tDiscriminator Loss: 0.505277\tGenerator Loss: 1.412283\n",
      "Train Epoch: 13 [17920/60000 (30%)]\tDiscriminator Loss: 0.521265\tGenerator Loss: 0.973891\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tDiscriminator Loss: 0.591772\tGenerator Loss: 1.298455\n",
      "Train Epoch: 13 [20480/60000 (34%)]\tDiscriminator Loss: 0.549185\tGenerator Loss: 1.142674\n",
      "Train Epoch: 13 [21760/60000 (36%)]\tDiscriminator Loss: 0.508597\tGenerator Loss: 1.339422\n",
      "Train Epoch: 13 [23040/60000 (38%)]\tDiscriminator Loss: 0.532399\tGenerator Loss: 0.892299\n",
      "Train Epoch: 13 [24320/60000 (41%)]\tDiscriminator Loss: 0.530546\tGenerator Loss: 1.513034\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tDiscriminator Loss: 0.523951\tGenerator Loss: 1.579402\n",
      "Train Epoch: 13 [26880/60000 (45%)]\tDiscriminator Loss: 0.695803\tGenerator Loss: 0.580038\n",
      "Train Epoch: 13 [28160/60000 (47%)]\tDiscriminator Loss: 0.639233\tGenerator Loss: 1.683743\n",
      "Train Epoch: 13 [29440/60000 (49%)]\tDiscriminator Loss: 0.555882\tGenerator Loss: 1.190986\n",
      "Train Epoch: 13 [30720/60000 (51%)]\tDiscriminator Loss: 0.514915\tGenerator Loss: 1.154566\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tDiscriminator Loss: 0.497296\tGenerator Loss: 1.360081\n",
      "Train Epoch: 13 [33280/60000 (55%)]\tDiscriminator Loss: 0.524762\tGenerator Loss: 1.125970\n",
      "Train Epoch: 13 [34560/60000 (58%)]\tDiscriminator Loss: 0.521458\tGenerator Loss: 0.974443\n",
      "Train Epoch: 13 [35840/60000 (60%)]\tDiscriminator Loss: 0.681972\tGenerator Loss: 0.690669\n",
      "Train Epoch: 13 [37120/60000 (62%)]\tDiscriminator Loss: 0.569353\tGenerator Loss: 1.326036\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tDiscriminator Loss: 0.473779\tGenerator Loss: 1.537455\n",
      "Train Epoch: 13 [39680/60000 (66%)]\tDiscriminator Loss: 0.478158\tGenerator Loss: 1.296312\n",
      "Train Epoch: 13 [40960/60000 (68%)]\tDiscriminator Loss: 0.566185\tGenerator Loss: 0.842550\n",
      "Train Epoch: 13 [42240/60000 (70%)]\tDiscriminator Loss: 0.499973\tGenerator Loss: 1.225287\n",
      "Train Epoch: 13 [43520/60000 (72%)]\tDiscriminator Loss: 0.555215\tGenerator Loss: 1.386494\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tDiscriminator Loss: 0.577277\tGenerator Loss: 0.960665\n",
      "Train Epoch: 13 [46080/60000 (77%)]\tDiscriminator Loss: 0.501546\tGenerator Loss: 1.186366\n",
      "Train Epoch: 13 [47360/60000 (79%)]\tDiscriminator Loss: 0.527129\tGenerator Loss: 1.201998\n",
      "Train Epoch: 13 [48640/60000 (81%)]\tDiscriminator Loss: 0.546536\tGenerator Loss: 1.102271\n",
      "Train Epoch: 13 [49920/60000 (83%)]\tDiscriminator Loss: 0.527419\tGenerator Loss: 1.386759\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tDiscriminator Loss: 0.541911\tGenerator Loss: 1.494399\n",
      "Train Epoch: 13 [52480/60000 (87%)]\tDiscriminator Loss: 0.532414\tGenerator Loss: 0.969493\n",
      "Train Epoch: 13 [53760/60000 (90%)]\tDiscriminator Loss: 0.482249\tGenerator Loss: 1.076555\n",
      "Train Epoch: 13 [55040/60000 (92%)]\tDiscriminator Loss: 0.557112\tGenerator Loss: 0.937821\n",
      "Train Epoch: 13 [56320/60000 (94%)]\tDiscriminator Loss: 0.508218\tGenerator Loss: 1.367368\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tDiscriminator Loss: 0.509723\tGenerator Loss: 1.459473\n",
      "Train Epoch: 13 [58880/60000 (98%)]\tDiscriminator Loss: 0.551153\tGenerator Loss: 0.943362\n",
      "Train Epoch: 14 [0/60000 (0%)]\tDiscriminator Loss: 0.541056\tGenerator Loss: 0.941082\n",
      "Train Epoch: 14 [1280/60000 (2%)]\tDiscriminator Loss: 0.483461\tGenerator Loss: 1.279288\n",
      "Train Epoch: 14 [2560/60000 (4%)]\tDiscriminator Loss: 0.545236\tGenerator Loss: 0.864367\n",
      "Train Epoch: 14 [3840/60000 (6%)]\tDiscriminator Loss: 0.536587\tGenerator Loss: 1.106473\n",
      "Train Epoch: 14 [5120/60000 (9%)]\tDiscriminator Loss: 0.571025\tGenerator Loss: 0.988545\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tDiscriminator Loss: 0.515142\tGenerator Loss: 1.149040\n",
      "Train Epoch: 14 [7680/60000 (13%)]\tDiscriminator Loss: 0.578828\tGenerator Loss: 1.041947\n",
      "Train Epoch: 14 [8960/60000 (15%)]\tDiscriminator Loss: 0.529181\tGenerator Loss: 1.013468\n",
      "Train Epoch: 14 [10240/60000 (17%)]\tDiscriminator Loss: 0.543053\tGenerator Loss: 1.375836\n",
      "Train Epoch: 14 [11520/60000 (19%)]\tDiscriminator Loss: 0.520166\tGenerator Loss: 1.150290\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tDiscriminator Loss: 0.516311\tGenerator Loss: 1.178203\n",
      "Train Epoch: 14 [14080/60000 (23%)]\tDiscriminator Loss: 0.536731\tGenerator Loss: 1.740418\n",
      "Train Epoch: 14 [15360/60000 (26%)]\tDiscriminator Loss: 0.559157\tGenerator Loss: 1.583143\n",
      "Train Epoch: 14 [16640/60000 (28%)]\tDiscriminator Loss: 0.546101\tGenerator Loss: 0.836077\n",
      "Train Epoch: 14 [17920/60000 (30%)]\tDiscriminator Loss: 0.553887\tGenerator Loss: 0.896145\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tDiscriminator Loss: 0.570633\tGenerator Loss: 1.070877\n",
      "Train Epoch: 14 [20480/60000 (34%)]\tDiscriminator Loss: 0.566877\tGenerator Loss: 1.278825\n",
      "Train Epoch: 14 [21760/60000 (36%)]\tDiscriminator Loss: 0.508085\tGenerator Loss: 1.204101\n",
      "Train Epoch: 14 [23040/60000 (38%)]\tDiscriminator Loss: 0.484574\tGenerator Loss: 1.170237\n",
      "Train Epoch: 14 [24320/60000 (41%)]\tDiscriminator Loss: 0.525531\tGenerator Loss: 1.186800\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tDiscriminator Loss: 0.606056\tGenerator Loss: 0.640120\n",
      "Train Epoch: 14 [26880/60000 (45%)]\tDiscriminator Loss: 0.486327\tGenerator Loss: 0.979486\n",
      "Train Epoch: 14 [28160/60000 (47%)]\tDiscriminator Loss: 0.536066\tGenerator Loss: 1.133577\n",
      "Train Epoch: 14 [29440/60000 (49%)]\tDiscriminator Loss: 0.453152\tGenerator Loss: 1.543856\n",
      "Train Epoch: 14 [30720/60000 (51%)]\tDiscriminator Loss: 0.486555\tGenerator Loss: 1.505619\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tDiscriminator Loss: 0.541796\tGenerator Loss: 0.920218\n",
      "Train Epoch: 14 [33280/60000 (55%)]\tDiscriminator Loss: 0.492731\tGenerator Loss: 1.275396\n",
      "Train Epoch: 14 [34560/60000 (58%)]\tDiscriminator Loss: 0.633451\tGenerator Loss: 1.227639\n",
      "Train Epoch: 14 [35840/60000 (60%)]\tDiscriminator Loss: 0.571751\tGenerator Loss: 1.194449\n",
      "Train Epoch: 14 [37120/60000 (62%)]\tDiscriminator Loss: 0.712875\tGenerator Loss: 0.600518\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tDiscriminator Loss: 0.590879\tGenerator Loss: 0.869452\n",
      "Train Epoch: 14 [39680/60000 (66%)]\tDiscriminator Loss: 0.558515\tGenerator Loss: 1.020599\n",
      "Train Epoch: 14 [40960/60000 (68%)]\tDiscriminator Loss: 0.551783\tGenerator Loss: 1.056287\n",
      "Train Epoch: 14 [42240/60000 (70%)]\tDiscriminator Loss: 0.540166\tGenerator Loss: 0.969692\n",
      "Train Epoch: 14 [43520/60000 (72%)]\tDiscriminator Loss: 0.552199\tGenerator Loss: 1.213761\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tDiscriminator Loss: 0.561861\tGenerator Loss: 1.524188\n",
      "Train Epoch: 14 [46080/60000 (77%)]\tDiscriminator Loss: 0.477092\tGenerator Loss: 1.468233\n",
      "Train Epoch: 14 [47360/60000 (79%)]\tDiscriminator Loss: 0.490945\tGenerator Loss: 1.571210\n",
      "Train Epoch: 14 [48640/60000 (81%)]\tDiscriminator Loss: 0.520383\tGenerator Loss: 1.661399\n",
      "Train Epoch: 14 [49920/60000 (83%)]\tDiscriminator Loss: 0.519417\tGenerator Loss: 1.076471\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tDiscriminator Loss: 0.569439\tGenerator Loss: 0.770494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 14 [52480/60000 (87%)]\tDiscriminator Loss: 0.538628\tGenerator Loss: 1.443670\n",
      "Train Epoch: 14 [53760/60000 (90%)]\tDiscriminator Loss: 0.508660\tGenerator Loss: 0.999741\n",
      "Train Epoch: 14 [55040/60000 (92%)]\tDiscriminator Loss: 0.523611\tGenerator Loss: 1.300646\n",
      "Train Epoch: 14 [56320/60000 (94%)]\tDiscriminator Loss: 0.555411\tGenerator Loss: 1.414713\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tDiscriminator Loss: 0.477529\tGenerator Loss: 1.473264\n",
      "Train Epoch: 14 [58880/60000 (98%)]\tDiscriminator Loss: 0.508669\tGenerator Loss: 1.623636\n",
      "Train Epoch: 15 [0/60000 (0%)]\tDiscriminator Loss: 0.450378\tGenerator Loss: 1.545410\n",
      "Train Epoch: 15 [1280/60000 (2%)]\tDiscriminator Loss: 0.534068\tGenerator Loss: 1.425853\n",
      "Train Epoch: 15 [2560/60000 (4%)]\tDiscriminator Loss: 0.561366\tGenerator Loss: 1.114850\n",
      "Train Epoch: 15 [3840/60000 (6%)]\tDiscriminator Loss: 0.593656\tGenerator Loss: 1.147661\n",
      "Train Epoch: 15 [5120/60000 (9%)]\tDiscriminator Loss: 0.544688\tGenerator Loss: 1.122046\n",
      "Train Epoch: 15 [6400/60000 (11%)]\tDiscriminator Loss: 0.530518\tGenerator Loss: 1.292556\n",
      "Train Epoch: 15 [7680/60000 (13%)]\tDiscriminator Loss: 0.498115\tGenerator Loss: 1.286414\n",
      "Train Epoch: 15 [8960/60000 (15%)]\tDiscriminator Loss: 0.620876\tGenerator Loss: 0.749094\n",
      "Train Epoch: 15 [10240/60000 (17%)]\tDiscriminator Loss: 0.502035\tGenerator Loss: 1.309612\n",
      "Train Epoch: 15 [11520/60000 (19%)]\tDiscriminator Loss: 0.606141\tGenerator Loss: 1.078010\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tDiscriminator Loss: 0.523122\tGenerator Loss: 1.029325\n",
      "Train Epoch: 15 [14080/60000 (23%)]\tDiscriminator Loss: 0.535754\tGenerator Loss: 1.453770\n",
      "Train Epoch: 15 [15360/60000 (26%)]\tDiscriminator Loss: 0.510869\tGenerator Loss: 1.417856\n",
      "Train Epoch: 15 [16640/60000 (28%)]\tDiscriminator Loss: 0.448675\tGenerator Loss: 1.223351\n",
      "Train Epoch: 15 [17920/60000 (30%)]\tDiscriminator Loss: 0.527343\tGenerator Loss: 1.471600\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tDiscriminator Loss: 0.483508\tGenerator Loss: 1.016850\n",
      "Train Epoch: 15 [20480/60000 (34%)]\tDiscriminator Loss: 0.586763\tGenerator Loss: 0.923560\n",
      "Train Epoch: 15 [21760/60000 (36%)]\tDiscriminator Loss: 0.562039\tGenerator Loss: 1.393421\n",
      "Train Epoch: 15 [23040/60000 (38%)]\tDiscriminator Loss: 0.549897\tGenerator Loss: 1.141806\n",
      "Train Epoch: 15 [24320/60000 (41%)]\tDiscriminator Loss: 0.528748\tGenerator Loss: 1.152320\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tDiscriminator Loss: 0.521653\tGenerator Loss: 1.159184\n",
      "Train Epoch: 15 [26880/60000 (45%)]\tDiscriminator Loss: 0.661318\tGenerator Loss: 1.905576\n",
      "Train Epoch: 15 [28160/60000 (47%)]\tDiscriminator Loss: 0.575435\tGenerator Loss: 1.218440\n",
      "Train Epoch: 15 [29440/60000 (49%)]\tDiscriminator Loss: 0.481679\tGenerator Loss: 1.304253\n",
      "Train Epoch: 15 [30720/60000 (51%)]\tDiscriminator Loss: 0.529356\tGenerator Loss: 1.169929\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tDiscriminator Loss: 0.466361\tGenerator Loss: 1.652560\n",
      "Train Epoch: 15 [33280/60000 (55%)]\tDiscriminator Loss: 0.497108\tGenerator Loss: 1.180696\n",
      "Train Epoch: 15 [34560/60000 (58%)]\tDiscriminator Loss: 0.525572\tGenerator Loss: 1.024772\n",
      "Train Epoch: 15 [35840/60000 (60%)]\tDiscriminator Loss: 0.512758\tGenerator Loss: 1.262549\n",
      "Train Epoch: 15 [37120/60000 (62%)]\tDiscriminator Loss: 0.610498\tGenerator Loss: 0.654400\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tDiscriminator Loss: 0.557657\tGenerator Loss: 1.091841\n",
      "Train Epoch: 15 [39680/60000 (66%)]\tDiscriminator Loss: 0.533070\tGenerator Loss: 1.357800\n",
      "Train Epoch: 15 [40960/60000 (68%)]\tDiscriminator Loss: 0.679448\tGenerator Loss: 2.001972\n",
      "Train Epoch: 15 [42240/60000 (70%)]\tDiscriminator Loss: 0.553702\tGenerator Loss: 1.061025\n",
      "Train Epoch: 15 [43520/60000 (72%)]\tDiscriminator Loss: 0.531606\tGenerator Loss: 1.201253\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tDiscriminator Loss: 0.508034\tGenerator Loss: 1.053856\n",
      "Train Epoch: 15 [46080/60000 (77%)]\tDiscriminator Loss: 0.463843\tGenerator Loss: 1.522558\n",
      "Train Epoch: 15 [47360/60000 (79%)]\tDiscriminator Loss: 0.587025\tGenerator Loss: 0.893409\n",
      "Train Epoch: 15 [48640/60000 (81%)]\tDiscriminator Loss: 0.570472\tGenerator Loss: 1.039672\n",
      "Train Epoch: 15 [49920/60000 (83%)]\tDiscriminator Loss: 0.485720\tGenerator Loss: 1.314166\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tDiscriminator Loss: 0.521620\tGenerator Loss: 0.954509\n",
      "Train Epoch: 15 [52480/60000 (87%)]\tDiscriminator Loss: 0.512575\tGenerator Loss: 1.129826\n",
      "Train Epoch: 15 [53760/60000 (90%)]\tDiscriminator Loss: 0.633106\tGenerator Loss: 1.733473\n",
      "Train Epoch: 15 [55040/60000 (92%)]\tDiscriminator Loss: 0.529105\tGenerator Loss: 1.460750\n",
      "Train Epoch: 15 [56320/60000 (94%)]\tDiscriminator Loss: 0.592044\tGenerator Loss: 1.375831\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tDiscriminator Loss: 0.529336\tGenerator Loss: 1.751967\n",
      "Train Epoch: 15 [58880/60000 (98%)]\tDiscriminator Loss: 0.574627\tGenerator Loss: 1.005833\n",
      "Train Epoch: 16 [0/60000 (0%)]\tDiscriminator Loss: 0.616793\tGenerator Loss: 1.596286\n",
      "Train Epoch: 16 [1280/60000 (2%)]\tDiscriminator Loss: 0.584217\tGenerator Loss: 0.819041\n",
      "Train Epoch: 16 [2560/60000 (4%)]\tDiscriminator Loss: 0.546931\tGenerator Loss: 1.568191\n",
      "Train Epoch: 16 [3840/60000 (6%)]\tDiscriminator Loss: 0.524201\tGenerator Loss: 1.241065\n",
      "Train Epoch: 16 [5120/60000 (9%)]\tDiscriminator Loss: 0.532431\tGenerator Loss: 1.040818\n",
      "Train Epoch: 16 [6400/60000 (11%)]\tDiscriminator Loss: 0.571856\tGenerator Loss: 1.099581\n",
      "Train Epoch: 16 [7680/60000 (13%)]\tDiscriminator Loss: 0.519973\tGenerator Loss: 0.992139\n",
      "Train Epoch: 16 [8960/60000 (15%)]\tDiscriminator Loss: 0.507807\tGenerator Loss: 1.605191\n",
      "Train Epoch: 16 [10240/60000 (17%)]\tDiscriminator Loss: 0.511580\tGenerator Loss: 1.464355\n",
      "Train Epoch: 16 [11520/60000 (19%)]\tDiscriminator Loss: 0.568015\tGenerator Loss: 0.900069\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tDiscriminator Loss: 0.513095\tGenerator Loss: 1.180948\n",
      "Train Epoch: 16 [14080/60000 (23%)]\tDiscriminator Loss: 0.622673\tGenerator Loss: 1.013579\n",
      "Train Epoch: 16 [15360/60000 (26%)]\tDiscriminator Loss: 0.562421\tGenerator Loss: 1.179438\n",
      "Train Epoch: 16 [16640/60000 (28%)]\tDiscriminator Loss: 0.577987\tGenerator Loss: 1.181621\n",
      "Train Epoch: 16 [17920/60000 (30%)]\tDiscriminator Loss: 0.531344\tGenerator Loss: 1.078444\n",
      "Train Epoch: 16 [19200/60000 (32%)]\tDiscriminator Loss: 0.559550\tGenerator Loss: 1.343837\n",
      "Train Epoch: 16 [20480/60000 (34%)]\tDiscriminator Loss: 0.583386\tGenerator Loss: 1.091700\n",
      "Train Epoch: 16 [21760/60000 (36%)]\tDiscriminator Loss: 0.532354\tGenerator Loss: 1.224979\n",
      "Train Epoch: 16 [23040/60000 (38%)]\tDiscriminator Loss: 0.531021\tGenerator Loss: 1.248300\n",
      "Train Epoch: 16 [24320/60000 (41%)]\tDiscriminator Loss: 0.561137\tGenerator Loss: 1.087493\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tDiscriminator Loss: 0.532855\tGenerator Loss: 1.135933\n",
      "Train Epoch: 16 [26880/60000 (45%)]\tDiscriminator Loss: 0.532130\tGenerator Loss: 1.037809\n",
      "Train Epoch: 16 [28160/60000 (47%)]\tDiscriminator Loss: 0.496393\tGenerator Loss: 1.217991\n",
      "Train Epoch: 16 [29440/60000 (49%)]\tDiscriminator Loss: 0.540727\tGenerator Loss: 1.158878\n",
      "Train Epoch: 16 [30720/60000 (51%)]\tDiscriminator Loss: 0.546373\tGenerator Loss: 1.230726\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tDiscriminator Loss: 0.533354\tGenerator Loss: 0.853759\n",
      "Train Epoch: 16 [33280/60000 (55%)]\tDiscriminator Loss: 0.522002\tGenerator Loss: 1.473880\n",
      "Train Epoch: 16 [34560/60000 (58%)]\tDiscriminator Loss: 0.550609\tGenerator Loss: 0.936035\n",
      "Train Epoch: 16 [35840/60000 (60%)]\tDiscriminator Loss: 0.541024\tGenerator Loss: 1.301328\n",
      "Train Epoch: 16 [37120/60000 (62%)]\tDiscriminator Loss: 0.549936\tGenerator Loss: 1.190109\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tDiscriminator Loss: 0.490057\tGenerator Loss: 1.337078\n",
      "Train Epoch: 16 [39680/60000 (66%)]\tDiscriminator Loss: 0.502975\tGenerator Loss: 1.212670\n",
      "Train Epoch: 16 [40960/60000 (68%)]\tDiscriminator Loss: 0.591873\tGenerator Loss: 1.387672\n",
      "Train Epoch: 16 [42240/60000 (70%)]\tDiscriminator Loss: 0.574587\tGenerator Loss: 1.177809\n",
      "Train Epoch: 16 [43520/60000 (72%)]\tDiscriminator Loss: 0.541745\tGenerator Loss: 1.372617\n",
      "Train Epoch: 16 [44800/60000 (75%)]\tDiscriminator Loss: 0.568177\tGenerator Loss: 1.024692\n",
      "Train Epoch: 16 [46080/60000 (77%)]\tDiscriminator Loss: 0.501462\tGenerator Loss: 1.074791\n",
      "Train Epoch: 16 [47360/60000 (79%)]\tDiscriminator Loss: 0.465730\tGenerator Loss: 1.329897\n",
      "Train Epoch: 16 [48640/60000 (81%)]\tDiscriminator Loss: 0.553304\tGenerator Loss: 2.093245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 16 [49920/60000 (83%)]\tDiscriminator Loss: 0.499106\tGenerator Loss: 1.081389\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tDiscriminator Loss: 0.534190\tGenerator Loss: 1.361527\n",
      "Train Epoch: 16 [52480/60000 (87%)]\tDiscriminator Loss: 0.529982\tGenerator Loss: 1.325095\n",
      "Train Epoch: 16 [53760/60000 (90%)]\tDiscriminator Loss: 0.542263\tGenerator Loss: 1.062722\n",
      "Train Epoch: 16 [55040/60000 (92%)]\tDiscriminator Loss: 0.572815\tGenerator Loss: 1.290868\n",
      "Train Epoch: 16 [56320/60000 (94%)]\tDiscriminator Loss: 0.500016\tGenerator Loss: 1.208550\n",
      "Train Epoch: 16 [57600/60000 (96%)]\tDiscriminator Loss: 0.573389\tGenerator Loss: 0.852052\n",
      "Train Epoch: 16 [58880/60000 (98%)]\tDiscriminator Loss: 0.496877\tGenerator Loss: 1.534616\n",
      "Train Epoch: 17 [0/60000 (0%)]\tDiscriminator Loss: 0.579329\tGenerator Loss: 1.067205\n",
      "Train Epoch: 17 [1280/60000 (2%)]\tDiscriminator Loss: 0.532109\tGenerator Loss: 1.389898\n",
      "Train Epoch: 17 [2560/60000 (4%)]\tDiscriminator Loss: 0.502844\tGenerator Loss: 1.198857\n",
      "Train Epoch: 17 [3840/60000 (6%)]\tDiscriminator Loss: 0.531343\tGenerator Loss: 1.427657\n",
      "Train Epoch: 17 [5120/60000 (9%)]\tDiscriminator Loss: 0.503519\tGenerator Loss: 1.137474\n",
      "Train Epoch: 17 [6400/60000 (11%)]\tDiscriminator Loss: 0.545944\tGenerator Loss: 1.053610\n",
      "Train Epoch: 17 [7680/60000 (13%)]\tDiscriminator Loss: 0.495101\tGenerator Loss: 1.323618\n",
      "Train Epoch: 17 [8960/60000 (15%)]\tDiscriminator Loss: 0.508852\tGenerator Loss: 1.533612\n",
      "Train Epoch: 17 [10240/60000 (17%)]\tDiscriminator Loss: 0.541215\tGenerator Loss: 1.047638\n",
      "Train Epoch: 17 [11520/60000 (19%)]\tDiscriminator Loss: 0.657019\tGenerator Loss: 1.648572\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tDiscriminator Loss: 0.614447\tGenerator Loss: 1.014695\n",
      "Train Epoch: 17 [14080/60000 (23%)]\tDiscriminator Loss: 0.499347\tGenerator Loss: 1.334839\n",
      "Train Epoch: 17 [15360/60000 (26%)]\tDiscriminator Loss: 0.466075\tGenerator Loss: 1.316420\n",
      "Train Epoch: 17 [16640/60000 (28%)]\tDiscriminator Loss: 0.477281\tGenerator Loss: 1.134572\n",
      "Train Epoch: 17 [17920/60000 (30%)]\tDiscriminator Loss: 0.521934\tGenerator Loss: 1.326558\n",
      "Train Epoch: 17 [19200/60000 (32%)]\tDiscriminator Loss: 0.582256\tGenerator Loss: 0.819179\n",
      "Train Epoch: 17 [20480/60000 (34%)]\tDiscriminator Loss: 0.581110\tGenerator Loss: 1.615910\n",
      "Train Epoch: 17 [21760/60000 (36%)]\tDiscriminator Loss: 0.532759\tGenerator Loss: 1.283754\n",
      "Train Epoch: 17 [23040/60000 (38%)]\tDiscriminator Loss: 0.495572\tGenerator Loss: 1.163901\n",
      "Train Epoch: 17 [24320/60000 (41%)]\tDiscriminator Loss: 0.605411\tGenerator Loss: 1.002036\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tDiscriminator Loss: 0.522258\tGenerator Loss: 1.008017\n",
      "Train Epoch: 17 [26880/60000 (45%)]\tDiscriminator Loss: 0.525428\tGenerator Loss: 1.437983\n",
      "Train Epoch: 17 [28160/60000 (47%)]\tDiscriminator Loss: 0.516281\tGenerator Loss: 1.254746\n",
      "Train Epoch: 17 [29440/60000 (49%)]\tDiscriminator Loss: 0.481572\tGenerator Loss: 1.282510\n",
      "Train Epoch: 17 [30720/60000 (51%)]\tDiscriminator Loss: 0.606144\tGenerator Loss: 1.455435\n",
      "Train Epoch: 17 [32000/60000 (53%)]\tDiscriminator Loss: 0.543941\tGenerator Loss: 1.141468\n",
      "Train Epoch: 17 [33280/60000 (55%)]\tDiscriminator Loss: 0.603292\tGenerator Loss: 1.671556\n",
      "Train Epoch: 17 [34560/60000 (58%)]\tDiscriminator Loss: 0.514476\tGenerator Loss: 1.350294\n",
      "Train Epoch: 17 [35840/60000 (60%)]\tDiscriminator Loss: 0.509228\tGenerator Loss: 1.176753\n",
      "Train Epoch: 17 [37120/60000 (62%)]\tDiscriminator Loss: 0.535029\tGenerator Loss: 1.080614\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tDiscriminator Loss: 0.579921\tGenerator Loss: 1.444301\n",
      "Train Epoch: 17 [39680/60000 (66%)]\tDiscriminator Loss: 0.620308\tGenerator Loss: 1.529925\n",
      "Train Epoch: 17 [40960/60000 (68%)]\tDiscriminator Loss: 0.572045\tGenerator Loss: 1.370664\n",
      "Train Epoch: 17 [42240/60000 (70%)]\tDiscriminator Loss: 0.570312\tGenerator Loss: 1.453012\n",
      "Train Epoch: 17 [43520/60000 (72%)]\tDiscriminator Loss: 0.522025\tGenerator Loss: 1.250170\n",
      "Train Epoch: 17 [44800/60000 (75%)]\tDiscriminator Loss: 0.541339\tGenerator Loss: 1.472551\n",
      "Train Epoch: 17 [46080/60000 (77%)]\tDiscriminator Loss: 0.648212\tGenerator Loss: 0.876113\n",
      "Train Epoch: 17 [47360/60000 (79%)]\tDiscriminator Loss: 0.597415\tGenerator Loss: 1.164464\n",
      "Train Epoch: 17 [48640/60000 (81%)]\tDiscriminator Loss: 0.567306\tGenerator Loss: 1.831454\n",
      "Train Epoch: 17 [49920/60000 (83%)]\tDiscriminator Loss: 0.501730\tGenerator Loss: 1.338413\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tDiscriminator Loss: 0.521823\tGenerator Loss: 1.182161\n",
      "Train Epoch: 17 [52480/60000 (87%)]\tDiscriminator Loss: 0.522030\tGenerator Loss: 1.538113\n",
      "Train Epoch: 17 [53760/60000 (90%)]\tDiscriminator Loss: 0.498618\tGenerator Loss: 1.045985\n",
      "Train Epoch: 17 [55040/60000 (92%)]\tDiscriminator Loss: 0.547194\tGenerator Loss: 0.880816\n",
      "Train Epoch: 17 [56320/60000 (94%)]\tDiscriminator Loss: 0.647842\tGenerator Loss: 1.086873\n",
      "Train Epoch: 17 [57600/60000 (96%)]\tDiscriminator Loss: 0.562977\tGenerator Loss: 1.924742\n",
      "Train Epoch: 17 [58880/60000 (98%)]\tDiscriminator Loss: 0.592789\tGenerator Loss: 1.303328\n",
      "Train Epoch: 18 [0/60000 (0%)]\tDiscriminator Loss: 0.627097\tGenerator Loss: 1.457726\n",
      "Train Epoch: 18 [1280/60000 (2%)]\tDiscriminator Loss: 0.524315\tGenerator Loss: 1.213771\n",
      "Train Epoch: 18 [2560/60000 (4%)]\tDiscriminator Loss: 0.568374\tGenerator Loss: 1.110999\n",
      "Train Epoch: 18 [3840/60000 (6%)]\tDiscriminator Loss: 0.510349\tGenerator Loss: 1.005515\n",
      "Train Epoch: 18 [5120/60000 (9%)]\tDiscriminator Loss: 0.537838\tGenerator Loss: 1.164700\n",
      "Train Epoch: 18 [6400/60000 (11%)]\tDiscriminator Loss: 0.511650\tGenerator Loss: 1.013530\n",
      "Train Epoch: 18 [7680/60000 (13%)]\tDiscriminator Loss: 0.519296\tGenerator Loss: 1.556844\n",
      "Train Epoch: 18 [8960/60000 (15%)]\tDiscriminator Loss: 0.597369\tGenerator Loss: 1.374244\n",
      "Train Epoch: 18 [10240/60000 (17%)]\tDiscriminator Loss: 0.526719\tGenerator Loss: 1.223287\n",
      "Train Epoch: 18 [11520/60000 (19%)]\tDiscriminator Loss: 0.565351\tGenerator Loss: 0.913964\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tDiscriminator Loss: 0.536463\tGenerator Loss: 1.263700\n",
      "Train Epoch: 18 [14080/60000 (23%)]\tDiscriminator Loss: 0.538267\tGenerator Loss: 1.110454\n",
      "Train Epoch: 18 [15360/60000 (26%)]\tDiscriminator Loss: 0.510834\tGenerator Loss: 1.098262\n",
      "Train Epoch: 18 [16640/60000 (28%)]\tDiscriminator Loss: 0.516821\tGenerator Loss: 1.350386\n",
      "Train Epoch: 18 [17920/60000 (30%)]\tDiscriminator Loss: 0.587928\tGenerator Loss: 0.799003\n",
      "Train Epoch: 18 [19200/60000 (32%)]\tDiscriminator Loss: 0.583706\tGenerator Loss: 0.881362\n",
      "Train Epoch: 18 [20480/60000 (34%)]\tDiscriminator Loss: 0.518024\tGenerator Loss: 0.994995\n",
      "Train Epoch: 18 [21760/60000 (36%)]\tDiscriminator Loss: 0.540114\tGenerator Loss: 1.361816\n",
      "Train Epoch: 18 [23040/60000 (38%)]\tDiscriminator Loss: 0.559675\tGenerator Loss: 1.022036\n",
      "Train Epoch: 18 [24320/60000 (41%)]\tDiscriminator Loss: 0.516449\tGenerator Loss: 1.254813\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tDiscriminator Loss: 0.535548\tGenerator Loss: 1.434646\n",
      "Train Epoch: 18 [26880/60000 (45%)]\tDiscriminator Loss: 0.516361\tGenerator Loss: 1.179416\n",
      "Train Epoch: 18 [28160/60000 (47%)]\tDiscriminator Loss: 0.484168\tGenerator Loss: 1.549290\n",
      "Train Epoch: 18 [29440/60000 (49%)]\tDiscriminator Loss: 0.562743\tGenerator Loss: 1.175894\n",
      "Train Epoch: 18 [30720/60000 (51%)]\tDiscriminator Loss: 0.690166\tGenerator Loss: 0.945284\n",
      "Train Epoch: 18 [32000/60000 (53%)]\tDiscriminator Loss: 0.492818\tGenerator Loss: 1.039405\n",
      "Train Epoch: 18 [33280/60000 (55%)]\tDiscriminator Loss: 0.536395\tGenerator Loss: 1.333588\n",
      "Train Epoch: 18 [34560/60000 (58%)]\tDiscriminator Loss: 0.490012\tGenerator Loss: 1.477636\n",
      "Train Epoch: 18 [35840/60000 (60%)]\tDiscriminator Loss: 0.563438\tGenerator Loss: 1.112538\n",
      "Train Epoch: 18 [37120/60000 (62%)]\tDiscriminator Loss: 0.577779\tGenerator Loss: 0.765750\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tDiscriminator Loss: 0.680757\tGenerator Loss: 0.659251\n",
      "Train Epoch: 18 [39680/60000 (66%)]\tDiscriminator Loss: 0.582567\tGenerator Loss: 1.074988\n",
      "Train Epoch: 18 [40960/60000 (68%)]\tDiscriminator Loss: 0.675976\tGenerator Loss: 0.593974\n",
      "Train Epoch: 18 [42240/60000 (70%)]\tDiscriminator Loss: 0.596614\tGenerator Loss: 1.069231\n",
      "Train Epoch: 18 [43520/60000 (72%)]\tDiscriminator Loss: 0.628362\tGenerator Loss: 1.650379\n",
      "Train Epoch: 18 [44800/60000 (75%)]\tDiscriminator Loss: 0.556984\tGenerator Loss: 0.968486\n",
      "Train Epoch: 18 [46080/60000 (77%)]\tDiscriminator Loss: 0.558815\tGenerator Loss: 1.046043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 18 [47360/60000 (79%)]\tDiscriminator Loss: 0.550012\tGenerator Loss: 1.172411\n",
      "Train Epoch: 18 [48640/60000 (81%)]\tDiscriminator Loss: 0.557478\tGenerator Loss: 1.279696\n",
      "Train Epoch: 18 [49920/60000 (83%)]\tDiscriminator Loss: 0.486240\tGenerator Loss: 1.085091\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tDiscriminator Loss: 0.486230\tGenerator Loss: 1.405794\n",
      "Train Epoch: 18 [52480/60000 (87%)]\tDiscriminator Loss: 0.554884\tGenerator Loss: 1.599312\n",
      "Train Epoch: 18 [53760/60000 (90%)]\tDiscriminator Loss: 0.472125\tGenerator Loss: 1.460963\n",
      "Train Epoch: 18 [55040/60000 (92%)]\tDiscriminator Loss: 0.628402\tGenerator Loss: 0.881361\n",
      "Train Epoch: 18 [56320/60000 (94%)]\tDiscriminator Loss: 0.609294\tGenerator Loss: 1.466404\n",
      "Train Epoch: 18 [57600/60000 (96%)]\tDiscriminator Loss: 0.574834\tGenerator Loss: 0.727860\n",
      "Train Epoch: 18 [58880/60000 (98%)]\tDiscriminator Loss: 0.460340\tGenerator Loss: 1.269472\n",
      "Train Epoch: 19 [0/60000 (0%)]\tDiscriminator Loss: 0.507647\tGenerator Loss: 1.288370\n",
      "Train Epoch: 19 [1280/60000 (2%)]\tDiscriminator Loss: 0.525753\tGenerator Loss: 0.988153\n",
      "Train Epoch: 19 [2560/60000 (4%)]\tDiscriminator Loss: 0.516010\tGenerator Loss: 1.455125\n",
      "Train Epoch: 19 [3840/60000 (6%)]\tDiscriminator Loss: 0.550736\tGenerator Loss: 1.003402\n",
      "Train Epoch: 19 [5120/60000 (9%)]\tDiscriminator Loss: 0.530109\tGenerator Loss: 1.292640\n",
      "Train Epoch: 19 [6400/60000 (11%)]\tDiscriminator Loss: 0.527067\tGenerator Loss: 1.218027\n",
      "Train Epoch: 19 [7680/60000 (13%)]\tDiscriminator Loss: 0.477830\tGenerator Loss: 1.327568\n",
      "Train Epoch: 19 [8960/60000 (15%)]\tDiscriminator Loss: 0.545007\tGenerator Loss: 1.302106\n",
      "Train Epoch: 19 [10240/60000 (17%)]\tDiscriminator Loss: 0.527193\tGenerator Loss: 1.313092\n",
      "Train Epoch: 19 [11520/60000 (19%)]\tDiscriminator Loss: 0.530828\tGenerator Loss: 1.439354\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tDiscriminator Loss: 0.576251\tGenerator Loss: 0.964434\n",
      "Train Epoch: 19 [14080/60000 (23%)]\tDiscriminator Loss: 0.532139\tGenerator Loss: 1.569813\n",
      "Train Epoch: 19 [15360/60000 (26%)]\tDiscriminator Loss: 0.514334\tGenerator Loss: 1.358263\n",
      "Train Epoch: 19 [16640/60000 (28%)]\tDiscriminator Loss: 0.495912\tGenerator Loss: 1.228229\n",
      "Train Epoch: 19 [17920/60000 (30%)]\tDiscriminator Loss: 0.483523\tGenerator Loss: 1.252085\n",
      "Train Epoch: 19 [19200/60000 (32%)]\tDiscriminator Loss: 0.560036\tGenerator Loss: 1.343313\n",
      "Train Epoch: 19 [20480/60000 (34%)]\tDiscriminator Loss: 0.523038\tGenerator Loss: 1.340175\n",
      "Train Epoch: 19 [21760/60000 (36%)]\tDiscriminator Loss: 0.532785\tGenerator Loss: 1.279655\n",
      "Train Epoch: 19 [23040/60000 (38%)]\tDiscriminator Loss: 0.518893\tGenerator Loss: 1.142765\n",
      "Train Epoch: 19 [24320/60000 (41%)]\tDiscriminator Loss: 0.541484\tGenerator Loss: 1.562806\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tDiscriminator Loss: 0.678381\tGenerator Loss: 1.574522\n",
      "Train Epoch: 19 [26880/60000 (45%)]\tDiscriminator Loss: 0.542901\tGenerator Loss: 1.255413\n",
      "Train Epoch: 19 [28160/60000 (47%)]\tDiscriminator Loss: 0.492093\tGenerator Loss: 1.945578\n",
      "Train Epoch: 19 [29440/60000 (49%)]\tDiscriminator Loss: 0.488776\tGenerator Loss: 1.303441\n",
      "Train Epoch: 19 [30720/60000 (51%)]\tDiscriminator Loss: 0.577170\tGenerator Loss: 1.011254\n",
      "Train Epoch: 19 [32000/60000 (53%)]\tDiscriminator Loss: 0.572690\tGenerator Loss: 1.394274\n",
      "Train Epoch: 19 [33280/60000 (55%)]\tDiscriminator Loss: 0.526425\tGenerator Loss: 1.111357\n",
      "Train Epoch: 19 [34560/60000 (58%)]\tDiscriminator Loss: 0.501580\tGenerator Loss: 1.615141\n",
      "Train Epoch: 19 [35840/60000 (60%)]\tDiscriminator Loss: 0.491930\tGenerator Loss: 1.373708\n",
      "Train Epoch: 19 [37120/60000 (62%)]\tDiscriminator Loss: 0.532178\tGenerator Loss: 1.440977\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tDiscriminator Loss: 0.555273\tGenerator Loss: 2.061528\n",
      "Train Epoch: 19 [39680/60000 (66%)]\tDiscriminator Loss: 0.556378\tGenerator Loss: 1.210821\n",
      "Train Epoch: 19 [40960/60000 (68%)]\tDiscriminator Loss: 0.581600\tGenerator Loss: 0.773523\n",
      "Train Epoch: 19 [42240/60000 (70%)]\tDiscriminator Loss: 0.617086\tGenerator Loss: 1.140906\n",
      "Train Epoch: 19 [43520/60000 (72%)]\tDiscriminator Loss: 0.516659\tGenerator Loss: 0.950934\n",
      "Train Epoch: 19 [44800/60000 (75%)]\tDiscriminator Loss: 0.651187\tGenerator Loss: 1.448362\n",
      "Train Epoch: 19 [46080/60000 (77%)]\tDiscriminator Loss: 0.490854\tGenerator Loss: 1.865146\n",
      "Train Epoch: 19 [47360/60000 (79%)]\tDiscriminator Loss: 0.476219\tGenerator Loss: 1.440594\n",
      "Train Epoch: 19 [48640/60000 (81%)]\tDiscriminator Loss: 0.511713\tGenerator Loss: 1.129937\n",
      "Train Epoch: 19 [49920/60000 (83%)]\tDiscriminator Loss: 0.567982\tGenerator Loss: 1.667732\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tDiscriminator Loss: 0.559624\tGenerator Loss: 1.391437\n",
      "Train Epoch: 19 [52480/60000 (87%)]\tDiscriminator Loss: 0.465764\tGenerator Loss: 1.498916\n",
      "Train Epoch: 19 [53760/60000 (90%)]\tDiscriminator Loss: 0.565252\tGenerator Loss: 0.824457\n",
      "Train Epoch: 19 [55040/60000 (92%)]\tDiscriminator Loss: 0.551334\tGenerator Loss: 1.078103\n",
      "Train Epoch: 19 [56320/60000 (94%)]\tDiscriminator Loss: 0.531595\tGenerator Loss: 1.291375\n",
      "Train Epoch: 19 [57600/60000 (96%)]\tDiscriminator Loss: 0.518022\tGenerator Loss: 1.213485\n",
      "Train Epoch: 19 [58880/60000 (98%)]\tDiscriminator Loss: 0.559942\tGenerator Loss: 1.329343\n",
      "Train Epoch: 20 [0/60000 (0%)]\tDiscriminator Loss: 0.568967\tGenerator Loss: 0.982744\n",
      "Train Epoch: 20 [1280/60000 (2%)]\tDiscriminator Loss: 0.557752\tGenerator Loss: 1.031289\n",
      "Train Epoch: 20 [2560/60000 (4%)]\tDiscriminator Loss: 0.573860\tGenerator Loss: 1.434080\n",
      "Train Epoch: 20 [3840/60000 (6%)]\tDiscriminator Loss: 0.507597\tGenerator Loss: 1.058887\n",
      "Train Epoch: 20 [5120/60000 (9%)]\tDiscriminator Loss: 0.540356\tGenerator Loss: 1.637026\n",
      "Train Epoch: 20 [6400/60000 (11%)]\tDiscriminator Loss: 0.481850\tGenerator Loss: 1.411282\n",
      "Train Epoch: 20 [7680/60000 (13%)]\tDiscriminator Loss: 0.487989\tGenerator Loss: 1.748063\n",
      "Train Epoch: 20 [8960/60000 (15%)]\tDiscriminator Loss: 0.577617\tGenerator Loss: 1.097187\n",
      "Train Epoch: 20 [10240/60000 (17%)]\tDiscriminator Loss: 0.604155\tGenerator Loss: 0.678650\n",
      "Train Epoch: 20 [11520/60000 (19%)]\tDiscriminator Loss: 0.680021\tGenerator Loss: 0.932242\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tDiscriminator Loss: 0.517592\tGenerator Loss: 1.462300\n",
      "Train Epoch: 20 [14080/60000 (23%)]\tDiscriminator Loss: 0.504884\tGenerator Loss: 1.009178\n",
      "Train Epoch: 20 [15360/60000 (26%)]\tDiscriminator Loss: 0.468466\tGenerator Loss: 1.350889\n",
      "Train Epoch: 20 [16640/60000 (28%)]\tDiscriminator Loss: 0.517705\tGenerator Loss: 1.256488\n",
      "Train Epoch: 20 [17920/60000 (30%)]\tDiscriminator Loss: 0.602368\tGenerator Loss: 0.733858\n",
      "Train Epoch: 20 [19200/60000 (32%)]\tDiscriminator Loss: 0.568606\tGenerator Loss: 1.097637\n",
      "Train Epoch: 20 [20480/60000 (34%)]\tDiscriminator Loss: 0.546610\tGenerator Loss: 1.401767\n",
      "Train Epoch: 20 [21760/60000 (36%)]\tDiscriminator Loss: 0.540323\tGenerator Loss: 1.448362\n",
      "Train Epoch: 20 [23040/60000 (38%)]\tDiscriminator Loss: 0.587216\tGenerator Loss: 1.050290\n",
      "Train Epoch: 20 [24320/60000 (41%)]\tDiscriminator Loss: 0.541692\tGenerator Loss: 1.404147\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tDiscriminator Loss: 0.520455\tGenerator Loss: 1.283406\n",
      "Train Epoch: 20 [26880/60000 (45%)]\tDiscriminator Loss: 0.579122\tGenerator Loss: 0.993219\n",
      "Train Epoch: 20 [28160/60000 (47%)]\tDiscriminator Loss: 0.510352\tGenerator Loss: 1.112855\n",
      "Train Epoch: 20 [29440/60000 (49%)]\tDiscriminator Loss: 0.528233\tGenerator Loss: 1.167549\n",
      "Train Epoch: 20 [30720/60000 (51%)]\tDiscriminator Loss: 0.565517\tGenerator Loss: 1.188566\n",
      "Train Epoch: 20 [32000/60000 (53%)]\tDiscriminator Loss: 0.591211\tGenerator Loss: 0.854636\n",
      "Train Epoch: 20 [33280/60000 (55%)]\tDiscriminator Loss: 0.570663\tGenerator Loss: 1.103235\n",
      "Train Epoch: 20 [34560/60000 (58%)]\tDiscriminator Loss: 0.585816\tGenerator Loss: 1.308209\n",
      "Train Epoch: 20 [35840/60000 (60%)]\tDiscriminator Loss: 0.533640\tGenerator Loss: 1.105656\n",
      "Train Epoch: 20 [37120/60000 (62%)]\tDiscriminator Loss: 0.581162\tGenerator Loss: 1.088771\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tDiscriminator Loss: 0.513774\tGenerator Loss: 1.321331\n",
      "Train Epoch: 20 [39680/60000 (66%)]\tDiscriminator Loss: 0.558990\tGenerator Loss: 0.981435\n",
      "Train Epoch: 20 [40960/60000 (68%)]\tDiscriminator Loss: 0.558576\tGenerator Loss: 1.345219\n",
      "Train Epoch: 20 [42240/60000 (70%)]\tDiscriminator Loss: 0.519739\tGenerator Loss: 1.094624\n",
      "Train Epoch: 20 [43520/60000 (72%)]\tDiscriminator Loss: 0.513212\tGenerator Loss: 1.055166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 20 [44800/60000 (75%)]\tDiscriminator Loss: 0.549874\tGenerator Loss: 1.228436\n",
      "Train Epoch: 20 [46080/60000 (77%)]\tDiscriminator Loss: 0.545451\tGenerator Loss: 1.096630\n",
      "Train Epoch: 20 [47360/60000 (79%)]\tDiscriminator Loss: 0.483019\tGenerator Loss: 1.309628\n",
      "Train Epoch: 20 [48640/60000 (81%)]\tDiscriminator Loss: 0.592211\tGenerator Loss: 0.944210\n",
      "Train Epoch: 20 [49920/60000 (83%)]\tDiscriminator Loss: 0.489602\tGenerator Loss: 1.167183\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tDiscriminator Loss: 0.519515\tGenerator Loss: 1.087282\n",
      "Train Epoch: 20 [52480/60000 (87%)]\tDiscriminator Loss: 0.539168\tGenerator Loss: 1.248648\n",
      "Train Epoch: 20 [53760/60000 (90%)]\tDiscriminator Loss: 0.519160\tGenerator Loss: 1.215478\n",
      "Train Epoch: 20 [55040/60000 (92%)]\tDiscriminator Loss: 0.515640\tGenerator Loss: 1.009371\n",
      "Train Epoch: 20 [56320/60000 (94%)]\tDiscriminator Loss: 0.576039\tGenerator Loss: 0.995470\n",
      "Train Epoch: 20 [57600/60000 (96%)]\tDiscriminator Loss: 0.634603\tGenerator Loss: 1.197356\n",
      "Train Epoch: 20 [58880/60000 (98%)]\tDiscriminator Loss: 0.554016\tGenerator Loss: 1.207836\n",
      "Train Epoch: 21 [0/60000 (0%)]\tDiscriminator Loss: 0.566426\tGenerator Loss: 0.812267\n",
      "Train Epoch: 21 [1280/60000 (2%)]\tDiscriminator Loss: 0.517879\tGenerator Loss: 1.166237\n",
      "Train Epoch: 21 [2560/60000 (4%)]\tDiscriminator Loss: 0.507144\tGenerator Loss: 1.204814\n",
      "Train Epoch: 21 [3840/60000 (6%)]\tDiscriminator Loss: 0.556916\tGenerator Loss: 1.009671\n",
      "Train Epoch: 21 [5120/60000 (9%)]\tDiscriminator Loss: 0.596205\tGenerator Loss: 0.802355\n",
      "Train Epoch: 21 [6400/60000 (11%)]\tDiscriminator Loss: 0.560027\tGenerator Loss: 1.089352\n",
      "Train Epoch: 21 [7680/60000 (13%)]\tDiscriminator Loss: 0.458656\tGenerator Loss: 1.424517\n",
      "Train Epoch: 21 [8960/60000 (15%)]\tDiscriminator Loss: 0.530411\tGenerator Loss: 1.094123\n",
      "Train Epoch: 21 [10240/60000 (17%)]\tDiscriminator Loss: 0.578934\tGenerator Loss: 1.565220\n",
      "Train Epoch: 21 [11520/60000 (19%)]\tDiscriminator Loss: 0.555851\tGenerator Loss: 1.394990\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tDiscriminator Loss: 0.585841\tGenerator Loss: 1.383587\n",
      "Train Epoch: 21 [14080/60000 (23%)]\tDiscriminator Loss: 0.494787\tGenerator Loss: 1.122981\n",
      "Train Epoch: 21 [15360/60000 (26%)]\tDiscriminator Loss: 0.595370\tGenerator Loss: 0.981550\n",
      "Train Epoch: 21 [16640/60000 (28%)]\tDiscriminator Loss: 0.616728\tGenerator Loss: 1.588022\n",
      "Train Epoch: 21 [17920/60000 (30%)]\tDiscriminator Loss: 0.517051\tGenerator Loss: 1.325812\n",
      "Train Epoch: 21 [19200/60000 (32%)]\tDiscriminator Loss: 0.535344\tGenerator Loss: 1.325570\n",
      "Train Epoch: 21 [20480/60000 (34%)]\tDiscriminator Loss: 0.498367\tGenerator Loss: 1.306760\n",
      "Train Epoch: 21 [21760/60000 (36%)]\tDiscriminator Loss: 0.496857\tGenerator Loss: 1.247541\n",
      "Train Epoch: 21 [23040/60000 (38%)]\tDiscriminator Loss: 0.523987\tGenerator Loss: 1.356296\n",
      "Train Epoch: 21 [24320/60000 (41%)]\tDiscriminator Loss: 0.539446\tGenerator Loss: 1.024229\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tDiscriminator Loss: 0.594755\tGenerator Loss: 0.800851\n",
      "Train Epoch: 21 [26880/60000 (45%)]\tDiscriminator Loss: 0.542640\tGenerator Loss: 1.100863\n",
      "Train Epoch: 21 [28160/60000 (47%)]\tDiscriminator Loss: 0.477025\tGenerator Loss: 1.664782\n",
      "Train Epoch: 21 [29440/60000 (49%)]\tDiscriminator Loss: 0.495673\tGenerator Loss: 1.172805\n",
      "Train Epoch: 21 [30720/60000 (51%)]\tDiscriminator Loss: 0.508132\tGenerator Loss: 1.234301\n",
      "Train Epoch: 21 [32000/60000 (53%)]\tDiscriminator Loss: 0.530980\tGenerator Loss: 1.171320\n",
      "Train Epoch: 21 [33280/60000 (55%)]\tDiscriminator Loss: 0.498361\tGenerator Loss: 1.540596\n",
      "Train Epoch: 21 [34560/60000 (58%)]\tDiscriminator Loss: 0.536994\tGenerator Loss: 1.148535\n",
      "Train Epoch: 21 [35840/60000 (60%)]\tDiscriminator Loss: 0.593810\tGenerator Loss: 1.177947\n",
      "Train Epoch: 21 [37120/60000 (62%)]\tDiscriminator Loss: 0.542962\tGenerator Loss: 1.165238\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tDiscriminator Loss: 0.489675\tGenerator Loss: 1.181058\n",
      "Train Epoch: 21 [39680/60000 (66%)]\tDiscriminator Loss: 0.512378\tGenerator Loss: 1.099786\n",
      "Train Epoch: 21 [40960/60000 (68%)]\tDiscriminator Loss: 0.559880\tGenerator Loss: 0.942387\n",
      "Train Epoch: 21 [42240/60000 (70%)]\tDiscriminator Loss: 0.513883\tGenerator Loss: 1.115376\n",
      "Train Epoch: 21 [43520/60000 (72%)]\tDiscriminator Loss: 0.520954\tGenerator Loss: 1.373589\n",
      "Train Epoch: 21 [44800/60000 (75%)]\tDiscriminator Loss: 0.501516\tGenerator Loss: 1.498951\n",
      "Train Epoch: 21 [46080/60000 (77%)]\tDiscriminator Loss: 0.529517\tGenerator Loss: 0.954480\n",
      "Train Epoch: 21 [47360/60000 (79%)]\tDiscriminator Loss: 0.511195\tGenerator Loss: 1.079265\n",
      "Train Epoch: 21 [48640/60000 (81%)]\tDiscriminator Loss: 0.505220\tGenerator Loss: 1.248714\n",
      "Train Epoch: 21 [49920/60000 (83%)]\tDiscriminator Loss: 0.501819\tGenerator Loss: 1.535471\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tDiscriminator Loss: 0.555309\tGenerator Loss: 1.238301\n",
      "Train Epoch: 21 [52480/60000 (87%)]\tDiscriminator Loss: 0.548651\tGenerator Loss: 1.115675\n",
      "Train Epoch: 21 [53760/60000 (90%)]\tDiscriminator Loss: 0.611539\tGenerator Loss: 1.376389\n",
      "Train Epoch: 21 [55040/60000 (92%)]\tDiscriminator Loss: 0.467990\tGenerator Loss: 1.278402\n",
      "Train Epoch: 21 [56320/60000 (94%)]\tDiscriminator Loss: 0.538383\tGenerator Loss: 1.318001\n",
      "Train Epoch: 21 [57600/60000 (96%)]\tDiscriminator Loss: 0.627259\tGenerator Loss: 1.075804\n",
      "Train Epoch: 21 [58880/60000 (98%)]\tDiscriminator Loss: 0.580802\tGenerator Loss: 0.918201\n",
      "Train Epoch: 22 [0/60000 (0%)]\tDiscriminator Loss: 0.577291\tGenerator Loss: 1.141652\n",
      "Train Epoch: 22 [1280/60000 (2%)]\tDiscriminator Loss: 0.591735\tGenerator Loss: 0.859748\n",
      "Train Epoch: 22 [2560/60000 (4%)]\tDiscriminator Loss: 0.483738\tGenerator Loss: 1.344272\n",
      "Train Epoch: 22 [3840/60000 (6%)]\tDiscriminator Loss: 0.457865\tGenerator Loss: 1.513157\n",
      "Train Epoch: 22 [5120/60000 (9%)]\tDiscriminator Loss: 0.503816\tGenerator Loss: 1.218024\n",
      "Train Epoch: 22 [6400/60000 (11%)]\tDiscriminator Loss: 0.605270\tGenerator Loss: 0.635089\n",
      "Train Epoch: 22 [7680/60000 (13%)]\tDiscriminator Loss: 0.555377\tGenerator Loss: 1.278633\n",
      "Train Epoch: 22 [8960/60000 (15%)]\tDiscriminator Loss: 0.555708\tGenerator Loss: 1.012882\n",
      "Train Epoch: 22 [10240/60000 (17%)]\tDiscriminator Loss: 0.537351\tGenerator Loss: 1.379112\n",
      "Train Epoch: 22 [11520/60000 (19%)]\tDiscriminator Loss: 0.512655\tGenerator Loss: 1.306185\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tDiscriminator Loss: 0.529383\tGenerator Loss: 1.337413\n",
      "Train Epoch: 22 [14080/60000 (23%)]\tDiscriminator Loss: 0.522628\tGenerator Loss: 0.911802\n",
      "Train Epoch: 22 [15360/60000 (26%)]\tDiscriminator Loss: 0.509942\tGenerator Loss: 0.958566\n",
      "Train Epoch: 22 [16640/60000 (28%)]\tDiscriminator Loss: 0.503496\tGenerator Loss: 1.351662\n",
      "Train Epoch: 22 [17920/60000 (30%)]\tDiscriminator Loss: 0.603295\tGenerator Loss: 0.840953\n",
      "Train Epoch: 22 [19200/60000 (32%)]\tDiscriminator Loss: 0.527228\tGenerator Loss: 0.976243\n",
      "Train Epoch: 22 [20480/60000 (34%)]\tDiscriminator Loss: 0.547716\tGenerator Loss: 1.134344\n",
      "Train Epoch: 22 [21760/60000 (36%)]\tDiscriminator Loss: 0.512385\tGenerator Loss: 1.067195\n",
      "Train Epoch: 22 [23040/60000 (38%)]\tDiscriminator Loss: 0.542901\tGenerator Loss: 1.194019\n",
      "Train Epoch: 22 [24320/60000 (41%)]\tDiscriminator Loss: 0.531375\tGenerator Loss: 1.387564\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tDiscriminator Loss: 0.509984\tGenerator Loss: 1.366425\n",
      "Train Epoch: 22 [26880/60000 (45%)]\tDiscriminator Loss: 0.527913\tGenerator Loss: 1.069067\n",
      "Train Epoch: 22 [28160/60000 (47%)]\tDiscriminator Loss: 0.542416\tGenerator Loss: 1.246670\n",
      "Train Epoch: 22 [29440/60000 (49%)]\tDiscriminator Loss: 0.605084\tGenerator Loss: 1.481201\n",
      "Train Epoch: 22 [30720/60000 (51%)]\tDiscriminator Loss: 0.558962\tGenerator Loss: 1.051932\n",
      "Train Epoch: 22 [32000/60000 (53%)]\tDiscriminator Loss: 0.556066\tGenerator Loss: 1.409550\n",
      "Train Epoch: 22 [33280/60000 (55%)]\tDiscriminator Loss: 0.608084\tGenerator Loss: 0.691294\n",
      "Train Epoch: 22 [34560/60000 (58%)]\tDiscriminator Loss: 0.430128\tGenerator Loss: 1.297039\n",
      "Train Epoch: 22 [35840/60000 (60%)]\tDiscriminator Loss: 0.548765\tGenerator Loss: 1.471215\n",
      "Train Epoch: 22 [37120/60000 (62%)]\tDiscriminator Loss: 0.609587\tGenerator Loss: 0.766273\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tDiscriminator Loss: 0.611917\tGenerator Loss: 0.937917\n",
      "Train Epoch: 22 [39680/60000 (66%)]\tDiscriminator Loss: 0.562531\tGenerator Loss: 1.157664\n",
      "Train Epoch: 22 [40960/60000 (68%)]\tDiscriminator Loss: 0.579658\tGenerator Loss: 1.432582\n",
      "Train Epoch: 22 [42240/60000 (70%)]\tDiscriminator Loss: 0.471924\tGenerator Loss: 1.237094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 22 [43520/60000 (72%)]\tDiscriminator Loss: 0.504394\tGenerator Loss: 1.104135\n",
      "Train Epoch: 22 [44800/60000 (75%)]\tDiscriminator Loss: 0.487505\tGenerator Loss: 1.144144\n",
      "Train Epoch: 22 [46080/60000 (77%)]\tDiscriminator Loss: 0.546115\tGenerator Loss: 1.479222\n",
      "Train Epoch: 22 [47360/60000 (79%)]\tDiscriminator Loss: 0.577510\tGenerator Loss: 0.948085\n",
      "Train Epoch: 22 [48640/60000 (81%)]\tDiscriminator Loss: 0.594832\tGenerator Loss: 0.746207\n",
      "Train Epoch: 22 [49920/60000 (83%)]\tDiscriminator Loss: 0.570066\tGenerator Loss: 1.542203\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tDiscriminator Loss: 0.541103\tGenerator Loss: 1.272593\n",
      "Train Epoch: 22 [52480/60000 (87%)]\tDiscriminator Loss: 0.541238\tGenerator Loss: 0.926847\n",
      "Train Epoch: 22 [53760/60000 (90%)]\tDiscriminator Loss: 0.585605\tGenerator Loss: 1.059255\n",
      "Train Epoch: 22 [55040/60000 (92%)]\tDiscriminator Loss: 0.550010\tGenerator Loss: 1.127657\n",
      "Train Epoch: 22 [56320/60000 (94%)]\tDiscriminator Loss: 0.640948\tGenerator Loss: 0.815476\n",
      "Train Epoch: 22 [57600/60000 (96%)]\tDiscriminator Loss: 0.538463\tGenerator Loss: 1.384667\n",
      "Train Epoch: 22 [58880/60000 (98%)]\tDiscriminator Loss: 0.541995\tGenerator Loss: 1.072869\n",
      "Train Epoch: 23 [0/60000 (0%)]\tDiscriminator Loss: 0.571470\tGenerator Loss: 1.590542\n",
      "Train Epoch: 23 [1280/60000 (2%)]\tDiscriminator Loss: 0.510215\tGenerator Loss: 1.376166\n",
      "Train Epoch: 23 [2560/60000 (4%)]\tDiscriminator Loss: 0.575874\tGenerator Loss: 1.481220\n",
      "Train Epoch: 23 [3840/60000 (6%)]\tDiscriminator Loss: 0.451554\tGenerator Loss: 1.435549\n",
      "Train Epoch: 23 [5120/60000 (9%)]\tDiscriminator Loss: 0.568710\tGenerator Loss: 1.266092\n",
      "Train Epoch: 23 [6400/60000 (11%)]\tDiscriminator Loss: 0.550963\tGenerator Loss: 1.296518\n",
      "Train Epoch: 23 [7680/60000 (13%)]\tDiscriminator Loss: 0.516269\tGenerator Loss: 1.108900\n",
      "Train Epoch: 23 [8960/60000 (15%)]\tDiscriminator Loss: 0.490687\tGenerator Loss: 1.610800\n",
      "Train Epoch: 23 [10240/60000 (17%)]\tDiscriminator Loss: 0.520549\tGenerator Loss: 1.234445\n",
      "Train Epoch: 23 [11520/60000 (19%)]\tDiscriminator Loss: 0.575367\tGenerator Loss: 1.477468\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tDiscriminator Loss: 0.500544\tGenerator Loss: 1.057761\n",
      "Train Epoch: 23 [14080/60000 (23%)]\tDiscriminator Loss: 0.563888\tGenerator Loss: 1.244422\n",
      "Train Epoch: 23 [15360/60000 (26%)]\tDiscriminator Loss: 0.572378\tGenerator Loss: 1.174752\n",
      "Train Epoch: 23 [16640/60000 (28%)]\tDiscriminator Loss: 0.553515\tGenerator Loss: 0.884661\n",
      "Train Epoch: 23 [17920/60000 (30%)]\tDiscriminator Loss: 0.518763\tGenerator Loss: 1.315529\n",
      "Train Epoch: 23 [19200/60000 (32%)]\tDiscriminator Loss: 0.537690\tGenerator Loss: 1.252438\n",
      "Train Epoch: 23 [20480/60000 (34%)]\tDiscriminator Loss: 0.514487\tGenerator Loss: 1.007634\n",
      "Train Epoch: 23 [21760/60000 (36%)]\tDiscriminator Loss: 0.496858\tGenerator Loss: 1.145468\n",
      "Train Epoch: 23 [23040/60000 (38%)]\tDiscriminator Loss: 0.517000\tGenerator Loss: 1.276502\n",
      "Train Epoch: 23 [24320/60000 (41%)]\tDiscriminator Loss: 0.518874\tGenerator Loss: 1.280097\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tDiscriminator Loss: 0.536199\tGenerator Loss: 1.609734\n",
      "Train Epoch: 23 [26880/60000 (45%)]\tDiscriminator Loss: 0.538479\tGenerator Loss: 1.075317\n",
      "Train Epoch: 23 [28160/60000 (47%)]\tDiscriminator Loss: 0.575807\tGenerator Loss: 1.416935\n",
      "Train Epoch: 23 [29440/60000 (49%)]\tDiscriminator Loss: 0.582316\tGenerator Loss: 1.066018\n",
      "Train Epoch: 23 [30720/60000 (51%)]\tDiscriminator Loss: 0.579284\tGenerator Loss: 1.043688\n",
      "Train Epoch: 23 [32000/60000 (53%)]\tDiscriminator Loss: 0.487696\tGenerator Loss: 1.632996\n",
      "Train Epoch: 23 [33280/60000 (55%)]\tDiscriminator Loss: 0.491259\tGenerator Loss: 1.370421\n",
      "Train Epoch: 23 [34560/60000 (58%)]\tDiscriminator Loss: 0.562555\tGenerator Loss: 1.369035\n",
      "Train Epoch: 23 [35840/60000 (60%)]\tDiscriminator Loss: 0.585290\tGenerator Loss: 1.052995\n",
      "Train Epoch: 23 [37120/60000 (62%)]\tDiscriminator Loss: 0.507460\tGenerator Loss: 1.489717\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tDiscriminator Loss: 0.526708\tGenerator Loss: 1.043148\n",
      "Train Epoch: 23 [39680/60000 (66%)]\tDiscriminator Loss: 0.504666\tGenerator Loss: 1.218760\n",
      "Train Epoch: 23 [40960/60000 (68%)]\tDiscriminator Loss: 0.588556\tGenerator Loss: 1.119298\n",
      "Train Epoch: 23 [42240/60000 (70%)]\tDiscriminator Loss: 0.522066\tGenerator Loss: 1.263973\n",
      "Train Epoch: 23 [43520/60000 (72%)]\tDiscriminator Loss: 0.534820\tGenerator Loss: 1.225613\n",
      "Train Epoch: 23 [44800/60000 (75%)]\tDiscriminator Loss: 0.545309\tGenerator Loss: 0.997388\n",
      "Train Epoch: 23 [46080/60000 (77%)]\tDiscriminator Loss: 0.591103\tGenerator Loss: 0.849757\n",
      "Train Epoch: 23 [47360/60000 (79%)]\tDiscriminator Loss: 0.582187\tGenerator Loss: 0.955784\n",
      "Train Epoch: 23 [48640/60000 (81%)]\tDiscriminator Loss: 0.540843\tGenerator Loss: 0.964707\n",
      "Train Epoch: 23 [49920/60000 (83%)]\tDiscriminator Loss: 0.500611\tGenerator Loss: 1.376279\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tDiscriminator Loss: 0.533662\tGenerator Loss: 1.318795\n",
      "Train Epoch: 23 [52480/60000 (87%)]\tDiscriminator Loss: 0.496089\tGenerator Loss: 1.186945\n",
      "Train Epoch: 23 [53760/60000 (90%)]\tDiscriminator Loss: 0.608296\tGenerator Loss: 0.671268\n",
      "Train Epoch: 23 [55040/60000 (92%)]\tDiscriminator Loss: 0.544599\tGenerator Loss: 1.395684\n",
      "Train Epoch: 23 [56320/60000 (94%)]\tDiscriminator Loss: 0.507602\tGenerator Loss: 1.247930\n",
      "Train Epoch: 23 [57600/60000 (96%)]\tDiscriminator Loss: 0.517397\tGenerator Loss: 1.331644\n",
      "Train Epoch: 23 [58880/60000 (98%)]\tDiscriminator Loss: 0.521526\tGenerator Loss: 0.924166\n",
      "Train Epoch: 24 [0/60000 (0%)]\tDiscriminator Loss: 0.499302\tGenerator Loss: 1.363080\n",
      "Train Epoch: 24 [1280/60000 (2%)]\tDiscriminator Loss: 0.562245\tGenerator Loss: 0.858843\n",
      "Train Epoch: 24 [2560/60000 (4%)]\tDiscriminator Loss: 0.647329\tGenerator Loss: 0.700962\n",
      "Train Epoch: 24 [3840/60000 (6%)]\tDiscriminator Loss: 0.651037\tGenerator Loss: 1.392160\n",
      "Train Epoch: 24 [5120/60000 (9%)]\tDiscriminator Loss: 0.587737\tGenerator Loss: 0.924232\n",
      "Train Epoch: 24 [6400/60000 (11%)]\tDiscriminator Loss: 0.548351\tGenerator Loss: 1.369833\n",
      "Train Epoch: 24 [7680/60000 (13%)]\tDiscriminator Loss: 0.513598\tGenerator Loss: 1.191927\n",
      "Train Epoch: 24 [8960/60000 (15%)]\tDiscriminator Loss: 0.498752\tGenerator Loss: 1.141447\n",
      "Train Epoch: 24 [10240/60000 (17%)]\tDiscriminator Loss: 0.626697\tGenerator Loss: 0.839937\n",
      "Train Epoch: 24 [11520/60000 (19%)]\tDiscriminator Loss: 0.587341\tGenerator Loss: 0.990209\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tDiscriminator Loss: 0.546269\tGenerator Loss: 1.320394\n",
      "Train Epoch: 24 [14080/60000 (23%)]\tDiscriminator Loss: 0.590153\tGenerator Loss: 0.903003\n",
      "Train Epoch: 24 [15360/60000 (26%)]\tDiscriminator Loss: 0.573094\tGenerator Loss: 1.092731\n",
      "Train Epoch: 24 [16640/60000 (28%)]\tDiscriminator Loss: 0.549637\tGenerator Loss: 0.954871\n",
      "Train Epoch: 24 [17920/60000 (30%)]\tDiscriminator Loss: 0.573062\tGenerator Loss: 1.194986\n",
      "Train Epoch: 24 [19200/60000 (32%)]\tDiscriminator Loss: 0.577798\tGenerator Loss: 1.319828\n",
      "Train Epoch: 24 [20480/60000 (34%)]\tDiscriminator Loss: 0.530576\tGenerator Loss: 1.416512\n",
      "Train Epoch: 24 [21760/60000 (36%)]\tDiscriminator Loss: 0.558697\tGenerator Loss: 0.853358\n",
      "Train Epoch: 24 [23040/60000 (38%)]\tDiscriminator Loss: 0.493097\tGenerator Loss: 1.127454\n",
      "Train Epoch: 24 [24320/60000 (41%)]\tDiscriminator Loss: 0.640479\tGenerator Loss: 0.619931\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tDiscriminator Loss: 0.562740\tGenerator Loss: 1.028077\n",
      "Train Epoch: 24 [26880/60000 (45%)]\tDiscriminator Loss: 0.529456\tGenerator Loss: 1.129817\n",
      "Train Epoch: 24 [28160/60000 (47%)]\tDiscriminator Loss: 0.549479\tGenerator Loss: 1.254644\n",
      "Train Epoch: 24 [29440/60000 (49%)]\tDiscriminator Loss: 0.467154\tGenerator Loss: 1.629390\n",
      "Train Epoch: 24 [30720/60000 (51%)]\tDiscriminator Loss: 0.531689\tGenerator Loss: 1.163240\n",
      "Train Epoch: 24 [32000/60000 (53%)]\tDiscriminator Loss: 0.565461\tGenerator Loss: 1.154283\n",
      "Train Epoch: 24 [33280/60000 (55%)]\tDiscriminator Loss: 0.534303\tGenerator Loss: 1.280205\n",
      "Train Epoch: 24 [34560/60000 (58%)]\tDiscriminator Loss: 0.534405\tGenerator Loss: 1.198305\n",
      "Train Epoch: 24 [35840/60000 (60%)]\tDiscriminator Loss: 0.508284\tGenerator Loss: 1.302628\n",
      "Train Epoch: 24 [37120/60000 (62%)]\tDiscriminator Loss: 0.552226\tGenerator Loss: 1.398413\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tDiscriminator Loss: 0.554163\tGenerator Loss: 1.437936\n",
      "Train Epoch: 24 [39680/60000 (66%)]\tDiscriminator Loss: 0.557229\tGenerator Loss: 1.244217\n",
      "Train Epoch: 24 [40960/60000 (68%)]\tDiscriminator Loss: 0.500136\tGenerator Loss: 1.354695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 24 [42240/60000 (70%)]\tDiscriminator Loss: 0.581603\tGenerator Loss: 1.502810\n",
      "Train Epoch: 24 [43520/60000 (72%)]\tDiscriminator Loss: 0.562742\tGenerator Loss: 0.799877\n",
      "Train Epoch: 24 [44800/60000 (75%)]\tDiscriminator Loss: 0.509104\tGenerator Loss: 1.179193\n",
      "Train Epoch: 24 [46080/60000 (77%)]\tDiscriminator Loss: 0.532748\tGenerator Loss: 1.423159\n",
      "Train Epoch: 24 [47360/60000 (79%)]\tDiscriminator Loss: 0.544320\tGenerator Loss: 1.258998\n",
      "Train Epoch: 24 [48640/60000 (81%)]\tDiscriminator Loss: 0.534677\tGenerator Loss: 0.982018\n",
      "Train Epoch: 24 [49920/60000 (83%)]\tDiscriminator Loss: 0.541842\tGenerator Loss: 1.111052\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tDiscriminator Loss: 0.580302\tGenerator Loss: 1.176540\n",
      "Train Epoch: 24 [52480/60000 (87%)]\tDiscriminator Loss: 0.500315\tGenerator Loss: 1.200411\n",
      "Train Epoch: 24 [53760/60000 (90%)]\tDiscriminator Loss: 0.558930\tGenerator Loss: 1.303594\n",
      "Train Epoch: 24 [55040/60000 (92%)]\tDiscriminator Loss: 0.553038\tGenerator Loss: 0.974883\n",
      "Train Epoch: 24 [56320/60000 (94%)]\tDiscriminator Loss: 0.584393\tGenerator Loss: 1.308521\n",
      "Train Epoch: 24 [57600/60000 (96%)]\tDiscriminator Loss: 0.566957\tGenerator Loss: 1.282140\n",
      "Train Epoch: 24 [58880/60000 (98%)]\tDiscriminator Loss: 0.531765\tGenerator Loss: 0.880385\n",
      "Train Epoch: 25 [0/60000 (0%)]\tDiscriminator Loss: 0.523783\tGenerator Loss: 1.019523\n",
      "Train Epoch: 25 [1280/60000 (2%)]\tDiscriminator Loss: 0.550612\tGenerator Loss: 1.198805\n",
      "Train Epoch: 25 [2560/60000 (4%)]\tDiscriminator Loss: 0.521275\tGenerator Loss: 1.153730\n",
      "Train Epoch: 25 [3840/60000 (6%)]\tDiscriminator Loss: 0.519632\tGenerator Loss: 1.365509\n",
      "Train Epoch: 25 [5120/60000 (9%)]\tDiscriminator Loss: 0.506217\tGenerator Loss: 1.174890\n",
      "Train Epoch: 25 [6400/60000 (11%)]\tDiscriminator Loss: 0.536342\tGenerator Loss: 0.974544\n",
      "Train Epoch: 25 [7680/60000 (13%)]\tDiscriminator Loss: 0.579076\tGenerator Loss: 1.478953\n",
      "Train Epoch: 25 [8960/60000 (15%)]\tDiscriminator Loss: 0.578868\tGenerator Loss: 0.787567\n",
      "Train Epoch: 25 [10240/60000 (17%)]\tDiscriminator Loss: 0.575000\tGenerator Loss: 1.370702\n",
      "Train Epoch: 25 [11520/60000 (19%)]\tDiscriminator Loss: 0.543709\tGenerator Loss: 1.165747\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tDiscriminator Loss: 0.565394\tGenerator Loss: 1.117021\n",
      "Train Epoch: 25 [14080/60000 (23%)]\tDiscriminator Loss: 0.620370\tGenerator Loss: 1.200069\n",
      "Train Epoch: 25 [15360/60000 (26%)]\tDiscriminator Loss: 0.600295\tGenerator Loss: 1.384502\n",
      "Train Epoch: 25 [16640/60000 (28%)]\tDiscriminator Loss: 0.504483\tGenerator Loss: 1.779475\n",
      "Train Epoch: 25 [17920/60000 (30%)]\tDiscriminator Loss: 0.531980\tGenerator Loss: 1.402933\n",
      "Train Epoch: 25 [19200/60000 (32%)]\tDiscriminator Loss: 0.609130\tGenerator Loss: 0.811351\n",
      "Train Epoch: 25 [20480/60000 (34%)]\tDiscriminator Loss: 0.607970\tGenerator Loss: 1.352143\n",
      "Train Epoch: 25 [21760/60000 (36%)]\tDiscriminator Loss: 0.479268\tGenerator Loss: 1.295196\n",
      "Train Epoch: 25 [23040/60000 (38%)]\tDiscriminator Loss: 0.533459\tGenerator Loss: 1.309647\n",
      "Train Epoch: 25 [24320/60000 (41%)]\tDiscriminator Loss: 0.577767\tGenerator Loss: 0.801706\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tDiscriminator Loss: 0.564681\tGenerator Loss: 0.890385\n",
      "Train Epoch: 25 [26880/60000 (45%)]\tDiscriminator Loss: 0.546553\tGenerator Loss: 1.062567\n",
      "Train Epoch: 25 [28160/60000 (47%)]\tDiscriminator Loss: 0.561309\tGenerator Loss: 1.309663\n",
      "Train Epoch: 25 [29440/60000 (49%)]\tDiscriminator Loss: 0.537511\tGenerator Loss: 1.333850\n",
      "Train Epoch: 25 [30720/60000 (51%)]\tDiscriminator Loss: 0.482840\tGenerator Loss: 1.397254\n",
      "Train Epoch: 25 [32000/60000 (53%)]\tDiscriminator Loss: 0.480832\tGenerator Loss: 1.255137\n",
      "Train Epoch: 25 [33280/60000 (55%)]\tDiscriminator Loss: 0.572637\tGenerator Loss: 0.935249\n",
      "Train Epoch: 25 [34560/60000 (58%)]\tDiscriminator Loss: 0.580751\tGenerator Loss: 0.929816\n",
      "Train Epoch: 25 [35840/60000 (60%)]\tDiscriminator Loss: 0.493641\tGenerator Loss: 1.558034\n",
      "Train Epoch: 25 [37120/60000 (62%)]\tDiscriminator Loss: 0.467346\tGenerator Loss: 1.371162\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tDiscriminator Loss: 0.549316\tGenerator Loss: 1.153722\n",
      "Train Epoch: 25 [39680/60000 (66%)]\tDiscriminator Loss: 0.587805\tGenerator Loss: 1.181730\n",
      "Train Epoch: 25 [40960/60000 (68%)]\tDiscriminator Loss: 0.546423\tGenerator Loss: 0.925675\n",
      "Train Epoch: 25 [42240/60000 (70%)]\tDiscriminator Loss: 0.539636\tGenerator Loss: 1.245792\n",
      "Train Epoch: 25 [43520/60000 (72%)]\tDiscriminator Loss: 0.557675\tGenerator Loss: 0.981419\n",
      "Train Epoch: 25 [44800/60000 (75%)]\tDiscriminator Loss: 0.596928\tGenerator Loss: 1.354923\n",
      "Train Epoch: 25 [46080/60000 (77%)]\tDiscriminator Loss: 0.527095\tGenerator Loss: 1.626343\n",
      "Train Epoch: 25 [47360/60000 (79%)]\tDiscriminator Loss: 0.567637\tGenerator Loss: 1.231978\n",
      "Train Epoch: 25 [48640/60000 (81%)]\tDiscriminator Loss: 0.604411\tGenerator Loss: 1.747665\n",
      "Train Epoch: 25 [49920/60000 (83%)]\tDiscriminator Loss: 0.590202\tGenerator Loss: 0.907187\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tDiscriminator Loss: 0.545240\tGenerator Loss: 0.883977\n",
      "Train Epoch: 25 [52480/60000 (87%)]\tDiscriminator Loss: 0.456438\tGenerator Loss: 1.307482\n",
      "Train Epoch: 25 [53760/60000 (90%)]\tDiscriminator Loss: 0.480532\tGenerator Loss: 1.299149\n",
      "Train Epoch: 25 [55040/60000 (92%)]\tDiscriminator Loss: 0.580997\tGenerator Loss: 1.477718\n",
      "Train Epoch: 25 [56320/60000 (94%)]\tDiscriminator Loss: 0.588484\tGenerator Loss: 0.885725\n",
      "Train Epoch: 25 [57600/60000 (96%)]\tDiscriminator Loss: 0.529071\tGenerator Loss: 1.326962\n",
      "Train Epoch: 25 [58880/60000 (98%)]\tDiscriminator Loss: 0.517067\tGenerator Loss: 1.169709\n",
      "Train Epoch: 26 [0/60000 (0%)]\tDiscriminator Loss: 0.519974\tGenerator Loss: 1.155681\n",
      "Train Epoch: 26 [1280/60000 (2%)]\tDiscriminator Loss: 0.585547\tGenerator Loss: 1.320782\n",
      "Train Epoch: 26 [2560/60000 (4%)]\tDiscriminator Loss: 0.514817\tGenerator Loss: 1.647050\n",
      "Train Epoch: 26 [3840/60000 (6%)]\tDiscriminator Loss: 0.548063\tGenerator Loss: 1.076393\n",
      "Train Epoch: 26 [5120/60000 (9%)]\tDiscriminator Loss: 0.557641\tGenerator Loss: 0.855893\n",
      "Train Epoch: 26 [6400/60000 (11%)]\tDiscriminator Loss: 0.468902\tGenerator Loss: 1.145948\n",
      "Train Epoch: 26 [7680/60000 (13%)]\tDiscriminator Loss: 0.490893\tGenerator Loss: 1.204325\n",
      "Train Epoch: 26 [8960/60000 (15%)]\tDiscriminator Loss: 0.545382\tGenerator Loss: 1.488230\n",
      "Train Epoch: 26 [10240/60000 (17%)]\tDiscriminator Loss: 0.623871\tGenerator Loss: 1.668858\n",
      "Train Epoch: 26 [11520/60000 (19%)]\tDiscriminator Loss: 0.560520\tGenerator Loss: 1.379001\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tDiscriminator Loss: 0.496605\tGenerator Loss: 1.132747\n",
      "Train Epoch: 26 [14080/60000 (23%)]\tDiscriminator Loss: 0.462699\tGenerator Loss: 1.311885\n",
      "Train Epoch: 26 [15360/60000 (26%)]\tDiscriminator Loss: 0.673584\tGenerator Loss: 0.613838\n",
      "Train Epoch: 26 [16640/60000 (28%)]\tDiscriminator Loss: 0.565457\tGenerator Loss: 1.439203\n",
      "Train Epoch: 26 [17920/60000 (30%)]\tDiscriminator Loss: 0.517583\tGenerator Loss: 1.412179\n",
      "Train Epoch: 26 [19200/60000 (32%)]\tDiscriminator Loss: 0.552887\tGenerator Loss: 1.269064\n",
      "Train Epoch: 26 [20480/60000 (34%)]\tDiscriminator Loss: 0.502076\tGenerator Loss: 1.164187\n",
      "Train Epoch: 26 [21760/60000 (36%)]\tDiscriminator Loss: 0.493275\tGenerator Loss: 1.124630\n",
      "Train Epoch: 26 [23040/60000 (38%)]\tDiscriminator Loss: 0.449790\tGenerator Loss: 1.014648\n",
      "Train Epoch: 26 [24320/60000 (41%)]\tDiscriminator Loss: 0.507308\tGenerator Loss: 1.018323\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tDiscriminator Loss: 0.571393\tGenerator Loss: 1.162106\n",
      "Train Epoch: 26 [26880/60000 (45%)]\tDiscriminator Loss: 0.547452\tGenerator Loss: 1.065473\n",
      "Train Epoch: 26 [28160/60000 (47%)]\tDiscriminator Loss: 0.557868\tGenerator Loss: 1.289773\n",
      "Train Epoch: 26 [29440/60000 (49%)]\tDiscriminator Loss: 0.578384\tGenerator Loss: 0.991474\n",
      "Train Epoch: 26 [30720/60000 (51%)]\tDiscriminator Loss: 0.450965\tGenerator Loss: 1.428663\n",
      "Train Epoch: 26 [32000/60000 (53%)]\tDiscriminator Loss: 0.542585\tGenerator Loss: 0.906702\n",
      "Train Epoch: 26 [33280/60000 (55%)]\tDiscriminator Loss: 0.565139\tGenerator Loss: 1.280734\n",
      "Train Epoch: 26 [34560/60000 (58%)]\tDiscriminator Loss: 0.608525\tGenerator Loss: 1.187868\n",
      "Train Epoch: 26 [35840/60000 (60%)]\tDiscriminator Loss: 0.531626\tGenerator Loss: 1.176577\n",
      "Train Epoch: 26 [37120/60000 (62%)]\tDiscriminator Loss: 0.508274\tGenerator Loss: 1.499830\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tDiscriminator Loss: 0.578411\tGenerator Loss: 1.194552\n",
      "Train Epoch: 26 [39680/60000 (66%)]\tDiscriminator Loss: 0.581667\tGenerator Loss: 1.140185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 26 [40960/60000 (68%)]\tDiscriminator Loss: 0.532574\tGenerator Loss: 0.981191\n",
      "Train Epoch: 26 [42240/60000 (70%)]\tDiscriminator Loss: 0.555750\tGenerator Loss: 1.389959\n",
      "Train Epoch: 26 [43520/60000 (72%)]\tDiscriminator Loss: 0.523020\tGenerator Loss: 1.282933\n",
      "Train Epoch: 26 [44800/60000 (75%)]\tDiscriminator Loss: 0.546186\tGenerator Loss: 1.278774\n",
      "Train Epoch: 26 [46080/60000 (77%)]\tDiscriminator Loss: 0.468517\tGenerator Loss: 1.052864\n",
      "Train Epoch: 26 [47360/60000 (79%)]\tDiscriminator Loss: 0.565257\tGenerator Loss: 1.119027\n",
      "Train Epoch: 26 [48640/60000 (81%)]\tDiscriminator Loss: 0.539437\tGenerator Loss: 1.522147\n",
      "Train Epoch: 26 [49920/60000 (83%)]\tDiscriminator Loss: 0.510090\tGenerator Loss: 1.191722\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tDiscriminator Loss: 0.470363\tGenerator Loss: 1.432379\n",
      "Train Epoch: 26 [52480/60000 (87%)]\tDiscriminator Loss: 0.509279\tGenerator Loss: 1.366858\n",
      "Train Epoch: 26 [53760/60000 (90%)]\tDiscriminator Loss: 0.602637\tGenerator Loss: 0.715273\n",
      "Train Epoch: 26 [55040/60000 (92%)]\tDiscriminator Loss: 0.534500\tGenerator Loss: 0.886277\n",
      "Train Epoch: 26 [56320/60000 (94%)]\tDiscriminator Loss: 0.577936\tGenerator Loss: 1.728853\n",
      "Train Epoch: 26 [57600/60000 (96%)]\tDiscriminator Loss: 0.595618\tGenerator Loss: 1.755814\n",
      "Train Epoch: 26 [58880/60000 (98%)]\tDiscriminator Loss: 0.498021\tGenerator Loss: 1.209190\n",
      "Train Epoch: 27 [0/60000 (0%)]\tDiscriminator Loss: 0.523710\tGenerator Loss: 1.269404\n",
      "Train Epoch: 27 [1280/60000 (2%)]\tDiscriminator Loss: 0.551126\tGenerator Loss: 1.285112\n",
      "Train Epoch: 27 [2560/60000 (4%)]\tDiscriminator Loss: 0.517086\tGenerator Loss: 1.141096\n",
      "Train Epoch: 27 [3840/60000 (6%)]\tDiscriminator Loss: 0.515689\tGenerator Loss: 1.219240\n",
      "Train Epoch: 27 [5120/60000 (9%)]\tDiscriminator Loss: 0.487042\tGenerator Loss: 1.553803\n",
      "Train Epoch: 27 [6400/60000 (11%)]\tDiscriminator Loss: 0.604724\tGenerator Loss: 0.982073\n",
      "Train Epoch: 27 [7680/60000 (13%)]\tDiscriminator Loss: 0.545613\tGenerator Loss: 1.100419\n",
      "Train Epoch: 27 [8960/60000 (15%)]\tDiscriminator Loss: 0.516454\tGenerator Loss: 1.250328\n",
      "Train Epoch: 27 [10240/60000 (17%)]\tDiscriminator Loss: 0.495015\tGenerator Loss: 0.980874\n",
      "Train Epoch: 27 [11520/60000 (19%)]\tDiscriminator Loss: 0.488589\tGenerator Loss: 1.399303\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tDiscriminator Loss: 0.488793\tGenerator Loss: 1.395789\n",
      "Train Epoch: 27 [14080/60000 (23%)]\tDiscriminator Loss: 0.569262\tGenerator Loss: 0.904085\n",
      "Train Epoch: 27 [15360/60000 (26%)]\tDiscriminator Loss: 0.531458\tGenerator Loss: 1.283934\n",
      "Train Epoch: 27 [16640/60000 (28%)]\tDiscriminator Loss: 0.555789\tGenerator Loss: 1.084723\n",
      "Train Epoch: 27 [17920/60000 (30%)]\tDiscriminator Loss: 0.500604\tGenerator Loss: 1.146491\n",
      "Train Epoch: 27 [19200/60000 (32%)]\tDiscriminator Loss: 0.468288\tGenerator Loss: 1.111358\n",
      "Train Epoch: 27 [20480/60000 (34%)]\tDiscriminator Loss: 0.608604\tGenerator Loss: 1.240409\n",
      "Train Epoch: 27 [21760/60000 (36%)]\tDiscriminator Loss: 0.577718\tGenerator Loss: 1.257647\n",
      "Train Epoch: 27 [23040/60000 (38%)]\tDiscriminator Loss: 0.477454\tGenerator Loss: 1.421282\n",
      "Train Epoch: 27 [24320/60000 (41%)]\tDiscriminator Loss: 0.494938\tGenerator Loss: 1.360075\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tDiscriminator Loss: 0.484007\tGenerator Loss: 1.209316\n",
      "Train Epoch: 27 [26880/60000 (45%)]\tDiscriminator Loss: 0.530957\tGenerator Loss: 1.356129\n",
      "Train Epoch: 27 [28160/60000 (47%)]\tDiscriminator Loss: 0.550905\tGenerator Loss: 1.000696\n",
      "Train Epoch: 27 [29440/60000 (49%)]\tDiscriminator Loss: 0.563283\tGenerator Loss: 1.335183\n",
      "Train Epoch: 27 [30720/60000 (51%)]\tDiscriminator Loss: 0.496748\tGenerator Loss: 1.104096\n",
      "Train Epoch: 27 [32000/60000 (53%)]\tDiscriminator Loss: 0.503589\tGenerator Loss: 1.294861\n",
      "Train Epoch: 27 [33280/60000 (55%)]\tDiscriminator Loss: 0.552382\tGenerator Loss: 1.253299\n",
      "Train Epoch: 27 [34560/60000 (58%)]\tDiscriminator Loss: 0.564718\tGenerator Loss: 0.908058\n",
      "Train Epoch: 27 [35840/60000 (60%)]\tDiscriminator Loss: 0.542769\tGenerator Loss: 1.052602\n",
      "Train Epoch: 27 [37120/60000 (62%)]\tDiscriminator Loss: 0.534282\tGenerator Loss: 1.319448\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tDiscriminator Loss: 0.548295\tGenerator Loss: 0.944360\n",
      "Train Epoch: 27 [39680/60000 (66%)]\tDiscriminator Loss: 0.515964\tGenerator Loss: 1.449788\n",
      "Train Epoch: 27 [40960/60000 (68%)]\tDiscriminator Loss: 0.508119\tGenerator Loss: 1.430254\n",
      "Train Epoch: 27 [42240/60000 (70%)]\tDiscriminator Loss: 0.552598\tGenerator Loss: 1.188152\n",
      "Train Epoch: 27 [43520/60000 (72%)]\tDiscriminator Loss: 0.727116\tGenerator Loss: 0.495801\n",
      "Train Epoch: 27 [44800/60000 (75%)]\tDiscriminator Loss: 0.555773\tGenerator Loss: 1.405342\n",
      "Train Epoch: 27 [46080/60000 (77%)]\tDiscriminator Loss: 0.523550\tGenerator Loss: 1.224451\n",
      "Train Epoch: 27 [47360/60000 (79%)]\tDiscriminator Loss: 0.443472\tGenerator Loss: 1.568005\n",
      "Train Epoch: 27 [48640/60000 (81%)]\tDiscriminator Loss: 0.530154\tGenerator Loss: 0.986747\n",
      "Train Epoch: 27 [49920/60000 (83%)]\tDiscriminator Loss: 0.667413\tGenerator Loss: 0.535815\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tDiscriminator Loss: 0.572722\tGenerator Loss: 1.196731\n",
      "Train Epoch: 27 [52480/60000 (87%)]\tDiscriminator Loss: 0.493288\tGenerator Loss: 1.295952\n",
      "Train Epoch: 27 [53760/60000 (90%)]\tDiscriminator Loss: 0.589581\tGenerator Loss: 1.641350\n",
      "Train Epoch: 27 [55040/60000 (92%)]\tDiscriminator Loss: 0.547785\tGenerator Loss: 1.293769\n",
      "Train Epoch: 27 [56320/60000 (94%)]\tDiscriminator Loss: 0.516773\tGenerator Loss: 1.542384\n",
      "Train Epoch: 27 [57600/60000 (96%)]\tDiscriminator Loss: 0.534879\tGenerator Loss: 1.141200\n",
      "Train Epoch: 27 [58880/60000 (98%)]\tDiscriminator Loss: 0.516600\tGenerator Loss: 1.184535\n",
      "Train Epoch: 28 [0/60000 (0%)]\tDiscriminator Loss: 0.569307\tGenerator Loss: 1.074308\n",
      "Train Epoch: 28 [1280/60000 (2%)]\tDiscriminator Loss: 0.571707\tGenerator Loss: 1.176737\n",
      "Train Epoch: 28 [2560/60000 (4%)]\tDiscriminator Loss: 0.504191\tGenerator Loss: 1.503559\n",
      "Train Epoch: 28 [3840/60000 (6%)]\tDiscriminator Loss: 0.488632\tGenerator Loss: 1.217724\n",
      "Train Epoch: 28 [5120/60000 (9%)]\tDiscriminator Loss: 0.564458\tGenerator Loss: 1.016847\n",
      "Train Epoch: 28 [6400/60000 (11%)]\tDiscriminator Loss: 0.490830\tGenerator Loss: 1.382670\n",
      "Train Epoch: 28 [7680/60000 (13%)]\tDiscriminator Loss: 0.521896\tGenerator Loss: 1.493867\n",
      "Train Epoch: 28 [8960/60000 (15%)]\tDiscriminator Loss: 0.624107\tGenerator Loss: 1.329589\n",
      "Train Epoch: 28 [10240/60000 (17%)]\tDiscriminator Loss: 0.694821\tGenerator Loss: 0.658427\n",
      "Train Epoch: 28 [11520/60000 (19%)]\tDiscriminator Loss: 0.532966\tGenerator Loss: 0.985026\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tDiscriminator Loss: 0.536305\tGenerator Loss: 0.964088\n",
      "Train Epoch: 28 [14080/60000 (23%)]\tDiscriminator Loss: 0.512595\tGenerator Loss: 0.960691\n",
      "Train Epoch: 28 [15360/60000 (26%)]\tDiscriminator Loss: 0.604736\tGenerator Loss: 1.220850\n",
      "Train Epoch: 28 [16640/60000 (28%)]\tDiscriminator Loss: 0.513414\tGenerator Loss: 1.422327\n",
      "Train Epoch: 28 [17920/60000 (30%)]\tDiscriminator Loss: 0.540394\tGenerator Loss: 1.681314\n",
      "Train Epoch: 28 [19200/60000 (32%)]\tDiscriminator Loss: 0.535136\tGenerator Loss: 1.141374\n",
      "Train Epoch: 28 [20480/60000 (34%)]\tDiscriminator Loss: 0.608422\tGenerator Loss: 0.985397\n",
      "Train Epoch: 28 [21760/60000 (36%)]\tDiscriminator Loss: 0.551233\tGenerator Loss: 1.108427\n",
      "Train Epoch: 28 [23040/60000 (38%)]\tDiscriminator Loss: 0.485853\tGenerator Loss: 1.291266\n",
      "Train Epoch: 28 [24320/60000 (41%)]\tDiscriminator Loss: 0.525913\tGenerator Loss: 0.991381\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tDiscriminator Loss: 0.647694\tGenerator Loss: 0.618139\n",
      "Train Epoch: 28 [26880/60000 (45%)]\tDiscriminator Loss: 0.524600\tGenerator Loss: 1.132943\n",
      "Train Epoch: 28 [28160/60000 (47%)]\tDiscriminator Loss: 0.479700\tGenerator Loss: 1.189731\n",
      "Train Epoch: 28 [29440/60000 (49%)]\tDiscriminator Loss: 0.572811\tGenerator Loss: 1.094049\n",
      "Train Epoch: 28 [30720/60000 (51%)]\tDiscriminator Loss: 0.622256\tGenerator Loss: 0.768175\n",
      "Train Epoch: 28 [32000/60000 (53%)]\tDiscriminator Loss: 0.546712\tGenerator Loss: 1.138365\n",
      "Train Epoch: 28 [33280/60000 (55%)]\tDiscriminator Loss: 0.545611\tGenerator Loss: 1.267321\n",
      "Train Epoch: 28 [34560/60000 (58%)]\tDiscriminator Loss: 0.520072\tGenerator Loss: 1.387792\n",
      "Train Epoch: 28 [35840/60000 (60%)]\tDiscriminator Loss: 0.604242\tGenerator Loss: 1.433128\n",
      "Train Epoch: 28 [37120/60000 (62%)]\tDiscriminator Loss: 0.536545\tGenerator Loss: 1.097840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 28 [38400/60000 (64%)]\tDiscriminator Loss: 0.557723\tGenerator Loss: 1.089310\n",
      "Train Epoch: 28 [39680/60000 (66%)]\tDiscriminator Loss: 0.538488\tGenerator Loss: 1.199623\n",
      "Train Epoch: 28 [40960/60000 (68%)]\tDiscriminator Loss: 0.543776\tGenerator Loss: 1.037009\n",
      "Train Epoch: 28 [42240/60000 (70%)]\tDiscriminator Loss: 0.468917\tGenerator Loss: 1.272261\n",
      "Train Epoch: 28 [43520/60000 (72%)]\tDiscriminator Loss: 0.538328\tGenerator Loss: 1.307213\n",
      "Train Epoch: 28 [44800/60000 (75%)]\tDiscriminator Loss: 0.657555\tGenerator Loss: 0.944868\n",
      "Train Epoch: 28 [46080/60000 (77%)]\tDiscriminator Loss: 0.594942\tGenerator Loss: 0.855563\n",
      "Train Epoch: 28 [47360/60000 (79%)]\tDiscriminator Loss: 0.470976\tGenerator Loss: 1.161939\n",
      "Train Epoch: 28 [48640/60000 (81%)]\tDiscriminator Loss: 0.462512\tGenerator Loss: 1.453974\n",
      "Train Epoch: 28 [49920/60000 (83%)]\tDiscriminator Loss: 0.483430\tGenerator Loss: 1.494099\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tDiscriminator Loss: 0.635916\tGenerator Loss: 1.680985\n",
      "Train Epoch: 28 [52480/60000 (87%)]\tDiscriminator Loss: 0.544844\tGenerator Loss: 1.045803\n",
      "Train Epoch: 28 [53760/60000 (90%)]\tDiscriminator Loss: 0.566701\tGenerator Loss: 1.155890\n",
      "Train Epoch: 28 [55040/60000 (92%)]\tDiscriminator Loss: 0.558966\tGenerator Loss: 1.287189\n",
      "Train Epoch: 28 [56320/60000 (94%)]\tDiscriminator Loss: 0.578086\tGenerator Loss: 0.970817\n",
      "Train Epoch: 28 [57600/60000 (96%)]\tDiscriminator Loss: 0.570045\tGenerator Loss: 1.009089\n",
      "Train Epoch: 28 [58880/60000 (98%)]\tDiscriminator Loss: 0.542477\tGenerator Loss: 1.125737\n",
      "Train Epoch: 29 [0/60000 (0%)]\tDiscriminator Loss: 0.633586\tGenerator Loss: 0.630648\n",
      "Train Epoch: 29 [1280/60000 (2%)]\tDiscriminator Loss: 0.498084\tGenerator Loss: 1.186175\n",
      "Train Epoch: 29 [2560/60000 (4%)]\tDiscriminator Loss: 0.520764\tGenerator Loss: 1.452892\n",
      "Train Epoch: 29 [3840/60000 (6%)]\tDiscriminator Loss: 0.472776\tGenerator Loss: 1.279401\n",
      "Train Epoch: 29 [5120/60000 (9%)]\tDiscriminator Loss: 0.572929\tGenerator Loss: 1.414776\n",
      "Train Epoch: 29 [6400/60000 (11%)]\tDiscriminator Loss: 0.556394\tGenerator Loss: 1.087014\n",
      "Train Epoch: 29 [7680/60000 (13%)]\tDiscriminator Loss: 0.525372\tGenerator Loss: 1.063544\n",
      "Train Epoch: 29 [8960/60000 (15%)]\tDiscriminator Loss: 0.491922\tGenerator Loss: 1.333592\n",
      "Train Epoch: 29 [10240/60000 (17%)]\tDiscriminator Loss: 0.530639\tGenerator Loss: 1.287740\n",
      "Train Epoch: 29 [11520/60000 (19%)]\tDiscriminator Loss: 0.517698\tGenerator Loss: 1.216757\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tDiscriminator Loss: 0.505087\tGenerator Loss: 1.658613\n",
      "Train Epoch: 29 [14080/60000 (23%)]\tDiscriminator Loss: 0.505725\tGenerator Loss: 1.210644\n",
      "Train Epoch: 29 [15360/60000 (26%)]\tDiscriminator Loss: 0.596173\tGenerator Loss: 1.224898\n",
      "Train Epoch: 29 [16640/60000 (28%)]\tDiscriminator Loss: 0.552571\tGenerator Loss: 1.037048\n",
      "Train Epoch: 29 [17920/60000 (30%)]\tDiscriminator Loss: 0.529007\tGenerator Loss: 1.035520\n",
      "Train Epoch: 29 [19200/60000 (32%)]\tDiscriminator Loss: 0.540024\tGenerator Loss: 1.276557\n",
      "Train Epoch: 29 [20480/60000 (34%)]\tDiscriminator Loss: 0.514032\tGenerator Loss: 1.543720\n",
      "Train Epoch: 29 [21760/60000 (36%)]\tDiscriminator Loss: 0.597174\tGenerator Loss: 1.817780\n",
      "Train Epoch: 29 [23040/60000 (38%)]\tDiscriminator Loss: 0.562950\tGenerator Loss: 1.460356\n",
      "Train Epoch: 29 [24320/60000 (41%)]\tDiscriminator Loss: 0.605037\tGenerator Loss: 1.189629\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tDiscriminator Loss: 0.496169\tGenerator Loss: 1.178079\n",
      "Train Epoch: 29 [26880/60000 (45%)]\tDiscriminator Loss: 0.464094\tGenerator Loss: 1.307733\n",
      "Train Epoch: 29 [28160/60000 (47%)]\tDiscriminator Loss: 0.513804\tGenerator Loss: 0.911378\n",
      "Train Epoch: 29 [29440/60000 (49%)]\tDiscriminator Loss: 0.515522\tGenerator Loss: 1.093821\n",
      "Train Epoch: 29 [30720/60000 (51%)]\tDiscriminator Loss: 0.547050\tGenerator Loss: 1.280229\n",
      "Train Epoch: 29 [32000/60000 (53%)]\tDiscriminator Loss: 0.595565\tGenerator Loss: 0.737323\n",
      "Train Epoch: 29 [33280/60000 (55%)]\tDiscriminator Loss: 0.501451\tGenerator Loss: 1.237776\n",
      "Train Epoch: 29 [34560/60000 (58%)]\tDiscriminator Loss: 0.541891\tGenerator Loss: 1.294433\n",
      "Train Epoch: 29 [35840/60000 (60%)]\tDiscriminator Loss: 0.742590\tGenerator Loss: 0.539105\n",
      "Train Epoch: 29 [37120/60000 (62%)]\tDiscriminator Loss: 0.578326\tGenerator Loss: 1.126560\n",
      "Train Epoch: 29 [38400/60000 (64%)]\tDiscriminator Loss: 0.483259\tGenerator Loss: 1.113860\n",
      "Train Epoch: 29 [39680/60000 (66%)]\tDiscriminator Loss: 0.537599\tGenerator Loss: 1.207912\n",
      "Train Epoch: 29 [40960/60000 (68%)]\tDiscriminator Loss: 0.621547\tGenerator Loss: 1.676834\n",
      "Train Epoch: 29 [42240/60000 (70%)]\tDiscriminator Loss: 0.521657\tGenerator Loss: 1.002827\n",
      "Train Epoch: 29 [43520/60000 (72%)]\tDiscriminator Loss: 0.561543\tGenerator Loss: 1.335448\n",
      "Train Epoch: 29 [44800/60000 (75%)]\tDiscriminator Loss: 0.551757\tGenerator Loss: 1.134218\n",
      "Train Epoch: 29 [46080/60000 (77%)]\tDiscriminator Loss: 0.627158\tGenerator Loss: 0.780515\n",
      "Train Epoch: 29 [47360/60000 (79%)]\tDiscriminator Loss: 0.599001\tGenerator Loss: 0.997812\n",
      "Train Epoch: 29 [48640/60000 (81%)]\tDiscriminator Loss: 0.484511\tGenerator Loss: 1.212347\n",
      "Train Epoch: 29 [49920/60000 (83%)]\tDiscriminator Loss: 0.535794\tGenerator Loss: 1.145984\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tDiscriminator Loss: 0.604134\tGenerator Loss: 0.848365\n",
      "Train Epoch: 29 [52480/60000 (87%)]\tDiscriminator Loss: 0.516440\tGenerator Loss: 0.990693\n",
      "Train Epoch: 29 [53760/60000 (90%)]\tDiscriminator Loss: 0.532489\tGenerator Loss: 1.366194\n",
      "Train Epoch: 29 [55040/60000 (92%)]\tDiscriminator Loss: 0.517910\tGenerator Loss: 1.152972\n",
      "Train Epoch: 29 [56320/60000 (94%)]\tDiscriminator Loss: 0.635988\tGenerator Loss: 0.863920\n",
      "Train Epoch: 29 [57600/60000 (96%)]\tDiscriminator Loss: 0.507389\tGenerator Loss: 1.035418\n",
      "Train Epoch: 29 [58880/60000 (98%)]\tDiscriminator Loss: 0.522706\tGenerator Loss: 1.026332\n",
      "Train Epoch: 30 [0/60000 (0%)]\tDiscriminator Loss: 0.584367\tGenerator Loss: 1.539615\n",
      "Train Epoch: 30 [1280/60000 (2%)]\tDiscriminator Loss: 0.541594\tGenerator Loss: 1.154951\n",
      "Train Epoch: 30 [2560/60000 (4%)]\tDiscriminator Loss: 0.507232\tGenerator Loss: 1.398222\n",
      "Train Epoch: 30 [3840/60000 (6%)]\tDiscriminator Loss: 0.461635\tGenerator Loss: 1.466923\n",
      "Train Epoch: 30 [5120/60000 (9%)]\tDiscriminator Loss: 0.553861\tGenerator Loss: 0.925454\n",
      "Train Epoch: 30 [6400/60000 (11%)]\tDiscriminator Loss: 0.594668\tGenerator Loss: 0.786909\n",
      "Train Epoch: 30 [7680/60000 (13%)]\tDiscriminator Loss: 0.539153\tGenerator Loss: 1.045753\n",
      "Train Epoch: 30 [8960/60000 (15%)]\tDiscriminator Loss: 0.510812\tGenerator Loss: 1.178234\n",
      "Train Epoch: 30 [10240/60000 (17%)]\tDiscriminator Loss: 0.450287\tGenerator Loss: 1.382562\n",
      "Train Epoch: 30 [11520/60000 (19%)]\tDiscriminator Loss: 0.530171\tGenerator Loss: 1.385178\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tDiscriminator Loss: 0.594390\tGenerator Loss: 0.865237\n",
      "Train Epoch: 30 [14080/60000 (23%)]\tDiscriminator Loss: 0.610004\tGenerator Loss: 0.645203\n",
      "Train Epoch: 30 [15360/60000 (26%)]\tDiscriminator Loss: 0.537449\tGenerator Loss: 0.992109\n",
      "Train Epoch: 30 [16640/60000 (28%)]\tDiscriminator Loss: 0.452925\tGenerator Loss: 1.589773\n",
      "Train Epoch: 30 [17920/60000 (30%)]\tDiscriminator Loss: 0.513703\tGenerator Loss: 1.485724\n",
      "Train Epoch: 30 [19200/60000 (32%)]\tDiscriminator Loss: 0.527559\tGenerator Loss: 1.243372\n",
      "Train Epoch: 30 [20480/60000 (34%)]\tDiscriminator Loss: 0.559105\tGenerator Loss: 1.380136\n",
      "Train Epoch: 30 [21760/60000 (36%)]\tDiscriminator Loss: 0.531280\tGenerator Loss: 1.180278\n",
      "Train Epoch: 30 [23040/60000 (38%)]\tDiscriminator Loss: 0.503654\tGenerator Loss: 1.593275\n",
      "Train Epoch: 30 [24320/60000 (41%)]\tDiscriminator Loss: 0.509784\tGenerator Loss: 1.262907\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tDiscriminator Loss: 0.594812\tGenerator Loss: 1.534597\n",
      "Train Epoch: 30 [26880/60000 (45%)]\tDiscriminator Loss: 0.474774\tGenerator Loss: 1.263332\n",
      "Train Epoch: 30 [28160/60000 (47%)]\tDiscriminator Loss: 0.527698\tGenerator Loss: 1.175519\n",
      "Train Epoch: 30 [29440/60000 (49%)]\tDiscriminator Loss: 0.580879\tGenerator Loss: 1.566065\n",
      "Train Epoch: 30 [30720/60000 (51%)]\tDiscriminator Loss: 0.591886\tGenerator Loss: 1.531368\n",
      "Train Epoch: 30 [32000/60000 (53%)]\tDiscriminator Loss: 0.521935\tGenerator Loss: 1.119469\n",
      "Train Epoch: 30 [33280/60000 (55%)]\tDiscriminator Loss: 0.716003\tGenerator Loss: 2.194292\n",
      "Train Epoch: 30 [34560/60000 (58%)]\tDiscriminator Loss: 0.498947\tGenerator Loss: 1.035928\n",
      "Train Epoch: 30 [35840/60000 (60%)]\tDiscriminator Loss: 0.581425\tGenerator Loss: 1.413548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 30 [37120/60000 (62%)]\tDiscriminator Loss: 0.515644\tGenerator Loss: 1.134552\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tDiscriminator Loss: 0.503524\tGenerator Loss: 1.342466\n",
      "Train Epoch: 30 [39680/60000 (66%)]\tDiscriminator Loss: 0.547721\tGenerator Loss: 0.859993\n",
      "Train Epoch: 30 [40960/60000 (68%)]\tDiscriminator Loss: 0.471371\tGenerator Loss: 1.221571\n",
      "Train Epoch: 30 [42240/60000 (70%)]\tDiscriminator Loss: 0.590793\tGenerator Loss: 1.420209\n",
      "Train Epoch: 30 [43520/60000 (72%)]\tDiscriminator Loss: 0.612110\tGenerator Loss: 1.117713\n",
      "Train Epoch: 30 [44800/60000 (75%)]\tDiscriminator Loss: 0.582181\tGenerator Loss: 1.080198\n",
      "Train Epoch: 30 [46080/60000 (77%)]\tDiscriminator Loss: 0.615796\tGenerator Loss: 0.776092\n",
      "Train Epoch: 30 [47360/60000 (79%)]\tDiscriminator Loss: 0.500719\tGenerator Loss: 1.174550\n",
      "Train Epoch: 30 [48640/60000 (81%)]\tDiscriminator Loss: 0.595976\tGenerator Loss: 1.353940\n",
      "Train Epoch: 30 [49920/60000 (83%)]\tDiscriminator Loss: 0.603281\tGenerator Loss: 0.663825\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tDiscriminator Loss: 0.509045\tGenerator Loss: 1.617918\n",
      "Train Epoch: 30 [52480/60000 (87%)]\tDiscriminator Loss: 0.495940\tGenerator Loss: 1.314510\n",
      "Train Epoch: 30 [53760/60000 (90%)]\tDiscriminator Loss: 0.494560\tGenerator Loss: 1.286540\n",
      "Train Epoch: 30 [55040/60000 (92%)]\tDiscriminator Loss: 0.679996\tGenerator Loss: 1.548160\n",
      "Train Epoch: 30 [56320/60000 (94%)]\tDiscriminator Loss: 0.583462\tGenerator Loss: 0.843649\n",
      "Train Epoch: 30 [57600/60000 (96%)]\tDiscriminator Loss: 0.546098\tGenerator Loss: 0.896985\n",
      "Train Epoch: 30 [58880/60000 (98%)]\tDiscriminator Loss: 0.524737\tGenerator Loss: 1.199771\n",
      "Train Epoch: 31 [0/60000 (0%)]\tDiscriminator Loss: 0.617546\tGenerator Loss: 1.134159\n",
      "Train Epoch: 31 [1280/60000 (2%)]\tDiscriminator Loss: 0.558743\tGenerator Loss: 1.096706\n",
      "Train Epoch: 31 [2560/60000 (4%)]\tDiscriminator Loss: 0.565639\tGenerator Loss: 1.064481\n",
      "Train Epoch: 31 [3840/60000 (6%)]\tDiscriminator Loss: 0.507013\tGenerator Loss: 1.213527\n",
      "Train Epoch: 31 [5120/60000 (9%)]\tDiscriminator Loss: 0.535409\tGenerator Loss: 1.036540\n",
      "Train Epoch: 31 [6400/60000 (11%)]\tDiscriminator Loss: 0.568565\tGenerator Loss: 1.016446\n",
      "Train Epoch: 31 [7680/60000 (13%)]\tDiscriminator Loss: 0.568805\tGenerator Loss: 1.320106\n",
      "Train Epoch: 31 [8960/60000 (15%)]\tDiscriminator Loss: 0.549071\tGenerator Loss: 1.077403\n",
      "Train Epoch: 31 [10240/60000 (17%)]\tDiscriminator Loss: 0.517283\tGenerator Loss: 1.604856\n",
      "Train Epoch: 31 [11520/60000 (19%)]\tDiscriminator Loss: 0.587811\tGenerator Loss: 0.833906\n",
      "Train Epoch: 31 [12800/60000 (21%)]\tDiscriminator Loss: 0.490802\tGenerator Loss: 1.214289\n",
      "Train Epoch: 31 [14080/60000 (23%)]\tDiscriminator Loss: 0.539716\tGenerator Loss: 1.303816\n",
      "Train Epoch: 31 [15360/60000 (26%)]\tDiscriminator Loss: 0.524118\tGenerator Loss: 0.971894\n",
      "Train Epoch: 31 [16640/60000 (28%)]\tDiscriminator Loss: 0.537512\tGenerator Loss: 1.156759\n",
      "Train Epoch: 31 [17920/60000 (30%)]\tDiscriminator Loss: 0.568683\tGenerator Loss: 1.217313\n",
      "Train Epoch: 31 [19200/60000 (32%)]\tDiscriminator Loss: 0.529988\tGenerator Loss: 1.771045\n",
      "Train Epoch: 31 [20480/60000 (34%)]\tDiscriminator Loss: 0.543558\tGenerator Loss: 1.737934\n",
      "Train Epoch: 31 [21760/60000 (36%)]\tDiscriminator Loss: 0.521170\tGenerator Loss: 1.158323\n",
      "Train Epoch: 31 [23040/60000 (38%)]\tDiscriminator Loss: 0.531677\tGenerator Loss: 1.096202\n",
      "Train Epoch: 31 [24320/60000 (41%)]\tDiscriminator Loss: 0.569967\tGenerator Loss: 1.312264\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tDiscriminator Loss: 0.521214\tGenerator Loss: 1.074004\n",
      "Train Epoch: 31 [26880/60000 (45%)]\tDiscriminator Loss: 0.515025\tGenerator Loss: 0.994498\n",
      "Train Epoch: 31 [28160/60000 (47%)]\tDiscriminator Loss: 0.495447\tGenerator Loss: 1.540127\n",
      "Train Epoch: 31 [29440/60000 (49%)]\tDiscriminator Loss: 0.511197\tGenerator Loss: 1.322665\n",
      "Train Epoch: 31 [30720/60000 (51%)]\tDiscriminator Loss: 0.535317\tGenerator Loss: 1.016027\n",
      "Train Epoch: 31 [32000/60000 (53%)]\tDiscriminator Loss: 0.532142\tGenerator Loss: 1.386660\n",
      "Train Epoch: 31 [33280/60000 (55%)]\tDiscriminator Loss: 0.566771\tGenerator Loss: 0.942068\n",
      "Train Epoch: 31 [34560/60000 (58%)]\tDiscriminator Loss: 0.530093\tGenerator Loss: 0.941013\n",
      "Train Epoch: 31 [35840/60000 (60%)]\tDiscriminator Loss: 0.531278\tGenerator Loss: 1.314993\n",
      "Train Epoch: 31 [37120/60000 (62%)]\tDiscriminator Loss: 0.593594\tGenerator Loss: 0.987890\n",
      "Train Epoch: 31 [38400/60000 (64%)]\tDiscriminator Loss: 0.561411\tGenerator Loss: 1.160339\n",
      "Train Epoch: 31 [39680/60000 (66%)]\tDiscriminator Loss: 0.504785\tGenerator Loss: 1.482523\n",
      "Train Epoch: 31 [40960/60000 (68%)]\tDiscriminator Loss: 0.533236\tGenerator Loss: 1.256318\n",
      "Train Epoch: 31 [42240/60000 (70%)]\tDiscriminator Loss: 0.495982\tGenerator Loss: 1.441402\n",
      "Train Epoch: 31 [43520/60000 (72%)]\tDiscriminator Loss: 0.583600\tGenerator Loss: 1.110461\n",
      "Train Epoch: 31 [44800/60000 (75%)]\tDiscriminator Loss: 0.496833\tGenerator Loss: 1.291759\n",
      "Train Epoch: 31 [46080/60000 (77%)]\tDiscriminator Loss: 0.538934\tGenerator Loss: 1.047469\n",
      "Train Epoch: 31 [47360/60000 (79%)]\tDiscriminator Loss: 0.527798\tGenerator Loss: 1.608822\n",
      "Train Epoch: 31 [48640/60000 (81%)]\tDiscriminator Loss: 0.575429\tGenerator Loss: 0.913994\n",
      "Train Epoch: 31 [49920/60000 (83%)]\tDiscriminator Loss: 0.544837\tGenerator Loss: 1.432523\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tDiscriminator Loss: 0.516178\tGenerator Loss: 1.227639\n",
      "Train Epoch: 31 [52480/60000 (87%)]\tDiscriminator Loss: 0.523807\tGenerator Loss: 1.523640\n",
      "Train Epoch: 31 [53760/60000 (90%)]\tDiscriminator Loss: 0.552135\tGenerator Loss: 1.079560\n",
      "Train Epoch: 31 [55040/60000 (92%)]\tDiscriminator Loss: 0.572901\tGenerator Loss: 1.100136\n",
      "Train Epoch: 31 [56320/60000 (94%)]\tDiscriminator Loss: 0.571824\tGenerator Loss: 1.450162\n",
      "Train Epoch: 31 [57600/60000 (96%)]\tDiscriminator Loss: 0.521309\tGenerator Loss: 1.027295\n",
      "Train Epoch: 31 [58880/60000 (98%)]\tDiscriminator Loss: 0.530952\tGenerator Loss: 0.892760\n",
      "Train Epoch: 32 [0/60000 (0%)]\tDiscriminator Loss: 0.500173\tGenerator Loss: 1.396017\n",
      "Train Epoch: 32 [1280/60000 (2%)]\tDiscriminator Loss: 0.538071\tGenerator Loss: 1.337075\n",
      "Train Epoch: 32 [2560/60000 (4%)]\tDiscriminator Loss: 0.511452\tGenerator Loss: 1.845459\n",
      "Train Epoch: 32 [3840/60000 (6%)]\tDiscriminator Loss: 0.547096\tGenerator Loss: 0.946791\n",
      "Train Epoch: 32 [5120/60000 (9%)]\tDiscriminator Loss: 0.510550\tGenerator Loss: 1.220168\n",
      "Train Epoch: 32 [6400/60000 (11%)]\tDiscriminator Loss: 0.543673\tGenerator Loss: 1.314713\n",
      "Train Epoch: 32 [7680/60000 (13%)]\tDiscriminator Loss: 0.551133\tGenerator Loss: 1.137931\n",
      "Train Epoch: 32 [8960/60000 (15%)]\tDiscriminator Loss: 0.500136\tGenerator Loss: 1.195695\n",
      "Train Epoch: 32 [10240/60000 (17%)]\tDiscriminator Loss: 0.555322\tGenerator Loss: 1.324925\n",
      "Train Epoch: 32 [11520/60000 (19%)]\tDiscriminator Loss: 0.553694\tGenerator Loss: 1.243169\n",
      "Train Epoch: 32 [12800/60000 (21%)]\tDiscriminator Loss: 0.537874\tGenerator Loss: 1.000383\n",
      "Train Epoch: 32 [14080/60000 (23%)]\tDiscriminator Loss: 0.465320\tGenerator Loss: 1.236569\n",
      "Train Epoch: 32 [15360/60000 (26%)]\tDiscriminator Loss: 0.593062\tGenerator Loss: 1.774905\n",
      "Train Epoch: 32 [16640/60000 (28%)]\tDiscriminator Loss: 0.609977\tGenerator Loss: 1.067557\n",
      "Train Epoch: 32 [17920/60000 (30%)]\tDiscriminator Loss: 0.462434\tGenerator Loss: 1.410534\n",
      "Train Epoch: 32 [19200/60000 (32%)]\tDiscriminator Loss: 0.542178\tGenerator Loss: 1.537343\n",
      "Train Epoch: 32 [20480/60000 (34%)]\tDiscriminator Loss: 0.527105\tGenerator Loss: 1.217750\n",
      "Train Epoch: 32 [21760/60000 (36%)]\tDiscriminator Loss: 0.566112\tGenerator Loss: 0.896755\n",
      "Train Epoch: 32 [23040/60000 (38%)]\tDiscriminator Loss: 0.491848\tGenerator Loss: 1.164009\n",
      "Train Epoch: 32 [24320/60000 (41%)]\tDiscriminator Loss: 0.512633\tGenerator Loss: 1.551742\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tDiscriminator Loss: 0.566033\tGenerator Loss: 0.944164\n",
      "Train Epoch: 32 [26880/60000 (45%)]\tDiscriminator Loss: 0.502488\tGenerator Loss: 1.043323\n",
      "Train Epoch: 32 [28160/60000 (47%)]\tDiscriminator Loss: 0.586713\tGenerator Loss: 1.239556\n",
      "Train Epoch: 32 [29440/60000 (49%)]\tDiscriminator Loss: 0.488109\tGenerator Loss: 1.456515\n",
      "Train Epoch: 32 [30720/60000 (51%)]\tDiscriminator Loss: 0.509767\tGenerator Loss: 1.159446\n",
      "Train Epoch: 32 [32000/60000 (53%)]\tDiscriminator Loss: 0.575410\tGenerator Loss: 0.978359\n",
      "Train Epoch: 32 [33280/60000 (55%)]\tDiscriminator Loss: 0.585728\tGenerator Loss: 1.406510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 32 [34560/60000 (58%)]\tDiscriminator Loss: 0.584836\tGenerator Loss: 1.126171\n",
      "Train Epoch: 32 [35840/60000 (60%)]\tDiscriminator Loss: 0.503263\tGenerator Loss: 1.498719\n",
      "Train Epoch: 32 [37120/60000 (62%)]\tDiscriminator Loss: 0.519175\tGenerator Loss: 1.743746\n",
      "Train Epoch: 32 [38400/60000 (64%)]\tDiscriminator Loss: 0.600326\tGenerator Loss: 1.277865\n",
      "Train Epoch: 32 [39680/60000 (66%)]\tDiscriminator Loss: 0.517003\tGenerator Loss: 1.204258\n",
      "Train Epoch: 32 [40960/60000 (68%)]\tDiscriminator Loss: 0.535046\tGenerator Loss: 1.406968\n",
      "Train Epoch: 32 [42240/60000 (70%)]\tDiscriminator Loss: 0.585513\tGenerator Loss: 1.014559\n",
      "Train Epoch: 32 [43520/60000 (72%)]\tDiscriminator Loss: 0.516001\tGenerator Loss: 1.333284\n",
      "Train Epoch: 32 [44800/60000 (75%)]\tDiscriminator Loss: 0.530814\tGenerator Loss: 0.974996\n",
      "Train Epoch: 32 [46080/60000 (77%)]\tDiscriminator Loss: 0.532654\tGenerator Loss: 1.067183\n",
      "Train Epoch: 32 [47360/60000 (79%)]\tDiscriminator Loss: 0.517966\tGenerator Loss: 1.145623\n",
      "Train Epoch: 32 [48640/60000 (81%)]\tDiscriminator Loss: 0.535307\tGenerator Loss: 0.946728\n",
      "Train Epoch: 32 [49920/60000 (83%)]\tDiscriminator Loss: 0.512409\tGenerator Loss: 1.584727\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tDiscriminator Loss: 0.564210\tGenerator Loss: 1.065275\n",
      "Train Epoch: 32 [52480/60000 (87%)]\tDiscriminator Loss: 0.486311\tGenerator Loss: 1.379376\n",
      "Train Epoch: 32 [53760/60000 (90%)]\tDiscriminator Loss: 0.521010\tGenerator Loss: 0.948770\n",
      "Train Epoch: 32 [55040/60000 (92%)]\tDiscriminator Loss: 0.592340\tGenerator Loss: 1.303866\n",
      "Train Epoch: 32 [56320/60000 (94%)]\tDiscriminator Loss: 0.499319\tGenerator Loss: 1.312116\n",
      "Train Epoch: 32 [57600/60000 (96%)]\tDiscriminator Loss: 0.511767\tGenerator Loss: 1.470670\n",
      "Train Epoch: 32 [58880/60000 (98%)]\tDiscriminator Loss: 0.533585\tGenerator Loss: 1.558839\n",
      "Train Epoch: 33 [0/60000 (0%)]\tDiscriminator Loss: 0.528190\tGenerator Loss: 0.932896\n",
      "Train Epoch: 33 [1280/60000 (2%)]\tDiscriminator Loss: 0.571453\tGenerator Loss: 1.097973\n",
      "Train Epoch: 33 [2560/60000 (4%)]\tDiscriminator Loss: 0.450722\tGenerator Loss: 1.419848\n",
      "Train Epoch: 33 [3840/60000 (6%)]\tDiscriminator Loss: 0.555625\tGenerator Loss: 1.408709\n",
      "Train Epoch: 33 [5120/60000 (9%)]\tDiscriminator Loss: 0.617684\tGenerator Loss: 0.633939\n",
      "Train Epoch: 33 [6400/60000 (11%)]\tDiscriminator Loss: 0.502863\tGenerator Loss: 1.196285\n",
      "Train Epoch: 33 [7680/60000 (13%)]\tDiscriminator Loss: 0.579585\tGenerator Loss: 0.765353\n",
      "Train Epoch: 33 [8960/60000 (15%)]\tDiscriminator Loss: 0.527860\tGenerator Loss: 1.218370\n",
      "Train Epoch: 33 [10240/60000 (17%)]\tDiscriminator Loss: 0.485491\tGenerator Loss: 1.279397\n",
      "Train Epoch: 33 [11520/60000 (19%)]\tDiscriminator Loss: 0.520339\tGenerator Loss: 0.968127\n",
      "Train Epoch: 33 [12800/60000 (21%)]\tDiscriminator Loss: 0.548374\tGenerator Loss: 1.219812\n",
      "Train Epoch: 33 [14080/60000 (23%)]\tDiscriminator Loss: 0.508486\tGenerator Loss: 1.552297\n",
      "Train Epoch: 33 [15360/60000 (26%)]\tDiscriminator Loss: 0.475517\tGenerator Loss: 1.325868\n",
      "Train Epoch: 33 [16640/60000 (28%)]\tDiscriminator Loss: 0.520414\tGenerator Loss: 1.047641\n",
      "Train Epoch: 33 [17920/60000 (30%)]\tDiscriminator Loss: 0.597747\tGenerator Loss: 1.072612\n",
      "Train Epoch: 33 [19200/60000 (32%)]\tDiscriminator Loss: 0.558509\tGenerator Loss: 1.078978\n",
      "Train Epoch: 33 [20480/60000 (34%)]\tDiscriminator Loss: 0.491085\tGenerator Loss: 1.192284\n",
      "Train Epoch: 33 [21760/60000 (36%)]\tDiscriminator Loss: 0.491338\tGenerator Loss: 1.145955\n",
      "Train Epoch: 33 [23040/60000 (38%)]\tDiscriminator Loss: 0.581540\tGenerator Loss: 0.846042\n",
      "Train Epoch: 33 [24320/60000 (41%)]\tDiscriminator Loss: 0.537837\tGenerator Loss: 0.928143\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tDiscriminator Loss: 0.532952\tGenerator Loss: 1.066260\n",
      "Train Epoch: 33 [26880/60000 (45%)]\tDiscriminator Loss: 0.459589\tGenerator Loss: 1.456120\n",
      "Train Epoch: 33 [28160/60000 (47%)]\tDiscriminator Loss: 0.483119\tGenerator Loss: 1.388153\n",
      "Train Epoch: 33 [29440/60000 (49%)]\tDiscriminator Loss: 0.554722\tGenerator Loss: 1.290147\n",
      "Train Epoch: 33 [30720/60000 (51%)]\tDiscriminator Loss: 0.526063\tGenerator Loss: 0.978167\n",
      "Train Epoch: 33 [32000/60000 (53%)]\tDiscriminator Loss: 0.489971\tGenerator Loss: 1.353768\n",
      "Train Epoch: 33 [33280/60000 (55%)]\tDiscriminator Loss: 0.608318\tGenerator Loss: 1.800680\n",
      "Train Epoch: 33 [34560/60000 (58%)]\tDiscriminator Loss: 0.557761\tGenerator Loss: 0.854700\n",
      "Train Epoch: 33 [35840/60000 (60%)]\tDiscriminator Loss: 0.437151\tGenerator Loss: 1.299755\n",
      "Train Epoch: 33 [37120/60000 (62%)]\tDiscriminator Loss: 0.533839\tGenerator Loss: 1.217919\n",
      "Train Epoch: 33 [38400/60000 (64%)]\tDiscriminator Loss: 0.533865\tGenerator Loss: 1.319400\n",
      "Train Epoch: 33 [39680/60000 (66%)]\tDiscriminator Loss: 0.563841\tGenerator Loss: 1.127502\n",
      "Train Epoch: 33 [40960/60000 (68%)]\tDiscriminator Loss: 0.519713\tGenerator Loss: 1.102067\n",
      "Train Epoch: 33 [42240/60000 (70%)]\tDiscriminator Loss: 0.493305\tGenerator Loss: 1.055604\n",
      "Train Epoch: 33 [43520/60000 (72%)]\tDiscriminator Loss: 0.629087\tGenerator Loss: 0.864812\n",
      "Train Epoch: 33 [44800/60000 (75%)]\tDiscriminator Loss: 0.561094\tGenerator Loss: 1.605986\n",
      "Train Epoch: 33 [46080/60000 (77%)]\tDiscriminator Loss: 0.468751\tGenerator Loss: 1.155708\n",
      "Train Epoch: 33 [47360/60000 (79%)]\tDiscriminator Loss: 0.553661\tGenerator Loss: 1.557351\n",
      "Train Epoch: 33 [48640/60000 (81%)]\tDiscriminator Loss: 0.655639\tGenerator Loss: 0.934393\n",
      "Train Epoch: 33 [49920/60000 (83%)]\tDiscriminator Loss: 0.522125\tGenerator Loss: 1.129452\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tDiscriminator Loss: 0.557738\tGenerator Loss: 1.235551\n",
      "Train Epoch: 33 [52480/60000 (87%)]\tDiscriminator Loss: 0.477896\tGenerator Loss: 1.426027\n",
      "Train Epoch: 33 [53760/60000 (90%)]\tDiscriminator Loss: 0.537561\tGenerator Loss: 1.347571\n",
      "Train Epoch: 33 [55040/60000 (92%)]\tDiscriminator Loss: 0.616102\tGenerator Loss: 1.144890\n",
      "Train Epoch: 33 [56320/60000 (94%)]\tDiscriminator Loss: 0.491092\tGenerator Loss: 1.108785\n",
      "Train Epoch: 33 [57600/60000 (96%)]\tDiscriminator Loss: 0.474028\tGenerator Loss: 1.359531\n",
      "Train Epoch: 33 [58880/60000 (98%)]\tDiscriminator Loss: 0.544913\tGenerator Loss: 1.165302\n",
      "Train Epoch: 34 [0/60000 (0%)]\tDiscriminator Loss: 0.626493\tGenerator Loss: 1.837925\n",
      "Train Epoch: 34 [1280/60000 (2%)]\tDiscriminator Loss: 0.548146\tGenerator Loss: 1.035044\n",
      "Train Epoch: 34 [2560/60000 (4%)]\tDiscriminator Loss: 0.497311\tGenerator Loss: 1.249814\n",
      "Train Epoch: 34 [3840/60000 (6%)]\tDiscriminator Loss: 0.555134\tGenerator Loss: 1.229387\n",
      "Train Epoch: 34 [5120/60000 (9%)]\tDiscriminator Loss: 0.544122\tGenerator Loss: 1.135791\n",
      "Train Epoch: 34 [6400/60000 (11%)]\tDiscriminator Loss: 0.527598\tGenerator Loss: 1.002757\n",
      "Train Epoch: 34 [7680/60000 (13%)]\tDiscriminator Loss: 0.513536\tGenerator Loss: 1.324702\n",
      "Train Epoch: 34 [8960/60000 (15%)]\tDiscriminator Loss: 0.538585\tGenerator Loss: 0.816968\n",
      "Train Epoch: 34 [10240/60000 (17%)]\tDiscriminator Loss: 0.513956\tGenerator Loss: 1.129035\n",
      "Train Epoch: 34 [11520/60000 (19%)]\tDiscriminator Loss: 0.499528\tGenerator Loss: 1.400505\n",
      "Train Epoch: 34 [12800/60000 (21%)]\tDiscriminator Loss: 0.513472\tGenerator Loss: 1.317308\n",
      "Train Epoch: 34 [14080/60000 (23%)]\tDiscriminator Loss: 0.566651\tGenerator Loss: 1.248734\n",
      "Train Epoch: 34 [15360/60000 (26%)]\tDiscriminator Loss: 0.554141\tGenerator Loss: 0.958050\n",
      "Train Epoch: 34 [16640/60000 (28%)]\tDiscriminator Loss: 0.524454\tGenerator Loss: 1.066381\n",
      "Train Epoch: 34 [17920/60000 (30%)]\tDiscriminator Loss: 0.533817\tGenerator Loss: 1.468791\n",
      "Train Epoch: 34 [19200/60000 (32%)]\tDiscriminator Loss: 0.605155\tGenerator Loss: 1.245584\n",
      "Train Epoch: 34 [20480/60000 (34%)]\tDiscriminator Loss: 0.497896\tGenerator Loss: 1.128743\n",
      "Train Epoch: 34 [21760/60000 (36%)]\tDiscriminator Loss: 0.568397\tGenerator Loss: 1.072248\n",
      "Train Epoch: 34 [23040/60000 (38%)]\tDiscriminator Loss: 0.548061\tGenerator Loss: 0.884427\n",
      "Train Epoch: 34 [24320/60000 (41%)]\tDiscriminator Loss: 0.516820\tGenerator Loss: 1.608592\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tDiscriminator Loss: 0.517195\tGenerator Loss: 1.652793\n",
      "Train Epoch: 34 [26880/60000 (45%)]\tDiscriminator Loss: 0.652233\tGenerator Loss: 1.630541\n",
      "Train Epoch: 34 [28160/60000 (47%)]\tDiscriminator Loss: 0.572622\tGenerator Loss: 1.203395\n",
      "Train Epoch: 34 [29440/60000 (49%)]\tDiscriminator Loss: 0.514964\tGenerator Loss: 1.343637\n",
      "Train Epoch: 34 [30720/60000 (51%)]\tDiscriminator Loss: 0.611020\tGenerator Loss: 0.930275\n",
      "Train Epoch: 34 [32000/60000 (53%)]\tDiscriminator Loss: 0.533692\tGenerator Loss: 0.889648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 34 [33280/60000 (55%)]\tDiscriminator Loss: 0.512552\tGenerator Loss: 1.297657\n",
      "Train Epoch: 34 [34560/60000 (58%)]\tDiscriminator Loss: 0.641564\tGenerator Loss: 1.513640\n",
      "Train Epoch: 34 [35840/60000 (60%)]\tDiscriminator Loss: 0.559015\tGenerator Loss: 1.200447\n",
      "Train Epoch: 34 [37120/60000 (62%)]\tDiscriminator Loss: 0.615284\tGenerator Loss: 0.977654\n",
      "Train Epoch: 34 [38400/60000 (64%)]\tDiscriminator Loss: 0.516159\tGenerator Loss: 1.250706\n",
      "Train Epoch: 34 [39680/60000 (66%)]\tDiscriminator Loss: 0.590064\tGenerator Loss: 1.185067\n",
      "Train Epoch: 34 [40960/60000 (68%)]\tDiscriminator Loss: 0.457993\tGenerator Loss: 1.078341\n",
      "Train Epoch: 34 [42240/60000 (70%)]\tDiscriminator Loss: 0.488852\tGenerator Loss: 1.407409\n",
      "Train Epoch: 34 [43520/60000 (72%)]\tDiscriminator Loss: 0.571086\tGenerator Loss: 1.014021\n",
      "Train Epoch: 34 [44800/60000 (75%)]\tDiscriminator Loss: 0.565886\tGenerator Loss: 1.374885\n",
      "Train Epoch: 34 [46080/60000 (77%)]\tDiscriminator Loss: 0.536119\tGenerator Loss: 1.454378\n",
      "Train Epoch: 34 [47360/60000 (79%)]\tDiscriminator Loss: 0.576029\tGenerator Loss: 1.126853\n",
      "Train Epoch: 34 [48640/60000 (81%)]\tDiscriminator Loss: 0.608091\tGenerator Loss: 0.984939\n",
      "Train Epoch: 34 [49920/60000 (83%)]\tDiscriminator Loss: 0.530856\tGenerator Loss: 1.512404\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tDiscriminator Loss: 0.534231\tGenerator Loss: 1.270347\n",
      "Train Epoch: 34 [52480/60000 (87%)]\tDiscriminator Loss: 0.511170\tGenerator Loss: 1.373881\n",
      "Train Epoch: 34 [53760/60000 (90%)]\tDiscriminator Loss: 0.585019\tGenerator Loss: 1.058596\n",
      "Train Epoch: 34 [55040/60000 (92%)]\tDiscriminator Loss: 0.505538\tGenerator Loss: 1.440152\n",
      "Train Epoch: 34 [56320/60000 (94%)]\tDiscriminator Loss: 0.555960\tGenerator Loss: 1.167212\n",
      "Train Epoch: 34 [57600/60000 (96%)]\tDiscriminator Loss: 0.514575\tGenerator Loss: 1.264398\n",
      "Train Epoch: 34 [58880/60000 (98%)]\tDiscriminator Loss: 0.534214\tGenerator Loss: 1.162879\n",
      "Train Epoch: 35 [0/60000 (0%)]\tDiscriminator Loss: 0.544901\tGenerator Loss: 1.271615\n",
      "Train Epoch: 35 [1280/60000 (2%)]\tDiscriminator Loss: 0.528226\tGenerator Loss: 1.341964\n",
      "Train Epoch: 35 [2560/60000 (4%)]\tDiscriminator Loss: 0.593996\tGenerator Loss: 1.254110\n",
      "Train Epoch: 35 [3840/60000 (6%)]\tDiscriminator Loss: 0.591171\tGenerator Loss: 1.497377\n",
      "Train Epoch: 35 [5120/60000 (9%)]\tDiscriminator Loss: 0.458925\tGenerator Loss: 1.045555\n",
      "Train Epoch: 35 [6400/60000 (11%)]\tDiscriminator Loss: 0.495974\tGenerator Loss: 1.117823\n",
      "Train Epoch: 35 [7680/60000 (13%)]\tDiscriminator Loss: 0.561312\tGenerator Loss: 1.241419\n",
      "Train Epoch: 35 [8960/60000 (15%)]\tDiscriminator Loss: 0.603385\tGenerator Loss: 1.121979\n",
      "Train Epoch: 35 [10240/60000 (17%)]\tDiscriminator Loss: 0.501998\tGenerator Loss: 1.274988\n",
      "Train Epoch: 35 [11520/60000 (19%)]\tDiscriminator Loss: 0.605049\tGenerator Loss: 1.887877\n",
      "Train Epoch: 35 [12800/60000 (21%)]\tDiscriminator Loss: 0.540985\tGenerator Loss: 1.037571\n",
      "Train Epoch: 35 [14080/60000 (23%)]\tDiscriminator Loss: 0.488757\tGenerator Loss: 1.950835\n",
      "Train Epoch: 35 [15360/60000 (26%)]\tDiscriminator Loss: 0.526374\tGenerator Loss: 1.234615\n",
      "Train Epoch: 35 [16640/60000 (28%)]\tDiscriminator Loss: 0.546602\tGenerator Loss: 1.627017\n",
      "Train Epoch: 35 [17920/60000 (30%)]\tDiscriminator Loss: 0.488578\tGenerator Loss: 1.246679\n",
      "Train Epoch: 35 [19200/60000 (32%)]\tDiscriminator Loss: 0.532845\tGenerator Loss: 1.347501\n",
      "Train Epoch: 35 [20480/60000 (34%)]\tDiscriminator Loss: 0.529608\tGenerator Loss: 1.192721\n",
      "Train Epoch: 35 [21760/60000 (36%)]\tDiscriminator Loss: 0.529159\tGenerator Loss: 1.118762\n",
      "Train Epoch: 35 [23040/60000 (38%)]\tDiscriminator Loss: 0.560738\tGenerator Loss: 0.936861\n",
      "Train Epoch: 35 [24320/60000 (41%)]\tDiscriminator Loss: 0.493999\tGenerator Loss: 1.400874\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tDiscriminator Loss: 0.454795\tGenerator Loss: 1.322387\n",
      "Train Epoch: 35 [26880/60000 (45%)]\tDiscriminator Loss: 0.690465\tGenerator Loss: 0.577024\n",
      "Train Epoch: 35 [28160/60000 (47%)]\tDiscriminator Loss: 0.531021\tGenerator Loss: 1.052751\n",
      "Train Epoch: 35 [29440/60000 (49%)]\tDiscriminator Loss: 0.608540\tGenerator Loss: 1.155804\n",
      "Train Epoch: 35 [30720/60000 (51%)]\tDiscriminator Loss: 0.519696\tGenerator Loss: 0.959463\n",
      "Train Epoch: 35 [32000/60000 (53%)]\tDiscriminator Loss: 0.549162\tGenerator Loss: 1.234019\n",
      "Train Epoch: 35 [33280/60000 (55%)]\tDiscriminator Loss: 0.546774\tGenerator Loss: 1.265854\n",
      "Train Epoch: 35 [34560/60000 (58%)]\tDiscriminator Loss: 0.505245\tGenerator Loss: 1.419429\n",
      "Train Epoch: 35 [35840/60000 (60%)]\tDiscriminator Loss: 0.539294\tGenerator Loss: 1.473421\n",
      "Train Epoch: 35 [37120/60000 (62%)]\tDiscriminator Loss: 0.625274\tGenerator Loss: 1.053668\n",
      "Train Epoch: 35 [38400/60000 (64%)]\tDiscriminator Loss: 0.543242\tGenerator Loss: 1.193042\n",
      "Train Epoch: 35 [39680/60000 (66%)]\tDiscriminator Loss: 0.520363\tGenerator Loss: 1.346665\n",
      "Train Epoch: 35 [40960/60000 (68%)]\tDiscriminator Loss: 0.540329\tGenerator Loss: 1.405672\n",
      "Train Epoch: 35 [42240/60000 (70%)]\tDiscriminator Loss: 0.554422\tGenerator Loss: 1.250768\n",
      "Train Epoch: 35 [43520/60000 (72%)]\tDiscriminator Loss: 0.498231\tGenerator Loss: 1.292795\n",
      "Train Epoch: 35 [44800/60000 (75%)]\tDiscriminator Loss: 0.524262\tGenerator Loss: 1.017259\n",
      "Train Epoch: 35 [46080/60000 (77%)]\tDiscriminator Loss: 0.528418\tGenerator Loss: 1.226250\n",
      "Train Epoch: 35 [47360/60000 (79%)]\tDiscriminator Loss: 0.481708\tGenerator Loss: 1.277044\n",
      "Train Epoch: 35 [48640/60000 (81%)]\tDiscriminator Loss: 0.563728\tGenerator Loss: 0.918940\n",
      "Train Epoch: 35 [49920/60000 (83%)]\tDiscriminator Loss: 0.580932\tGenerator Loss: 1.384953\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tDiscriminator Loss: 0.509439\tGenerator Loss: 1.581995\n",
      "Train Epoch: 35 [52480/60000 (87%)]\tDiscriminator Loss: 0.494687\tGenerator Loss: 1.279347\n",
      "Train Epoch: 35 [53760/60000 (90%)]\tDiscriminator Loss: 0.585074\tGenerator Loss: 1.468582\n",
      "Train Epoch: 35 [55040/60000 (92%)]\tDiscriminator Loss: 0.505015\tGenerator Loss: 1.122624\n",
      "Train Epoch: 35 [56320/60000 (94%)]\tDiscriminator Loss: 0.491166\tGenerator Loss: 1.252552\n",
      "Train Epoch: 35 [57600/60000 (96%)]\tDiscriminator Loss: 0.517220\tGenerator Loss: 1.370435\n",
      "Train Epoch: 35 [58880/60000 (98%)]\tDiscriminator Loss: 0.606203\tGenerator Loss: 0.691729\n",
      "Train Epoch: 36 [0/60000 (0%)]\tDiscriminator Loss: 0.500451\tGenerator Loss: 1.037113\n",
      "Train Epoch: 36 [1280/60000 (2%)]\tDiscriminator Loss: 0.492115\tGenerator Loss: 0.958157\n",
      "Train Epoch: 36 [2560/60000 (4%)]\tDiscriminator Loss: 0.558055\tGenerator Loss: 0.905834\n",
      "Train Epoch: 36 [3840/60000 (6%)]\tDiscriminator Loss: 0.535570\tGenerator Loss: 1.437966\n",
      "Train Epoch: 36 [5120/60000 (9%)]\tDiscriminator Loss: 0.533383\tGenerator Loss: 1.358627\n",
      "Train Epoch: 36 [6400/60000 (11%)]\tDiscriminator Loss: 0.506462\tGenerator Loss: 0.991514\n",
      "Train Epoch: 36 [7680/60000 (13%)]\tDiscriminator Loss: 0.619452\tGenerator Loss: 1.095127\n",
      "Train Epoch: 36 [8960/60000 (15%)]\tDiscriminator Loss: 0.505354\tGenerator Loss: 1.184429\n",
      "Train Epoch: 36 [10240/60000 (17%)]\tDiscriminator Loss: 0.520302\tGenerator Loss: 1.219972\n",
      "Train Epoch: 36 [11520/60000 (19%)]\tDiscriminator Loss: 0.609806\tGenerator Loss: 1.468080\n",
      "Train Epoch: 36 [12800/60000 (21%)]\tDiscriminator Loss: 0.546720\tGenerator Loss: 0.805520\n",
      "Train Epoch: 36 [14080/60000 (23%)]\tDiscriminator Loss: 0.515283\tGenerator Loss: 1.191672\n",
      "Train Epoch: 36 [15360/60000 (26%)]\tDiscriminator Loss: 0.515033\tGenerator Loss: 1.241160\n",
      "Train Epoch: 36 [16640/60000 (28%)]\tDiscriminator Loss: 0.523444\tGenerator Loss: 0.962116\n",
      "Train Epoch: 36 [17920/60000 (30%)]\tDiscriminator Loss: 0.516393\tGenerator Loss: 1.081222\n",
      "Train Epoch: 36 [19200/60000 (32%)]\tDiscriminator Loss: 0.527982\tGenerator Loss: 1.461147\n",
      "Train Epoch: 36 [20480/60000 (34%)]\tDiscriminator Loss: 0.531022\tGenerator Loss: 1.617524\n",
      "Train Epoch: 36 [21760/60000 (36%)]\tDiscriminator Loss: 0.515679\tGenerator Loss: 1.337976\n",
      "Train Epoch: 36 [23040/60000 (38%)]\tDiscriminator Loss: 0.546249\tGenerator Loss: 1.216384\n",
      "Train Epoch: 36 [24320/60000 (41%)]\tDiscriminator Loss: 0.561540\tGenerator Loss: 0.951396\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tDiscriminator Loss: 0.538437\tGenerator Loss: 1.521322\n",
      "Train Epoch: 36 [26880/60000 (45%)]\tDiscriminator Loss: 0.496060\tGenerator Loss: 1.341891\n",
      "Train Epoch: 36 [28160/60000 (47%)]\tDiscriminator Loss: 0.542365\tGenerator Loss: 1.307043\n",
      "Train Epoch: 36 [29440/60000 (49%)]\tDiscriminator Loss: 0.571942\tGenerator Loss: 1.049807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 36 [30720/60000 (51%)]\tDiscriminator Loss: 0.517514\tGenerator Loss: 1.364705\n",
      "Train Epoch: 36 [32000/60000 (53%)]\tDiscriminator Loss: 0.545301\tGenerator Loss: 1.027765\n",
      "Train Epoch: 36 [33280/60000 (55%)]\tDiscriminator Loss: 0.656533\tGenerator Loss: 1.062939\n",
      "Train Epoch: 36 [34560/60000 (58%)]\tDiscriminator Loss: 0.539354\tGenerator Loss: 1.319090\n",
      "Train Epoch: 36 [35840/60000 (60%)]\tDiscriminator Loss: 0.511840\tGenerator Loss: 1.148815\n",
      "Train Epoch: 36 [37120/60000 (62%)]\tDiscriminator Loss: 0.548794\tGenerator Loss: 0.913056\n",
      "Train Epoch: 36 [38400/60000 (64%)]\tDiscriminator Loss: 0.555973\tGenerator Loss: 0.935761\n",
      "Train Epoch: 36 [39680/60000 (66%)]\tDiscriminator Loss: 0.533972\tGenerator Loss: 1.147722\n",
      "Train Epoch: 36 [40960/60000 (68%)]\tDiscriminator Loss: 0.511369\tGenerator Loss: 1.075286\n",
      "Train Epoch: 36 [42240/60000 (70%)]\tDiscriminator Loss: 0.533587\tGenerator Loss: 1.656163\n",
      "Train Epoch: 36 [43520/60000 (72%)]\tDiscriminator Loss: 0.515151\tGenerator Loss: 1.324753\n",
      "Train Epoch: 36 [44800/60000 (75%)]\tDiscriminator Loss: 0.497665\tGenerator Loss: 1.332368\n",
      "Train Epoch: 36 [46080/60000 (77%)]\tDiscriminator Loss: 0.577582\tGenerator Loss: 1.566855\n",
      "Train Epoch: 36 [47360/60000 (79%)]\tDiscriminator Loss: 0.501634\tGenerator Loss: 1.054616\n",
      "Train Epoch: 36 [48640/60000 (81%)]\tDiscriminator Loss: 0.557235\tGenerator Loss: 1.571185\n",
      "Train Epoch: 36 [49920/60000 (83%)]\tDiscriminator Loss: 0.571565\tGenerator Loss: 1.314641\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tDiscriminator Loss: 0.504250\tGenerator Loss: 1.182902\n",
      "Train Epoch: 36 [52480/60000 (87%)]\tDiscriminator Loss: 0.521444\tGenerator Loss: 1.396651\n",
      "Train Epoch: 36 [53760/60000 (90%)]\tDiscriminator Loss: 0.588960\tGenerator Loss: 2.140176\n",
      "Train Epoch: 36 [55040/60000 (92%)]\tDiscriminator Loss: 0.577775\tGenerator Loss: 1.129878\n",
      "Train Epoch: 36 [56320/60000 (94%)]\tDiscriminator Loss: 0.621964\tGenerator Loss: 0.877517\n",
      "Train Epoch: 36 [57600/60000 (96%)]\tDiscriminator Loss: 0.498817\tGenerator Loss: 1.347810\n",
      "Train Epoch: 36 [58880/60000 (98%)]\tDiscriminator Loss: 0.458884\tGenerator Loss: 1.435210\n",
      "Train Epoch: 37 [0/60000 (0%)]\tDiscriminator Loss: 0.566357\tGenerator Loss: 0.797422\n",
      "Train Epoch: 37 [1280/60000 (2%)]\tDiscriminator Loss: 0.486290\tGenerator Loss: 1.436725\n",
      "Train Epoch: 37 [2560/60000 (4%)]\tDiscriminator Loss: 0.487204\tGenerator Loss: 1.140133\n",
      "Train Epoch: 37 [3840/60000 (6%)]\tDiscriminator Loss: 0.516796\tGenerator Loss: 1.264049\n",
      "Train Epoch: 37 [5120/60000 (9%)]\tDiscriminator Loss: 0.622701\tGenerator Loss: 0.929181\n",
      "Train Epoch: 37 [6400/60000 (11%)]\tDiscriminator Loss: 0.521035\tGenerator Loss: 1.343463\n",
      "Train Epoch: 37 [7680/60000 (13%)]\tDiscriminator Loss: 0.482588\tGenerator Loss: 1.876107\n",
      "Train Epoch: 37 [8960/60000 (15%)]\tDiscriminator Loss: 0.613474\tGenerator Loss: 0.976793\n",
      "Train Epoch: 37 [10240/60000 (17%)]\tDiscriminator Loss: 0.535839\tGenerator Loss: 1.028313\n",
      "Train Epoch: 37 [11520/60000 (19%)]\tDiscriminator Loss: 0.526364\tGenerator Loss: 1.189032\n",
      "Train Epoch: 37 [12800/60000 (21%)]\tDiscriminator Loss: 0.594845\tGenerator Loss: 1.553679\n",
      "Train Epoch: 37 [14080/60000 (23%)]\tDiscriminator Loss: 0.569423\tGenerator Loss: 0.870265\n",
      "Train Epoch: 37 [15360/60000 (26%)]\tDiscriminator Loss: 0.522252\tGenerator Loss: 1.274623\n",
      "Train Epoch: 37 [16640/60000 (28%)]\tDiscriminator Loss: 0.535663\tGenerator Loss: 1.612416\n",
      "Train Epoch: 37 [17920/60000 (30%)]\tDiscriminator Loss: 0.531654\tGenerator Loss: 1.065136\n",
      "Train Epoch: 37 [19200/60000 (32%)]\tDiscriminator Loss: 0.529684\tGenerator Loss: 1.610184\n",
      "Train Epoch: 37 [20480/60000 (34%)]\tDiscriminator Loss: 0.608065\tGenerator Loss: 0.748973\n",
      "Train Epoch: 37 [21760/60000 (36%)]\tDiscriminator Loss: 0.540623\tGenerator Loss: 1.594180\n",
      "Train Epoch: 37 [23040/60000 (38%)]\tDiscriminator Loss: 0.597035\tGenerator Loss: 1.240541\n",
      "Train Epoch: 37 [24320/60000 (41%)]\tDiscriminator Loss: 0.520065\tGenerator Loss: 1.519530\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tDiscriminator Loss: 0.508013\tGenerator Loss: 1.488209\n",
      "Train Epoch: 37 [26880/60000 (45%)]\tDiscriminator Loss: 0.503809\tGenerator Loss: 1.277911\n",
      "Train Epoch: 37 [28160/60000 (47%)]\tDiscriminator Loss: 0.541053\tGenerator Loss: 1.561635\n",
      "Train Epoch: 37 [29440/60000 (49%)]\tDiscriminator Loss: 0.515457\tGenerator Loss: 1.536215\n",
      "Train Epoch: 37 [30720/60000 (51%)]\tDiscriminator Loss: 0.532067\tGenerator Loss: 1.239431\n",
      "Train Epoch: 37 [32000/60000 (53%)]\tDiscriminator Loss: 0.520924\tGenerator Loss: 0.970991\n",
      "Train Epoch: 37 [33280/60000 (55%)]\tDiscriminator Loss: 0.528559\tGenerator Loss: 1.325218\n",
      "Train Epoch: 37 [34560/60000 (58%)]\tDiscriminator Loss: 0.539656\tGenerator Loss: 1.504602\n",
      "Train Epoch: 37 [35840/60000 (60%)]\tDiscriminator Loss: 0.535582\tGenerator Loss: 0.956421\n",
      "Train Epoch: 37 [37120/60000 (62%)]\tDiscriminator Loss: 0.541245\tGenerator Loss: 0.889142\n",
      "Train Epoch: 37 [38400/60000 (64%)]\tDiscriminator Loss: 0.587675\tGenerator Loss: 1.150043\n",
      "Train Epoch: 37 [39680/60000 (66%)]\tDiscriminator Loss: 0.534488\tGenerator Loss: 0.934723\n",
      "Train Epoch: 37 [40960/60000 (68%)]\tDiscriminator Loss: 0.554306\tGenerator Loss: 1.147316\n",
      "Train Epoch: 37 [42240/60000 (70%)]\tDiscriminator Loss: 0.507668\tGenerator Loss: 1.354465\n",
      "Train Epoch: 37 [43520/60000 (72%)]\tDiscriminator Loss: 0.483208\tGenerator Loss: 1.263332\n",
      "Train Epoch: 37 [44800/60000 (75%)]\tDiscriminator Loss: 0.566584\tGenerator Loss: 1.090195\n",
      "Train Epoch: 37 [46080/60000 (77%)]\tDiscriminator Loss: 0.534714\tGenerator Loss: 1.352099\n",
      "Train Epoch: 37 [47360/60000 (79%)]\tDiscriminator Loss: 0.536598\tGenerator Loss: 1.432350\n",
      "Train Epoch: 37 [48640/60000 (81%)]\tDiscriminator Loss: 0.535535\tGenerator Loss: 1.691053\n",
      "Train Epoch: 37 [49920/60000 (83%)]\tDiscriminator Loss: 0.537300\tGenerator Loss: 1.444916\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tDiscriminator Loss: 0.554865\tGenerator Loss: 1.081922\n",
      "Train Epoch: 37 [52480/60000 (87%)]\tDiscriminator Loss: 0.529388\tGenerator Loss: 1.320248\n",
      "Train Epoch: 37 [53760/60000 (90%)]\tDiscriminator Loss: 0.504905\tGenerator Loss: 1.156213\n",
      "Train Epoch: 37 [55040/60000 (92%)]\tDiscriminator Loss: 0.565337\tGenerator Loss: 1.061942\n",
      "Train Epoch: 37 [56320/60000 (94%)]\tDiscriminator Loss: 0.529567\tGenerator Loss: 1.218846\n",
      "Train Epoch: 37 [57600/60000 (96%)]\tDiscriminator Loss: 0.533495\tGenerator Loss: 1.009337\n",
      "Train Epoch: 37 [58880/60000 (98%)]\tDiscriminator Loss: 0.493970\tGenerator Loss: 1.122295\n",
      "Train Epoch: 38 [0/60000 (0%)]\tDiscriminator Loss: 0.521274\tGenerator Loss: 1.419401\n",
      "Train Epoch: 38 [1280/60000 (2%)]\tDiscriminator Loss: 0.502619\tGenerator Loss: 1.318403\n",
      "Train Epoch: 38 [2560/60000 (4%)]\tDiscriminator Loss: 0.557827\tGenerator Loss: 1.080540\n",
      "Train Epoch: 38 [3840/60000 (6%)]\tDiscriminator Loss: 0.534917\tGenerator Loss: 1.012811\n",
      "Train Epoch: 38 [5120/60000 (9%)]\tDiscriminator Loss: 0.593507\tGenerator Loss: 0.989393\n",
      "Train Epoch: 38 [6400/60000 (11%)]\tDiscriminator Loss: 0.481971\tGenerator Loss: 1.156600\n",
      "Train Epoch: 38 [7680/60000 (13%)]\tDiscriminator Loss: 0.547886\tGenerator Loss: 1.481032\n",
      "Train Epoch: 38 [8960/60000 (15%)]\tDiscriminator Loss: 0.574441\tGenerator Loss: 1.328370\n",
      "Train Epoch: 38 [10240/60000 (17%)]\tDiscriminator Loss: 0.525537\tGenerator Loss: 1.334185\n",
      "Train Epoch: 38 [11520/60000 (19%)]\tDiscriminator Loss: 0.538863\tGenerator Loss: 1.044597\n",
      "Train Epoch: 38 [12800/60000 (21%)]\tDiscriminator Loss: 0.521919\tGenerator Loss: 0.979059\n",
      "Train Epoch: 38 [14080/60000 (23%)]\tDiscriminator Loss: 0.565524\tGenerator Loss: 1.014961\n",
      "Train Epoch: 38 [15360/60000 (26%)]\tDiscriminator Loss: 0.508772\tGenerator Loss: 1.144650\n",
      "Train Epoch: 38 [16640/60000 (28%)]\tDiscriminator Loss: 0.531590\tGenerator Loss: 1.089179\n",
      "Train Epoch: 38 [17920/60000 (30%)]\tDiscriminator Loss: 0.577236\tGenerator Loss: 1.301230\n",
      "Train Epoch: 38 [19200/60000 (32%)]\tDiscriminator Loss: 0.493494\tGenerator Loss: 1.332168\n",
      "Train Epoch: 38 [20480/60000 (34%)]\tDiscriminator Loss: 0.808911\tGenerator Loss: 0.450034\n",
      "Train Epoch: 38 [21760/60000 (36%)]\tDiscriminator Loss: 0.503480\tGenerator Loss: 1.036420\n",
      "Train Epoch: 38 [23040/60000 (38%)]\tDiscriminator Loss: 0.594155\tGenerator Loss: 1.356752\n",
      "Train Epoch: 38 [24320/60000 (41%)]\tDiscriminator Loss: 0.512405\tGenerator Loss: 1.380822\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tDiscriminator Loss: 0.498895\tGenerator Loss: 1.257593\n",
      "Train Epoch: 38 [26880/60000 (45%)]\tDiscriminator Loss: 0.529483\tGenerator Loss: 1.242775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 38 [28160/60000 (47%)]\tDiscriminator Loss: 0.607594\tGenerator Loss: 0.775515\n",
      "Train Epoch: 38 [29440/60000 (49%)]\tDiscriminator Loss: 0.591798\tGenerator Loss: 1.387769\n",
      "Train Epoch: 38 [30720/60000 (51%)]\tDiscriminator Loss: 0.515117\tGenerator Loss: 1.264497\n",
      "Train Epoch: 38 [32000/60000 (53%)]\tDiscriminator Loss: 0.566127\tGenerator Loss: 1.214988\n",
      "Train Epoch: 38 [33280/60000 (55%)]\tDiscriminator Loss: 0.605096\tGenerator Loss: 0.838362\n",
      "Train Epoch: 38 [34560/60000 (58%)]\tDiscriminator Loss: 0.489410\tGenerator Loss: 1.152408\n",
      "Train Epoch: 38 [35840/60000 (60%)]\tDiscriminator Loss: 0.461387\tGenerator Loss: 1.239165\n",
      "Train Epoch: 38 [37120/60000 (62%)]\tDiscriminator Loss: 0.559203\tGenerator Loss: 0.986157\n",
      "Train Epoch: 38 [38400/60000 (64%)]\tDiscriminator Loss: 0.537003\tGenerator Loss: 1.041543\n",
      "Train Epoch: 38 [39680/60000 (66%)]\tDiscriminator Loss: 0.531193\tGenerator Loss: 1.109638\n",
      "Train Epoch: 38 [40960/60000 (68%)]\tDiscriminator Loss: 0.566514\tGenerator Loss: 1.487455\n",
      "Train Epoch: 38 [42240/60000 (70%)]\tDiscriminator Loss: 0.502811\tGenerator Loss: 1.485683\n",
      "Train Epoch: 38 [43520/60000 (72%)]\tDiscriminator Loss: 0.578675\tGenerator Loss: 1.159697\n",
      "Train Epoch: 38 [44800/60000 (75%)]\tDiscriminator Loss: 0.521620\tGenerator Loss: 1.215011\n",
      "Train Epoch: 38 [46080/60000 (77%)]\tDiscriminator Loss: 0.613920\tGenerator Loss: 1.027023\n",
      "Train Epoch: 38 [47360/60000 (79%)]\tDiscriminator Loss: 0.496916\tGenerator Loss: 1.002357\n",
      "Train Epoch: 38 [48640/60000 (81%)]\tDiscriminator Loss: 0.488998\tGenerator Loss: 1.316621\n",
      "Train Epoch: 38 [49920/60000 (83%)]\tDiscriminator Loss: 0.585714\tGenerator Loss: 0.856435\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tDiscriminator Loss: 0.549396\tGenerator Loss: 1.112645\n",
      "Train Epoch: 38 [52480/60000 (87%)]\tDiscriminator Loss: 0.507725\tGenerator Loss: 1.158859\n",
      "Train Epoch: 38 [53760/60000 (90%)]\tDiscriminator Loss: 0.538227\tGenerator Loss: 1.093335\n",
      "Train Epoch: 38 [55040/60000 (92%)]\tDiscriminator Loss: 0.492056\tGenerator Loss: 1.258257\n",
      "Train Epoch: 38 [56320/60000 (94%)]\tDiscriminator Loss: 0.478304\tGenerator Loss: 1.520039\n",
      "Train Epoch: 38 [57600/60000 (96%)]\tDiscriminator Loss: 0.525452\tGenerator Loss: 0.956498\n",
      "Train Epoch: 38 [58880/60000 (98%)]\tDiscriminator Loss: 0.539848\tGenerator Loss: 1.179904\n",
      "Train Epoch: 39 [0/60000 (0%)]\tDiscriminator Loss: 0.520713\tGenerator Loss: 1.229770\n",
      "Train Epoch: 39 [1280/60000 (2%)]\tDiscriminator Loss: 0.520020\tGenerator Loss: 1.087247\n",
      "Train Epoch: 39 [2560/60000 (4%)]\tDiscriminator Loss: 0.608518\tGenerator Loss: 0.752217\n",
      "Train Epoch: 39 [3840/60000 (6%)]\tDiscriminator Loss: 0.517899\tGenerator Loss: 1.158836\n",
      "Train Epoch: 39 [5120/60000 (9%)]\tDiscriminator Loss: 0.468316\tGenerator Loss: 1.395777\n",
      "Train Epoch: 39 [6400/60000 (11%)]\tDiscriminator Loss: 0.472208\tGenerator Loss: 1.485990\n",
      "Train Epoch: 39 [7680/60000 (13%)]\tDiscriminator Loss: 0.522912\tGenerator Loss: 1.064794\n",
      "Train Epoch: 39 [8960/60000 (15%)]\tDiscriminator Loss: 0.500952\tGenerator Loss: 1.332587\n",
      "Train Epoch: 39 [10240/60000 (17%)]\tDiscriminator Loss: 0.513696\tGenerator Loss: 1.089688\n",
      "Train Epoch: 39 [11520/60000 (19%)]\tDiscriminator Loss: 0.478057\tGenerator Loss: 1.405205\n",
      "Train Epoch: 39 [12800/60000 (21%)]\tDiscriminator Loss: 0.515977\tGenerator Loss: 1.077573\n",
      "Train Epoch: 39 [14080/60000 (23%)]\tDiscriminator Loss: 0.548714\tGenerator Loss: 1.146566\n",
      "Train Epoch: 39 [15360/60000 (26%)]\tDiscriminator Loss: 0.473861\tGenerator Loss: 1.431769\n",
      "Train Epoch: 39 [16640/60000 (28%)]\tDiscriminator Loss: 0.525127\tGenerator Loss: 1.060366\n",
      "Train Epoch: 39 [17920/60000 (30%)]\tDiscriminator Loss: 0.525016\tGenerator Loss: 1.278098\n",
      "Train Epoch: 39 [19200/60000 (32%)]\tDiscriminator Loss: 0.529864\tGenerator Loss: 1.476491\n",
      "Train Epoch: 39 [20480/60000 (34%)]\tDiscriminator Loss: 0.512054\tGenerator Loss: 1.361369\n",
      "Train Epoch: 39 [21760/60000 (36%)]\tDiscriminator Loss: 0.576111\tGenerator Loss: 1.081491\n",
      "Train Epoch: 39 [23040/60000 (38%)]\tDiscriminator Loss: 0.499585\tGenerator Loss: 1.177692\n",
      "Train Epoch: 39 [24320/60000 (41%)]\tDiscriminator Loss: 0.531812\tGenerator Loss: 1.253740\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tDiscriminator Loss: 0.482127\tGenerator Loss: 1.193527\n",
      "Train Epoch: 39 [26880/60000 (45%)]\tDiscriminator Loss: 0.524962\tGenerator Loss: 1.058608\n",
      "Train Epoch: 39 [28160/60000 (47%)]\tDiscriminator Loss: 0.565773\tGenerator Loss: 0.973691\n",
      "Train Epoch: 39 [29440/60000 (49%)]\tDiscriminator Loss: 0.510576\tGenerator Loss: 1.335065\n",
      "Train Epoch: 39 [30720/60000 (51%)]\tDiscriminator Loss: 0.554149\tGenerator Loss: 1.153438\n",
      "Train Epoch: 39 [32000/60000 (53%)]\tDiscriminator Loss: 0.588696\tGenerator Loss: 1.374980\n",
      "Train Epoch: 39 [33280/60000 (55%)]\tDiscriminator Loss: 0.480379\tGenerator Loss: 1.307233\n",
      "Train Epoch: 39 [34560/60000 (58%)]\tDiscriminator Loss: 0.538413\tGenerator Loss: 1.275934\n",
      "Train Epoch: 39 [35840/60000 (60%)]\tDiscriminator Loss: 0.549447\tGenerator Loss: 1.313148\n",
      "Train Epoch: 39 [37120/60000 (62%)]\tDiscriminator Loss: 0.530474\tGenerator Loss: 1.111078\n",
      "Train Epoch: 39 [38400/60000 (64%)]\tDiscriminator Loss: 0.545108\tGenerator Loss: 1.600871\n",
      "Train Epoch: 39 [39680/60000 (66%)]\tDiscriminator Loss: 0.632375\tGenerator Loss: 0.752805\n",
      "Train Epoch: 39 [40960/60000 (68%)]\tDiscriminator Loss: 0.462731\tGenerator Loss: 1.367083\n",
      "Train Epoch: 39 [42240/60000 (70%)]\tDiscriminator Loss: 0.454374\tGenerator Loss: 1.326078\n",
      "Train Epoch: 39 [43520/60000 (72%)]\tDiscriminator Loss: 0.489677\tGenerator Loss: 1.391799\n",
      "Train Epoch: 39 [44800/60000 (75%)]\tDiscriminator Loss: 0.561513\tGenerator Loss: 0.918063\n",
      "Train Epoch: 39 [46080/60000 (77%)]\tDiscriminator Loss: 0.522387\tGenerator Loss: 1.235952\n",
      "Train Epoch: 39 [47360/60000 (79%)]\tDiscriminator Loss: 0.485598\tGenerator Loss: 1.156654\n",
      "Train Epoch: 39 [48640/60000 (81%)]\tDiscriminator Loss: 0.535596\tGenerator Loss: 1.266514\n",
      "Train Epoch: 39 [49920/60000 (83%)]\tDiscriminator Loss: 0.505675\tGenerator Loss: 0.986911\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tDiscriminator Loss: 0.580278\tGenerator Loss: 0.896491\n",
      "Train Epoch: 39 [52480/60000 (87%)]\tDiscriminator Loss: 0.555272\tGenerator Loss: 1.326959\n",
      "Train Epoch: 39 [53760/60000 (90%)]\tDiscriminator Loss: 0.506802\tGenerator Loss: 1.127096\n",
      "Train Epoch: 39 [55040/60000 (92%)]\tDiscriminator Loss: 0.554345\tGenerator Loss: 1.273777\n",
      "Train Epoch: 39 [56320/60000 (94%)]\tDiscriminator Loss: 0.501065\tGenerator Loss: 1.199047\n",
      "Train Epoch: 39 [57600/60000 (96%)]\tDiscriminator Loss: 0.509430\tGenerator Loss: 1.142749\n",
      "Train Epoch: 39 [58880/60000 (98%)]\tDiscriminator Loss: 0.522638\tGenerator Loss: 1.060745\n",
      "Train Epoch: 40 [0/60000 (0%)]\tDiscriminator Loss: 0.501210\tGenerator Loss: 1.544738\n",
      "Train Epoch: 40 [1280/60000 (2%)]\tDiscriminator Loss: 0.585502\tGenerator Loss: 1.397700\n",
      "Train Epoch: 40 [2560/60000 (4%)]\tDiscriminator Loss: 0.475518\tGenerator Loss: 1.111875\n",
      "Train Epoch: 40 [3840/60000 (6%)]\tDiscriminator Loss: 0.521457\tGenerator Loss: 1.263557\n",
      "Train Epoch: 40 [5120/60000 (9%)]\tDiscriminator Loss: 0.548216\tGenerator Loss: 1.063700\n",
      "Train Epoch: 40 [6400/60000 (11%)]\tDiscriminator Loss: 0.521018\tGenerator Loss: 1.313025\n",
      "Train Epoch: 40 [7680/60000 (13%)]\tDiscriminator Loss: 0.503814\tGenerator Loss: 1.116773\n",
      "Train Epoch: 40 [8960/60000 (15%)]\tDiscriminator Loss: 0.541094\tGenerator Loss: 1.292243\n",
      "Train Epoch: 40 [10240/60000 (17%)]\tDiscriminator Loss: 0.516462\tGenerator Loss: 1.146233\n",
      "Train Epoch: 40 [11520/60000 (19%)]\tDiscriminator Loss: 0.526879\tGenerator Loss: 1.148840\n",
      "Train Epoch: 40 [12800/60000 (21%)]\tDiscriminator Loss: 0.522539\tGenerator Loss: 1.225471\n",
      "Train Epoch: 40 [14080/60000 (23%)]\tDiscriminator Loss: 0.469025\tGenerator Loss: 1.561749\n",
      "Train Epoch: 40 [15360/60000 (26%)]\tDiscriminator Loss: 0.562511\tGenerator Loss: 1.414528\n",
      "Train Epoch: 40 [16640/60000 (28%)]\tDiscriminator Loss: 0.518206\tGenerator Loss: 1.280188\n",
      "Train Epoch: 40 [17920/60000 (30%)]\tDiscriminator Loss: 0.464617\tGenerator Loss: 1.369652\n",
      "Train Epoch: 40 [19200/60000 (32%)]\tDiscriminator Loss: 0.507551\tGenerator Loss: 1.403497\n",
      "Train Epoch: 40 [20480/60000 (34%)]\tDiscriminator Loss: 0.556344\tGenerator Loss: 0.873384\n",
      "Train Epoch: 40 [21760/60000 (36%)]\tDiscriminator Loss: 0.547178\tGenerator Loss: 1.291119\n",
      "Train Epoch: 40 [23040/60000 (38%)]\tDiscriminator Loss: 0.502714\tGenerator Loss: 1.256733\n",
      "Train Epoch: 40 [24320/60000 (41%)]\tDiscriminator Loss: 0.544654\tGenerator Loss: 1.287879\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tDiscriminator Loss: 0.536183\tGenerator Loss: 0.935092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 40 [26880/60000 (45%)]\tDiscriminator Loss: 0.549076\tGenerator Loss: 1.267510\n",
      "Train Epoch: 40 [28160/60000 (47%)]\tDiscriminator Loss: 0.508258\tGenerator Loss: 1.348583\n",
      "Train Epoch: 40 [29440/60000 (49%)]\tDiscriminator Loss: 0.485743\tGenerator Loss: 1.284869\n",
      "Train Epoch: 40 [30720/60000 (51%)]\tDiscriminator Loss: 0.500755\tGenerator Loss: 1.146862\n",
      "Train Epoch: 40 [32000/60000 (53%)]\tDiscriminator Loss: 0.529499\tGenerator Loss: 0.899431\n",
      "Train Epoch: 40 [33280/60000 (55%)]\tDiscriminator Loss: 0.554776\tGenerator Loss: 1.309846\n",
      "Train Epoch: 40 [34560/60000 (58%)]\tDiscriminator Loss: 0.476457\tGenerator Loss: 1.411573\n",
      "Train Epoch: 40 [35840/60000 (60%)]\tDiscriminator Loss: 0.538596\tGenerator Loss: 1.252512\n",
      "Train Epoch: 40 [37120/60000 (62%)]\tDiscriminator Loss: 0.512936\tGenerator Loss: 1.170350\n",
      "Train Epoch: 40 [38400/60000 (64%)]\tDiscriminator Loss: 0.544896\tGenerator Loss: 1.421143\n",
      "Train Epoch: 40 [39680/60000 (66%)]\tDiscriminator Loss: 0.578796\tGenerator Loss: 1.215419\n",
      "Train Epoch: 40 [40960/60000 (68%)]\tDiscriminator Loss: 0.446434\tGenerator Loss: 1.439194\n",
      "Train Epoch: 40 [42240/60000 (70%)]\tDiscriminator Loss: 0.494411\tGenerator Loss: 1.238818\n",
      "Train Epoch: 40 [43520/60000 (72%)]\tDiscriminator Loss: 0.517229\tGenerator Loss: 1.304449\n",
      "Train Epoch: 40 [44800/60000 (75%)]\tDiscriminator Loss: 0.492062\tGenerator Loss: 1.128045\n",
      "Train Epoch: 40 [46080/60000 (77%)]\tDiscriminator Loss: 0.570025\tGenerator Loss: 0.942967\n",
      "Train Epoch: 40 [47360/60000 (79%)]\tDiscriminator Loss: 0.614451\tGenerator Loss: 1.325923\n",
      "Train Epoch: 40 [48640/60000 (81%)]\tDiscriminator Loss: 0.544158\tGenerator Loss: 0.981951\n",
      "Train Epoch: 40 [49920/60000 (83%)]\tDiscriminator Loss: 0.497322\tGenerator Loss: 1.157342\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tDiscriminator Loss: 0.560764\tGenerator Loss: 1.024010\n",
      "Train Epoch: 40 [52480/60000 (87%)]\tDiscriminator Loss: 0.587172\tGenerator Loss: 1.196210\n",
      "Train Epoch: 40 [53760/60000 (90%)]\tDiscriminator Loss: 0.431923\tGenerator Loss: 1.423136\n",
      "Train Epoch: 40 [55040/60000 (92%)]\tDiscriminator Loss: 0.506542\tGenerator Loss: 1.050100\n",
      "Train Epoch: 40 [56320/60000 (94%)]\tDiscriminator Loss: 0.546875\tGenerator Loss: 1.306897\n",
      "Train Epoch: 40 [57600/60000 (96%)]\tDiscriminator Loss: 0.516389\tGenerator Loss: 1.140869\n",
      "Train Epoch: 40 [58880/60000 (98%)]\tDiscriminator Loss: 0.566506\tGenerator Loss: 1.461379\n",
      "Train Epoch: 41 [0/60000 (0%)]\tDiscriminator Loss: 0.487515\tGenerator Loss: 1.398486\n",
      "Train Epoch: 41 [1280/60000 (2%)]\tDiscriminator Loss: 0.501722\tGenerator Loss: 1.537633\n",
      "Train Epoch: 41 [2560/60000 (4%)]\tDiscriminator Loss: 0.565549\tGenerator Loss: 1.018719\n",
      "Train Epoch: 41 [3840/60000 (6%)]\tDiscriminator Loss: 0.512224\tGenerator Loss: 1.018898\n",
      "Train Epoch: 41 [5120/60000 (9%)]\tDiscriminator Loss: 0.537172\tGenerator Loss: 1.287321\n",
      "Train Epoch: 41 [6400/60000 (11%)]\tDiscriminator Loss: 0.518280\tGenerator Loss: 1.223744\n",
      "Train Epoch: 41 [7680/60000 (13%)]\tDiscriminator Loss: 0.563494\tGenerator Loss: 1.185766\n",
      "Train Epoch: 41 [8960/60000 (15%)]\tDiscriminator Loss: 0.489227\tGenerator Loss: 1.221609\n",
      "Train Epoch: 41 [10240/60000 (17%)]\tDiscriminator Loss: 0.479992\tGenerator Loss: 1.456568\n",
      "Train Epoch: 41 [11520/60000 (19%)]\tDiscriminator Loss: 0.514088\tGenerator Loss: 1.598901\n",
      "Train Epoch: 41 [12800/60000 (21%)]\tDiscriminator Loss: 0.507145\tGenerator Loss: 1.312072\n",
      "Train Epoch: 41 [14080/60000 (23%)]\tDiscriminator Loss: 0.476027\tGenerator Loss: 1.461222\n",
      "Train Epoch: 41 [15360/60000 (26%)]\tDiscriminator Loss: 0.464079\tGenerator Loss: 1.362422\n",
      "Train Epoch: 41 [16640/60000 (28%)]\tDiscriminator Loss: 0.559035\tGenerator Loss: 1.135856\n",
      "Train Epoch: 41 [17920/60000 (30%)]\tDiscriminator Loss: 0.527636\tGenerator Loss: 1.385740\n",
      "Train Epoch: 41 [19200/60000 (32%)]\tDiscriminator Loss: 0.496827\tGenerator Loss: 1.136972\n",
      "Train Epoch: 41 [20480/60000 (34%)]\tDiscriminator Loss: 0.521365\tGenerator Loss: 1.182873\n",
      "Train Epoch: 41 [21760/60000 (36%)]\tDiscriminator Loss: 0.512663\tGenerator Loss: 1.203428\n",
      "Train Epoch: 41 [23040/60000 (38%)]\tDiscriminator Loss: 0.517528\tGenerator Loss: 1.473195\n",
      "Train Epoch: 41 [24320/60000 (41%)]\tDiscriminator Loss: 0.534435\tGenerator Loss: 1.188784\n",
      "Train Epoch: 41 [25600/60000 (43%)]\tDiscriminator Loss: 0.526371\tGenerator Loss: 1.088636\n",
      "Train Epoch: 41 [26880/60000 (45%)]\tDiscriminator Loss: 0.549538\tGenerator Loss: 1.477529\n",
      "Train Epoch: 41 [28160/60000 (47%)]\tDiscriminator Loss: 0.531333\tGenerator Loss: 1.028200\n",
      "Train Epoch: 41 [29440/60000 (49%)]\tDiscriminator Loss: 0.487293\tGenerator Loss: 1.558957\n",
      "Train Epoch: 41 [30720/60000 (51%)]\tDiscriminator Loss: 0.564936\tGenerator Loss: 0.923957\n",
      "Train Epoch: 41 [32000/60000 (53%)]\tDiscriminator Loss: 0.528489\tGenerator Loss: 1.192439\n",
      "Train Epoch: 41 [33280/60000 (55%)]\tDiscriminator Loss: 0.605107\tGenerator Loss: 0.749050\n",
      "Train Epoch: 41 [34560/60000 (58%)]\tDiscriminator Loss: 0.619427\tGenerator Loss: 1.395186\n",
      "Train Epoch: 41 [35840/60000 (60%)]\tDiscriminator Loss: 0.498066\tGenerator Loss: 1.499131\n",
      "Train Epoch: 41 [37120/60000 (62%)]\tDiscriminator Loss: 0.544940\tGenerator Loss: 1.029876\n",
      "Train Epoch: 41 [38400/60000 (64%)]\tDiscriminator Loss: 0.475642\tGenerator Loss: 1.202214\n",
      "Train Epoch: 41 [39680/60000 (66%)]\tDiscriminator Loss: 0.564157\tGenerator Loss: 1.089630\n",
      "Train Epoch: 41 [40960/60000 (68%)]\tDiscriminator Loss: 0.510225\tGenerator Loss: 1.107525\n",
      "Train Epoch: 41 [42240/60000 (70%)]\tDiscriminator Loss: 0.576749\tGenerator Loss: 1.028301\n",
      "Train Epoch: 41 [43520/60000 (72%)]\tDiscriminator Loss: 0.569059\tGenerator Loss: 1.397280\n",
      "Train Epoch: 41 [44800/60000 (75%)]\tDiscriminator Loss: 0.520319\tGenerator Loss: 1.388041\n",
      "Train Epoch: 41 [46080/60000 (77%)]\tDiscriminator Loss: 0.537129\tGenerator Loss: 1.110453\n",
      "Train Epoch: 41 [47360/60000 (79%)]\tDiscriminator Loss: 0.520613\tGenerator Loss: 1.282909\n",
      "Train Epoch: 41 [48640/60000 (81%)]\tDiscriminator Loss: 0.527249\tGenerator Loss: 0.956159\n",
      "Train Epoch: 41 [49920/60000 (83%)]\tDiscriminator Loss: 0.538203\tGenerator Loss: 1.054582\n",
      "Train Epoch: 41 [51200/60000 (85%)]\tDiscriminator Loss: 0.535853\tGenerator Loss: 1.266663\n",
      "Train Epoch: 41 [52480/60000 (87%)]\tDiscriminator Loss: 0.502183\tGenerator Loss: 1.112395\n",
      "Train Epoch: 41 [53760/60000 (90%)]\tDiscriminator Loss: 0.510777\tGenerator Loss: 1.269691\n",
      "Train Epoch: 41 [55040/60000 (92%)]\tDiscriminator Loss: 0.543690\tGenerator Loss: 1.467625\n",
      "Train Epoch: 41 [56320/60000 (94%)]\tDiscriminator Loss: 0.524120\tGenerator Loss: 1.464518\n",
      "Train Epoch: 41 [57600/60000 (96%)]\tDiscriminator Loss: 0.512741\tGenerator Loss: 1.433366\n",
      "Train Epoch: 41 [58880/60000 (98%)]\tDiscriminator Loss: 0.464718\tGenerator Loss: 1.223056\n",
      "Train Epoch: 42 [0/60000 (0%)]\tDiscriminator Loss: 0.494036\tGenerator Loss: 1.133318\n",
      "Train Epoch: 42 [1280/60000 (2%)]\tDiscriminator Loss: 0.526722\tGenerator Loss: 1.375173\n",
      "Train Epoch: 42 [2560/60000 (4%)]\tDiscriminator Loss: 0.513834\tGenerator Loss: 1.085542\n",
      "Train Epoch: 42 [3840/60000 (6%)]\tDiscriminator Loss: 0.522619\tGenerator Loss: 1.228671\n",
      "Train Epoch: 42 [5120/60000 (9%)]\tDiscriminator Loss: 0.488121\tGenerator Loss: 1.446610\n",
      "Train Epoch: 42 [6400/60000 (11%)]\tDiscriminator Loss: 0.526928\tGenerator Loss: 1.237625\n",
      "Train Epoch: 42 [7680/60000 (13%)]\tDiscriminator Loss: 0.621025\tGenerator Loss: 1.621738\n",
      "Train Epoch: 42 [8960/60000 (15%)]\tDiscriminator Loss: 0.529869\tGenerator Loss: 1.151690\n",
      "Train Epoch: 42 [10240/60000 (17%)]\tDiscriminator Loss: 0.554621\tGenerator Loss: 1.378125\n",
      "Train Epoch: 42 [11520/60000 (19%)]\tDiscriminator Loss: 0.574668\tGenerator Loss: 1.393287\n",
      "Train Epoch: 42 [12800/60000 (21%)]\tDiscriminator Loss: 0.493382\tGenerator Loss: 1.794711\n",
      "Train Epoch: 42 [14080/60000 (23%)]\tDiscriminator Loss: 0.467724\tGenerator Loss: 1.183967\n",
      "Train Epoch: 42 [15360/60000 (26%)]\tDiscriminator Loss: 0.560041\tGenerator Loss: 0.796391\n",
      "Train Epoch: 42 [16640/60000 (28%)]\tDiscriminator Loss: 0.520069\tGenerator Loss: 1.330034\n",
      "Train Epoch: 42 [17920/60000 (30%)]\tDiscriminator Loss: 0.520111\tGenerator Loss: 1.238702\n",
      "Train Epoch: 42 [19200/60000 (32%)]\tDiscriminator Loss: 0.546285\tGenerator Loss: 1.420150\n",
      "Train Epoch: 42 [20480/60000 (34%)]\tDiscriminator Loss: 0.454034\tGenerator Loss: 1.243612\n",
      "Train Epoch: 42 [21760/60000 (36%)]\tDiscriminator Loss: 0.498024\tGenerator Loss: 1.171476\n",
      "Train Epoch: 42 [23040/60000 (38%)]\tDiscriminator Loss: 0.513761\tGenerator Loss: 1.176830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 42 [24320/60000 (41%)]\tDiscriminator Loss: 0.493319\tGenerator Loss: 1.150354\n",
      "Train Epoch: 42 [25600/60000 (43%)]\tDiscriminator Loss: 0.573335\tGenerator Loss: 1.588285\n",
      "Train Epoch: 42 [26880/60000 (45%)]\tDiscriminator Loss: 0.495956\tGenerator Loss: 1.306872\n",
      "Train Epoch: 42 [28160/60000 (47%)]\tDiscriminator Loss: 0.531168\tGenerator Loss: 1.147005\n",
      "Train Epoch: 42 [29440/60000 (49%)]\tDiscriminator Loss: 0.517547\tGenerator Loss: 1.341410\n",
      "Train Epoch: 42 [30720/60000 (51%)]\tDiscriminator Loss: 0.584258\tGenerator Loss: 1.814332\n",
      "Train Epoch: 42 [32000/60000 (53%)]\tDiscriminator Loss: 0.536799\tGenerator Loss: 1.143621\n",
      "Train Epoch: 42 [33280/60000 (55%)]\tDiscriminator Loss: 0.514279\tGenerator Loss: 1.211502\n",
      "Train Epoch: 42 [34560/60000 (58%)]\tDiscriminator Loss: 0.571181\tGenerator Loss: 1.292714\n",
      "Train Epoch: 42 [35840/60000 (60%)]\tDiscriminator Loss: 0.548238\tGenerator Loss: 0.991127\n",
      "Train Epoch: 42 [37120/60000 (62%)]\tDiscriminator Loss: 0.553062\tGenerator Loss: 0.912346\n",
      "Train Epoch: 42 [38400/60000 (64%)]\tDiscriminator Loss: 0.512805\tGenerator Loss: 1.183267\n",
      "Train Epoch: 42 [39680/60000 (66%)]\tDiscriminator Loss: 0.541274\tGenerator Loss: 1.021643\n",
      "Train Epoch: 42 [40960/60000 (68%)]\tDiscriminator Loss: 0.470869\tGenerator Loss: 1.265840\n",
      "Train Epoch: 42 [42240/60000 (70%)]\tDiscriminator Loss: 0.600993\tGenerator Loss: 0.913490\n",
      "Train Epoch: 42 [43520/60000 (72%)]\tDiscriminator Loss: 0.528708\tGenerator Loss: 1.540460\n",
      "Train Epoch: 42 [44800/60000 (75%)]\tDiscriminator Loss: 0.517833\tGenerator Loss: 1.219657\n",
      "Train Epoch: 42 [46080/60000 (77%)]\tDiscriminator Loss: 0.519669\tGenerator Loss: 1.103403\n",
      "Train Epoch: 42 [47360/60000 (79%)]\tDiscriminator Loss: 0.567971\tGenerator Loss: 0.993885\n",
      "Train Epoch: 42 [48640/60000 (81%)]\tDiscriminator Loss: 0.498922\tGenerator Loss: 1.343965\n",
      "Train Epoch: 42 [49920/60000 (83%)]\tDiscriminator Loss: 0.551429\tGenerator Loss: 1.148520\n",
      "Train Epoch: 42 [51200/60000 (85%)]\tDiscriminator Loss: 0.504145\tGenerator Loss: 1.040018\n",
      "Train Epoch: 42 [52480/60000 (87%)]\tDiscriminator Loss: 0.546066\tGenerator Loss: 1.280707\n",
      "Train Epoch: 42 [53760/60000 (90%)]\tDiscriminator Loss: 0.550158\tGenerator Loss: 1.057714\n",
      "Train Epoch: 42 [55040/60000 (92%)]\tDiscriminator Loss: 0.568860\tGenerator Loss: 1.378535\n",
      "Train Epoch: 42 [56320/60000 (94%)]\tDiscriminator Loss: 0.477585\tGenerator Loss: 1.222459\n",
      "Train Epoch: 42 [57600/60000 (96%)]\tDiscriminator Loss: 0.486788\tGenerator Loss: 1.224567\n",
      "Train Epoch: 42 [58880/60000 (98%)]\tDiscriminator Loss: 0.639574\tGenerator Loss: 0.645182\n",
      "Train Epoch: 43 [0/60000 (0%)]\tDiscriminator Loss: 0.543059\tGenerator Loss: 1.528312\n",
      "Train Epoch: 43 [1280/60000 (2%)]\tDiscriminator Loss: 0.472801\tGenerator Loss: 1.290183\n",
      "Train Epoch: 43 [2560/60000 (4%)]\tDiscriminator Loss: 0.531506\tGenerator Loss: 1.699680\n",
      "Train Epoch: 43 [3840/60000 (6%)]\tDiscriminator Loss: 0.534604\tGenerator Loss: 0.854032\n",
      "Train Epoch: 43 [5120/60000 (9%)]\tDiscriminator Loss: 0.517611\tGenerator Loss: 1.348099\n",
      "Train Epoch: 43 [6400/60000 (11%)]\tDiscriminator Loss: 0.534328\tGenerator Loss: 1.519210\n",
      "Train Epoch: 43 [7680/60000 (13%)]\tDiscriminator Loss: 0.455324\tGenerator Loss: 1.782951\n",
      "Train Epoch: 43 [8960/60000 (15%)]\tDiscriminator Loss: 0.510957\tGenerator Loss: 1.736809\n",
      "Train Epoch: 43 [10240/60000 (17%)]\tDiscriminator Loss: 0.597743\tGenerator Loss: 1.484800\n",
      "Train Epoch: 43 [11520/60000 (19%)]\tDiscriminator Loss: 0.488036\tGenerator Loss: 1.432840\n",
      "Train Epoch: 43 [12800/60000 (21%)]\tDiscriminator Loss: 0.511678\tGenerator Loss: 1.424914\n",
      "Train Epoch: 43 [14080/60000 (23%)]\tDiscriminator Loss: 0.534246\tGenerator Loss: 1.105474\n",
      "Train Epoch: 43 [15360/60000 (26%)]\tDiscriminator Loss: 0.525929\tGenerator Loss: 1.516153\n",
      "Train Epoch: 43 [16640/60000 (28%)]\tDiscriminator Loss: 0.555458\tGenerator Loss: 1.294873\n",
      "Train Epoch: 43 [17920/60000 (30%)]\tDiscriminator Loss: 0.566828\tGenerator Loss: 1.391716\n",
      "Train Epoch: 43 [19200/60000 (32%)]\tDiscriminator Loss: 0.515795\tGenerator Loss: 1.204327\n",
      "Train Epoch: 43 [20480/60000 (34%)]\tDiscriminator Loss: 0.592647\tGenerator Loss: 1.174035\n",
      "Train Epoch: 43 [21760/60000 (36%)]\tDiscriminator Loss: 0.582270\tGenerator Loss: 1.374896\n",
      "Train Epoch: 43 [23040/60000 (38%)]\tDiscriminator Loss: 0.490532\tGenerator Loss: 1.188324\n",
      "Train Epoch: 43 [24320/60000 (41%)]\tDiscriminator Loss: 0.612423\tGenerator Loss: 0.658307\n",
      "Train Epoch: 43 [25600/60000 (43%)]\tDiscriminator Loss: 0.553221\tGenerator Loss: 1.436530\n",
      "Train Epoch: 43 [26880/60000 (45%)]\tDiscriminator Loss: 0.512603\tGenerator Loss: 1.052872\n",
      "Train Epoch: 43 [28160/60000 (47%)]\tDiscriminator Loss: 0.505833\tGenerator Loss: 1.371238\n",
      "Train Epoch: 43 [29440/60000 (49%)]\tDiscriminator Loss: 0.475962\tGenerator Loss: 1.191773\n",
      "Train Epoch: 43 [30720/60000 (51%)]\tDiscriminator Loss: 0.565036\tGenerator Loss: 0.829170\n",
      "Train Epoch: 43 [32000/60000 (53%)]\tDiscriminator Loss: 0.530874\tGenerator Loss: 1.241526\n",
      "Train Epoch: 43 [33280/60000 (55%)]\tDiscriminator Loss: 0.485496\tGenerator Loss: 1.512205\n",
      "Train Epoch: 43 [34560/60000 (58%)]\tDiscriminator Loss: 0.518203\tGenerator Loss: 1.232926\n",
      "Train Epoch: 43 [35840/60000 (60%)]\tDiscriminator Loss: 0.553679\tGenerator Loss: 1.190439\n",
      "Train Epoch: 43 [37120/60000 (62%)]\tDiscriminator Loss: 0.591403\tGenerator Loss: 0.975311\n",
      "Train Epoch: 43 [38400/60000 (64%)]\tDiscriminator Loss: 0.610561\tGenerator Loss: 1.396452\n",
      "Train Epoch: 43 [39680/60000 (66%)]\tDiscriminator Loss: 0.456183\tGenerator Loss: 1.287001\n",
      "Train Epoch: 43 [40960/60000 (68%)]\tDiscriminator Loss: 0.487509\tGenerator Loss: 1.442603\n",
      "Train Epoch: 43 [42240/60000 (70%)]\tDiscriminator Loss: 0.508889\tGenerator Loss: 1.140292\n",
      "Train Epoch: 43 [43520/60000 (72%)]\tDiscriminator Loss: 0.558274\tGenerator Loss: 0.859317\n",
      "Train Epoch: 43 [44800/60000 (75%)]\tDiscriminator Loss: 0.511849\tGenerator Loss: 1.119509\n",
      "Train Epoch: 43 [46080/60000 (77%)]\tDiscriminator Loss: 0.556565\tGenerator Loss: 0.898460\n",
      "Train Epoch: 43 [47360/60000 (79%)]\tDiscriminator Loss: 0.551566\tGenerator Loss: 1.049593\n",
      "Train Epoch: 43 [48640/60000 (81%)]\tDiscriminator Loss: 0.629259\tGenerator Loss: 1.402416\n",
      "Train Epoch: 43 [49920/60000 (83%)]\tDiscriminator Loss: 0.577728\tGenerator Loss: 1.096527\n",
      "Train Epoch: 43 [51200/60000 (85%)]\tDiscriminator Loss: 0.528329\tGenerator Loss: 1.709430\n",
      "Train Epoch: 43 [52480/60000 (87%)]\tDiscriminator Loss: 0.558497\tGenerator Loss: 1.071645\n",
      "Train Epoch: 43 [53760/60000 (90%)]\tDiscriminator Loss: 0.548298\tGenerator Loss: 1.372267\n",
      "Train Epoch: 43 [55040/60000 (92%)]\tDiscriminator Loss: 0.538643\tGenerator Loss: 1.303026\n",
      "Train Epoch: 43 [56320/60000 (94%)]\tDiscriminator Loss: 0.549433\tGenerator Loss: 0.890625\n",
      "Train Epoch: 43 [57600/60000 (96%)]\tDiscriminator Loss: 0.527550\tGenerator Loss: 1.325821\n",
      "Train Epoch: 43 [58880/60000 (98%)]\tDiscriminator Loss: 0.526287\tGenerator Loss: 1.444862\n",
      "Train Epoch: 44 [0/60000 (0%)]\tDiscriminator Loss: 0.574443\tGenerator Loss: 1.415961\n",
      "Train Epoch: 44 [1280/60000 (2%)]\tDiscriminator Loss: 0.501393\tGenerator Loss: 1.452929\n",
      "Train Epoch: 44 [2560/60000 (4%)]\tDiscriminator Loss: 0.509614\tGenerator Loss: 1.445311\n",
      "Train Epoch: 44 [3840/60000 (6%)]\tDiscriminator Loss: 0.497457\tGenerator Loss: 1.171840\n",
      "Train Epoch: 44 [5120/60000 (9%)]\tDiscriminator Loss: 0.530405\tGenerator Loss: 0.906197\n",
      "Train Epoch: 44 [6400/60000 (11%)]\tDiscriminator Loss: 0.568881\tGenerator Loss: 1.212881\n",
      "Train Epoch: 44 [7680/60000 (13%)]\tDiscriminator Loss: 0.492049\tGenerator Loss: 0.948017\n",
      "Train Epoch: 44 [8960/60000 (15%)]\tDiscriminator Loss: 0.481398\tGenerator Loss: 1.467189\n",
      "Train Epoch: 44 [10240/60000 (17%)]\tDiscriminator Loss: 0.561903\tGenerator Loss: 1.260664\n",
      "Train Epoch: 44 [11520/60000 (19%)]\tDiscriminator Loss: 0.550841\tGenerator Loss: 1.590441\n",
      "Train Epoch: 44 [12800/60000 (21%)]\tDiscriminator Loss: 0.498969\tGenerator Loss: 1.256068\n",
      "Train Epoch: 44 [14080/60000 (23%)]\tDiscriminator Loss: 0.549819\tGenerator Loss: 1.338063\n",
      "Train Epoch: 44 [15360/60000 (26%)]\tDiscriminator Loss: 0.502757\tGenerator Loss: 1.474435\n",
      "Train Epoch: 44 [16640/60000 (28%)]\tDiscriminator Loss: 0.592331\tGenerator Loss: 0.975850\n",
      "Train Epoch: 44 [17920/60000 (30%)]\tDiscriminator Loss: 0.587478\tGenerator Loss: 0.916510\n",
      "Train Epoch: 44 [19200/60000 (32%)]\tDiscriminator Loss: 0.511291\tGenerator Loss: 1.218584\n",
      "Train Epoch: 44 [20480/60000 (34%)]\tDiscriminator Loss: 0.707640\tGenerator Loss: 0.558425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 44 [21760/60000 (36%)]\tDiscriminator Loss: 0.563415\tGenerator Loss: 1.219422\n",
      "Train Epoch: 44 [23040/60000 (38%)]\tDiscriminator Loss: 0.504260\tGenerator Loss: 1.426455\n",
      "Train Epoch: 44 [24320/60000 (41%)]\tDiscriminator Loss: 0.580298\tGenerator Loss: 1.032664\n",
      "Train Epoch: 44 [25600/60000 (43%)]\tDiscriminator Loss: 0.485157\tGenerator Loss: 1.508733\n",
      "Train Epoch: 44 [26880/60000 (45%)]\tDiscriminator Loss: 0.513499\tGenerator Loss: 1.460418\n",
      "Train Epoch: 44 [28160/60000 (47%)]\tDiscriminator Loss: 0.537206\tGenerator Loss: 1.136797\n",
      "Train Epoch: 44 [29440/60000 (49%)]\tDiscriminator Loss: 0.536018\tGenerator Loss: 1.239918\n",
      "Train Epoch: 44 [30720/60000 (51%)]\tDiscriminator Loss: 0.482741\tGenerator Loss: 1.207349\n",
      "Train Epoch: 44 [32000/60000 (53%)]\tDiscriminator Loss: 0.521708\tGenerator Loss: 1.437571\n",
      "Train Epoch: 44 [33280/60000 (55%)]\tDiscriminator Loss: 0.589443\tGenerator Loss: 1.389016\n",
      "Train Epoch: 44 [34560/60000 (58%)]\tDiscriminator Loss: 0.515344\tGenerator Loss: 1.552736\n",
      "Train Epoch: 44 [35840/60000 (60%)]\tDiscriminator Loss: 0.533071\tGenerator Loss: 1.111205\n",
      "Train Epoch: 44 [37120/60000 (62%)]\tDiscriminator Loss: 0.511088\tGenerator Loss: 1.414375\n",
      "Train Epoch: 44 [38400/60000 (64%)]\tDiscriminator Loss: 0.567642\tGenerator Loss: 1.283067\n",
      "Train Epoch: 44 [39680/60000 (66%)]\tDiscriminator Loss: 0.452964\tGenerator Loss: 1.450157\n",
      "Train Epoch: 44 [40960/60000 (68%)]\tDiscriminator Loss: 0.483193\tGenerator Loss: 1.627943\n",
      "Train Epoch: 44 [42240/60000 (70%)]\tDiscriminator Loss: 0.505987\tGenerator Loss: 1.321348\n",
      "Train Epoch: 44 [43520/60000 (72%)]\tDiscriminator Loss: 0.552334\tGenerator Loss: 0.908119\n",
      "Train Epoch: 44 [44800/60000 (75%)]\tDiscriminator Loss: 0.502850\tGenerator Loss: 1.248991\n",
      "Train Epoch: 44 [46080/60000 (77%)]\tDiscriminator Loss: 0.496554\tGenerator Loss: 1.119315\n",
      "Train Epoch: 44 [47360/60000 (79%)]\tDiscriminator Loss: 0.550313\tGenerator Loss: 1.457528\n",
      "Train Epoch: 44 [48640/60000 (81%)]\tDiscriminator Loss: 0.532095\tGenerator Loss: 1.156449\n",
      "Train Epoch: 44 [49920/60000 (83%)]\tDiscriminator Loss: 0.486080\tGenerator Loss: 1.294015\n",
      "Train Epoch: 44 [51200/60000 (85%)]\tDiscriminator Loss: 0.518952\tGenerator Loss: 1.143288\n",
      "Train Epoch: 44 [52480/60000 (87%)]\tDiscriminator Loss: 0.589817\tGenerator Loss: 1.791581\n",
      "Train Epoch: 44 [53760/60000 (90%)]\tDiscriminator Loss: 0.483956\tGenerator Loss: 1.104168\n",
      "Train Epoch: 44 [55040/60000 (92%)]\tDiscriminator Loss: 0.512694\tGenerator Loss: 1.009820\n",
      "Train Epoch: 44 [56320/60000 (94%)]\tDiscriminator Loss: 0.532543\tGenerator Loss: 1.540573\n",
      "Train Epoch: 44 [57600/60000 (96%)]\tDiscriminator Loss: 0.497120\tGenerator Loss: 1.510013\n",
      "Train Epoch: 44 [58880/60000 (98%)]\tDiscriminator Loss: 0.556996\tGenerator Loss: 1.251849\n",
      "Train Epoch: 45 [0/60000 (0%)]\tDiscriminator Loss: 0.497022\tGenerator Loss: 1.406483\n",
      "Train Epoch: 45 [1280/60000 (2%)]\tDiscriminator Loss: 0.490791\tGenerator Loss: 1.193074\n",
      "Train Epoch: 45 [2560/60000 (4%)]\tDiscriminator Loss: 0.568274\tGenerator Loss: 1.133565\n",
      "Train Epoch: 45 [3840/60000 (6%)]\tDiscriminator Loss: 0.522873\tGenerator Loss: 1.534992\n",
      "Train Epoch: 45 [5120/60000 (9%)]\tDiscriminator Loss: 0.521039\tGenerator Loss: 1.089928\n",
      "Train Epoch: 45 [6400/60000 (11%)]\tDiscriminator Loss: 0.473514\tGenerator Loss: 1.209873\n",
      "Train Epoch: 45 [7680/60000 (13%)]\tDiscriminator Loss: 0.506795\tGenerator Loss: 1.366711\n",
      "Train Epoch: 45 [8960/60000 (15%)]\tDiscriminator Loss: 0.537649\tGenerator Loss: 1.424713\n",
      "Train Epoch: 45 [10240/60000 (17%)]\tDiscriminator Loss: 0.499814\tGenerator Loss: 1.441308\n",
      "Train Epoch: 45 [11520/60000 (19%)]\tDiscriminator Loss: 0.559766\tGenerator Loss: 1.595107\n",
      "Train Epoch: 45 [12800/60000 (21%)]\tDiscriminator Loss: 0.594035\tGenerator Loss: 0.965328\n",
      "Train Epoch: 45 [14080/60000 (23%)]\tDiscriminator Loss: 0.490813\tGenerator Loss: 1.217405\n",
      "Train Epoch: 45 [15360/60000 (26%)]\tDiscriminator Loss: 0.561762\tGenerator Loss: 1.490669\n",
      "Train Epoch: 45 [16640/60000 (28%)]\tDiscriminator Loss: 0.490114\tGenerator Loss: 1.275506\n",
      "Train Epoch: 45 [17920/60000 (30%)]\tDiscriminator Loss: 0.496380\tGenerator Loss: 1.571742\n",
      "Train Epoch: 45 [19200/60000 (32%)]\tDiscriminator Loss: 0.523529\tGenerator Loss: 1.254523\n",
      "Train Epoch: 45 [20480/60000 (34%)]\tDiscriminator Loss: 0.511839\tGenerator Loss: 1.566192\n",
      "Train Epoch: 45 [21760/60000 (36%)]\tDiscriminator Loss: 0.532408\tGenerator Loss: 1.274595\n",
      "Train Epoch: 45 [23040/60000 (38%)]\tDiscriminator Loss: 0.587451\tGenerator Loss: 1.045269\n",
      "Train Epoch: 45 [24320/60000 (41%)]\tDiscriminator Loss: 0.519532\tGenerator Loss: 1.229161\n",
      "Train Epoch: 45 [25600/60000 (43%)]\tDiscriminator Loss: 0.558281\tGenerator Loss: 1.403800\n",
      "Train Epoch: 45 [26880/60000 (45%)]\tDiscriminator Loss: 0.534578\tGenerator Loss: 1.188506\n",
      "Train Epoch: 45 [28160/60000 (47%)]\tDiscriminator Loss: 0.476543\tGenerator Loss: 1.116755\n",
      "Train Epoch: 45 [29440/60000 (49%)]\tDiscriminator Loss: 0.547200\tGenerator Loss: 1.047988\n",
      "Train Epoch: 45 [30720/60000 (51%)]\tDiscriminator Loss: 0.574000\tGenerator Loss: 1.026389\n",
      "Train Epoch: 45 [32000/60000 (53%)]\tDiscriminator Loss: 0.497425\tGenerator Loss: 1.567505\n",
      "Train Epoch: 45 [33280/60000 (55%)]\tDiscriminator Loss: 0.495270\tGenerator Loss: 1.316977\n",
      "Train Epoch: 45 [34560/60000 (58%)]\tDiscriminator Loss: 0.515332\tGenerator Loss: 1.350447\n",
      "Train Epoch: 45 [35840/60000 (60%)]\tDiscriminator Loss: 0.582763\tGenerator Loss: 0.973487\n",
      "Train Epoch: 45 [37120/60000 (62%)]\tDiscriminator Loss: 0.577461\tGenerator Loss: 1.304959\n",
      "Train Epoch: 45 [38400/60000 (64%)]\tDiscriminator Loss: 0.516800\tGenerator Loss: 1.560460\n",
      "Train Epoch: 45 [39680/60000 (66%)]\tDiscriminator Loss: 0.561203\tGenerator Loss: 0.728579\n",
      "Train Epoch: 45 [40960/60000 (68%)]\tDiscriminator Loss: 0.510647\tGenerator Loss: 1.107907\n",
      "Train Epoch: 45 [42240/60000 (70%)]\tDiscriminator Loss: 0.547908\tGenerator Loss: 1.282342\n",
      "Train Epoch: 45 [43520/60000 (72%)]\tDiscriminator Loss: 0.460707\tGenerator Loss: 1.664509\n",
      "Train Epoch: 45 [44800/60000 (75%)]\tDiscriminator Loss: 0.510694\tGenerator Loss: 1.351014\n",
      "Train Epoch: 45 [46080/60000 (77%)]\tDiscriminator Loss: 0.469227\tGenerator Loss: 1.533429\n",
      "Train Epoch: 45 [47360/60000 (79%)]\tDiscriminator Loss: 0.510922\tGenerator Loss: 1.129431\n",
      "Train Epoch: 45 [48640/60000 (81%)]\tDiscriminator Loss: 0.505404\tGenerator Loss: 1.282478\n",
      "Train Epoch: 45 [49920/60000 (83%)]\tDiscriminator Loss: 0.510753\tGenerator Loss: 1.092548\n",
      "Train Epoch: 45 [51200/60000 (85%)]\tDiscriminator Loss: 0.563729\tGenerator Loss: 1.283147\n",
      "Train Epoch: 45 [52480/60000 (87%)]\tDiscriminator Loss: 0.505686\tGenerator Loss: 1.497696\n",
      "Train Epoch: 45 [53760/60000 (90%)]\tDiscriminator Loss: 0.513089\tGenerator Loss: 1.156917\n",
      "Train Epoch: 45 [55040/60000 (92%)]\tDiscriminator Loss: 0.520498\tGenerator Loss: 1.601353\n",
      "Train Epoch: 45 [56320/60000 (94%)]\tDiscriminator Loss: 0.587249\tGenerator Loss: 1.471103\n",
      "Train Epoch: 45 [57600/60000 (96%)]\tDiscriminator Loss: 0.535728\tGenerator Loss: 1.351029\n",
      "Train Epoch: 45 [58880/60000 (98%)]\tDiscriminator Loss: 0.470991\tGenerator Loss: 1.484439\n",
      "Train Epoch: 46 [0/60000 (0%)]\tDiscriminator Loss: 0.457311\tGenerator Loss: 1.547170\n",
      "Train Epoch: 46 [1280/60000 (2%)]\tDiscriminator Loss: 0.532291\tGenerator Loss: 1.148710\n",
      "Train Epoch: 46 [2560/60000 (4%)]\tDiscriminator Loss: 0.479378\tGenerator Loss: 1.662247\n",
      "Train Epoch: 46 [3840/60000 (6%)]\tDiscriminator Loss: 0.536378\tGenerator Loss: 1.520626\n",
      "Train Epoch: 46 [5120/60000 (9%)]\tDiscriminator Loss: 0.516113\tGenerator Loss: 1.066948\n",
      "Train Epoch: 46 [6400/60000 (11%)]\tDiscriminator Loss: 0.500599\tGenerator Loss: 1.749233\n",
      "Train Epoch: 46 [7680/60000 (13%)]\tDiscriminator Loss: 0.590553\tGenerator Loss: 1.711243\n",
      "Train Epoch: 46 [8960/60000 (15%)]\tDiscriminator Loss: 0.511361\tGenerator Loss: 1.209807\n",
      "Train Epoch: 46 [10240/60000 (17%)]\tDiscriminator Loss: 0.472965\tGenerator Loss: 1.271279\n",
      "Train Epoch: 46 [11520/60000 (19%)]\tDiscriminator Loss: 0.539539\tGenerator Loss: 1.645744\n",
      "Train Epoch: 46 [12800/60000 (21%)]\tDiscriminator Loss: 0.494539\tGenerator Loss: 1.112911\n",
      "Train Epoch: 46 [14080/60000 (23%)]\tDiscriminator Loss: 0.499505\tGenerator Loss: 1.551673\n",
      "Train Epoch: 46 [15360/60000 (26%)]\tDiscriminator Loss: 0.567947\tGenerator Loss: 1.329762\n",
      "Train Epoch: 46 [16640/60000 (28%)]\tDiscriminator Loss: 0.574745\tGenerator Loss: 0.999410\n",
      "Train Epoch: 46 [17920/60000 (30%)]\tDiscriminator Loss: 0.531151\tGenerator Loss: 1.142243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 46 [19200/60000 (32%)]\tDiscriminator Loss: 0.558210\tGenerator Loss: 1.743986\n",
      "Train Epoch: 46 [20480/60000 (34%)]\tDiscriminator Loss: 0.507601\tGenerator Loss: 1.251572\n",
      "Train Epoch: 46 [21760/60000 (36%)]\tDiscriminator Loss: 0.531931\tGenerator Loss: 1.317496\n",
      "Train Epoch: 46 [23040/60000 (38%)]\tDiscriminator Loss: 0.551009\tGenerator Loss: 1.029391\n",
      "Train Epoch: 46 [24320/60000 (41%)]\tDiscriminator Loss: 0.625843\tGenerator Loss: 0.702211\n",
      "Train Epoch: 46 [25600/60000 (43%)]\tDiscriminator Loss: 0.561684\tGenerator Loss: 1.341066\n",
      "Train Epoch: 46 [26880/60000 (45%)]\tDiscriminator Loss: 0.489851\tGenerator Loss: 1.435926\n",
      "Train Epoch: 46 [28160/60000 (47%)]\tDiscriminator Loss: 0.529261\tGenerator Loss: 1.062702\n",
      "Train Epoch: 46 [29440/60000 (49%)]\tDiscriminator Loss: 0.493927\tGenerator Loss: 1.292454\n",
      "Train Epoch: 46 [30720/60000 (51%)]\tDiscriminator Loss: 0.566408\tGenerator Loss: 1.374839\n",
      "Train Epoch: 46 [32000/60000 (53%)]\tDiscriminator Loss: 0.562182\tGenerator Loss: 1.070494\n",
      "Train Epoch: 46 [33280/60000 (55%)]\tDiscriminator Loss: 0.453175\tGenerator Loss: 1.374480\n",
      "Train Epoch: 46 [34560/60000 (58%)]\tDiscriminator Loss: 0.500521\tGenerator Loss: 1.118788\n",
      "Train Epoch: 46 [35840/60000 (60%)]\tDiscriminator Loss: 0.501237\tGenerator Loss: 1.764722\n",
      "Train Epoch: 46 [37120/60000 (62%)]\tDiscriminator Loss: 0.472110\tGenerator Loss: 1.222613\n",
      "Train Epoch: 46 [38400/60000 (64%)]\tDiscriminator Loss: 0.541394\tGenerator Loss: 1.396510\n",
      "Train Epoch: 46 [39680/60000 (66%)]\tDiscriminator Loss: 0.458012\tGenerator Loss: 1.384147\n",
      "Train Epoch: 46 [40960/60000 (68%)]\tDiscriminator Loss: 0.520150\tGenerator Loss: 1.248300\n",
      "Train Epoch: 46 [42240/60000 (70%)]\tDiscriminator Loss: 0.550780\tGenerator Loss: 1.216272\n",
      "Train Epoch: 46 [43520/60000 (72%)]\tDiscriminator Loss: 0.544423\tGenerator Loss: 1.458446\n",
      "Train Epoch: 46 [44800/60000 (75%)]\tDiscriminator Loss: 0.514321\tGenerator Loss: 1.302496\n",
      "Train Epoch: 46 [46080/60000 (77%)]\tDiscriminator Loss: 0.533614\tGenerator Loss: 1.021239\n",
      "Train Epoch: 46 [47360/60000 (79%)]\tDiscriminator Loss: 0.525016\tGenerator Loss: 1.158983\n",
      "Train Epoch: 46 [48640/60000 (81%)]\tDiscriminator Loss: 0.541152\tGenerator Loss: 0.975660\n",
      "Train Epoch: 46 [49920/60000 (83%)]\tDiscriminator Loss: 0.550798\tGenerator Loss: 1.105888\n",
      "Train Epoch: 46 [51200/60000 (85%)]\tDiscriminator Loss: 0.491500\tGenerator Loss: 1.159725\n",
      "Train Epoch: 46 [52480/60000 (87%)]\tDiscriminator Loss: 0.514969\tGenerator Loss: 1.264535\n",
      "Train Epoch: 46 [53760/60000 (90%)]\tDiscriminator Loss: 0.577784\tGenerator Loss: 0.860018\n",
      "Train Epoch: 46 [55040/60000 (92%)]\tDiscriminator Loss: 0.502865\tGenerator Loss: 1.247698\n",
      "Train Epoch: 46 [56320/60000 (94%)]\tDiscriminator Loss: 0.574104\tGenerator Loss: 1.037765\n",
      "Train Epoch: 46 [57600/60000 (96%)]\tDiscriminator Loss: 0.470304\tGenerator Loss: 1.639552\n",
      "Train Epoch: 46 [58880/60000 (98%)]\tDiscriminator Loss: 0.540529\tGenerator Loss: 1.219477\n",
      "Train Epoch: 47 [0/60000 (0%)]\tDiscriminator Loss: 0.540712\tGenerator Loss: 1.693130\n",
      "Train Epoch: 47 [1280/60000 (2%)]\tDiscriminator Loss: 0.513490\tGenerator Loss: 1.062390\n",
      "Train Epoch: 47 [2560/60000 (4%)]\tDiscriminator Loss: 0.447219\tGenerator Loss: 1.357178\n",
      "Train Epoch: 47 [3840/60000 (6%)]\tDiscriminator Loss: 0.520477\tGenerator Loss: 1.480968\n",
      "Train Epoch: 47 [5120/60000 (9%)]\tDiscriminator Loss: 0.495367\tGenerator Loss: 1.315133\n",
      "Train Epoch: 47 [6400/60000 (11%)]\tDiscriminator Loss: 0.651486\tGenerator Loss: 0.707011\n",
      "Train Epoch: 47 [7680/60000 (13%)]\tDiscriminator Loss: 0.486921\tGenerator Loss: 1.327909\n",
      "Train Epoch: 47 [8960/60000 (15%)]\tDiscriminator Loss: 0.580013\tGenerator Loss: 1.053562\n",
      "Train Epoch: 47 [10240/60000 (17%)]\tDiscriminator Loss: 0.585466\tGenerator Loss: 0.914017\n",
      "Train Epoch: 47 [11520/60000 (19%)]\tDiscriminator Loss: 0.572117\tGenerator Loss: 1.766325\n",
      "Train Epoch: 47 [12800/60000 (21%)]\tDiscriminator Loss: 0.431292\tGenerator Loss: 1.531139\n",
      "Train Epoch: 47 [14080/60000 (23%)]\tDiscriminator Loss: 0.554234\tGenerator Loss: 1.080331\n",
      "Train Epoch: 47 [15360/60000 (26%)]\tDiscriminator Loss: 0.504049\tGenerator Loss: 1.303032\n",
      "Train Epoch: 47 [16640/60000 (28%)]\tDiscriminator Loss: 0.493466\tGenerator Loss: 1.287600\n",
      "Train Epoch: 47 [17920/60000 (30%)]\tDiscriminator Loss: 0.552716\tGenerator Loss: 1.145166\n",
      "Train Epoch: 47 [19200/60000 (32%)]\tDiscriminator Loss: 0.536939\tGenerator Loss: 1.290113\n",
      "Train Epoch: 47 [20480/60000 (34%)]\tDiscriminator Loss: 0.535474\tGenerator Loss: 1.009278\n",
      "Train Epoch: 47 [21760/60000 (36%)]\tDiscriminator Loss: 0.556893\tGenerator Loss: 1.197125\n",
      "Train Epoch: 47 [23040/60000 (38%)]\tDiscriminator Loss: 0.620541\tGenerator Loss: 0.782818\n",
      "Train Epoch: 47 [24320/60000 (41%)]\tDiscriminator Loss: 0.515543\tGenerator Loss: 1.196233\n",
      "Train Epoch: 47 [25600/60000 (43%)]\tDiscriminator Loss: 0.466506\tGenerator Loss: 1.566372\n",
      "Train Epoch: 47 [26880/60000 (45%)]\tDiscriminator Loss: 0.511392\tGenerator Loss: 1.543063\n",
      "Train Epoch: 47 [28160/60000 (47%)]\tDiscriminator Loss: 0.529538\tGenerator Loss: 1.149177\n",
      "Train Epoch: 47 [29440/60000 (49%)]\tDiscriminator Loss: 0.503232\tGenerator Loss: 1.307704\n",
      "Train Epoch: 47 [30720/60000 (51%)]\tDiscriminator Loss: 0.537042\tGenerator Loss: 1.519877\n",
      "Train Epoch: 47 [32000/60000 (53%)]\tDiscriminator Loss: 0.536148\tGenerator Loss: 1.161196\n",
      "Train Epoch: 47 [33280/60000 (55%)]\tDiscriminator Loss: 0.504541\tGenerator Loss: 1.199188\n",
      "Train Epoch: 47 [34560/60000 (58%)]\tDiscriminator Loss: 0.490465\tGenerator Loss: 1.642940\n",
      "Train Epoch: 47 [35840/60000 (60%)]\tDiscriminator Loss: 0.507987\tGenerator Loss: 1.515495\n",
      "Train Epoch: 47 [37120/60000 (62%)]\tDiscriminator Loss: 0.517051\tGenerator Loss: 1.412770\n",
      "Train Epoch: 47 [38400/60000 (64%)]\tDiscriminator Loss: 0.510378\tGenerator Loss: 1.322501\n",
      "Train Epoch: 47 [39680/60000 (66%)]\tDiscriminator Loss: 0.492504\tGenerator Loss: 1.241680\n",
      "Train Epoch: 47 [40960/60000 (68%)]\tDiscriminator Loss: 0.498965\tGenerator Loss: 1.260857\n",
      "Train Epoch: 47 [42240/60000 (70%)]\tDiscriminator Loss: 0.503487\tGenerator Loss: 1.234656\n",
      "Train Epoch: 47 [43520/60000 (72%)]\tDiscriminator Loss: 0.562701\tGenerator Loss: 1.218887\n",
      "Train Epoch: 47 [44800/60000 (75%)]\tDiscriminator Loss: 0.572814\tGenerator Loss: 1.402710\n",
      "Train Epoch: 47 [46080/60000 (77%)]\tDiscriminator Loss: 0.502515\tGenerator Loss: 1.430121\n",
      "Train Epoch: 47 [47360/60000 (79%)]\tDiscriminator Loss: 0.503556\tGenerator Loss: 1.298798\n",
      "Train Epoch: 47 [48640/60000 (81%)]\tDiscriminator Loss: 0.526539\tGenerator Loss: 1.620042\n",
      "Train Epoch: 47 [49920/60000 (83%)]\tDiscriminator Loss: 0.572729\tGenerator Loss: 1.113792\n",
      "Train Epoch: 47 [51200/60000 (85%)]\tDiscriminator Loss: 0.523944\tGenerator Loss: 1.413968\n",
      "Train Epoch: 47 [52480/60000 (87%)]\tDiscriminator Loss: 0.545060\tGenerator Loss: 1.363287\n",
      "Train Epoch: 47 [53760/60000 (90%)]\tDiscriminator Loss: 0.531905\tGenerator Loss: 1.267236\n",
      "Train Epoch: 47 [55040/60000 (92%)]\tDiscriminator Loss: 0.565111\tGenerator Loss: 0.991186\n",
      "Train Epoch: 47 [56320/60000 (94%)]\tDiscriminator Loss: 0.522961\tGenerator Loss: 1.204555\n",
      "Train Epoch: 47 [57600/60000 (96%)]\tDiscriminator Loss: 0.515563\tGenerator Loss: 1.485005\n",
      "Train Epoch: 47 [58880/60000 (98%)]\tDiscriminator Loss: 0.496003\tGenerator Loss: 1.273488\n",
      "Train Epoch: 48 [0/60000 (0%)]\tDiscriminator Loss: 0.529573\tGenerator Loss: 0.905602\n",
      "Train Epoch: 48 [1280/60000 (2%)]\tDiscriminator Loss: 0.548485\tGenerator Loss: 1.252007\n",
      "Train Epoch: 48 [2560/60000 (4%)]\tDiscriminator Loss: 0.572129\tGenerator Loss: 1.419556\n",
      "Train Epoch: 48 [3840/60000 (6%)]\tDiscriminator Loss: 0.521811\tGenerator Loss: 1.884298\n",
      "Train Epoch: 48 [5120/60000 (9%)]\tDiscriminator Loss: 0.528249\tGenerator Loss: 1.613642\n",
      "Train Epoch: 48 [6400/60000 (11%)]\tDiscriminator Loss: 0.518205\tGenerator Loss: 1.665344\n",
      "Train Epoch: 48 [7680/60000 (13%)]\tDiscriminator Loss: 0.511036\tGenerator Loss: 1.072309\n",
      "Train Epoch: 48 [8960/60000 (15%)]\tDiscriminator Loss: 0.574851\tGenerator Loss: 1.395417\n",
      "Train Epoch: 48 [10240/60000 (17%)]\tDiscriminator Loss: 0.494531\tGenerator Loss: 1.437911\n",
      "Train Epoch: 48 [11520/60000 (19%)]\tDiscriminator Loss: 0.555463\tGenerator Loss: 1.251270\n",
      "Train Epoch: 48 [12800/60000 (21%)]\tDiscriminator Loss: 0.474868\tGenerator Loss: 1.337503\n",
      "Train Epoch: 48 [14080/60000 (23%)]\tDiscriminator Loss: 0.623165\tGenerator Loss: 1.641662\n",
      "Train Epoch: 48 [15360/60000 (26%)]\tDiscriminator Loss: 0.507432\tGenerator Loss: 1.123271\n",
      "Train Epoch: 48 [16640/60000 (28%)]\tDiscriminator Loss: 0.533292\tGenerator Loss: 1.014130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 48 [17920/60000 (30%)]\tDiscriminator Loss: 0.483548\tGenerator Loss: 1.305024\n",
      "Train Epoch: 48 [19200/60000 (32%)]\tDiscriminator Loss: 0.566931\tGenerator Loss: 0.909843\n",
      "Train Epoch: 48 [20480/60000 (34%)]\tDiscriminator Loss: 0.517807\tGenerator Loss: 1.342186\n",
      "Train Epoch: 48 [21760/60000 (36%)]\tDiscriminator Loss: 0.566063\tGenerator Loss: 1.024651\n",
      "Train Epoch: 48 [23040/60000 (38%)]\tDiscriminator Loss: 0.573524\tGenerator Loss: 1.117575\n",
      "Train Epoch: 48 [24320/60000 (41%)]\tDiscriminator Loss: 0.558730\tGenerator Loss: 1.069746\n",
      "Train Epoch: 48 [25600/60000 (43%)]\tDiscriminator Loss: 0.512978\tGenerator Loss: 1.297363\n",
      "Train Epoch: 48 [26880/60000 (45%)]\tDiscriminator Loss: 0.533207\tGenerator Loss: 1.154533\n",
      "Train Epoch: 48 [28160/60000 (47%)]\tDiscriminator Loss: 0.565606\tGenerator Loss: 0.982177\n",
      "Train Epoch: 48 [29440/60000 (49%)]\tDiscriminator Loss: 0.455576\tGenerator Loss: 1.253632\n",
      "Train Epoch: 48 [30720/60000 (51%)]\tDiscriminator Loss: 0.528505\tGenerator Loss: 0.968210\n",
      "Train Epoch: 48 [32000/60000 (53%)]\tDiscriminator Loss: 0.510983\tGenerator Loss: 1.203470\n",
      "Train Epoch: 48 [33280/60000 (55%)]\tDiscriminator Loss: 0.521120\tGenerator Loss: 1.395383\n",
      "Train Epoch: 48 [34560/60000 (58%)]\tDiscriminator Loss: 0.581531\tGenerator Loss: 1.649658\n",
      "Train Epoch: 48 [35840/60000 (60%)]\tDiscriminator Loss: 0.522221\tGenerator Loss: 1.188701\n",
      "Train Epoch: 48 [37120/60000 (62%)]\tDiscriminator Loss: 0.575816\tGenerator Loss: 1.265260\n",
      "Train Epoch: 48 [38400/60000 (64%)]\tDiscriminator Loss: 0.517807\tGenerator Loss: 1.380198\n",
      "Train Epoch: 48 [39680/60000 (66%)]\tDiscriminator Loss: 0.499429\tGenerator Loss: 1.480746\n",
      "Train Epoch: 48 [40960/60000 (68%)]\tDiscriminator Loss: 0.509540\tGenerator Loss: 1.256779\n",
      "Train Epoch: 48 [42240/60000 (70%)]\tDiscriminator Loss: 0.529647\tGenerator Loss: 1.479296\n",
      "Train Epoch: 48 [43520/60000 (72%)]\tDiscriminator Loss: 0.496754\tGenerator Loss: 1.209922\n",
      "Train Epoch: 48 [44800/60000 (75%)]\tDiscriminator Loss: 0.512975\tGenerator Loss: 1.156912\n",
      "Train Epoch: 48 [46080/60000 (77%)]\tDiscriminator Loss: 0.532346\tGenerator Loss: 1.501191\n",
      "Train Epoch: 48 [47360/60000 (79%)]\tDiscriminator Loss: 0.585466\tGenerator Loss: 1.236325\n",
      "Train Epoch: 48 [48640/60000 (81%)]\tDiscriminator Loss: 0.550502\tGenerator Loss: 1.068359\n",
      "Train Epoch: 48 [49920/60000 (83%)]\tDiscriminator Loss: 0.557217\tGenerator Loss: 1.499532\n",
      "Train Epoch: 48 [51200/60000 (85%)]\tDiscriminator Loss: 0.571991\tGenerator Loss: 1.111090\n",
      "Train Epoch: 48 [52480/60000 (87%)]\tDiscriminator Loss: 0.585275\tGenerator Loss: 1.694554\n",
      "Train Epoch: 48 [53760/60000 (90%)]\tDiscriminator Loss: 0.479691\tGenerator Loss: 1.166445\n",
      "Train Epoch: 48 [55040/60000 (92%)]\tDiscriminator Loss: 0.511935\tGenerator Loss: 1.009588\n",
      "Train Epoch: 48 [56320/60000 (94%)]\tDiscriminator Loss: 0.554709\tGenerator Loss: 0.879022\n",
      "Train Epoch: 48 [57600/60000 (96%)]\tDiscriminator Loss: 0.540240\tGenerator Loss: 1.715070\n",
      "Train Epoch: 48 [58880/60000 (98%)]\tDiscriminator Loss: 0.536561\tGenerator Loss: 0.992845\n",
      "Train Epoch: 49 [0/60000 (0%)]\tDiscriminator Loss: 0.536834\tGenerator Loss: 1.233977\n",
      "Train Epoch: 49 [1280/60000 (2%)]\tDiscriminator Loss: 0.530084\tGenerator Loss: 1.409981\n",
      "Train Epoch: 49 [2560/60000 (4%)]\tDiscriminator Loss: 0.555792\tGenerator Loss: 1.266698\n",
      "Train Epoch: 49 [3840/60000 (6%)]\tDiscriminator Loss: 0.488076\tGenerator Loss: 1.954248\n",
      "Train Epoch: 49 [5120/60000 (9%)]\tDiscriminator Loss: 0.544492\tGenerator Loss: 1.651087\n",
      "Train Epoch: 49 [6400/60000 (11%)]\tDiscriminator Loss: 0.483601\tGenerator Loss: 1.277194\n",
      "Train Epoch: 49 [7680/60000 (13%)]\tDiscriminator Loss: 0.591240\tGenerator Loss: 1.659430\n",
      "Train Epoch: 49 [8960/60000 (15%)]\tDiscriminator Loss: 0.553472\tGenerator Loss: 0.826114\n",
      "Train Epoch: 49 [10240/60000 (17%)]\tDiscriminator Loss: 0.478689\tGenerator Loss: 1.160586\n",
      "Train Epoch: 49 [11520/60000 (19%)]\tDiscriminator Loss: 0.510524\tGenerator Loss: 1.257591\n",
      "Train Epoch: 49 [12800/60000 (21%)]\tDiscriminator Loss: 0.503668\tGenerator Loss: 1.367204\n",
      "Train Epoch: 49 [14080/60000 (23%)]\tDiscriminator Loss: 0.611250\tGenerator Loss: 1.434307\n",
      "Train Epoch: 49 [15360/60000 (26%)]\tDiscriminator Loss: 0.525916\tGenerator Loss: 1.598552\n",
      "Train Epoch: 49 [16640/60000 (28%)]\tDiscriminator Loss: 0.478730\tGenerator Loss: 1.245830\n",
      "Train Epoch: 49 [17920/60000 (30%)]\tDiscriminator Loss: 0.481073\tGenerator Loss: 1.590771\n",
      "Train Epoch: 49 [19200/60000 (32%)]\tDiscriminator Loss: 0.482806\tGenerator Loss: 1.166789\n",
      "Train Epoch: 49 [20480/60000 (34%)]\tDiscriminator Loss: 0.533093\tGenerator Loss: 1.309006\n",
      "Train Epoch: 49 [21760/60000 (36%)]\tDiscriminator Loss: 0.515159\tGenerator Loss: 1.320341\n",
      "Train Epoch: 49 [23040/60000 (38%)]\tDiscriminator Loss: 0.558136\tGenerator Loss: 1.156060\n",
      "Train Epoch: 49 [24320/60000 (41%)]\tDiscriminator Loss: 0.503449\tGenerator Loss: 1.384594\n",
      "Train Epoch: 49 [25600/60000 (43%)]\tDiscriminator Loss: 0.550999\tGenerator Loss: 1.410273\n",
      "Train Epoch: 49 [26880/60000 (45%)]\tDiscriminator Loss: 0.536568\tGenerator Loss: 1.302764\n",
      "Train Epoch: 49 [28160/60000 (47%)]\tDiscriminator Loss: 0.505887\tGenerator Loss: 1.256432\n",
      "Train Epoch: 49 [29440/60000 (49%)]\tDiscriminator Loss: 0.534535\tGenerator Loss: 0.875689\n",
      "Train Epoch: 49 [30720/60000 (51%)]\tDiscriminator Loss: 0.529203\tGenerator Loss: 1.162734\n",
      "Train Epoch: 49 [32000/60000 (53%)]\tDiscriminator Loss: 0.543879\tGenerator Loss: 1.307094\n",
      "Train Epoch: 49 [33280/60000 (55%)]\tDiscriminator Loss: 0.493396\tGenerator Loss: 1.394676\n",
      "Train Epoch: 49 [34560/60000 (58%)]\tDiscriminator Loss: 0.506496\tGenerator Loss: 1.077555\n",
      "Train Epoch: 49 [35840/60000 (60%)]\tDiscriminator Loss: 0.480947\tGenerator Loss: 1.386551\n",
      "Train Epoch: 49 [37120/60000 (62%)]\tDiscriminator Loss: 0.481714\tGenerator Loss: 1.214521\n",
      "Train Epoch: 49 [38400/60000 (64%)]\tDiscriminator Loss: 0.562393\tGenerator Loss: 1.096185\n",
      "Train Epoch: 49 [39680/60000 (66%)]\tDiscriminator Loss: 0.541323\tGenerator Loss: 1.088846\n",
      "Train Epoch: 49 [40960/60000 (68%)]\tDiscriminator Loss: 0.495639\tGenerator Loss: 1.468390\n",
      "Train Epoch: 49 [42240/60000 (70%)]\tDiscriminator Loss: 0.469354\tGenerator Loss: 1.361070\n",
      "Train Epoch: 49 [43520/60000 (72%)]\tDiscriminator Loss: 0.564855\tGenerator Loss: 0.866404\n",
      "Train Epoch: 49 [44800/60000 (75%)]\tDiscriminator Loss: 0.519603\tGenerator Loss: 1.154118\n",
      "Train Epoch: 49 [46080/60000 (77%)]\tDiscriminator Loss: 0.570240\tGenerator Loss: 0.952564\n",
      "Train Epoch: 49 [47360/60000 (79%)]\tDiscriminator Loss: 0.521886\tGenerator Loss: 1.185222\n",
      "Train Epoch: 49 [48640/60000 (81%)]\tDiscriminator Loss: 0.501588\tGenerator Loss: 1.488974\n",
      "Train Epoch: 49 [49920/60000 (83%)]\tDiscriminator Loss: 0.543569\tGenerator Loss: 0.935542\n",
      "Train Epoch: 49 [51200/60000 (85%)]\tDiscriminator Loss: 0.535757\tGenerator Loss: 1.357678\n",
      "Train Epoch: 49 [52480/60000 (87%)]\tDiscriminator Loss: 0.574151\tGenerator Loss: 0.996105\n",
      "Train Epoch: 49 [53760/60000 (90%)]\tDiscriminator Loss: 0.501922\tGenerator Loss: 1.349310\n",
      "Train Epoch: 49 [55040/60000 (92%)]\tDiscriminator Loss: 0.475495\tGenerator Loss: 1.406099\n",
      "Train Epoch: 49 [56320/60000 (94%)]\tDiscriminator Loss: 0.580598\tGenerator Loss: 1.146953\n",
      "Train Epoch: 49 [57600/60000 (96%)]\tDiscriminator Loss: 0.511489\tGenerator Loss: 1.262292\n",
      "Train Epoch: 49 [58880/60000 (98%)]\tDiscriminator Loss: 0.557611\tGenerator Loss: 0.756755\n",
      "Train Epoch: 50 [0/60000 (0%)]\tDiscriminator Loss: 0.543528\tGenerator Loss: 1.371914\n",
      "Train Epoch: 50 [1280/60000 (2%)]\tDiscriminator Loss: 0.483756\tGenerator Loss: 1.503233\n",
      "Train Epoch: 50 [2560/60000 (4%)]\tDiscriminator Loss: 0.500159\tGenerator Loss: 1.015495\n",
      "Train Epoch: 50 [3840/60000 (6%)]\tDiscriminator Loss: 0.488478\tGenerator Loss: 1.316884\n",
      "Train Epoch: 50 [5120/60000 (9%)]\tDiscriminator Loss: 0.522803\tGenerator Loss: 0.937627\n",
      "Train Epoch: 50 [6400/60000 (11%)]\tDiscriminator Loss: 0.585274\tGenerator Loss: 1.805861\n",
      "Train Epoch: 50 [7680/60000 (13%)]\tDiscriminator Loss: 0.502309\tGenerator Loss: 1.485966\n",
      "Train Epoch: 50 [8960/60000 (15%)]\tDiscriminator Loss: 0.570397\tGenerator Loss: 0.773096\n",
      "Train Epoch: 50 [10240/60000 (17%)]\tDiscriminator Loss: 0.470779\tGenerator Loss: 1.584403\n",
      "Train Epoch: 50 [11520/60000 (19%)]\tDiscriminator Loss: 0.590863\tGenerator Loss: 0.884887\n",
      "Train Epoch: 50 [12800/60000 (21%)]\tDiscriminator Loss: 0.542918\tGenerator Loss: 1.178107\n",
      "Train Epoch: 50 [14080/60000 (23%)]\tDiscriminator Loss: 0.502502\tGenerator Loss: 1.382352\n",
      "Train Epoch: 50 [15360/60000 (26%)]\tDiscriminator Loss: 0.523403\tGenerator Loss: 1.210829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 50 [16640/60000 (28%)]\tDiscriminator Loss: 0.560712\tGenerator Loss: 1.037817\n",
      "Train Epoch: 50 [17920/60000 (30%)]\tDiscriminator Loss: 0.547048\tGenerator Loss: 1.244304\n",
      "Train Epoch: 50 [19200/60000 (32%)]\tDiscriminator Loss: 0.499017\tGenerator Loss: 1.325514\n",
      "Train Epoch: 50 [20480/60000 (34%)]\tDiscriminator Loss: 0.598007\tGenerator Loss: 1.192666\n",
      "Train Epoch: 50 [21760/60000 (36%)]\tDiscriminator Loss: 0.563400\tGenerator Loss: 1.224630\n",
      "Train Epoch: 50 [23040/60000 (38%)]\tDiscriminator Loss: 0.497510\tGenerator Loss: 1.573343\n",
      "Train Epoch: 50 [24320/60000 (41%)]\tDiscriminator Loss: 0.497434\tGenerator Loss: 1.173881\n",
      "Train Epoch: 50 [25600/60000 (43%)]\tDiscriminator Loss: 0.468438\tGenerator Loss: 1.555703\n",
      "Train Epoch: 50 [26880/60000 (45%)]\tDiscriminator Loss: 0.512998\tGenerator Loss: 1.152015\n",
      "Train Epoch: 50 [28160/60000 (47%)]\tDiscriminator Loss: 0.519852\tGenerator Loss: 0.976613\n",
      "Train Epoch: 50 [29440/60000 (49%)]\tDiscriminator Loss: 0.500691\tGenerator Loss: 1.259285\n",
      "Train Epoch: 50 [30720/60000 (51%)]\tDiscriminator Loss: 0.519401\tGenerator Loss: 1.276947\n",
      "Train Epoch: 50 [32000/60000 (53%)]\tDiscriminator Loss: 0.542810\tGenerator Loss: 1.361965\n",
      "Train Epoch: 50 [33280/60000 (55%)]\tDiscriminator Loss: 0.532591\tGenerator Loss: 1.369839\n",
      "Train Epoch: 50 [34560/60000 (58%)]\tDiscriminator Loss: 0.579193\tGenerator Loss: 1.579095\n",
      "Train Epoch: 50 [35840/60000 (60%)]\tDiscriminator Loss: 0.486679\tGenerator Loss: 1.168735\n",
      "Train Epoch: 50 [37120/60000 (62%)]\tDiscriminator Loss: 0.502633\tGenerator Loss: 1.353058\n",
      "Train Epoch: 50 [38400/60000 (64%)]\tDiscriminator Loss: 0.563054\tGenerator Loss: 1.595900\n",
      "Train Epoch: 50 [39680/60000 (66%)]\tDiscriminator Loss: 0.572963\tGenerator Loss: 1.162732\n",
      "Train Epoch: 50 [40960/60000 (68%)]\tDiscriminator Loss: 0.527505\tGenerator Loss: 1.026358\n",
      "Train Epoch: 50 [42240/60000 (70%)]\tDiscriminator Loss: 0.494727\tGenerator Loss: 1.084707\n",
      "Train Epoch: 50 [43520/60000 (72%)]\tDiscriminator Loss: 0.502771\tGenerator Loss: 1.412983\n",
      "Train Epoch: 50 [44800/60000 (75%)]\tDiscriminator Loss: 0.555909\tGenerator Loss: 0.916324\n",
      "Train Epoch: 50 [46080/60000 (77%)]\tDiscriminator Loss: 0.469740\tGenerator Loss: 1.174618\n",
      "Train Epoch: 50 [47360/60000 (79%)]\tDiscriminator Loss: 0.643952\tGenerator Loss: 1.502853\n",
      "Train Epoch: 50 [48640/60000 (81%)]\tDiscriminator Loss: 0.499937\tGenerator Loss: 1.318576\n",
      "Train Epoch: 50 [49920/60000 (83%)]\tDiscriminator Loss: 0.590268\tGenerator Loss: 1.839831\n",
      "Train Epoch: 50 [51200/60000 (85%)]\tDiscriminator Loss: 0.470490\tGenerator Loss: 1.326980\n",
      "Train Epoch: 50 [52480/60000 (87%)]\tDiscriminator Loss: 0.528714\tGenerator Loss: 1.007732\n",
      "Train Epoch: 50 [53760/60000 (90%)]\tDiscriminator Loss: 0.539016\tGenerator Loss: 1.251680\n",
      "Train Epoch: 50 [55040/60000 (92%)]\tDiscriminator Loss: 0.531210\tGenerator Loss: 1.438349\n",
      "Train Epoch: 50 [56320/60000 (94%)]\tDiscriminator Loss: 0.521780\tGenerator Loss: 1.404631\n",
      "Train Epoch: 50 [57600/60000 (96%)]\tDiscriminator Loss: 0.510329\tGenerator Loss: 1.430379\n",
      "Train Epoch: 50 [58880/60000 (98%)]\tDiscriminator Loss: 0.550300\tGenerator Loss: 1.217553\n",
      "Train Epoch: 51 [0/60000 (0%)]\tDiscriminator Loss: 0.570391\tGenerator Loss: 0.772564\n",
      "Train Epoch: 51 [1280/60000 (2%)]\tDiscriminator Loss: 0.532904\tGenerator Loss: 1.343209\n",
      "Train Epoch: 51 [2560/60000 (4%)]\tDiscriminator Loss: 0.494893\tGenerator Loss: 1.762407\n",
      "Train Epoch: 51 [3840/60000 (6%)]\tDiscriminator Loss: 0.493707\tGenerator Loss: 1.168829\n",
      "Train Epoch: 51 [5120/60000 (9%)]\tDiscriminator Loss: 0.457572\tGenerator Loss: 1.145310\n",
      "Train Epoch: 51 [6400/60000 (11%)]\tDiscriminator Loss: 0.537346\tGenerator Loss: 1.095892\n",
      "Train Epoch: 51 [7680/60000 (13%)]\tDiscriminator Loss: 0.497018\tGenerator Loss: 1.097032\n",
      "Train Epoch: 51 [8960/60000 (15%)]\tDiscriminator Loss: 0.508041\tGenerator Loss: 1.199370\n",
      "Train Epoch: 51 [10240/60000 (17%)]\tDiscriminator Loss: 0.558345\tGenerator Loss: 1.236129\n",
      "Train Epoch: 51 [11520/60000 (19%)]\tDiscriminator Loss: 0.442960\tGenerator Loss: 1.404799\n",
      "Train Epoch: 51 [12800/60000 (21%)]\tDiscriminator Loss: 0.577510\tGenerator Loss: 1.051381\n",
      "Train Epoch: 51 [14080/60000 (23%)]\tDiscriminator Loss: 0.486056\tGenerator Loss: 1.317789\n",
      "Train Epoch: 51 [15360/60000 (26%)]\tDiscriminator Loss: 0.474875\tGenerator Loss: 1.370607\n",
      "Train Epoch: 51 [16640/60000 (28%)]\tDiscriminator Loss: 0.498095\tGenerator Loss: 1.534500\n",
      "Train Epoch: 51 [17920/60000 (30%)]\tDiscriminator Loss: 0.548965\tGenerator Loss: 1.111502\n",
      "Train Epoch: 51 [19200/60000 (32%)]\tDiscriminator Loss: 0.546665\tGenerator Loss: 1.536376\n",
      "Train Epoch: 51 [20480/60000 (34%)]\tDiscriminator Loss: 0.577411\tGenerator Loss: 1.724436\n",
      "Train Epoch: 51 [21760/60000 (36%)]\tDiscriminator Loss: 0.508072\tGenerator Loss: 1.255338\n",
      "Train Epoch: 51 [23040/60000 (38%)]\tDiscriminator Loss: 0.519693\tGenerator Loss: 1.073520\n",
      "Train Epoch: 51 [24320/60000 (41%)]\tDiscriminator Loss: 0.521626\tGenerator Loss: 1.324012\n",
      "Train Epoch: 51 [25600/60000 (43%)]\tDiscriminator Loss: 0.501140\tGenerator Loss: 1.354258\n",
      "Train Epoch: 51 [26880/60000 (45%)]\tDiscriminator Loss: 0.460588\tGenerator Loss: 1.431885\n",
      "Train Epoch: 51 [28160/60000 (47%)]\tDiscriminator Loss: 0.551939\tGenerator Loss: 1.730575\n",
      "Train Epoch: 51 [29440/60000 (49%)]\tDiscriminator Loss: 0.493801\tGenerator Loss: 1.387170\n",
      "Train Epoch: 51 [30720/60000 (51%)]\tDiscriminator Loss: 0.491424\tGenerator Loss: 1.307168\n",
      "Train Epoch: 51 [32000/60000 (53%)]\tDiscriminator Loss: 0.457652\tGenerator Loss: 1.650979\n",
      "Train Epoch: 51 [33280/60000 (55%)]\tDiscriminator Loss: 0.652004\tGenerator Loss: 0.570261\n",
      "Train Epoch: 51 [34560/60000 (58%)]\tDiscriminator Loss: 0.565696\tGenerator Loss: 1.188482\n",
      "Train Epoch: 51 [35840/60000 (60%)]\tDiscriminator Loss: 0.498025\tGenerator Loss: 1.280937\n",
      "Train Epoch: 51 [37120/60000 (62%)]\tDiscriminator Loss: 0.535667\tGenerator Loss: 1.215499\n",
      "Train Epoch: 51 [38400/60000 (64%)]\tDiscriminator Loss: 0.493005\tGenerator Loss: 1.548399\n",
      "Train Epoch: 51 [39680/60000 (66%)]\tDiscriminator Loss: 0.538360\tGenerator Loss: 1.098840\n",
      "Train Epoch: 51 [40960/60000 (68%)]\tDiscriminator Loss: 0.506569\tGenerator Loss: 1.502173\n",
      "Train Epoch: 51 [42240/60000 (70%)]\tDiscriminator Loss: 0.596109\tGenerator Loss: 1.006818\n",
      "Train Epoch: 51 [43520/60000 (72%)]\tDiscriminator Loss: 0.511923\tGenerator Loss: 1.123643\n",
      "Train Epoch: 51 [44800/60000 (75%)]\tDiscriminator Loss: 0.499906\tGenerator Loss: 1.262569\n",
      "Train Epoch: 51 [46080/60000 (77%)]\tDiscriminator Loss: 0.531936\tGenerator Loss: 0.956491\n",
      "Train Epoch: 51 [47360/60000 (79%)]\tDiscriminator Loss: 0.507744\tGenerator Loss: 0.979173\n",
      "Train Epoch: 51 [48640/60000 (81%)]\tDiscriminator Loss: 0.494043\tGenerator Loss: 1.068234\n",
      "Train Epoch: 51 [49920/60000 (83%)]\tDiscriminator Loss: 0.510053\tGenerator Loss: 1.404655\n",
      "Train Epoch: 51 [51200/60000 (85%)]\tDiscriminator Loss: 0.511323\tGenerator Loss: 0.939310\n",
      "Train Epoch: 51 [52480/60000 (87%)]\tDiscriminator Loss: 0.512569\tGenerator Loss: 1.198584\n",
      "Train Epoch: 51 [53760/60000 (90%)]\tDiscriminator Loss: 0.541086\tGenerator Loss: 1.312528\n",
      "Train Epoch: 51 [55040/60000 (92%)]\tDiscriminator Loss: 0.533811\tGenerator Loss: 1.025435\n",
      "Train Epoch: 51 [56320/60000 (94%)]\tDiscriminator Loss: 0.491560\tGenerator Loss: 1.286771\n",
      "Train Epoch: 51 [57600/60000 (96%)]\tDiscriminator Loss: 0.539306\tGenerator Loss: 1.056908\n",
      "Train Epoch: 51 [58880/60000 (98%)]\tDiscriminator Loss: 0.569589\tGenerator Loss: 1.010349\n",
      "Train Epoch: 52 [0/60000 (0%)]\tDiscriminator Loss: 0.537665\tGenerator Loss: 1.018685\n",
      "Train Epoch: 52 [1280/60000 (2%)]\tDiscriminator Loss: 0.493791\tGenerator Loss: 1.213673\n",
      "Train Epoch: 52 [2560/60000 (4%)]\tDiscriminator Loss: 0.551336\tGenerator Loss: 1.331280\n",
      "Train Epoch: 52 [3840/60000 (6%)]\tDiscriminator Loss: 0.497538\tGenerator Loss: 1.013152\n",
      "Train Epoch: 52 [5120/60000 (9%)]\tDiscriminator Loss: 0.572150\tGenerator Loss: 1.461599\n",
      "Train Epoch: 52 [6400/60000 (11%)]\tDiscriminator Loss: 0.532383\tGenerator Loss: 1.157643\n",
      "Train Epoch: 52 [7680/60000 (13%)]\tDiscriminator Loss: 0.482710\tGenerator Loss: 1.178532\n",
      "Train Epoch: 52 [8960/60000 (15%)]\tDiscriminator Loss: 0.478148\tGenerator Loss: 1.295600\n",
      "Train Epoch: 52 [10240/60000 (17%)]\tDiscriminator Loss: 0.529983\tGenerator Loss: 1.360976\n",
      "Train Epoch: 52 [11520/60000 (19%)]\tDiscriminator Loss: 0.470102\tGenerator Loss: 1.271926\n",
      "Train Epoch: 52 [12800/60000 (21%)]\tDiscriminator Loss: 0.623866\tGenerator Loss: 0.774880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 52 [14080/60000 (23%)]\tDiscriminator Loss: 0.463775\tGenerator Loss: 1.158856\n",
      "Train Epoch: 52 [15360/60000 (26%)]\tDiscriminator Loss: 0.539877\tGenerator Loss: 1.415132\n",
      "Train Epoch: 52 [16640/60000 (28%)]\tDiscriminator Loss: 0.523587\tGenerator Loss: 1.136212\n",
      "Train Epoch: 52 [17920/60000 (30%)]\tDiscriminator Loss: 0.479429\tGenerator Loss: 1.436337\n",
      "Train Epoch: 52 [19200/60000 (32%)]\tDiscriminator Loss: 0.527569\tGenerator Loss: 1.176229\n",
      "Train Epoch: 52 [20480/60000 (34%)]\tDiscriminator Loss: 0.561056\tGenerator Loss: 0.919210\n",
      "Train Epoch: 52 [21760/60000 (36%)]\tDiscriminator Loss: 0.524874\tGenerator Loss: 1.595574\n",
      "Train Epoch: 52 [23040/60000 (38%)]\tDiscriminator Loss: 0.489851\tGenerator Loss: 1.315585\n",
      "Train Epoch: 52 [24320/60000 (41%)]\tDiscriminator Loss: 0.489445\tGenerator Loss: 1.176906\n",
      "Train Epoch: 52 [25600/60000 (43%)]\tDiscriminator Loss: 0.509006\tGenerator Loss: 1.506856\n",
      "Train Epoch: 52 [26880/60000 (45%)]\tDiscriminator Loss: 0.490997\tGenerator Loss: 1.210309\n",
      "Train Epoch: 52 [28160/60000 (47%)]\tDiscriminator Loss: 0.554340\tGenerator Loss: 1.474455\n",
      "Train Epoch: 52 [29440/60000 (49%)]\tDiscriminator Loss: 0.520097\tGenerator Loss: 1.170049\n",
      "Train Epoch: 52 [30720/60000 (51%)]\tDiscriminator Loss: 0.508998\tGenerator Loss: 1.046309\n",
      "Train Epoch: 52 [32000/60000 (53%)]\tDiscriminator Loss: 0.518702\tGenerator Loss: 1.401686\n",
      "Train Epoch: 52 [33280/60000 (55%)]\tDiscriminator Loss: 0.533206\tGenerator Loss: 1.137850\n",
      "Train Epoch: 52 [34560/60000 (58%)]\tDiscriminator Loss: 0.543120\tGenerator Loss: 1.210659\n",
      "Train Epoch: 52 [35840/60000 (60%)]\tDiscriminator Loss: 0.616954\tGenerator Loss: 0.768474\n",
      "Train Epoch: 52 [37120/60000 (62%)]\tDiscriminator Loss: 0.504746\tGenerator Loss: 1.454173\n",
      "Train Epoch: 52 [38400/60000 (64%)]\tDiscriminator Loss: 0.466560\tGenerator Loss: 1.467311\n",
      "Train Epoch: 52 [39680/60000 (66%)]\tDiscriminator Loss: 0.518384\tGenerator Loss: 1.454559\n",
      "Train Epoch: 52 [40960/60000 (68%)]\tDiscriminator Loss: 0.509028\tGenerator Loss: 1.297516\n",
      "Train Epoch: 52 [42240/60000 (70%)]\tDiscriminator Loss: 0.510055\tGenerator Loss: 1.187643\n",
      "Train Epoch: 52 [43520/60000 (72%)]\tDiscriminator Loss: 0.505356\tGenerator Loss: 1.244635\n",
      "Train Epoch: 52 [44800/60000 (75%)]\tDiscriminator Loss: 0.495190\tGenerator Loss: 1.398672\n",
      "Train Epoch: 52 [46080/60000 (77%)]\tDiscriminator Loss: 0.503131\tGenerator Loss: 1.257979\n",
      "Train Epoch: 52 [47360/60000 (79%)]\tDiscriminator Loss: 0.532746\tGenerator Loss: 1.366904\n",
      "Train Epoch: 52 [48640/60000 (81%)]\tDiscriminator Loss: 0.529900\tGenerator Loss: 1.435782\n",
      "Train Epoch: 52 [49920/60000 (83%)]\tDiscriminator Loss: 0.568906\tGenerator Loss: 1.771019\n",
      "Train Epoch: 52 [51200/60000 (85%)]\tDiscriminator Loss: 0.491006\tGenerator Loss: 1.536513\n",
      "Train Epoch: 52 [52480/60000 (87%)]\tDiscriminator Loss: 0.604947\tGenerator Loss: 0.855290\n",
      "Train Epoch: 52 [53760/60000 (90%)]\tDiscriminator Loss: 0.517700\tGenerator Loss: 1.095220\n",
      "Train Epoch: 52 [55040/60000 (92%)]\tDiscriminator Loss: 0.459952\tGenerator Loss: 1.390939\n",
      "Train Epoch: 52 [56320/60000 (94%)]\tDiscriminator Loss: 0.539396\tGenerator Loss: 1.444276\n",
      "Train Epoch: 52 [57600/60000 (96%)]\tDiscriminator Loss: 0.515249\tGenerator Loss: 1.266508\n",
      "Train Epoch: 52 [58880/60000 (98%)]\tDiscriminator Loss: 0.481411\tGenerator Loss: 1.430600\n",
      "Train Epoch: 53 [0/60000 (0%)]\tDiscriminator Loss: 0.534479\tGenerator Loss: 1.589401\n",
      "Train Epoch: 53 [1280/60000 (2%)]\tDiscriminator Loss: 0.514330\tGenerator Loss: 1.200648\n",
      "Train Epoch: 53 [2560/60000 (4%)]\tDiscriminator Loss: 0.511619\tGenerator Loss: 1.290657\n",
      "Train Epoch: 53 [3840/60000 (6%)]\tDiscriminator Loss: 0.546689\tGenerator Loss: 1.027555\n",
      "Train Epoch: 53 [5120/60000 (9%)]\tDiscriminator Loss: 0.501907\tGenerator Loss: 1.254000\n",
      "Train Epoch: 53 [6400/60000 (11%)]\tDiscriminator Loss: 0.578634\tGenerator Loss: 1.825738\n",
      "Train Epoch: 53 [7680/60000 (13%)]\tDiscriminator Loss: 0.453595\tGenerator Loss: 1.184133\n",
      "Train Epoch: 53 [8960/60000 (15%)]\tDiscriminator Loss: 0.539177\tGenerator Loss: 1.663468\n",
      "Train Epoch: 53 [10240/60000 (17%)]\tDiscriminator Loss: 0.543371\tGenerator Loss: 1.238327\n",
      "Train Epoch: 53 [11520/60000 (19%)]\tDiscriminator Loss: 0.491129\tGenerator Loss: 1.485503\n",
      "Train Epoch: 53 [12800/60000 (21%)]\tDiscriminator Loss: 0.563862\tGenerator Loss: 1.253801\n",
      "Train Epoch: 53 [14080/60000 (23%)]\tDiscriminator Loss: 0.546433\tGenerator Loss: 1.088918\n",
      "Train Epoch: 53 [15360/60000 (26%)]\tDiscriminator Loss: 0.493626\tGenerator Loss: 1.511494\n",
      "Train Epoch: 53 [16640/60000 (28%)]\tDiscriminator Loss: 0.492262\tGenerator Loss: 1.272367\n",
      "Train Epoch: 53 [17920/60000 (30%)]\tDiscriminator Loss: 0.476332\tGenerator Loss: 1.354429\n",
      "Train Epoch: 53 [19200/60000 (32%)]\tDiscriminator Loss: 0.494358\tGenerator Loss: 1.374936\n",
      "Train Epoch: 53 [20480/60000 (34%)]\tDiscriminator Loss: 0.536537\tGenerator Loss: 1.364565\n",
      "Train Epoch: 53 [21760/60000 (36%)]\tDiscriminator Loss: 0.491141\tGenerator Loss: 1.495600\n",
      "Train Epoch: 53 [23040/60000 (38%)]\tDiscriminator Loss: 0.596259\tGenerator Loss: 1.625279\n",
      "Train Epoch: 53 [24320/60000 (41%)]\tDiscriminator Loss: 0.510222\tGenerator Loss: 1.290281\n",
      "Train Epoch: 53 [25600/60000 (43%)]\tDiscriminator Loss: 0.501808\tGenerator Loss: 1.095086\n",
      "Train Epoch: 53 [26880/60000 (45%)]\tDiscriminator Loss: 0.471444\tGenerator Loss: 1.399883\n",
      "Train Epoch: 53 [28160/60000 (47%)]\tDiscriminator Loss: 0.507894\tGenerator Loss: 1.374044\n",
      "Train Epoch: 53 [29440/60000 (49%)]\tDiscriminator Loss: 0.568239\tGenerator Loss: 1.179566\n",
      "Train Epoch: 53 [30720/60000 (51%)]\tDiscriminator Loss: 0.526696\tGenerator Loss: 1.277034\n",
      "Train Epoch: 53 [32000/60000 (53%)]\tDiscriminator Loss: 0.456027\tGenerator Loss: 1.453053\n",
      "Train Epoch: 53 [33280/60000 (55%)]\tDiscriminator Loss: 0.561284\tGenerator Loss: 0.940825\n",
      "Train Epoch: 53 [34560/60000 (58%)]\tDiscriminator Loss: 0.516892\tGenerator Loss: 1.582961\n",
      "Train Epoch: 53 [35840/60000 (60%)]\tDiscriminator Loss: 0.531716\tGenerator Loss: 1.053957\n",
      "Train Epoch: 53 [37120/60000 (62%)]\tDiscriminator Loss: 0.508356\tGenerator Loss: 1.295465\n",
      "Train Epoch: 53 [38400/60000 (64%)]\tDiscriminator Loss: 0.482987\tGenerator Loss: 1.211000\n",
      "Train Epoch: 53 [39680/60000 (66%)]\tDiscriminator Loss: 0.497789\tGenerator Loss: 1.297072\n",
      "Train Epoch: 53 [40960/60000 (68%)]\tDiscriminator Loss: 0.525653\tGenerator Loss: 1.244049\n",
      "Train Epoch: 53 [42240/60000 (70%)]\tDiscriminator Loss: 0.549158\tGenerator Loss: 1.248610\n",
      "Train Epoch: 53 [43520/60000 (72%)]\tDiscriminator Loss: 0.514233\tGenerator Loss: 1.402976\n",
      "Train Epoch: 53 [44800/60000 (75%)]\tDiscriminator Loss: 0.565561\tGenerator Loss: 1.359151\n",
      "Train Epoch: 53 [46080/60000 (77%)]\tDiscriminator Loss: 0.477387\tGenerator Loss: 1.215562\n",
      "Train Epoch: 53 [47360/60000 (79%)]\tDiscriminator Loss: 0.581599\tGenerator Loss: 0.966556\n",
      "Train Epoch: 53 [48640/60000 (81%)]\tDiscriminator Loss: 0.475874\tGenerator Loss: 1.420437\n",
      "Train Epoch: 53 [49920/60000 (83%)]\tDiscriminator Loss: 0.516195\tGenerator Loss: 1.728813\n",
      "Train Epoch: 53 [51200/60000 (85%)]\tDiscriminator Loss: 0.511794\tGenerator Loss: 1.191767\n",
      "Train Epoch: 53 [52480/60000 (87%)]\tDiscriminator Loss: 0.528874\tGenerator Loss: 1.488675\n",
      "Train Epoch: 53 [53760/60000 (90%)]\tDiscriminator Loss: 0.528975\tGenerator Loss: 1.387711\n",
      "Train Epoch: 53 [55040/60000 (92%)]\tDiscriminator Loss: 0.539853\tGenerator Loss: 1.054380\n",
      "Train Epoch: 53 [56320/60000 (94%)]\tDiscriminator Loss: 0.550621\tGenerator Loss: 1.071554\n",
      "Train Epoch: 53 [57600/60000 (96%)]\tDiscriminator Loss: 0.484833\tGenerator Loss: 1.225943\n",
      "Train Epoch: 53 [58880/60000 (98%)]\tDiscriminator Loss: 0.567405\tGenerator Loss: 1.120007\n",
      "Train Epoch: 54 [0/60000 (0%)]\tDiscriminator Loss: 0.537501\tGenerator Loss: 1.178862\n",
      "Train Epoch: 54 [1280/60000 (2%)]\tDiscriminator Loss: 0.460356\tGenerator Loss: 1.503634\n",
      "Train Epoch: 54 [2560/60000 (4%)]\tDiscriminator Loss: 0.559610\tGenerator Loss: 1.774386\n",
      "Train Epoch: 54 [3840/60000 (6%)]\tDiscriminator Loss: 0.514717\tGenerator Loss: 1.171067\n",
      "Train Epoch: 54 [5120/60000 (9%)]\tDiscriminator Loss: 0.461574\tGenerator Loss: 1.428406\n",
      "Train Epoch: 54 [6400/60000 (11%)]\tDiscriminator Loss: 0.524161\tGenerator Loss: 1.236373\n",
      "Train Epoch: 54 [7680/60000 (13%)]\tDiscriminator Loss: 0.471504\tGenerator Loss: 1.058257\n",
      "Train Epoch: 54 [8960/60000 (15%)]\tDiscriminator Loss: 0.493297\tGenerator Loss: 1.113654\n",
      "Train Epoch: 54 [10240/60000 (17%)]\tDiscriminator Loss: 0.524878\tGenerator Loss: 1.348144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 54 [11520/60000 (19%)]\tDiscriminator Loss: 0.510398\tGenerator Loss: 1.101325\n",
      "Train Epoch: 54 [12800/60000 (21%)]\tDiscriminator Loss: 0.508701\tGenerator Loss: 1.296733\n",
      "Train Epoch: 54 [14080/60000 (23%)]\tDiscriminator Loss: 0.492640\tGenerator Loss: 1.402547\n",
      "Train Epoch: 54 [15360/60000 (26%)]\tDiscriminator Loss: 0.630404\tGenerator Loss: 0.720470\n",
      "Train Epoch: 54 [16640/60000 (28%)]\tDiscriminator Loss: 0.471079\tGenerator Loss: 1.258047\n",
      "Train Epoch: 54 [17920/60000 (30%)]\tDiscriminator Loss: 0.541578\tGenerator Loss: 1.578490\n",
      "Train Epoch: 54 [19200/60000 (32%)]\tDiscriminator Loss: 0.508976\tGenerator Loss: 1.454882\n",
      "Train Epoch: 54 [20480/60000 (34%)]\tDiscriminator Loss: 0.498418\tGenerator Loss: 1.170405\n",
      "Train Epoch: 54 [21760/60000 (36%)]\tDiscriminator Loss: 0.459109\tGenerator Loss: 1.228386\n",
      "Train Epoch: 54 [23040/60000 (38%)]\tDiscriminator Loss: 0.617269\tGenerator Loss: 1.722095\n",
      "Train Epoch: 54 [24320/60000 (41%)]\tDiscriminator Loss: 0.515062\tGenerator Loss: 1.205403\n",
      "Train Epoch: 54 [25600/60000 (43%)]\tDiscriminator Loss: 0.494022\tGenerator Loss: 1.495068\n",
      "Train Epoch: 54 [26880/60000 (45%)]\tDiscriminator Loss: 0.537248\tGenerator Loss: 1.149193\n",
      "Train Epoch: 54 [28160/60000 (47%)]\tDiscriminator Loss: 0.524083\tGenerator Loss: 1.277000\n",
      "Train Epoch: 54 [29440/60000 (49%)]\tDiscriminator Loss: 0.453652\tGenerator Loss: 1.596270\n",
      "Train Epoch: 54 [30720/60000 (51%)]\tDiscriminator Loss: 0.509539\tGenerator Loss: 1.263734\n",
      "Train Epoch: 54 [32000/60000 (53%)]\tDiscriminator Loss: 0.483153\tGenerator Loss: 1.591635\n",
      "Train Epoch: 54 [33280/60000 (55%)]\tDiscriminator Loss: 0.508583\tGenerator Loss: 1.442067\n",
      "Train Epoch: 54 [34560/60000 (58%)]\tDiscriminator Loss: 0.493988\tGenerator Loss: 1.133376\n",
      "Train Epoch: 54 [35840/60000 (60%)]\tDiscriminator Loss: 0.482780\tGenerator Loss: 1.431632\n",
      "Train Epoch: 54 [37120/60000 (62%)]\tDiscriminator Loss: 0.489739\tGenerator Loss: 1.414124\n",
      "Train Epoch: 54 [38400/60000 (64%)]\tDiscriminator Loss: 0.502078\tGenerator Loss: 1.109529\n",
      "Train Epoch: 54 [39680/60000 (66%)]\tDiscriminator Loss: 0.498605\tGenerator Loss: 1.480840\n",
      "Train Epoch: 54 [40960/60000 (68%)]\tDiscriminator Loss: 0.502412\tGenerator Loss: 1.222016\n",
      "Train Epoch: 54 [42240/60000 (70%)]\tDiscriminator Loss: 0.562621\tGenerator Loss: 1.105861\n",
      "Train Epoch: 54 [43520/60000 (72%)]\tDiscriminator Loss: 0.504543\tGenerator Loss: 1.394616\n",
      "Train Epoch: 54 [44800/60000 (75%)]\tDiscriminator Loss: 0.527955\tGenerator Loss: 1.117443\n",
      "Train Epoch: 54 [46080/60000 (77%)]\tDiscriminator Loss: 0.472379\tGenerator Loss: 1.126371\n",
      "Train Epoch: 54 [47360/60000 (79%)]\tDiscriminator Loss: 0.641881\tGenerator Loss: 0.717036\n",
      "Train Epoch: 54 [48640/60000 (81%)]\tDiscriminator Loss: 0.533820\tGenerator Loss: 1.148719\n",
      "Train Epoch: 54 [49920/60000 (83%)]\tDiscriminator Loss: 0.510972\tGenerator Loss: 1.131855\n",
      "Train Epoch: 54 [51200/60000 (85%)]\tDiscriminator Loss: 0.542156\tGenerator Loss: 1.646196\n",
      "Train Epoch: 54 [52480/60000 (87%)]\tDiscriminator Loss: 0.515950\tGenerator Loss: 1.263057\n",
      "Train Epoch: 54 [53760/60000 (90%)]\tDiscriminator Loss: 0.567038\tGenerator Loss: 1.030508\n",
      "Train Epoch: 54 [55040/60000 (92%)]\tDiscriminator Loss: 0.594960\tGenerator Loss: 1.502464\n",
      "Train Epoch: 54 [56320/60000 (94%)]\tDiscriminator Loss: 0.511791\tGenerator Loss: 1.334310\n",
      "Train Epoch: 54 [57600/60000 (96%)]\tDiscriminator Loss: 0.487191\tGenerator Loss: 1.033562\n",
      "Train Epoch: 54 [58880/60000 (98%)]\tDiscriminator Loss: 0.477955\tGenerator Loss: 1.160844\n",
      "Train Epoch: 55 [0/60000 (0%)]\tDiscriminator Loss: 0.522750\tGenerator Loss: 1.212101\n",
      "Train Epoch: 55 [1280/60000 (2%)]\tDiscriminator Loss: 0.444753\tGenerator Loss: 1.444178\n",
      "Train Epoch: 55 [2560/60000 (4%)]\tDiscriminator Loss: 0.526234\tGenerator Loss: 1.231319\n",
      "Train Epoch: 55 [3840/60000 (6%)]\tDiscriminator Loss: 0.524445\tGenerator Loss: 1.433648\n",
      "Train Epoch: 55 [5120/60000 (9%)]\tDiscriminator Loss: 0.550910\tGenerator Loss: 1.312419\n",
      "Train Epoch: 55 [6400/60000 (11%)]\tDiscriminator Loss: 0.472900\tGenerator Loss: 1.167706\n",
      "Train Epoch: 55 [7680/60000 (13%)]\tDiscriminator Loss: 0.561193\tGenerator Loss: 1.351850\n",
      "Train Epoch: 55 [8960/60000 (15%)]\tDiscriminator Loss: 0.504404\tGenerator Loss: 1.068030\n",
      "Train Epoch: 55 [10240/60000 (17%)]\tDiscriminator Loss: 0.487834\tGenerator Loss: 1.489856\n",
      "Train Epoch: 55 [11520/60000 (19%)]\tDiscriminator Loss: 0.493671\tGenerator Loss: 1.474551\n",
      "Train Epoch: 55 [12800/60000 (21%)]\tDiscriminator Loss: 0.526615\tGenerator Loss: 1.528119\n",
      "Train Epoch: 55 [14080/60000 (23%)]\tDiscriminator Loss: 0.464222\tGenerator Loss: 1.563244\n",
      "Train Epoch: 55 [15360/60000 (26%)]\tDiscriminator Loss: 0.585917\tGenerator Loss: 1.992629\n",
      "Train Epoch: 55 [16640/60000 (28%)]\tDiscriminator Loss: 0.463802\tGenerator Loss: 1.543174\n",
      "Train Epoch: 55 [17920/60000 (30%)]\tDiscriminator Loss: 0.530167\tGenerator Loss: 1.247371\n",
      "Train Epoch: 55 [19200/60000 (32%)]\tDiscriminator Loss: 0.500448\tGenerator Loss: 1.179124\n",
      "Train Epoch: 55 [20480/60000 (34%)]\tDiscriminator Loss: 0.557467\tGenerator Loss: 1.509012\n",
      "Train Epoch: 55 [21760/60000 (36%)]\tDiscriminator Loss: 0.543255\tGenerator Loss: 1.356868\n",
      "Train Epoch: 55 [23040/60000 (38%)]\tDiscriminator Loss: 0.527278\tGenerator Loss: 1.280618\n",
      "Train Epoch: 55 [24320/60000 (41%)]\tDiscriminator Loss: 0.484675\tGenerator Loss: 1.407561\n",
      "Train Epoch: 55 [25600/60000 (43%)]\tDiscriminator Loss: 0.491219\tGenerator Loss: 1.175398\n",
      "Train Epoch: 55 [26880/60000 (45%)]\tDiscriminator Loss: 0.520641\tGenerator Loss: 1.193489\n",
      "Train Epoch: 55 [28160/60000 (47%)]\tDiscriminator Loss: 0.473803\tGenerator Loss: 1.362925\n",
      "Train Epoch: 55 [29440/60000 (49%)]\tDiscriminator Loss: 0.559564\tGenerator Loss: 1.296621\n",
      "Train Epoch: 55 [30720/60000 (51%)]\tDiscriminator Loss: 0.559697\tGenerator Loss: 1.313603\n",
      "Train Epoch: 55 [32000/60000 (53%)]\tDiscriminator Loss: 0.539774\tGenerator Loss: 1.288680\n",
      "Train Epoch: 55 [33280/60000 (55%)]\tDiscriminator Loss: 0.587618\tGenerator Loss: 0.957731\n",
      "Train Epoch: 55 [34560/60000 (58%)]\tDiscriminator Loss: 0.556753\tGenerator Loss: 0.958673\n",
      "Train Epoch: 55 [35840/60000 (60%)]\tDiscriminator Loss: 0.495129\tGenerator Loss: 1.232756\n",
      "Train Epoch: 55 [37120/60000 (62%)]\tDiscriminator Loss: 0.516864\tGenerator Loss: 1.163226\n",
      "Train Epoch: 55 [38400/60000 (64%)]\tDiscriminator Loss: 0.524674\tGenerator Loss: 1.688638\n",
      "Train Epoch: 55 [39680/60000 (66%)]\tDiscriminator Loss: 0.514821\tGenerator Loss: 1.313556\n",
      "Train Epoch: 55 [40960/60000 (68%)]\tDiscriminator Loss: 0.475147\tGenerator Loss: 1.255482\n",
      "Train Epoch: 55 [42240/60000 (70%)]\tDiscriminator Loss: 0.449007\tGenerator Loss: 1.379328\n",
      "Train Epoch: 55 [43520/60000 (72%)]\tDiscriminator Loss: 0.524931\tGenerator Loss: 1.370283\n",
      "Train Epoch: 55 [44800/60000 (75%)]\tDiscriminator Loss: 0.475613\tGenerator Loss: 1.208616\n",
      "Train Epoch: 55 [46080/60000 (77%)]\tDiscriminator Loss: 0.487612\tGenerator Loss: 1.569126\n",
      "Train Epoch: 55 [47360/60000 (79%)]\tDiscriminator Loss: 0.563944\tGenerator Loss: 1.271619\n",
      "Train Epoch: 55 [48640/60000 (81%)]\tDiscriminator Loss: 0.659203\tGenerator Loss: 0.750916\n",
      "Train Epoch: 55 [49920/60000 (83%)]\tDiscriminator Loss: 0.479128\tGenerator Loss: 1.366779\n",
      "Train Epoch: 55 [51200/60000 (85%)]\tDiscriminator Loss: 0.496705\tGenerator Loss: 1.358704\n",
      "Train Epoch: 55 [52480/60000 (87%)]\tDiscriminator Loss: 0.563923\tGenerator Loss: 1.086919\n",
      "Train Epoch: 55 [53760/60000 (90%)]\tDiscriminator Loss: 0.504711\tGenerator Loss: 1.450446\n",
      "Train Epoch: 55 [55040/60000 (92%)]\tDiscriminator Loss: 0.537080\tGenerator Loss: 1.273665\n",
      "Train Epoch: 55 [56320/60000 (94%)]\tDiscriminator Loss: 0.461573\tGenerator Loss: 1.120850\n",
      "Train Epoch: 55 [57600/60000 (96%)]\tDiscriminator Loss: 0.552611\tGenerator Loss: 0.887810\n",
      "Train Epoch: 55 [58880/60000 (98%)]\tDiscriminator Loss: 0.532940\tGenerator Loss: 1.531586\n",
      "Train Epoch: 56 [0/60000 (0%)]\tDiscriminator Loss: 0.467661\tGenerator Loss: 1.521525\n",
      "Train Epoch: 56 [1280/60000 (2%)]\tDiscriminator Loss: 0.514288\tGenerator Loss: 0.981157\n",
      "Train Epoch: 56 [2560/60000 (4%)]\tDiscriminator Loss: 0.503796\tGenerator Loss: 1.322774\n",
      "Train Epoch: 56 [3840/60000 (6%)]\tDiscriminator Loss: 0.491868\tGenerator Loss: 1.563895\n",
      "Train Epoch: 56 [5120/60000 (9%)]\tDiscriminator Loss: 0.507539\tGenerator Loss: 1.079360\n",
      "Train Epoch: 56 [6400/60000 (11%)]\tDiscriminator Loss: 0.487473\tGenerator Loss: 1.403989\n",
      "Train Epoch: 56 [7680/60000 (13%)]\tDiscriminator Loss: 0.525311\tGenerator Loss: 1.286170\n",
      "Train Epoch: 56 [8960/60000 (15%)]\tDiscriminator Loss: 0.509766\tGenerator Loss: 1.398108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 56 [10240/60000 (17%)]\tDiscriminator Loss: 0.539606\tGenerator Loss: 1.742041\n",
      "Train Epoch: 56 [11520/60000 (19%)]\tDiscriminator Loss: 0.529894\tGenerator Loss: 1.215336\n",
      "Train Epoch: 56 [12800/60000 (21%)]\tDiscriminator Loss: 0.524651\tGenerator Loss: 1.569064\n",
      "Train Epoch: 56 [14080/60000 (23%)]\tDiscriminator Loss: 0.551088\tGenerator Loss: 1.101531\n",
      "Train Epoch: 56 [15360/60000 (26%)]\tDiscriminator Loss: 0.523783\tGenerator Loss: 1.501978\n",
      "Train Epoch: 56 [16640/60000 (28%)]\tDiscriminator Loss: 0.491981\tGenerator Loss: 1.373397\n",
      "Train Epoch: 56 [17920/60000 (30%)]\tDiscriminator Loss: 0.511633\tGenerator Loss: 1.221405\n",
      "Train Epoch: 56 [19200/60000 (32%)]\tDiscriminator Loss: 0.566648\tGenerator Loss: 1.661786\n",
      "Train Epoch: 56 [20480/60000 (34%)]\tDiscriminator Loss: 0.576821\tGenerator Loss: 1.282400\n",
      "Train Epoch: 56 [21760/60000 (36%)]\tDiscriminator Loss: 0.456767\tGenerator Loss: 1.347053\n",
      "Train Epoch: 56 [23040/60000 (38%)]\tDiscriminator Loss: 0.485851\tGenerator Loss: 1.157920\n",
      "Train Epoch: 56 [24320/60000 (41%)]\tDiscriminator Loss: 0.538562\tGenerator Loss: 1.103097\n",
      "Train Epoch: 56 [25600/60000 (43%)]\tDiscriminator Loss: 0.466574\tGenerator Loss: 1.338889\n",
      "Train Epoch: 56 [26880/60000 (45%)]\tDiscriminator Loss: 0.529698\tGenerator Loss: 1.119598\n",
      "Train Epoch: 56 [28160/60000 (47%)]\tDiscriminator Loss: 0.536045\tGenerator Loss: 1.403261\n",
      "Train Epoch: 56 [29440/60000 (49%)]\tDiscriminator Loss: 0.505812\tGenerator Loss: 1.311219\n",
      "Train Epoch: 56 [30720/60000 (51%)]\tDiscriminator Loss: 0.501159\tGenerator Loss: 1.485227\n",
      "Train Epoch: 56 [32000/60000 (53%)]\tDiscriminator Loss: 0.486789\tGenerator Loss: 1.592641\n",
      "Train Epoch: 56 [33280/60000 (55%)]\tDiscriminator Loss: 0.595303\tGenerator Loss: 1.124373\n",
      "Train Epoch: 56 [34560/60000 (58%)]\tDiscriminator Loss: 0.681910\tGenerator Loss: 0.685167\n",
      "Train Epoch: 56 [35840/60000 (60%)]\tDiscriminator Loss: 0.598013\tGenerator Loss: 1.273003\n",
      "Train Epoch: 56 [37120/60000 (62%)]\tDiscriminator Loss: 0.469517\tGenerator Loss: 1.545360\n",
      "Train Epoch: 56 [38400/60000 (64%)]\tDiscriminator Loss: 0.508652\tGenerator Loss: 1.222394\n",
      "Train Epoch: 56 [39680/60000 (66%)]\tDiscriminator Loss: 0.484999\tGenerator Loss: 1.425698\n",
      "Train Epoch: 56 [40960/60000 (68%)]\tDiscriminator Loss: 0.566988\tGenerator Loss: 1.304950\n",
      "Train Epoch: 56 [42240/60000 (70%)]\tDiscriminator Loss: 0.474327\tGenerator Loss: 1.562306\n",
      "Train Epoch: 56 [43520/60000 (72%)]\tDiscriminator Loss: 0.519224\tGenerator Loss: 0.891614\n",
      "Train Epoch: 56 [44800/60000 (75%)]\tDiscriminator Loss: 0.503516\tGenerator Loss: 1.155192\n",
      "Train Epoch: 56 [46080/60000 (77%)]\tDiscriminator Loss: 0.538067\tGenerator Loss: 1.456030\n",
      "Train Epoch: 56 [47360/60000 (79%)]\tDiscriminator Loss: 0.458365\tGenerator Loss: 1.290786\n",
      "Train Epoch: 56 [48640/60000 (81%)]\tDiscriminator Loss: 0.512806\tGenerator Loss: 1.378178\n",
      "Train Epoch: 56 [49920/60000 (83%)]\tDiscriminator Loss: 0.516243\tGenerator Loss: 1.179084\n",
      "Train Epoch: 56 [51200/60000 (85%)]\tDiscriminator Loss: 0.518768\tGenerator Loss: 0.980337\n",
      "Train Epoch: 56 [52480/60000 (87%)]\tDiscriminator Loss: 0.541187\tGenerator Loss: 1.247300\n",
      "Train Epoch: 56 [53760/60000 (90%)]\tDiscriminator Loss: 0.493663\tGenerator Loss: 1.329314\n",
      "Train Epoch: 56 [55040/60000 (92%)]\tDiscriminator Loss: 0.550832\tGenerator Loss: 1.457448\n",
      "Train Epoch: 56 [56320/60000 (94%)]\tDiscriminator Loss: 0.471874\tGenerator Loss: 1.448970\n",
      "Train Epoch: 56 [57600/60000 (96%)]\tDiscriminator Loss: 0.501751\tGenerator Loss: 1.221330\n",
      "Train Epoch: 56 [58880/60000 (98%)]\tDiscriminator Loss: 0.443502\tGenerator Loss: 1.453212\n",
      "Train Epoch: 57 [0/60000 (0%)]\tDiscriminator Loss: 0.510132\tGenerator Loss: 1.165004\n",
      "Train Epoch: 57 [1280/60000 (2%)]\tDiscriminator Loss: 0.554996\tGenerator Loss: 1.684406\n",
      "Train Epoch: 57 [2560/60000 (4%)]\tDiscriminator Loss: 0.459925\tGenerator Loss: 1.379510\n",
      "Train Epoch: 57 [3840/60000 (6%)]\tDiscriminator Loss: 0.493037\tGenerator Loss: 1.197837\n",
      "Train Epoch: 57 [5120/60000 (9%)]\tDiscriminator Loss: 0.538620\tGenerator Loss: 1.185079\n",
      "Train Epoch: 57 [6400/60000 (11%)]\tDiscriminator Loss: 0.533672\tGenerator Loss: 1.383571\n",
      "Train Epoch: 57 [7680/60000 (13%)]\tDiscriminator Loss: 0.478525\tGenerator Loss: 1.264070\n",
      "Train Epoch: 57 [8960/60000 (15%)]\tDiscriminator Loss: 0.472979\tGenerator Loss: 1.534789\n",
      "Train Epoch: 57 [10240/60000 (17%)]\tDiscriminator Loss: 0.656439\tGenerator Loss: 0.737659\n",
      "Train Epoch: 57 [11520/60000 (19%)]\tDiscriminator Loss: 0.530044\tGenerator Loss: 1.381125\n",
      "Train Epoch: 57 [12800/60000 (21%)]\tDiscriminator Loss: 0.662115\tGenerator Loss: 0.618161\n",
      "Train Epoch: 57 [14080/60000 (23%)]\tDiscriminator Loss: 0.542128\tGenerator Loss: 1.457126\n",
      "Train Epoch: 57 [15360/60000 (26%)]\tDiscriminator Loss: 0.485778\tGenerator Loss: 1.283229\n",
      "Train Epoch: 57 [16640/60000 (28%)]\tDiscriminator Loss: 0.571325\tGenerator Loss: 0.941834\n",
      "Train Epoch: 57 [17920/60000 (30%)]\tDiscriminator Loss: 0.565412\tGenerator Loss: 0.864610\n",
      "Train Epoch: 57 [19200/60000 (32%)]\tDiscriminator Loss: 0.477422\tGenerator Loss: 1.359507\n",
      "Train Epoch: 57 [20480/60000 (34%)]\tDiscriminator Loss: 0.515933\tGenerator Loss: 1.279515\n",
      "Train Epoch: 57 [21760/60000 (36%)]\tDiscriminator Loss: 0.529544\tGenerator Loss: 1.758356\n",
      "Train Epoch: 57 [23040/60000 (38%)]\tDiscriminator Loss: 0.537685\tGenerator Loss: 1.436260\n",
      "Train Epoch: 57 [24320/60000 (41%)]\tDiscriminator Loss: 0.589006\tGenerator Loss: 0.815897\n",
      "Train Epoch: 57 [25600/60000 (43%)]\tDiscriminator Loss: 0.505423\tGenerator Loss: 1.055521\n",
      "Train Epoch: 57 [26880/60000 (45%)]\tDiscriminator Loss: 0.509738\tGenerator Loss: 1.596656\n",
      "Train Epoch: 57 [28160/60000 (47%)]\tDiscriminator Loss: 0.515207\tGenerator Loss: 1.102318\n",
      "Train Epoch: 57 [29440/60000 (49%)]\tDiscriminator Loss: 0.476577\tGenerator Loss: 1.274525\n",
      "Train Epoch: 57 [30720/60000 (51%)]\tDiscriminator Loss: 0.485887\tGenerator Loss: 1.216787\n",
      "Train Epoch: 57 [32000/60000 (53%)]\tDiscriminator Loss: 0.554401\tGenerator Loss: 1.284275\n",
      "Train Epoch: 57 [33280/60000 (55%)]\tDiscriminator Loss: 0.523885\tGenerator Loss: 1.399565\n",
      "Train Epoch: 57 [34560/60000 (58%)]\tDiscriminator Loss: 0.539295\tGenerator Loss: 0.990567\n",
      "Train Epoch: 57 [35840/60000 (60%)]\tDiscriminator Loss: 0.505480\tGenerator Loss: 1.468238\n",
      "Train Epoch: 57 [37120/60000 (62%)]\tDiscriminator Loss: 0.483050\tGenerator Loss: 1.156017\n",
      "Train Epoch: 57 [38400/60000 (64%)]\tDiscriminator Loss: 0.550929\tGenerator Loss: 1.571778\n",
      "Train Epoch: 57 [39680/60000 (66%)]\tDiscriminator Loss: 0.515064\tGenerator Loss: 1.072086\n",
      "Train Epoch: 57 [40960/60000 (68%)]\tDiscriminator Loss: 0.533327\tGenerator Loss: 1.142267\n",
      "Train Epoch: 57 [42240/60000 (70%)]\tDiscriminator Loss: 0.472360\tGenerator Loss: 1.575387\n",
      "Train Epoch: 57 [43520/60000 (72%)]\tDiscriminator Loss: 0.489753\tGenerator Loss: 1.140356\n",
      "Train Epoch: 57 [44800/60000 (75%)]\tDiscriminator Loss: 0.445497\tGenerator Loss: 1.277285\n",
      "Train Epoch: 57 [46080/60000 (77%)]\tDiscriminator Loss: 0.583243\tGenerator Loss: 0.990188\n",
      "Train Epoch: 57 [47360/60000 (79%)]\tDiscriminator Loss: 0.555757\tGenerator Loss: 0.947779\n",
      "Train Epoch: 57 [48640/60000 (81%)]\tDiscriminator Loss: 0.534858\tGenerator Loss: 1.348725\n",
      "Train Epoch: 57 [49920/60000 (83%)]\tDiscriminator Loss: 0.525064\tGenerator Loss: 1.308778\n",
      "Train Epoch: 57 [51200/60000 (85%)]\tDiscriminator Loss: 0.530424\tGenerator Loss: 1.705401\n",
      "Train Epoch: 57 [52480/60000 (87%)]\tDiscriminator Loss: 0.535420\tGenerator Loss: 1.339752\n",
      "Train Epoch: 57 [53760/60000 (90%)]\tDiscriminator Loss: 0.471157\tGenerator Loss: 1.572752\n",
      "Train Epoch: 57 [55040/60000 (92%)]\tDiscriminator Loss: 0.486791\tGenerator Loss: 1.359330\n",
      "Train Epoch: 57 [56320/60000 (94%)]\tDiscriminator Loss: 0.529608\tGenerator Loss: 1.179350\n",
      "Train Epoch: 57 [57600/60000 (96%)]\tDiscriminator Loss: 0.537530\tGenerator Loss: 1.414014\n",
      "Train Epoch: 57 [58880/60000 (98%)]\tDiscriminator Loss: 0.555242\tGenerator Loss: 1.239258\n",
      "Train Epoch: 58 [0/60000 (0%)]\tDiscriminator Loss: 0.565075\tGenerator Loss: 1.467455\n",
      "Train Epoch: 58 [1280/60000 (2%)]\tDiscriminator Loss: 0.489277\tGenerator Loss: 1.233127\n",
      "Train Epoch: 58 [2560/60000 (4%)]\tDiscriminator Loss: 0.497531\tGenerator Loss: 1.293976\n",
      "Train Epoch: 58 [3840/60000 (6%)]\tDiscriminator Loss: 0.548917\tGenerator Loss: 1.268905\n",
      "Train Epoch: 58 [5120/60000 (9%)]\tDiscriminator Loss: 0.505628\tGenerator Loss: 1.272110\n",
      "Train Epoch: 58 [6400/60000 (11%)]\tDiscriminator Loss: 0.485722\tGenerator Loss: 1.299651\n",
      "Train Epoch: 58 [7680/60000 (13%)]\tDiscriminator Loss: 0.508275\tGenerator Loss: 1.269969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 58 [8960/60000 (15%)]\tDiscriminator Loss: 0.490611\tGenerator Loss: 1.191402\n",
      "Train Epoch: 58 [10240/60000 (17%)]\tDiscriminator Loss: 0.635251\tGenerator Loss: 1.414498\n",
      "Train Epoch: 58 [11520/60000 (19%)]\tDiscriminator Loss: 0.434398\tGenerator Loss: 1.781325\n",
      "Train Epoch: 58 [12800/60000 (21%)]\tDiscriminator Loss: 0.473077\tGenerator Loss: 1.228796\n",
      "Train Epoch: 58 [14080/60000 (23%)]\tDiscriminator Loss: 0.490018\tGenerator Loss: 1.439111\n",
      "Train Epoch: 58 [15360/60000 (26%)]\tDiscriminator Loss: 0.521422\tGenerator Loss: 1.167659\n",
      "Train Epoch: 58 [16640/60000 (28%)]\tDiscriminator Loss: 0.485973\tGenerator Loss: 1.309639\n",
      "Train Epoch: 58 [17920/60000 (30%)]\tDiscriminator Loss: 0.512281\tGenerator Loss: 1.155017\n",
      "Train Epoch: 58 [19200/60000 (32%)]\tDiscriminator Loss: 0.490179\tGenerator Loss: 1.479567\n",
      "Train Epoch: 58 [20480/60000 (34%)]\tDiscriminator Loss: 0.521882\tGenerator Loss: 1.385181\n",
      "Train Epoch: 58 [21760/60000 (36%)]\tDiscriminator Loss: 0.468550\tGenerator Loss: 1.288702\n",
      "Train Epoch: 58 [23040/60000 (38%)]\tDiscriminator Loss: 0.573372\tGenerator Loss: 1.378728\n",
      "Train Epoch: 58 [24320/60000 (41%)]\tDiscriminator Loss: 0.539628\tGenerator Loss: 1.556286\n",
      "Train Epoch: 58 [25600/60000 (43%)]\tDiscriminator Loss: 0.567732\tGenerator Loss: 1.669884\n",
      "Train Epoch: 58 [26880/60000 (45%)]\tDiscriminator Loss: 0.507733\tGenerator Loss: 1.286189\n",
      "Train Epoch: 58 [28160/60000 (47%)]\tDiscriminator Loss: 0.545766\tGenerator Loss: 1.092611\n",
      "Train Epoch: 58 [29440/60000 (49%)]\tDiscriminator Loss: 0.476756\tGenerator Loss: 1.453604\n",
      "Train Epoch: 58 [30720/60000 (51%)]\tDiscriminator Loss: 0.522277\tGenerator Loss: 1.498228\n",
      "Train Epoch: 58 [32000/60000 (53%)]\tDiscriminator Loss: 0.552953\tGenerator Loss: 0.980921\n",
      "Train Epoch: 58 [33280/60000 (55%)]\tDiscriminator Loss: 0.536041\tGenerator Loss: 0.966094\n",
      "Train Epoch: 58 [34560/60000 (58%)]\tDiscriminator Loss: 0.517343\tGenerator Loss: 1.262711\n",
      "Train Epoch: 58 [35840/60000 (60%)]\tDiscriminator Loss: 0.494481\tGenerator Loss: 1.420363\n",
      "Train Epoch: 58 [37120/60000 (62%)]\tDiscriminator Loss: 0.475004\tGenerator Loss: 1.413005\n",
      "Train Epoch: 58 [38400/60000 (64%)]\tDiscriminator Loss: 0.539051\tGenerator Loss: 1.536141\n",
      "Train Epoch: 58 [39680/60000 (66%)]\tDiscriminator Loss: 0.508382\tGenerator Loss: 1.369899\n",
      "Train Epoch: 58 [40960/60000 (68%)]\tDiscriminator Loss: 0.502013\tGenerator Loss: 1.143623\n",
      "Train Epoch: 58 [42240/60000 (70%)]\tDiscriminator Loss: 0.567531\tGenerator Loss: 1.000337\n",
      "Train Epoch: 58 [43520/60000 (72%)]\tDiscriminator Loss: 0.481068\tGenerator Loss: 1.372073\n",
      "Train Epoch: 58 [44800/60000 (75%)]\tDiscriminator Loss: 0.507371\tGenerator Loss: 1.193384\n",
      "Train Epoch: 58 [46080/60000 (77%)]\tDiscriminator Loss: 0.539802\tGenerator Loss: 1.172956\n",
      "Train Epoch: 58 [47360/60000 (79%)]\tDiscriminator Loss: 0.504046\tGenerator Loss: 1.444165\n",
      "Train Epoch: 58 [48640/60000 (81%)]\tDiscriminator Loss: 0.474656\tGenerator Loss: 1.391945\n",
      "Train Epoch: 58 [49920/60000 (83%)]\tDiscriminator Loss: 0.620114\tGenerator Loss: 1.672046\n",
      "Train Epoch: 58 [51200/60000 (85%)]\tDiscriminator Loss: 0.506083\tGenerator Loss: 1.674330\n",
      "Train Epoch: 58 [52480/60000 (87%)]\tDiscriminator Loss: 0.468938\tGenerator Loss: 1.161219\n",
      "Train Epoch: 58 [53760/60000 (90%)]\tDiscriminator Loss: 0.498761\tGenerator Loss: 1.399705\n",
      "Train Epoch: 58 [55040/60000 (92%)]\tDiscriminator Loss: 0.520302\tGenerator Loss: 1.505904\n",
      "Train Epoch: 58 [56320/60000 (94%)]\tDiscriminator Loss: 0.467345\tGenerator Loss: 1.299066\n",
      "Train Epoch: 58 [57600/60000 (96%)]\tDiscriminator Loss: 0.521135\tGenerator Loss: 1.411826\n",
      "Train Epoch: 58 [58880/60000 (98%)]\tDiscriminator Loss: 0.475880\tGenerator Loss: 1.389470\n",
      "Train Epoch: 59 [0/60000 (0%)]\tDiscriminator Loss: 0.466591\tGenerator Loss: 1.624174\n",
      "Train Epoch: 59 [1280/60000 (2%)]\tDiscriminator Loss: 0.541298\tGenerator Loss: 1.444908\n",
      "Train Epoch: 59 [2560/60000 (4%)]\tDiscriminator Loss: 0.516403\tGenerator Loss: 1.313118\n",
      "Train Epoch: 59 [3840/60000 (6%)]\tDiscriminator Loss: 0.512726\tGenerator Loss: 1.219705\n",
      "Train Epoch: 59 [5120/60000 (9%)]\tDiscriminator Loss: 0.518713\tGenerator Loss: 1.091226\n",
      "Train Epoch: 59 [6400/60000 (11%)]\tDiscriminator Loss: 0.480543\tGenerator Loss: 1.543852\n",
      "Train Epoch: 59 [7680/60000 (13%)]\tDiscriminator Loss: 0.460648\tGenerator Loss: 1.251399\n",
      "Train Epoch: 59 [8960/60000 (15%)]\tDiscriminator Loss: 0.484095\tGenerator Loss: 1.567732\n",
      "Train Epoch: 59 [10240/60000 (17%)]\tDiscriminator Loss: 0.504259\tGenerator Loss: 1.317955\n",
      "Train Epoch: 59 [11520/60000 (19%)]\tDiscriminator Loss: 0.506757\tGenerator Loss: 1.192035\n",
      "Train Epoch: 59 [12800/60000 (21%)]\tDiscriminator Loss: 0.507913\tGenerator Loss: 1.643176\n",
      "Train Epoch: 59 [14080/60000 (23%)]\tDiscriminator Loss: 0.554473\tGenerator Loss: 1.197708\n",
      "Train Epoch: 59 [15360/60000 (26%)]\tDiscriminator Loss: 0.449941\tGenerator Loss: 1.616590\n",
      "Train Epoch: 59 [16640/60000 (28%)]\tDiscriminator Loss: 0.513757\tGenerator Loss: 1.301236\n",
      "Train Epoch: 59 [17920/60000 (30%)]\tDiscriminator Loss: 0.534926\tGenerator Loss: 1.530471\n",
      "Train Epoch: 59 [19200/60000 (32%)]\tDiscriminator Loss: 0.513206\tGenerator Loss: 1.372723\n",
      "Train Epoch: 59 [20480/60000 (34%)]\tDiscriminator Loss: 0.456457\tGenerator Loss: 1.153340\n",
      "Train Epoch: 59 [21760/60000 (36%)]\tDiscriminator Loss: 0.491440\tGenerator Loss: 1.156961\n",
      "Train Epoch: 59 [23040/60000 (38%)]\tDiscriminator Loss: 0.488240\tGenerator Loss: 1.250759\n",
      "Train Epoch: 59 [24320/60000 (41%)]\tDiscriminator Loss: 0.520370\tGenerator Loss: 1.240458\n",
      "Train Epoch: 59 [25600/60000 (43%)]\tDiscriminator Loss: 0.509058\tGenerator Loss: 1.421041\n",
      "Train Epoch: 59 [26880/60000 (45%)]\tDiscriminator Loss: 0.485321\tGenerator Loss: 1.338505\n",
      "Train Epoch: 59 [28160/60000 (47%)]\tDiscriminator Loss: 0.528515\tGenerator Loss: 1.073747\n",
      "Train Epoch: 59 [29440/60000 (49%)]\tDiscriminator Loss: 0.502561\tGenerator Loss: 1.441658\n",
      "Train Epoch: 59 [30720/60000 (51%)]\tDiscriminator Loss: 0.475783\tGenerator Loss: 1.438900\n",
      "Train Epoch: 59 [32000/60000 (53%)]\tDiscriminator Loss: 0.472338\tGenerator Loss: 1.468527\n",
      "Train Epoch: 59 [33280/60000 (55%)]\tDiscriminator Loss: 0.507538\tGenerator Loss: 1.361442\n",
      "Train Epoch: 59 [34560/60000 (58%)]\tDiscriminator Loss: 0.466586\tGenerator Loss: 1.253642\n",
      "Train Epoch: 59 [35840/60000 (60%)]\tDiscriminator Loss: 0.560217\tGenerator Loss: 1.300732\n",
      "Train Epoch: 59 [37120/60000 (62%)]\tDiscriminator Loss: 0.505922\tGenerator Loss: 1.623505\n",
      "Train Epoch: 59 [38400/60000 (64%)]\tDiscriminator Loss: 0.489149\tGenerator Loss: 1.305922\n",
      "Train Epoch: 59 [39680/60000 (66%)]\tDiscriminator Loss: 0.535540\tGenerator Loss: 1.369172\n",
      "Train Epoch: 59 [40960/60000 (68%)]\tDiscriminator Loss: 0.480099\tGenerator Loss: 1.239570\n",
      "Train Epoch: 59 [42240/60000 (70%)]\tDiscriminator Loss: 0.484704\tGenerator Loss: 1.339300\n",
      "Train Epoch: 59 [43520/60000 (72%)]\tDiscriminator Loss: 0.461706\tGenerator Loss: 1.413540\n",
      "Train Epoch: 59 [44800/60000 (75%)]\tDiscriminator Loss: 0.497416\tGenerator Loss: 1.416460\n",
      "Train Epoch: 59 [46080/60000 (77%)]\tDiscriminator Loss: 0.490389\tGenerator Loss: 1.741074\n",
      "Train Epoch: 59 [47360/60000 (79%)]\tDiscriminator Loss: 0.549561\tGenerator Loss: 1.466793\n",
      "Train Epoch: 59 [48640/60000 (81%)]\tDiscriminator Loss: 0.492424\tGenerator Loss: 1.070683\n",
      "Train Epoch: 59 [49920/60000 (83%)]\tDiscriminator Loss: 0.465463\tGenerator Loss: 1.647080\n",
      "Train Epoch: 59 [51200/60000 (85%)]\tDiscriminator Loss: 0.445013\tGenerator Loss: 1.173762\n",
      "Train Epoch: 59 [52480/60000 (87%)]\tDiscriminator Loss: 0.465080\tGenerator Loss: 1.301533\n",
      "Train Epoch: 59 [53760/60000 (90%)]\tDiscriminator Loss: 0.515470\tGenerator Loss: 1.446775\n",
      "Train Epoch: 59 [55040/60000 (92%)]\tDiscriminator Loss: 0.444173\tGenerator Loss: 1.314749\n",
      "Train Epoch: 59 [56320/60000 (94%)]\tDiscriminator Loss: 0.483135\tGenerator Loss: 1.267503\n",
      "Train Epoch: 59 [57600/60000 (96%)]\tDiscriminator Loss: 0.520221\tGenerator Loss: 0.960158\n",
      "Train Epoch: 59 [58880/60000 (98%)]\tDiscriminator Loss: 0.532447\tGenerator Loss: 1.668839\n",
      "Train Epoch: 60 [0/60000 (0%)]\tDiscriminator Loss: 0.444060\tGenerator Loss: 1.621246\n",
      "Train Epoch: 60 [1280/60000 (2%)]\tDiscriminator Loss: 0.519679\tGenerator Loss: 1.047103\n",
      "Train Epoch: 60 [2560/60000 (4%)]\tDiscriminator Loss: 0.481917\tGenerator Loss: 1.182615\n",
      "Train Epoch: 60 [3840/60000 (6%)]\tDiscriminator Loss: 0.514872\tGenerator Loss: 1.434081\n",
      "Train Epoch: 60 [5120/60000 (9%)]\tDiscriminator Loss: 0.438387\tGenerator Loss: 1.578904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 60 [6400/60000 (11%)]\tDiscriminator Loss: 0.471845\tGenerator Loss: 1.280259\n",
      "Train Epoch: 60 [7680/60000 (13%)]\tDiscriminator Loss: 0.500814\tGenerator Loss: 1.498899\n",
      "Train Epoch: 60 [8960/60000 (15%)]\tDiscriminator Loss: 0.469588\tGenerator Loss: 1.458992\n",
      "Train Epoch: 60 [10240/60000 (17%)]\tDiscriminator Loss: 0.536632\tGenerator Loss: 1.216502\n",
      "Train Epoch: 60 [11520/60000 (19%)]\tDiscriminator Loss: 0.529044\tGenerator Loss: 1.128225\n",
      "Train Epoch: 60 [12800/60000 (21%)]\tDiscriminator Loss: 0.502685\tGenerator Loss: 1.833280\n",
      "Train Epoch: 60 [14080/60000 (23%)]\tDiscriminator Loss: 0.492911\tGenerator Loss: 1.044822\n",
      "Train Epoch: 60 [15360/60000 (26%)]\tDiscriminator Loss: 0.561540\tGenerator Loss: 1.264643\n",
      "Train Epoch: 60 [16640/60000 (28%)]\tDiscriminator Loss: 0.536827\tGenerator Loss: 1.199220\n",
      "Train Epoch: 60 [17920/60000 (30%)]\tDiscriminator Loss: 0.513430\tGenerator Loss: 1.342051\n",
      "Train Epoch: 60 [19200/60000 (32%)]\tDiscriminator Loss: 0.461410\tGenerator Loss: 1.542491\n",
      "Train Epoch: 60 [20480/60000 (34%)]\tDiscriminator Loss: 0.506503\tGenerator Loss: 1.037858\n",
      "Train Epoch: 60 [21760/60000 (36%)]\tDiscriminator Loss: 0.514063\tGenerator Loss: 1.056993\n",
      "Train Epoch: 60 [23040/60000 (38%)]\tDiscriminator Loss: 0.495416\tGenerator Loss: 1.119200\n",
      "Train Epoch: 60 [24320/60000 (41%)]\tDiscriminator Loss: 0.527156\tGenerator Loss: 1.159079\n",
      "Train Epoch: 60 [25600/60000 (43%)]\tDiscriminator Loss: 0.498278\tGenerator Loss: 1.182868\n",
      "Train Epoch: 60 [26880/60000 (45%)]\tDiscriminator Loss: 0.588238\tGenerator Loss: 1.529968\n",
      "Train Epoch: 60 [28160/60000 (47%)]\tDiscriminator Loss: 0.475878\tGenerator Loss: 1.325266\n",
      "Train Epoch: 60 [29440/60000 (49%)]\tDiscriminator Loss: 0.640218\tGenerator Loss: 0.887060\n",
      "Train Epoch: 60 [30720/60000 (51%)]\tDiscriminator Loss: 0.476179\tGenerator Loss: 1.231611\n",
      "Train Epoch: 60 [32000/60000 (53%)]\tDiscriminator Loss: 0.544506\tGenerator Loss: 0.994073\n",
      "Train Epoch: 60 [33280/60000 (55%)]\tDiscriminator Loss: 0.470639\tGenerator Loss: 1.478172\n",
      "Train Epoch: 60 [34560/60000 (58%)]\tDiscriminator Loss: 0.512298\tGenerator Loss: 1.443450\n",
      "Train Epoch: 60 [35840/60000 (60%)]\tDiscriminator Loss: 0.514135\tGenerator Loss: 1.236643\n",
      "Train Epoch: 60 [37120/60000 (62%)]\tDiscriminator Loss: 0.643276\tGenerator Loss: 0.969020\n",
      "Train Epoch: 60 [38400/60000 (64%)]\tDiscriminator Loss: 0.521993\tGenerator Loss: 1.256459\n",
      "Train Epoch: 60 [39680/60000 (66%)]\tDiscriminator Loss: 0.539018\tGenerator Loss: 1.133646\n",
      "Train Epoch: 60 [40960/60000 (68%)]\tDiscriminator Loss: 0.479633\tGenerator Loss: 1.195887\n",
      "Train Epoch: 60 [42240/60000 (70%)]\tDiscriminator Loss: 0.555936\tGenerator Loss: 1.045763\n",
      "Train Epoch: 60 [43520/60000 (72%)]\tDiscriminator Loss: 0.466067\tGenerator Loss: 1.512072\n",
      "Train Epoch: 60 [44800/60000 (75%)]\tDiscriminator Loss: 0.559003\tGenerator Loss: 0.838918\n",
      "Train Epoch: 60 [46080/60000 (77%)]\tDiscriminator Loss: 0.491070\tGenerator Loss: 1.198743\n",
      "Train Epoch: 60 [47360/60000 (79%)]\tDiscriminator Loss: 0.506331\tGenerator Loss: 1.537397\n",
      "Train Epoch: 60 [48640/60000 (81%)]\tDiscriminator Loss: 0.489121\tGenerator Loss: 1.305499\n",
      "Train Epoch: 60 [49920/60000 (83%)]\tDiscriminator Loss: 0.493992\tGenerator Loss: 1.197428\n",
      "Train Epoch: 60 [51200/60000 (85%)]\tDiscriminator Loss: 0.505439\tGenerator Loss: 1.248541\n",
      "Train Epoch: 60 [52480/60000 (87%)]\tDiscriminator Loss: 0.484448\tGenerator Loss: 1.234904\n",
      "Train Epoch: 60 [53760/60000 (90%)]\tDiscriminator Loss: 0.529681\tGenerator Loss: 1.256330\n",
      "Train Epoch: 60 [55040/60000 (92%)]\tDiscriminator Loss: 0.512760\tGenerator Loss: 1.018991\n",
      "Train Epoch: 60 [56320/60000 (94%)]\tDiscriminator Loss: 0.491044\tGenerator Loss: 1.531741\n",
      "Train Epoch: 60 [57600/60000 (96%)]\tDiscriminator Loss: 0.527731\tGenerator Loss: 1.233292\n",
      "Train Epoch: 60 [58880/60000 (98%)]\tDiscriminator Loss: 0.563344\tGenerator Loss: 1.130493\n",
      "Train Epoch: 61 [0/60000 (0%)]\tDiscriminator Loss: 0.479002\tGenerator Loss: 1.312611\n",
      "Train Epoch: 61 [1280/60000 (2%)]\tDiscriminator Loss: 0.514237\tGenerator Loss: 1.216815\n",
      "Train Epoch: 61 [2560/60000 (4%)]\tDiscriminator Loss: 0.478479\tGenerator Loss: 1.276941\n",
      "Train Epoch: 61 [3840/60000 (6%)]\tDiscriminator Loss: 0.608633\tGenerator Loss: 1.133857\n",
      "Train Epoch: 61 [5120/60000 (9%)]\tDiscriminator Loss: 0.493693\tGenerator Loss: 1.322154\n",
      "Train Epoch: 61 [6400/60000 (11%)]\tDiscriminator Loss: 0.580346\tGenerator Loss: 1.430952\n",
      "Train Epoch: 61 [7680/60000 (13%)]\tDiscriminator Loss: 0.491016\tGenerator Loss: 1.779634\n",
      "Train Epoch: 61 [8960/60000 (15%)]\tDiscriminator Loss: 0.529030\tGenerator Loss: 1.162217\n",
      "Train Epoch: 61 [10240/60000 (17%)]\tDiscriminator Loss: 0.506327\tGenerator Loss: 1.462279\n",
      "Train Epoch: 61 [11520/60000 (19%)]\tDiscriminator Loss: 0.505129\tGenerator Loss: 1.147299\n",
      "Train Epoch: 61 [12800/60000 (21%)]\tDiscriminator Loss: 0.510266\tGenerator Loss: 0.904223\n",
      "Train Epoch: 61 [14080/60000 (23%)]\tDiscriminator Loss: 0.503634\tGenerator Loss: 1.289223\n",
      "Train Epoch: 61 [15360/60000 (26%)]\tDiscriminator Loss: 0.508248\tGenerator Loss: 1.438349\n",
      "Train Epoch: 61 [16640/60000 (28%)]\tDiscriminator Loss: 0.481080\tGenerator Loss: 1.475788\n",
      "Train Epoch: 61 [17920/60000 (30%)]\tDiscriminator Loss: 0.461716\tGenerator Loss: 1.473542\n",
      "Train Epoch: 61 [19200/60000 (32%)]\tDiscriminator Loss: 0.535139\tGenerator Loss: 1.079025\n",
      "Train Epoch: 61 [20480/60000 (34%)]\tDiscriminator Loss: 0.461226\tGenerator Loss: 1.851141\n",
      "Train Epoch: 61 [21760/60000 (36%)]\tDiscriminator Loss: 0.502578\tGenerator Loss: 1.458006\n",
      "Train Epoch: 61 [23040/60000 (38%)]\tDiscriminator Loss: 0.471874\tGenerator Loss: 1.296057\n",
      "Train Epoch: 61 [24320/60000 (41%)]\tDiscriminator Loss: 0.532066\tGenerator Loss: 1.359943\n",
      "Train Epoch: 61 [25600/60000 (43%)]\tDiscriminator Loss: 0.444272\tGenerator Loss: 1.604113\n",
      "Train Epoch: 61 [26880/60000 (45%)]\tDiscriminator Loss: 0.487974\tGenerator Loss: 1.560851\n",
      "Train Epoch: 61 [28160/60000 (47%)]\tDiscriminator Loss: 0.488052\tGenerator Loss: 1.193185\n",
      "Train Epoch: 61 [29440/60000 (49%)]\tDiscriminator Loss: 0.488940\tGenerator Loss: 1.251545\n",
      "Train Epoch: 61 [30720/60000 (51%)]\tDiscriminator Loss: 0.438664\tGenerator Loss: 1.453551\n",
      "Train Epoch: 61 [32000/60000 (53%)]\tDiscriminator Loss: 0.558859\tGenerator Loss: 1.650525\n",
      "Train Epoch: 61 [33280/60000 (55%)]\tDiscriminator Loss: 0.496319\tGenerator Loss: 1.750506\n",
      "Train Epoch: 61 [34560/60000 (58%)]\tDiscriminator Loss: 0.563868\tGenerator Loss: 1.384509\n",
      "Train Epoch: 61 [35840/60000 (60%)]\tDiscriminator Loss: 0.489740\tGenerator Loss: 1.474903\n",
      "Train Epoch: 61 [37120/60000 (62%)]\tDiscriminator Loss: 0.522739\tGenerator Loss: 1.227354\n",
      "Train Epoch: 61 [38400/60000 (64%)]\tDiscriminator Loss: 0.476473\tGenerator Loss: 1.582886\n",
      "Train Epoch: 61 [39680/60000 (66%)]\tDiscriminator Loss: 0.513067\tGenerator Loss: 1.049960\n",
      "Train Epoch: 61 [40960/60000 (68%)]\tDiscriminator Loss: 0.551121\tGenerator Loss: 1.238643\n",
      "Train Epoch: 61 [42240/60000 (70%)]\tDiscriminator Loss: 0.471878\tGenerator Loss: 1.527586\n",
      "Train Epoch: 61 [43520/60000 (72%)]\tDiscriminator Loss: 0.527111\tGenerator Loss: 1.465664\n",
      "Train Epoch: 61 [44800/60000 (75%)]\tDiscriminator Loss: 0.473379\tGenerator Loss: 1.678826\n",
      "Train Epoch: 61 [46080/60000 (77%)]\tDiscriminator Loss: 0.508318\tGenerator Loss: 1.116699\n",
      "Train Epoch: 61 [47360/60000 (79%)]\tDiscriminator Loss: 0.484627\tGenerator Loss: 1.322151\n",
      "Train Epoch: 61 [48640/60000 (81%)]\tDiscriminator Loss: 0.550503\tGenerator Loss: 1.479084\n",
      "Train Epoch: 61 [49920/60000 (83%)]\tDiscriminator Loss: 0.540973\tGenerator Loss: 1.488673\n",
      "Train Epoch: 61 [51200/60000 (85%)]\tDiscriminator Loss: 0.545663\tGenerator Loss: 1.218278\n",
      "Train Epoch: 61 [52480/60000 (87%)]\tDiscriminator Loss: 0.556562\tGenerator Loss: 1.517067\n",
      "Train Epoch: 61 [53760/60000 (90%)]\tDiscriminator Loss: 0.598514\tGenerator Loss: 0.808057\n",
      "Train Epoch: 61 [55040/60000 (92%)]\tDiscriminator Loss: 0.455003\tGenerator Loss: 1.132746\n",
      "Train Epoch: 61 [56320/60000 (94%)]\tDiscriminator Loss: 0.530629\tGenerator Loss: 1.636131\n",
      "Train Epoch: 61 [57600/60000 (96%)]\tDiscriminator Loss: 0.494025\tGenerator Loss: 1.528700\n",
      "Train Epoch: 61 [58880/60000 (98%)]\tDiscriminator Loss: 0.448717\tGenerator Loss: 1.530162\n",
      "Train Epoch: 62 [0/60000 (0%)]\tDiscriminator Loss: 0.543082\tGenerator Loss: 1.215271\n",
      "Train Epoch: 62 [1280/60000 (2%)]\tDiscriminator Loss: 0.515061\tGenerator Loss: 1.209348\n",
      "Train Epoch: 62 [2560/60000 (4%)]\tDiscriminator Loss: 0.507684\tGenerator Loss: 1.337863\n",
      "Train Epoch: 62 [3840/60000 (6%)]\tDiscriminator Loss: 0.484159\tGenerator Loss: 1.524775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 62 [5120/60000 (9%)]\tDiscriminator Loss: 0.545274\tGenerator Loss: 1.042472\n",
      "Train Epoch: 62 [6400/60000 (11%)]\tDiscriminator Loss: 0.527691\tGenerator Loss: 1.360509\n",
      "Train Epoch: 62 [7680/60000 (13%)]\tDiscriminator Loss: 0.517639\tGenerator Loss: 1.313219\n",
      "Train Epoch: 62 [8960/60000 (15%)]\tDiscriminator Loss: 0.495694\tGenerator Loss: 1.420121\n",
      "Train Epoch: 62 [10240/60000 (17%)]\tDiscriminator Loss: 0.501925\tGenerator Loss: 1.288724\n",
      "Train Epoch: 62 [11520/60000 (19%)]\tDiscriminator Loss: 0.514450\tGenerator Loss: 1.075604\n",
      "Train Epoch: 62 [12800/60000 (21%)]\tDiscriminator Loss: 0.467033\tGenerator Loss: 1.440871\n",
      "Train Epoch: 62 [14080/60000 (23%)]\tDiscriminator Loss: 0.548844\tGenerator Loss: 1.198525\n",
      "Train Epoch: 62 [15360/60000 (26%)]\tDiscriminator Loss: 0.483315\tGenerator Loss: 1.149977\n",
      "Train Epoch: 62 [16640/60000 (28%)]\tDiscriminator Loss: 0.513614\tGenerator Loss: 1.263285\n",
      "Train Epoch: 62 [17920/60000 (30%)]\tDiscriminator Loss: 0.534526\tGenerator Loss: 1.457525\n",
      "Train Epoch: 62 [19200/60000 (32%)]\tDiscriminator Loss: 0.472240\tGenerator Loss: 1.041300\n",
      "Train Epoch: 62 [20480/60000 (34%)]\tDiscriminator Loss: 0.524046\tGenerator Loss: 1.334493\n",
      "Train Epoch: 62 [21760/60000 (36%)]\tDiscriminator Loss: 0.485179\tGenerator Loss: 1.299996\n",
      "Train Epoch: 62 [23040/60000 (38%)]\tDiscriminator Loss: 0.458602\tGenerator Loss: 1.525789\n",
      "Train Epoch: 62 [24320/60000 (41%)]\tDiscriminator Loss: 0.520900\tGenerator Loss: 1.335958\n",
      "Train Epoch: 62 [25600/60000 (43%)]\tDiscriminator Loss: 0.543529\tGenerator Loss: 1.515791\n",
      "Train Epoch: 62 [26880/60000 (45%)]\tDiscriminator Loss: 0.544208\tGenerator Loss: 1.950879\n",
      "Train Epoch: 62 [28160/60000 (47%)]\tDiscriminator Loss: 0.523361\tGenerator Loss: 1.161961\n",
      "Train Epoch: 62 [29440/60000 (49%)]\tDiscriminator Loss: 0.476607\tGenerator Loss: 1.408971\n",
      "Train Epoch: 62 [30720/60000 (51%)]\tDiscriminator Loss: 0.555313\tGenerator Loss: 0.930185\n",
      "Train Epoch: 62 [32000/60000 (53%)]\tDiscriminator Loss: 0.512618\tGenerator Loss: 1.340931\n",
      "Train Epoch: 62 [33280/60000 (55%)]\tDiscriminator Loss: 0.475019\tGenerator Loss: 1.189348\n",
      "Train Epoch: 62 [34560/60000 (58%)]\tDiscriminator Loss: 0.464369\tGenerator Loss: 1.569012\n",
      "Train Epoch: 62 [35840/60000 (60%)]\tDiscriminator Loss: 0.575997\tGenerator Loss: 0.785771\n",
      "Train Epoch: 62 [37120/60000 (62%)]\tDiscriminator Loss: 0.520993\tGenerator Loss: 1.344643\n",
      "Train Epoch: 62 [38400/60000 (64%)]\tDiscriminator Loss: 0.513292\tGenerator Loss: 1.298979\n",
      "Train Epoch: 62 [39680/60000 (66%)]\tDiscriminator Loss: 0.538786\tGenerator Loss: 1.157881\n",
      "Train Epoch: 62 [40960/60000 (68%)]\tDiscriminator Loss: 0.487183\tGenerator Loss: 1.215161\n",
      "Train Epoch: 62 [42240/60000 (70%)]\tDiscriminator Loss: 0.517167\tGenerator Loss: 0.821891\n",
      "Train Epoch: 62 [43520/60000 (72%)]\tDiscriminator Loss: 0.502092\tGenerator Loss: 1.342893\n",
      "Train Epoch: 62 [44800/60000 (75%)]\tDiscriminator Loss: 0.527303\tGenerator Loss: 1.395638\n",
      "Train Epoch: 62 [46080/60000 (77%)]\tDiscriminator Loss: 0.466369\tGenerator Loss: 1.364432\n",
      "Train Epoch: 62 [47360/60000 (79%)]\tDiscriminator Loss: 0.545981\tGenerator Loss: 1.331195\n",
      "Train Epoch: 62 [48640/60000 (81%)]\tDiscriminator Loss: 0.550656\tGenerator Loss: 0.973200\n",
      "Train Epoch: 62 [49920/60000 (83%)]\tDiscriminator Loss: 0.498570\tGenerator Loss: 1.509512\n",
      "Train Epoch: 62 [51200/60000 (85%)]\tDiscriminator Loss: 0.543475\tGenerator Loss: 1.425701\n",
      "Train Epoch: 62 [52480/60000 (87%)]\tDiscriminator Loss: 0.468968\tGenerator Loss: 1.446765\n",
      "Train Epoch: 62 [53760/60000 (90%)]\tDiscriminator Loss: 0.492681\tGenerator Loss: 1.122571\n",
      "Train Epoch: 62 [55040/60000 (92%)]\tDiscriminator Loss: 0.579200\tGenerator Loss: 1.746505\n",
      "Train Epoch: 62 [56320/60000 (94%)]\tDiscriminator Loss: 0.492952\tGenerator Loss: 1.702956\n",
      "Train Epoch: 62 [57600/60000 (96%)]\tDiscriminator Loss: 0.514096\tGenerator Loss: 1.283872\n",
      "Train Epoch: 62 [58880/60000 (98%)]\tDiscriminator Loss: 0.518187\tGenerator Loss: 1.882581\n",
      "Train Epoch: 63 [0/60000 (0%)]\tDiscriminator Loss: 0.514442\tGenerator Loss: 1.474466\n",
      "Train Epoch: 63 [1280/60000 (2%)]\tDiscriminator Loss: 0.494684\tGenerator Loss: 1.386208\n",
      "Train Epoch: 63 [2560/60000 (4%)]\tDiscriminator Loss: 0.518278\tGenerator Loss: 1.300526\n",
      "Train Epoch: 63 [3840/60000 (6%)]\tDiscriminator Loss: 0.487009\tGenerator Loss: 1.304544\n",
      "Train Epoch: 63 [5120/60000 (9%)]\tDiscriminator Loss: 0.477991\tGenerator Loss: 1.347769\n",
      "Train Epoch: 63 [6400/60000 (11%)]\tDiscriminator Loss: 0.500318\tGenerator Loss: 1.270536\n",
      "Train Epoch: 63 [7680/60000 (13%)]\tDiscriminator Loss: 0.513663\tGenerator Loss: 1.528419\n",
      "Train Epoch: 63 [8960/60000 (15%)]\tDiscriminator Loss: 0.488549\tGenerator Loss: 1.227328\n",
      "Train Epoch: 63 [10240/60000 (17%)]\tDiscriminator Loss: 0.476214\tGenerator Loss: 1.343404\n",
      "Train Epoch: 63 [11520/60000 (19%)]\tDiscriminator Loss: 0.525046\tGenerator Loss: 1.233050\n",
      "Train Epoch: 63 [12800/60000 (21%)]\tDiscriminator Loss: 0.520305\tGenerator Loss: 1.426975\n",
      "Train Epoch: 63 [14080/60000 (23%)]\tDiscriminator Loss: 0.557965\tGenerator Loss: 1.643074\n",
      "Train Epoch: 63 [15360/60000 (26%)]\tDiscriminator Loss: 0.480002\tGenerator Loss: 1.935808\n",
      "Train Epoch: 63 [16640/60000 (28%)]\tDiscriminator Loss: 0.550532\tGenerator Loss: 1.120315\n",
      "Train Epoch: 63 [17920/60000 (30%)]\tDiscriminator Loss: 0.518234\tGenerator Loss: 1.382477\n",
      "Train Epoch: 63 [19200/60000 (32%)]\tDiscriminator Loss: 0.437673\tGenerator Loss: 1.690057\n",
      "Train Epoch: 63 [20480/60000 (34%)]\tDiscriminator Loss: 0.551600\tGenerator Loss: 1.494976\n",
      "Train Epoch: 63 [21760/60000 (36%)]\tDiscriminator Loss: 0.458106\tGenerator Loss: 1.206637\n",
      "Train Epoch: 63 [23040/60000 (38%)]\tDiscriminator Loss: 0.561426\tGenerator Loss: 1.314015\n",
      "Train Epoch: 63 [24320/60000 (41%)]\tDiscriminator Loss: 0.488060\tGenerator Loss: 1.135228\n",
      "Train Epoch: 63 [25600/60000 (43%)]\tDiscriminator Loss: 0.455603\tGenerator Loss: 1.826449\n",
      "Train Epoch: 63 [26880/60000 (45%)]\tDiscriminator Loss: 0.526019\tGenerator Loss: 1.652459\n",
      "Train Epoch: 63 [28160/60000 (47%)]\tDiscriminator Loss: 0.514481\tGenerator Loss: 1.269176\n",
      "Train Epoch: 63 [29440/60000 (49%)]\tDiscriminator Loss: 0.497799\tGenerator Loss: 1.139049\n",
      "Train Epoch: 63 [30720/60000 (51%)]\tDiscriminator Loss: 0.525873\tGenerator Loss: 1.740673\n",
      "Train Epoch: 63 [32000/60000 (53%)]\tDiscriminator Loss: 0.495257\tGenerator Loss: 1.261395\n",
      "Train Epoch: 63 [33280/60000 (55%)]\tDiscriminator Loss: 0.489954\tGenerator Loss: 1.140932\n",
      "Train Epoch: 63 [34560/60000 (58%)]\tDiscriminator Loss: 0.507361\tGenerator Loss: 1.735463\n",
      "Train Epoch: 63 [35840/60000 (60%)]\tDiscriminator Loss: 0.480459\tGenerator Loss: 1.462754\n",
      "Train Epoch: 63 [37120/60000 (62%)]\tDiscriminator Loss: 0.515030\tGenerator Loss: 1.565676\n",
      "Train Epoch: 63 [38400/60000 (64%)]\tDiscriminator Loss: 0.531619\tGenerator Loss: 0.921869\n",
      "Train Epoch: 63 [39680/60000 (66%)]\tDiscriminator Loss: 0.457528\tGenerator Loss: 1.597022\n",
      "Train Epoch: 63 [40960/60000 (68%)]\tDiscriminator Loss: 0.540589\tGenerator Loss: 1.082086\n",
      "Train Epoch: 63 [42240/60000 (70%)]\tDiscriminator Loss: 0.502076\tGenerator Loss: 1.401267\n",
      "Train Epoch: 63 [43520/60000 (72%)]\tDiscriminator Loss: 0.517664\tGenerator Loss: 1.174254\n",
      "Train Epoch: 63 [44800/60000 (75%)]\tDiscriminator Loss: 0.529338\tGenerator Loss: 1.264854\n",
      "Train Epoch: 63 [46080/60000 (77%)]\tDiscriminator Loss: 0.521895\tGenerator Loss: 1.434673\n",
      "Train Epoch: 63 [47360/60000 (79%)]\tDiscriminator Loss: 0.498944\tGenerator Loss: 1.149606\n",
      "Train Epoch: 63 [48640/60000 (81%)]\tDiscriminator Loss: 0.520274\tGenerator Loss: 1.076890\n",
      "Train Epoch: 63 [49920/60000 (83%)]\tDiscriminator Loss: 0.514440\tGenerator Loss: 1.461320\n",
      "Train Epoch: 63 [51200/60000 (85%)]\tDiscriminator Loss: 0.460120\tGenerator Loss: 1.795784\n",
      "Train Epoch: 63 [52480/60000 (87%)]\tDiscriminator Loss: 0.463094\tGenerator Loss: 1.518401\n",
      "Train Epoch: 63 [53760/60000 (90%)]\tDiscriminator Loss: 0.496763\tGenerator Loss: 1.234676\n",
      "Train Epoch: 63 [55040/60000 (92%)]\tDiscriminator Loss: 0.462526\tGenerator Loss: 1.472880\n",
      "Train Epoch: 63 [56320/60000 (94%)]\tDiscriminator Loss: 0.570742\tGenerator Loss: 1.070444\n",
      "Train Epoch: 63 [57600/60000 (96%)]\tDiscriminator Loss: 0.434743\tGenerator Loss: 1.305459\n",
      "Train Epoch: 63 [58880/60000 (98%)]\tDiscriminator Loss: 0.548743\tGenerator Loss: 1.358162\n",
      "Train Epoch: 64 [0/60000 (0%)]\tDiscriminator Loss: 0.472880\tGenerator Loss: 1.099020\n",
      "Train Epoch: 64 [1280/60000 (2%)]\tDiscriminator Loss: 0.491525\tGenerator Loss: 1.212501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 64 [2560/60000 (4%)]\tDiscriminator Loss: 0.478386\tGenerator Loss: 1.364958\n",
      "Train Epoch: 64 [3840/60000 (6%)]\tDiscriminator Loss: 0.484969\tGenerator Loss: 1.084606\n",
      "Train Epoch: 64 [5120/60000 (9%)]\tDiscriminator Loss: 0.462068\tGenerator Loss: 1.426652\n",
      "Train Epoch: 64 [6400/60000 (11%)]\tDiscriminator Loss: 0.487789\tGenerator Loss: 1.385109\n",
      "Train Epoch: 64 [7680/60000 (13%)]\tDiscriminator Loss: 0.481795\tGenerator Loss: 1.157650\n",
      "Train Epoch: 64 [8960/60000 (15%)]\tDiscriminator Loss: 0.515230\tGenerator Loss: 1.605976\n",
      "Train Epoch: 64 [10240/60000 (17%)]\tDiscriminator Loss: 0.492251\tGenerator Loss: 0.962418\n",
      "Train Epoch: 64 [11520/60000 (19%)]\tDiscriminator Loss: 0.526146\tGenerator Loss: 1.496058\n",
      "Train Epoch: 64 [12800/60000 (21%)]\tDiscriminator Loss: 0.468875\tGenerator Loss: 1.408225\n",
      "Train Epoch: 64 [14080/60000 (23%)]\tDiscriminator Loss: 0.494708\tGenerator Loss: 1.135227\n",
      "Train Epoch: 64 [15360/60000 (26%)]\tDiscriminator Loss: 0.474907\tGenerator Loss: 1.446310\n",
      "Train Epoch: 64 [16640/60000 (28%)]\tDiscriminator Loss: 0.494237\tGenerator Loss: 1.582808\n",
      "Train Epoch: 64 [17920/60000 (30%)]\tDiscriminator Loss: 0.446572\tGenerator Loss: 1.238461\n",
      "Train Epoch: 64 [19200/60000 (32%)]\tDiscriminator Loss: 0.511490\tGenerator Loss: 1.485549\n",
      "Train Epoch: 64 [20480/60000 (34%)]\tDiscriminator Loss: 0.562239\tGenerator Loss: 0.946924\n",
      "Train Epoch: 64 [21760/60000 (36%)]\tDiscriminator Loss: 0.486162\tGenerator Loss: 1.339464\n",
      "Train Epoch: 64 [23040/60000 (38%)]\tDiscriminator Loss: 0.483431\tGenerator Loss: 1.568353\n",
      "Train Epoch: 64 [24320/60000 (41%)]\tDiscriminator Loss: 0.490798\tGenerator Loss: 1.322133\n",
      "Train Epoch: 64 [25600/60000 (43%)]\tDiscriminator Loss: 0.436592\tGenerator Loss: 1.585928\n",
      "Train Epoch: 64 [26880/60000 (45%)]\tDiscriminator Loss: 0.440975\tGenerator Loss: 1.409537\n",
      "Train Epoch: 64 [28160/60000 (47%)]\tDiscriminator Loss: 0.507099\tGenerator Loss: 1.654924\n",
      "Train Epoch: 64 [29440/60000 (49%)]\tDiscriminator Loss: 0.521985\tGenerator Loss: 1.336555\n",
      "Train Epoch: 64 [30720/60000 (51%)]\tDiscriminator Loss: 0.518479\tGenerator Loss: 1.114782\n",
      "Train Epoch: 64 [32000/60000 (53%)]\tDiscriminator Loss: 0.483323\tGenerator Loss: 1.305318\n",
      "Train Epoch: 64 [33280/60000 (55%)]\tDiscriminator Loss: 0.474500\tGenerator Loss: 1.288044\n",
      "Train Epoch: 64 [34560/60000 (58%)]\tDiscriminator Loss: 0.543444\tGenerator Loss: 1.696089\n",
      "Train Epoch: 64 [35840/60000 (60%)]\tDiscriminator Loss: 0.451383\tGenerator Loss: 1.676111\n",
      "Train Epoch: 64 [37120/60000 (62%)]\tDiscriminator Loss: 0.517268\tGenerator Loss: 1.063666\n",
      "Train Epoch: 64 [38400/60000 (64%)]\tDiscriminator Loss: 0.538230\tGenerator Loss: 1.572202\n",
      "Train Epoch: 64 [39680/60000 (66%)]\tDiscriminator Loss: 0.502133\tGenerator Loss: 1.852319\n",
      "Train Epoch: 64 [40960/60000 (68%)]\tDiscriminator Loss: 0.521803\tGenerator Loss: 1.093676\n",
      "Train Epoch: 64 [42240/60000 (70%)]\tDiscriminator Loss: 0.553627\tGenerator Loss: 1.153219\n",
      "Train Epoch: 64 [43520/60000 (72%)]\tDiscriminator Loss: 0.499075\tGenerator Loss: 1.180023\n",
      "Train Epoch: 64 [44800/60000 (75%)]\tDiscriminator Loss: 0.495468\tGenerator Loss: 1.321520\n",
      "Train Epoch: 64 [46080/60000 (77%)]\tDiscriminator Loss: 0.488023\tGenerator Loss: 1.455455\n",
      "Train Epoch: 64 [47360/60000 (79%)]\tDiscriminator Loss: 0.548592\tGenerator Loss: 1.175170\n",
      "Train Epoch: 64 [48640/60000 (81%)]\tDiscriminator Loss: 0.528499\tGenerator Loss: 1.529878\n",
      "Train Epoch: 64 [49920/60000 (83%)]\tDiscriminator Loss: 0.474748\tGenerator Loss: 1.441873\n",
      "Train Epoch: 64 [51200/60000 (85%)]\tDiscriminator Loss: 0.505350\tGenerator Loss: 1.044160\n",
      "Train Epoch: 64 [52480/60000 (87%)]\tDiscriminator Loss: 0.498776\tGenerator Loss: 1.343149\n",
      "Train Epoch: 64 [53760/60000 (90%)]\tDiscriminator Loss: 0.492796\tGenerator Loss: 1.344560\n",
      "Train Epoch: 64 [55040/60000 (92%)]\tDiscriminator Loss: 0.497849\tGenerator Loss: 1.087476\n",
      "Train Epoch: 64 [56320/60000 (94%)]\tDiscriminator Loss: 0.493741\tGenerator Loss: 1.614541\n",
      "Train Epoch: 64 [57600/60000 (96%)]\tDiscriminator Loss: 0.490460\tGenerator Loss: 1.154586\n",
      "Train Epoch: 64 [58880/60000 (98%)]\tDiscriminator Loss: 0.471374\tGenerator Loss: 1.652618\n",
      "Train Epoch: 65 [0/60000 (0%)]\tDiscriminator Loss: 0.507787\tGenerator Loss: 1.168914\n",
      "Train Epoch: 65 [1280/60000 (2%)]\tDiscriminator Loss: 0.487943\tGenerator Loss: 1.108205\n",
      "Train Epoch: 65 [2560/60000 (4%)]\tDiscriminator Loss: 0.521262\tGenerator Loss: 1.507884\n",
      "Train Epoch: 65 [3840/60000 (6%)]\tDiscriminator Loss: 0.515451\tGenerator Loss: 1.035969\n",
      "Train Epoch: 65 [5120/60000 (9%)]\tDiscriminator Loss: 0.509969\tGenerator Loss: 1.267858\n",
      "Train Epoch: 65 [6400/60000 (11%)]\tDiscriminator Loss: 0.603011\tGenerator Loss: 1.154160\n",
      "Train Epoch: 65 [7680/60000 (13%)]\tDiscriminator Loss: 0.457874\tGenerator Loss: 1.505844\n",
      "Train Epoch: 65 [8960/60000 (15%)]\tDiscriminator Loss: 0.483600\tGenerator Loss: 1.393017\n",
      "Train Epoch: 65 [10240/60000 (17%)]\tDiscriminator Loss: 0.496821\tGenerator Loss: 1.570196\n",
      "Train Epoch: 65 [11520/60000 (19%)]\tDiscriminator Loss: 0.535217\tGenerator Loss: 1.639664\n",
      "Train Epoch: 65 [12800/60000 (21%)]\tDiscriminator Loss: 0.473481\tGenerator Loss: 1.235851\n",
      "Train Epoch: 65 [14080/60000 (23%)]\tDiscriminator Loss: 0.494732\tGenerator Loss: 1.533438\n",
      "Train Epoch: 65 [15360/60000 (26%)]\tDiscriminator Loss: 0.519547\tGenerator Loss: 1.027186\n",
      "Train Epoch: 65 [16640/60000 (28%)]\tDiscriminator Loss: 0.488787\tGenerator Loss: 1.334804\n",
      "Train Epoch: 65 [17920/60000 (30%)]\tDiscriminator Loss: 0.570788\tGenerator Loss: 1.794752\n",
      "Train Epoch: 65 [19200/60000 (32%)]\tDiscriminator Loss: 0.491152\tGenerator Loss: 1.270383\n",
      "Train Epoch: 65 [20480/60000 (34%)]\tDiscriminator Loss: 0.497106\tGenerator Loss: 1.232733\n",
      "Train Epoch: 65 [21760/60000 (36%)]\tDiscriminator Loss: 0.445922\tGenerator Loss: 1.303542\n",
      "Train Epoch: 65 [23040/60000 (38%)]\tDiscriminator Loss: 0.488897\tGenerator Loss: 1.215440\n",
      "Train Epoch: 65 [24320/60000 (41%)]\tDiscriminator Loss: 0.480153\tGenerator Loss: 1.471658\n",
      "Train Epoch: 65 [25600/60000 (43%)]\tDiscriminator Loss: 0.495289\tGenerator Loss: 1.351422\n",
      "Train Epoch: 65 [26880/60000 (45%)]\tDiscriminator Loss: 0.502001\tGenerator Loss: 1.335956\n",
      "Train Epoch: 65 [28160/60000 (47%)]\tDiscriminator Loss: 0.424624\tGenerator Loss: 1.313400\n",
      "Train Epoch: 65 [29440/60000 (49%)]\tDiscriminator Loss: 0.553063\tGenerator Loss: 1.024978\n",
      "Train Epoch: 65 [30720/60000 (51%)]\tDiscriminator Loss: 0.583009\tGenerator Loss: 1.554211\n",
      "Train Epoch: 65 [32000/60000 (53%)]\tDiscriminator Loss: 0.499099\tGenerator Loss: 1.654547\n",
      "Train Epoch: 65 [33280/60000 (55%)]\tDiscriminator Loss: 0.467579\tGenerator Loss: 1.503720\n",
      "Train Epoch: 65 [34560/60000 (58%)]\tDiscriminator Loss: 0.438795\tGenerator Loss: 1.337487\n",
      "Train Epoch: 65 [35840/60000 (60%)]\tDiscriminator Loss: 0.600952\tGenerator Loss: 1.416394\n",
      "Train Epoch: 65 [37120/60000 (62%)]\tDiscriminator Loss: 0.492241\tGenerator Loss: 1.390895\n",
      "Train Epoch: 65 [38400/60000 (64%)]\tDiscriminator Loss: 0.538106\tGenerator Loss: 0.940255\n",
      "Train Epoch: 65 [39680/60000 (66%)]\tDiscriminator Loss: 0.453484\tGenerator Loss: 1.415380\n",
      "Train Epoch: 65 [40960/60000 (68%)]\tDiscriminator Loss: 0.521543\tGenerator Loss: 1.182571\n",
      "Train Epoch: 65 [42240/60000 (70%)]\tDiscriminator Loss: 0.604111\tGenerator Loss: 0.841499\n",
      "Train Epoch: 65 [43520/60000 (72%)]\tDiscriminator Loss: 0.518025\tGenerator Loss: 1.047328\n",
      "Train Epoch: 65 [44800/60000 (75%)]\tDiscriminator Loss: 0.496345\tGenerator Loss: 1.622450\n",
      "Train Epoch: 65 [46080/60000 (77%)]\tDiscriminator Loss: 0.472673\tGenerator Loss: 1.404075\n",
      "Train Epoch: 65 [47360/60000 (79%)]\tDiscriminator Loss: 0.536466\tGenerator Loss: 1.184017\n",
      "Train Epoch: 65 [48640/60000 (81%)]\tDiscriminator Loss: 0.555221\tGenerator Loss: 0.828095\n",
      "Train Epoch: 65 [49920/60000 (83%)]\tDiscriminator Loss: 0.469878\tGenerator Loss: 1.327805\n",
      "Train Epoch: 65 [51200/60000 (85%)]\tDiscriminator Loss: 0.471991\tGenerator Loss: 1.203843\n",
      "Train Epoch: 65 [52480/60000 (87%)]\tDiscriminator Loss: 0.469627\tGenerator Loss: 1.326140\n",
      "Train Epoch: 65 [53760/60000 (90%)]\tDiscriminator Loss: 0.528967\tGenerator Loss: 1.140188\n",
      "Train Epoch: 65 [55040/60000 (92%)]\tDiscriminator Loss: 0.508327\tGenerator Loss: 1.613250\n",
      "Train Epoch: 65 [56320/60000 (94%)]\tDiscriminator Loss: 0.540376\tGenerator Loss: 1.188276\n",
      "Train Epoch: 65 [57600/60000 (96%)]\tDiscriminator Loss: 0.550134\tGenerator Loss: 1.321002\n",
      "Train Epoch: 65 [58880/60000 (98%)]\tDiscriminator Loss: 0.473539\tGenerator Loss: 1.340108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 66 [0/60000 (0%)]\tDiscriminator Loss: 0.534977\tGenerator Loss: 1.178241\n",
      "Train Epoch: 66 [1280/60000 (2%)]\tDiscriminator Loss: 0.461996\tGenerator Loss: 1.505393\n",
      "Train Epoch: 66 [2560/60000 (4%)]\tDiscriminator Loss: 0.513310\tGenerator Loss: 1.481196\n",
      "Train Epoch: 66 [3840/60000 (6%)]\tDiscriminator Loss: 0.475592\tGenerator Loss: 1.557942\n",
      "Train Epoch: 66 [5120/60000 (9%)]\tDiscriminator Loss: 0.530083\tGenerator Loss: 1.632293\n",
      "Train Epoch: 66 [6400/60000 (11%)]\tDiscriminator Loss: 0.533882\tGenerator Loss: 1.386034\n",
      "Train Epoch: 66 [7680/60000 (13%)]\tDiscriminator Loss: 0.471923\tGenerator Loss: 1.248389\n",
      "Train Epoch: 66 [8960/60000 (15%)]\tDiscriminator Loss: 0.489587\tGenerator Loss: 1.044244\n",
      "Train Epoch: 66 [10240/60000 (17%)]\tDiscriminator Loss: 0.467368\tGenerator Loss: 1.379207\n",
      "Train Epoch: 66 [11520/60000 (19%)]\tDiscriminator Loss: 0.519368\tGenerator Loss: 1.374430\n",
      "Train Epoch: 66 [12800/60000 (21%)]\tDiscriminator Loss: 0.546216\tGenerator Loss: 1.185904\n",
      "Train Epoch: 66 [14080/60000 (23%)]\tDiscriminator Loss: 0.560685\tGenerator Loss: 0.997051\n",
      "Train Epoch: 66 [15360/60000 (26%)]\tDiscriminator Loss: 0.490090\tGenerator Loss: 1.459991\n",
      "Train Epoch: 66 [16640/60000 (28%)]\tDiscriminator Loss: 0.430301\tGenerator Loss: 1.462761\n",
      "Train Epoch: 66 [17920/60000 (30%)]\tDiscriminator Loss: 0.512943\tGenerator Loss: 1.507239\n",
      "Train Epoch: 66 [19200/60000 (32%)]\tDiscriminator Loss: 0.451997\tGenerator Loss: 1.109508\n",
      "Train Epoch: 66 [20480/60000 (34%)]\tDiscriminator Loss: 0.523559\tGenerator Loss: 0.883400\n",
      "Train Epoch: 66 [21760/60000 (36%)]\tDiscriminator Loss: 0.518290\tGenerator Loss: 1.526880\n",
      "Train Epoch: 66 [23040/60000 (38%)]\tDiscriminator Loss: 0.516040\tGenerator Loss: 1.242205\n",
      "Train Epoch: 66 [24320/60000 (41%)]\tDiscriminator Loss: 0.547304\tGenerator Loss: 1.058921\n",
      "Train Epoch: 66 [25600/60000 (43%)]\tDiscriminator Loss: 0.515033\tGenerator Loss: 1.261624\n",
      "Train Epoch: 66 [26880/60000 (45%)]\tDiscriminator Loss: 0.498292\tGenerator Loss: 1.580854\n",
      "Train Epoch: 66 [28160/60000 (47%)]\tDiscriminator Loss: 0.535614\tGenerator Loss: 1.145748\n",
      "Train Epoch: 66 [29440/60000 (49%)]\tDiscriminator Loss: 0.467949\tGenerator Loss: 1.513355\n",
      "Train Epoch: 66 [30720/60000 (51%)]\tDiscriminator Loss: 0.545298\tGenerator Loss: 1.155086\n",
      "Train Epoch: 66 [32000/60000 (53%)]\tDiscriminator Loss: 0.447977\tGenerator Loss: 1.323066\n",
      "Train Epoch: 66 [33280/60000 (55%)]\tDiscriminator Loss: 0.521557\tGenerator Loss: 1.248686\n",
      "Train Epoch: 66 [34560/60000 (58%)]\tDiscriminator Loss: 0.494877\tGenerator Loss: 1.713146\n",
      "Train Epoch: 66 [35840/60000 (60%)]\tDiscriminator Loss: 0.468666\tGenerator Loss: 1.479213\n",
      "Train Epoch: 66 [37120/60000 (62%)]\tDiscriminator Loss: 0.499558\tGenerator Loss: 1.268852\n",
      "Train Epoch: 66 [38400/60000 (64%)]\tDiscriminator Loss: 0.508312\tGenerator Loss: 1.012189\n",
      "Train Epoch: 66 [39680/60000 (66%)]\tDiscriminator Loss: 0.528598\tGenerator Loss: 1.107459\n",
      "Train Epoch: 66 [40960/60000 (68%)]\tDiscriminator Loss: 0.512284\tGenerator Loss: 1.404868\n",
      "Train Epoch: 66 [42240/60000 (70%)]\tDiscriminator Loss: 0.612620\tGenerator Loss: 1.034101\n",
      "Train Epoch: 66 [43520/60000 (72%)]\tDiscriminator Loss: 0.494707\tGenerator Loss: 1.465564\n",
      "Train Epoch: 66 [44800/60000 (75%)]\tDiscriminator Loss: 0.561605\tGenerator Loss: 1.265510\n",
      "Train Epoch: 66 [46080/60000 (77%)]\tDiscriminator Loss: 0.445357\tGenerator Loss: 1.323501\n",
      "Train Epoch: 66 [47360/60000 (79%)]\tDiscriminator Loss: 0.581730\tGenerator Loss: 0.952302\n",
      "Train Epoch: 66 [48640/60000 (81%)]\tDiscriminator Loss: 0.511964\tGenerator Loss: 1.205581\n",
      "Train Epoch: 66 [49920/60000 (83%)]\tDiscriminator Loss: 0.517826\tGenerator Loss: 1.336864\n",
      "Train Epoch: 66 [51200/60000 (85%)]\tDiscriminator Loss: 0.461508\tGenerator Loss: 1.315288\n",
      "Train Epoch: 66 [52480/60000 (87%)]\tDiscriminator Loss: 0.445763\tGenerator Loss: 1.396592\n",
      "Train Epoch: 66 [53760/60000 (90%)]\tDiscriminator Loss: 0.508682\tGenerator Loss: 1.109499\n",
      "Train Epoch: 66 [55040/60000 (92%)]\tDiscriminator Loss: 0.447960\tGenerator Loss: 1.158404\n",
      "Train Epoch: 66 [56320/60000 (94%)]\tDiscriminator Loss: 0.494971\tGenerator Loss: 1.322821\n",
      "Train Epoch: 66 [57600/60000 (96%)]\tDiscriminator Loss: 0.527819\tGenerator Loss: 1.222002\n",
      "Train Epoch: 66 [58880/60000 (98%)]\tDiscriminator Loss: 0.485763\tGenerator Loss: 1.137927\n",
      "Train Epoch: 67 [0/60000 (0%)]\tDiscriminator Loss: 0.449933\tGenerator Loss: 1.543469\n",
      "Train Epoch: 67 [1280/60000 (2%)]\tDiscriminator Loss: 0.499893\tGenerator Loss: 1.541321\n",
      "Train Epoch: 67 [2560/60000 (4%)]\tDiscriminator Loss: 0.440037\tGenerator Loss: 1.146759\n",
      "Train Epoch: 67 [3840/60000 (6%)]\tDiscriminator Loss: 0.449333\tGenerator Loss: 1.477704\n",
      "Train Epoch: 67 [5120/60000 (9%)]\tDiscriminator Loss: 0.542865\tGenerator Loss: 1.331034\n",
      "Train Epoch: 67 [6400/60000 (11%)]\tDiscriminator Loss: 0.478095\tGenerator Loss: 1.507325\n",
      "Train Epoch: 67 [7680/60000 (13%)]\tDiscriminator Loss: 0.553252\tGenerator Loss: 1.775212\n",
      "Train Epoch: 67 [8960/60000 (15%)]\tDiscriminator Loss: 0.452955\tGenerator Loss: 1.843184\n",
      "Train Epoch: 67 [10240/60000 (17%)]\tDiscriminator Loss: 0.479400\tGenerator Loss: 1.231114\n",
      "Train Epoch: 67 [11520/60000 (19%)]\tDiscriminator Loss: 0.516806\tGenerator Loss: 1.053343\n",
      "Train Epoch: 67 [12800/60000 (21%)]\tDiscriminator Loss: 0.468411\tGenerator Loss: 1.125617\n",
      "Train Epoch: 67 [14080/60000 (23%)]\tDiscriminator Loss: 0.554840\tGenerator Loss: 0.924215\n",
      "Train Epoch: 67 [15360/60000 (26%)]\tDiscriminator Loss: 0.465090\tGenerator Loss: 1.074618\n",
      "Train Epoch: 67 [16640/60000 (28%)]\tDiscriminator Loss: 0.502363\tGenerator Loss: 1.302860\n",
      "Train Epoch: 67 [17920/60000 (30%)]\tDiscriminator Loss: 0.512104\tGenerator Loss: 1.447005\n",
      "Train Epoch: 67 [19200/60000 (32%)]\tDiscriminator Loss: 0.549635\tGenerator Loss: 1.606780\n",
      "Train Epoch: 67 [20480/60000 (34%)]\tDiscriminator Loss: 0.468484\tGenerator Loss: 1.379938\n",
      "Train Epoch: 67 [21760/60000 (36%)]\tDiscriminator Loss: 0.520542\tGenerator Loss: 1.442404\n",
      "Train Epoch: 67 [23040/60000 (38%)]\tDiscriminator Loss: 0.519716\tGenerator Loss: 1.828563\n",
      "Train Epoch: 67 [24320/60000 (41%)]\tDiscriminator Loss: 0.538466\tGenerator Loss: 1.422759\n",
      "Train Epoch: 67 [25600/60000 (43%)]\tDiscriminator Loss: 0.468814\tGenerator Loss: 1.321241\n",
      "Train Epoch: 67 [26880/60000 (45%)]\tDiscriminator Loss: 0.478741\tGenerator Loss: 1.537486\n",
      "Train Epoch: 67 [28160/60000 (47%)]\tDiscriminator Loss: 0.532746\tGenerator Loss: 1.944969\n",
      "Train Epoch: 67 [29440/60000 (49%)]\tDiscriminator Loss: 0.519212\tGenerator Loss: 1.340048\n",
      "Train Epoch: 67 [30720/60000 (51%)]\tDiscriminator Loss: 0.524254\tGenerator Loss: 1.290601\n",
      "Train Epoch: 67 [32000/60000 (53%)]\tDiscriminator Loss: 0.513189\tGenerator Loss: 1.528456\n",
      "Train Epoch: 67 [33280/60000 (55%)]\tDiscriminator Loss: 0.424378\tGenerator Loss: 1.478088\n",
      "Train Epoch: 67 [34560/60000 (58%)]\tDiscriminator Loss: 0.439140\tGenerator Loss: 1.492352\n",
      "Train Epoch: 67 [35840/60000 (60%)]\tDiscriminator Loss: 0.533055\tGenerator Loss: 1.149816\n",
      "Train Epoch: 67 [37120/60000 (62%)]\tDiscriminator Loss: 0.477676\tGenerator Loss: 1.536304\n",
      "Train Epoch: 67 [38400/60000 (64%)]\tDiscriminator Loss: 0.546575\tGenerator Loss: 1.561675\n",
      "Train Epoch: 67 [39680/60000 (66%)]\tDiscriminator Loss: 0.497671\tGenerator Loss: 1.527652\n",
      "Train Epoch: 67 [40960/60000 (68%)]\tDiscriminator Loss: 0.557085\tGenerator Loss: 1.261995\n",
      "Train Epoch: 67 [42240/60000 (70%)]\tDiscriminator Loss: 0.458895\tGenerator Loss: 1.459701\n",
      "Train Epoch: 67 [43520/60000 (72%)]\tDiscriminator Loss: 0.481454\tGenerator Loss: 1.492475\n",
      "Train Epoch: 67 [44800/60000 (75%)]\tDiscriminator Loss: 0.512643\tGenerator Loss: 1.122863\n",
      "Train Epoch: 67 [46080/60000 (77%)]\tDiscriminator Loss: 0.517191\tGenerator Loss: 1.303794\n",
      "Train Epoch: 67 [47360/60000 (79%)]\tDiscriminator Loss: 0.547113\tGenerator Loss: 0.978999\n",
      "Train Epoch: 67 [48640/60000 (81%)]\tDiscriminator Loss: 0.523224\tGenerator Loss: 1.426497\n",
      "Train Epoch: 67 [49920/60000 (83%)]\tDiscriminator Loss: 0.480481\tGenerator Loss: 1.576119\n",
      "Train Epoch: 67 [51200/60000 (85%)]\tDiscriminator Loss: 0.447485\tGenerator Loss: 1.307774\n",
      "Train Epoch: 67 [52480/60000 (87%)]\tDiscriminator Loss: 0.583323\tGenerator Loss: 1.013673\n",
      "Train Epoch: 67 [53760/60000 (90%)]\tDiscriminator Loss: 0.496877\tGenerator Loss: 1.340995\n",
      "Train Epoch: 67 [55040/60000 (92%)]\tDiscriminator Loss: 0.512829\tGenerator Loss: 1.107139\n",
      "Train Epoch: 67 [56320/60000 (94%)]\tDiscriminator Loss: 0.489291\tGenerator Loss: 1.584387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 67 [57600/60000 (96%)]\tDiscriminator Loss: 0.545406\tGenerator Loss: 1.133862\n",
      "Train Epoch: 67 [58880/60000 (98%)]\tDiscriminator Loss: 0.527273\tGenerator Loss: 1.762839\n",
      "Train Epoch: 68 [0/60000 (0%)]\tDiscriminator Loss: 0.483870\tGenerator Loss: 1.399086\n",
      "Train Epoch: 68 [1280/60000 (2%)]\tDiscriminator Loss: 0.482657\tGenerator Loss: 1.746574\n",
      "Train Epoch: 68 [2560/60000 (4%)]\tDiscriminator Loss: 0.429618\tGenerator Loss: 1.519043\n",
      "Train Epoch: 68 [3840/60000 (6%)]\tDiscriminator Loss: 0.529103\tGenerator Loss: 1.201527\n",
      "Train Epoch: 68 [5120/60000 (9%)]\tDiscriminator Loss: 0.471450\tGenerator Loss: 1.235310\n",
      "Train Epoch: 68 [6400/60000 (11%)]\tDiscriminator Loss: 0.548054\tGenerator Loss: 1.291709\n",
      "Train Epoch: 68 [7680/60000 (13%)]\tDiscriminator Loss: 0.474937\tGenerator Loss: 1.415556\n",
      "Train Epoch: 68 [8960/60000 (15%)]\tDiscriminator Loss: 0.544543\tGenerator Loss: 1.328056\n",
      "Train Epoch: 68 [10240/60000 (17%)]\tDiscriminator Loss: 0.510531\tGenerator Loss: 1.533350\n",
      "Train Epoch: 68 [11520/60000 (19%)]\tDiscriminator Loss: 0.505543\tGenerator Loss: 1.579647\n",
      "Train Epoch: 68 [12800/60000 (21%)]\tDiscriminator Loss: 0.543112\tGenerator Loss: 1.269877\n",
      "Train Epoch: 68 [14080/60000 (23%)]\tDiscriminator Loss: 0.512554\tGenerator Loss: 1.118715\n",
      "Train Epoch: 68 [15360/60000 (26%)]\tDiscriminator Loss: 0.456463\tGenerator Loss: 1.626837\n",
      "Train Epoch: 68 [16640/60000 (28%)]\tDiscriminator Loss: 0.539015\tGenerator Loss: 1.299571\n",
      "Train Epoch: 68 [17920/60000 (30%)]\tDiscriminator Loss: 0.509889\tGenerator Loss: 1.255173\n",
      "Train Epoch: 68 [19200/60000 (32%)]\tDiscriminator Loss: 0.538721\tGenerator Loss: 1.133567\n",
      "Train Epoch: 68 [20480/60000 (34%)]\tDiscriminator Loss: 0.455959\tGenerator Loss: 1.481198\n",
      "Train Epoch: 68 [21760/60000 (36%)]\tDiscriminator Loss: 0.463417\tGenerator Loss: 1.379489\n",
      "Train Epoch: 68 [23040/60000 (38%)]\tDiscriminator Loss: 0.572895\tGenerator Loss: 1.649997\n",
      "Train Epoch: 68 [24320/60000 (41%)]\tDiscriminator Loss: 0.494624\tGenerator Loss: 1.221885\n",
      "Train Epoch: 68 [25600/60000 (43%)]\tDiscriminator Loss: 0.588036\tGenerator Loss: 1.136281\n",
      "Train Epoch: 68 [26880/60000 (45%)]\tDiscriminator Loss: 0.478454\tGenerator Loss: 1.619533\n",
      "Train Epoch: 68 [28160/60000 (47%)]\tDiscriminator Loss: 0.518451\tGenerator Loss: 1.523898\n",
      "Train Epoch: 68 [29440/60000 (49%)]\tDiscriminator Loss: 0.437909\tGenerator Loss: 1.522128\n",
      "Train Epoch: 68 [30720/60000 (51%)]\tDiscriminator Loss: 0.536162\tGenerator Loss: 1.212842\n",
      "Train Epoch: 68 [32000/60000 (53%)]\tDiscriminator Loss: 0.521502\tGenerator Loss: 1.335551\n",
      "Train Epoch: 68 [33280/60000 (55%)]\tDiscriminator Loss: 0.482028\tGenerator Loss: 1.671010\n",
      "Train Epoch: 68 [34560/60000 (58%)]\tDiscriminator Loss: 0.610833\tGenerator Loss: 0.828151\n",
      "Train Epoch: 68 [35840/60000 (60%)]\tDiscriminator Loss: 0.483149\tGenerator Loss: 1.381876\n",
      "Train Epoch: 68 [37120/60000 (62%)]\tDiscriminator Loss: 0.517683\tGenerator Loss: 1.938704\n",
      "Train Epoch: 68 [38400/60000 (64%)]\tDiscriminator Loss: 0.459341\tGenerator Loss: 1.428224\n",
      "Train Epoch: 68 [39680/60000 (66%)]\tDiscriminator Loss: 0.462601\tGenerator Loss: 1.343120\n",
      "Train Epoch: 68 [40960/60000 (68%)]\tDiscriminator Loss: 0.430318\tGenerator Loss: 1.488793\n",
      "Train Epoch: 68 [42240/60000 (70%)]\tDiscriminator Loss: 0.515341\tGenerator Loss: 1.533317\n",
      "Train Epoch: 68 [43520/60000 (72%)]\tDiscriminator Loss: 0.620613\tGenerator Loss: 1.766158\n",
      "Train Epoch: 68 [44800/60000 (75%)]\tDiscriminator Loss: 0.468563\tGenerator Loss: 1.571113\n",
      "Train Epoch: 68 [46080/60000 (77%)]\tDiscriminator Loss: 0.482653\tGenerator Loss: 1.679103\n",
      "Train Epoch: 68 [47360/60000 (79%)]\tDiscriminator Loss: 0.481374\tGenerator Loss: 1.501965\n",
      "Train Epoch: 68 [48640/60000 (81%)]\tDiscriminator Loss: 0.483267\tGenerator Loss: 1.277702\n",
      "Train Epoch: 68 [49920/60000 (83%)]\tDiscriminator Loss: 0.529934\tGenerator Loss: 1.105841\n",
      "Train Epoch: 68 [51200/60000 (85%)]\tDiscriminator Loss: 0.481152\tGenerator Loss: 1.328475\n",
      "Train Epoch: 68 [52480/60000 (87%)]\tDiscriminator Loss: 0.484148\tGenerator Loss: 1.214630\n",
      "Train Epoch: 68 [53760/60000 (90%)]\tDiscriminator Loss: 0.512931\tGenerator Loss: 1.459917\n",
      "Train Epoch: 68 [55040/60000 (92%)]\tDiscriminator Loss: 0.482862\tGenerator Loss: 1.716814\n",
      "Train Epoch: 68 [56320/60000 (94%)]\tDiscriminator Loss: 0.546766\tGenerator Loss: 1.474573\n",
      "Train Epoch: 68 [57600/60000 (96%)]\tDiscriminator Loss: 0.510872\tGenerator Loss: 1.488990\n",
      "Train Epoch: 68 [58880/60000 (98%)]\tDiscriminator Loss: 0.544015\tGenerator Loss: 1.160159\n",
      "Train Epoch: 69 [0/60000 (0%)]\tDiscriminator Loss: 0.497706\tGenerator Loss: 1.752428\n",
      "Train Epoch: 69 [1280/60000 (2%)]\tDiscriminator Loss: 0.499026\tGenerator Loss: 1.209750\n",
      "Train Epoch: 69 [2560/60000 (4%)]\tDiscriminator Loss: 0.471943\tGenerator Loss: 1.360129\n",
      "Train Epoch: 69 [3840/60000 (6%)]\tDiscriminator Loss: 0.515378\tGenerator Loss: 1.200050\n",
      "Train Epoch: 69 [5120/60000 (9%)]\tDiscriminator Loss: 0.529225\tGenerator Loss: 1.051277\n",
      "Train Epoch: 69 [6400/60000 (11%)]\tDiscriminator Loss: 0.527696\tGenerator Loss: 1.813761\n",
      "Train Epoch: 69 [7680/60000 (13%)]\tDiscriminator Loss: 0.521338\tGenerator Loss: 1.098076\n",
      "Train Epoch: 69 [8960/60000 (15%)]\tDiscriminator Loss: 0.480834\tGenerator Loss: 1.340318\n",
      "Train Epoch: 69 [10240/60000 (17%)]\tDiscriminator Loss: 0.445622\tGenerator Loss: 1.295229\n",
      "Train Epoch: 69 [11520/60000 (19%)]\tDiscriminator Loss: 0.475516\tGenerator Loss: 1.786037\n",
      "Train Epoch: 69 [12800/60000 (21%)]\tDiscriminator Loss: 0.417809\tGenerator Loss: 1.560513\n",
      "Train Epoch: 69 [14080/60000 (23%)]\tDiscriminator Loss: 0.455746\tGenerator Loss: 1.316791\n",
      "Train Epoch: 69 [15360/60000 (26%)]\tDiscriminator Loss: 0.493381\tGenerator Loss: 1.445596\n",
      "Train Epoch: 69 [16640/60000 (28%)]\tDiscriminator Loss: 0.485405\tGenerator Loss: 1.420261\n",
      "Train Epoch: 69 [17920/60000 (30%)]\tDiscriminator Loss: 0.507080\tGenerator Loss: 1.370742\n",
      "Train Epoch: 69 [19200/60000 (32%)]\tDiscriminator Loss: 0.462384\tGenerator Loss: 1.291710\n",
      "Train Epoch: 69 [20480/60000 (34%)]\tDiscriminator Loss: 0.488379\tGenerator Loss: 1.247871\n",
      "Train Epoch: 69 [21760/60000 (36%)]\tDiscriminator Loss: 0.484913\tGenerator Loss: 1.303285\n",
      "Train Epoch: 69 [23040/60000 (38%)]\tDiscriminator Loss: 0.520360\tGenerator Loss: 1.445052\n",
      "Train Epoch: 69 [24320/60000 (41%)]\tDiscriminator Loss: 0.477779\tGenerator Loss: 1.424728\n",
      "Train Epoch: 69 [25600/60000 (43%)]\tDiscriminator Loss: 0.500885\tGenerator Loss: 1.366239\n",
      "Train Epoch: 69 [26880/60000 (45%)]\tDiscriminator Loss: 0.494068\tGenerator Loss: 1.099818\n",
      "Train Epoch: 69 [28160/60000 (47%)]\tDiscriminator Loss: 0.518059\tGenerator Loss: 1.399465\n",
      "Train Epoch: 69 [29440/60000 (49%)]\tDiscriminator Loss: 0.572540\tGenerator Loss: 0.917457\n",
      "Train Epoch: 69 [30720/60000 (51%)]\tDiscriminator Loss: 0.527810\tGenerator Loss: 1.096578\n",
      "Train Epoch: 69 [32000/60000 (53%)]\tDiscriminator Loss: 0.471927\tGenerator Loss: 1.392203\n",
      "Train Epoch: 69 [33280/60000 (55%)]\tDiscriminator Loss: 0.464059\tGenerator Loss: 1.654444\n",
      "Train Epoch: 69 [34560/60000 (58%)]\tDiscriminator Loss: 0.497632\tGenerator Loss: 1.721589\n",
      "Train Epoch: 69 [35840/60000 (60%)]\tDiscriminator Loss: 0.507486\tGenerator Loss: 1.034752\n",
      "Train Epoch: 69 [37120/60000 (62%)]\tDiscriminator Loss: 0.482848\tGenerator Loss: 1.192522\n",
      "Train Epoch: 69 [38400/60000 (64%)]\tDiscriminator Loss: 0.523348\tGenerator Loss: 1.204451\n",
      "Train Epoch: 69 [39680/60000 (66%)]\tDiscriminator Loss: 0.544662\tGenerator Loss: 1.256171\n",
      "Train Epoch: 69 [40960/60000 (68%)]\tDiscriminator Loss: 0.501652\tGenerator Loss: 1.443533\n",
      "Train Epoch: 69 [42240/60000 (70%)]\tDiscriminator Loss: 0.556369\tGenerator Loss: 1.163665\n",
      "Train Epoch: 69 [43520/60000 (72%)]\tDiscriminator Loss: 0.423146\tGenerator Loss: 1.441531\n",
      "Train Epoch: 69 [44800/60000 (75%)]\tDiscriminator Loss: 0.536968\tGenerator Loss: 1.179833\n",
      "Train Epoch: 69 [46080/60000 (77%)]\tDiscriminator Loss: 0.503578\tGenerator Loss: 1.204844\n",
      "Train Epoch: 69 [47360/60000 (79%)]\tDiscriminator Loss: 0.513865\tGenerator Loss: 1.316674\n",
      "Train Epoch: 69 [48640/60000 (81%)]\tDiscriminator Loss: 0.529382\tGenerator Loss: 1.386913\n",
      "Train Epoch: 69 [49920/60000 (83%)]\tDiscriminator Loss: 0.523612\tGenerator Loss: 1.334710\n",
      "Train Epoch: 69 [51200/60000 (85%)]\tDiscriminator Loss: 0.621125\tGenerator Loss: 0.666113\n",
      "Train Epoch: 69 [52480/60000 (87%)]\tDiscriminator Loss: 0.487483\tGenerator Loss: 1.235419\n",
      "Train Epoch: 69 [53760/60000 (90%)]\tDiscriminator Loss: 0.520250\tGenerator Loss: 1.521892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 69 [55040/60000 (92%)]\tDiscriminator Loss: 0.461144\tGenerator Loss: 1.564489\n",
      "Train Epoch: 69 [56320/60000 (94%)]\tDiscriminator Loss: 0.503535\tGenerator Loss: 1.066542\n",
      "Train Epoch: 69 [57600/60000 (96%)]\tDiscriminator Loss: 0.548192\tGenerator Loss: 1.383015\n",
      "Train Epoch: 69 [58880/60000 (98%)]\tDiscriminator Loss: 0.533181\tGenerator Loss: 1.680405\n",
      "Train Epoch: 70 [0/60000 (0%)]\tDiscriminator Loss: 0.505196\tGenerator Loss: 0.990762\n",
      "Train Epoch: 70 [1280/60000 (2%)]\tDiscriminator Loss: 0.450411\tGenerator Loss: 1.761992\n",
      "Train Epoch: 70 [2560/60000 (4%)]\tDiscriminator Loss: 0.490321\tGenerator Loss: 1.674571\n",
      "Train Epoch: 70 [3840/60000 (6%)]\tDiscriminator Loss: 0.447983\tGenerator Loss: 1.473768\n",
      "Train Epoch: 70 [5120/60000 (9%)]\tDiscriminator Loss: 0.527207\tGenerator Loss: 1.504292\n",
      "Train Epoch: 70 [6400/60000 (11%)]\tDiscriminator Loss: 0.515362\tGenerator Loss: 1.212027\n",
      "Train Epoch: 70 [7680/60000 (13%)]\tDiscriminator Loss: 0.538288\tGenerator Loss: 1.491025\n",
      "Train Epoch: 70 [8960/60000 (15%)]\tDiscriminator Loss: 0.570332\tGenerator Loss: 1.937519\n",
      "Train Epoch: 70 [10240/60000 (17%)]\tDiscriminator Loss: 0.493706\tGenerator Loss: 1.070851\n",
      "Train Epoch: 70 [11520/60000 (19%)]\tDiscriminator Loss: 0.440688\tGenerator Loss: 1.700670\n",
      "Train Epoch: 70 [12800/60000 (21%)]\tDiscriminator Loss: 0.531570\tGenerator Loss: 1.102950\n",
      "Train Epoch: 70 [14080/60000 (23%)]\tDiscriminator Loss: 0.485984\tGenerator Loss: 1.623185\n",
      "Train Epoch: 70 [15360/60000 (26%)]\tDiscriminator Loss: 0.546849\tGenerator Loss: 1.228921\n",
      "Train Epoch: 70 [16640/60000 (28%)]\tDiscriminator Loss: 0.545080\tGenerator Loss: 1.627996\n",
      "Train Epoch: 70 [17920/60000 (30%)]\tDiscriminator Loss: 0.537489\tGenerator Loss: 1.092456\n",
      "Train Epoch: 70 [19200/60000 (32%)]\tDiscriminator Loss: 0.480583\tGenerator Loss: 1.460679\n",
      "Train Epoch: 70 [20480/60000 (34%)]\tDiscriminator Loss: 0.481540\tGenerator Loss: 1.539633\n",
      "Train Epoch: 70 [21760/60000 (36%)]\tDiscriminator Loss: 0.490315\tGenerator Loss: 1.159794\n",
      "Train Epoch: 70 [23040/60000 (38%)]\tDiscriminator Loss: 0.468756\tGenerator Loss: 1.106550\n",
      "Train Epoch: 70 [24320/60000 (41%)]\tDiscriminator Loss: 0.571413\tGenerator Loss: 1.551643\n",
      "Train Epoch: 70 [25600/60000 (43%)]\tDiscriminator Loss: 0.531272\tGenerator Loss: 1.091581\n",
      "Train Epoch: 70 [26880/60000 (45%)]\tDiscriminator Loss: 0.477603\tGenerator Loss: 1.455718\n",
      "Train Epoch: 70 [28160/60000 (47%)]\tDiscriminator Loss: 0.540649\tGenerator Loss: 1.232426\n",
      "Train Epoch: 70 [29440/60000 (49%)]\tDiscriminator Loss: 0.483136\tGenerator Loss: 1.422791\n",
      "Train Epoch: 70 [30720/60000 (51%)]\tDiscriminator Loss: 0.483400\tGenerator Loss: 1.348942\n",
      "Train Epoch: 70 [32000/60000 (53%)]\tDiscriminator Loss: 0.474387\tGenerator Loss: 1.568297\n",
      "Train Epoch: 70 [33280/60000 (55%)]\tDiscriminator Loss: 0.522380\tGenerator Loss: 1.146962\n",
      "Train Epoch: 70 [34560/60000 (58%)]\tDiscriminator Loss: 0.495457\tGenerator Loss: 1.527990\n",
      "Train Epoch: 70 [35840/60000 (60%)]\tDiscriminator Loss: 0.521809\tGenerator Loss: 0.996688\n",
      "Train Epoch: 70 [37120/60000 (62%)]\tDiscriminator Loss: 0.472817\tGenerator Loss: 1.129808\n",
      "Train Epoch: 70 [38400/60000 (64%)]\tDiscriminator Loss: 0.476576\tGenerator Loss: 1.687674\n",
      "Train Epoch: 70 [39680/60000 (66%)]\tDiscriminator Loss: 0.524894\tGenerator Loss: 1.061644\n",
      "Train Epoch: 70 [40960/60000 (68%)]\tDiscriminator Loss: 0.482773\tGenerator Loss: 1.300740\n",
      "Train Epoch: 70 [42240/60000 (70%)]\tDiscriminator Loss: 0.505005\tGenerator Loss: 1.122172\n",
      "Train Epoch: 70 [43520/60000 (72%)]\tDiscriminator Loss: 0.519336\tGenerator Loss: 1.577284\n",
      "Train Epoch: 70 [44800/60000 (75%)]\tDiscriminator Loss: 0.488051\tGenerator Loss: 1.356618\n",
      "Train Epoch: 70 [46080/60000 (77%)]\tDiscriminator Loss: 0.495014\tGenerator Loss: 1.324799\n",
      "Train Epoch: 70 [47360/60000 (79%)]\tDiscriminator Loss: 0.497484\tGenerator Loss: 1.225417\n",
      "Train Epoch: 70 [48640/60000 (81%)]\tDiscriminator Loss: 0.530339\tGenerator Loss: 1.127064\n",
      "Train Epoch: 70 [49920/60000 (83%)]\tDiscriminator Loss: 0.448813\tGenerator Loss: 1.308074\n",
      "Train Epoch: 70 [51200/60000 (85%)]\tDiscriminator Loss: 0.506535\tGenerator Loss: 0.876853\n",
      "Train Epoch: 70 [52480/60000 (87%)]\tDiscriminator Loss: 0.497938\tGenerator Loss: 1.054407\n",
      "Train Epoch: 70 [53760/60000 (90%)]\tDiscriminator Loss: 0.485477\tGenerator Loss: 1.286014\n",
      "Train Epoch: 70 [55040/60000 (92%)]\tDiscriminator Loss: 0.522596\tGenerator Loss: 1.164965\n",
      "Train Epoch: 70 [56320/60000 (94%)]\tDiscriminator Loss: 0.526483\tGenerator Loss: 1.259795\n",
      "Train Epoch: 70 [57600/60000 (96%)]\tDiscriminator Loss: 0.438992\tGenerator Loss: 1.390162\n",
      "Train Epoch: 70 [58880/60000 (98%)]\tDiscriminator Loss: 0.516423\tGenerator Loss: 1.322743\n",
      "Train Epoch: 71 [0/60000 (0%)]\tDiscriminator Loss: 0.579418\tGenerator Loss: 0.917427\n",
      "Train Epoch: 71 [1280/60000 (2%)]\tDiscriminator Loss: 0.486224\tGenerator Loss: 1.188250\n",
      "Train Epoch: 71 [2560/60000 (4%)]\tDiscriminator Loss: 0.468059\tGenerator Loss: 1.306835\n",
      "Train Epoch: 71 [3840/60000 (6%)]\tDiscriminator Loss: 0.461406\tGenerator Loss: 1.376081\n",
      "Train Epoch: 71 [5120/60000 (9%)]\tDiscriminator Loss: 0.492868\tGenerator Loss: 1.501084\n",
      "Train Epoch: 71 [6400/60000 (11%)]\tDiscriminator Loss: 0.462016\tGenerator Loss: 1.360437\n",
      "Train Epoch: 71 [7680/60000 (13%)]\tDiscriminator Loss: 0.480224\tGenerator Loss: 1.114628\n",
      "Train Epoch: 71 [8960/60000 (15%)]\tDiscriminator Loss: 0.499857\tGenerator Loss: 1.115355\n",
      "Train Epoch: 71 [10240/60000 (17%)]\tDiscriminator Loss: 0.509872\tGenerator Loss: 1.506851\n",
      "Train Epoch: 71 [11520/60000 (19%)]\tDiscriminator Loss: 0.517988\tGenerator Loss: 1.756086\n",
      "Train Epoch: 71 [12800/60000 (21%)]\tDiscriminator Loss: 0.516479\tGenerator Loss: 1.214727\n",
      "Train Epoch: 71 [14080/60000 (23%)]\tDiscriminator Loss: 0.414828\tGenerator Loss: 1.553637\n",
      "Train Epoch: 71 [15360/60000 (26%)]\tDiscriminator Loss: 0.535106\tGenerator Loss: 1.685691\n",
      "Train Epoch: 71 [16640/60000 (28%)]\tDiscriminator Loss: 0.500690\tGenerator Loss: 1.641484\n",
      "Train Epoch: 71 [17920/60000 (30%)]\tDiscriminator Loss: 0.472505\tGenerator Loss: 1.463217\n",
      "Train Epoch: 71 [19200/60000 (32%)]\tDiscriminator Loss: 0.476378\tGenerator Loss: 0.960715\n",
      "Train Epoch: 71 [20480/60000 (34%)]\tDiscriminator Loss: 0.509777\tGenerator Loss: 1.478773\n",
      "Train Epoch: 71 [21760/60000 (36%)]\tDiscriminator Loss: 0.516947\tGenerator Loss: 1.098109\n",
      "Train Epoch: 71 [23040/60000 (38%)]\tDiscriminator Loss: 0.592341\tGenerator Loss: 0.973579\n",
      "Train Epoch: 71 [24320/60000 (41%)]\tDiscriminator Loss: 0.502760\tGenerator Loss: 1.484605\n",
      "Train Epoch: 71 [25600/60000 (43%)]\tDiscriminator Loss: 0.487783\tGenerator Loss: 1.262581\n",
      "Train Epoch: 71 [26880/60000 (45%)]\tDiscriminator Loss: 0.492956\tGenerator Loss: 1.391332\n",
      "Train Epoch: 71 [28160/60000 (47%)]\tDiscriminator Loss: 0.483313\tGenerator Loss: 1.326731\n",
      "Train Epoch: 71 [29440/60000 (49%)]\tDiscriminator Loss: 0.464428\tGenerator Loss: 1.324319\n",
      "Train Epoch: 71 [30720/60000 (51%)]\tDiscriminator Loss: 0.426703\tGenerator Loss: 1.377819\n",
      "Train Epoch: 71 [32000/60000 (53%)]\tDiscriminator Loss: 0.501017\tGenerator Loss: 1.468721\n",
      "Train Epoch: 71 [33280/60000 (55%)]\tDiscriminator Loss: 0.531672\tGenerator Loss: 1.400687\n",
      "Train Epoch: 71 [34560/60000 (58%)]\tDiscriminator Loss: 0.481547\tGenerator Loss: 1.451397\n",
      "Train Epoch: 71 [35840/60000 (60%)]\tDiscriminator Loss: 0.503985\tGenerator Loss: 1.817902\n",
      "Train Epoch: 71 [37120/60000 (62%)]\tDiscriminator Loss: 0.483837\tGenerator Loss: 1.189645\n",
      "Train Epoch: 71 [38400/60000 (64%)]\tDiscriminator Loss: 0.511852\tGenerator Loss: 0.912204\n",
      "Train Epoch: 71 [39680/60000 (66%)]\tDiscriminator Loss: 0.497722\tGenerator Loss: 1.318810\n",
      "Train Epoch: 71 [40960/60000 (68%)]\tDiscriminator Loss: 0.477283\tGenerator Loss: 1.364120\n",
      "Train Epoch: 71 [42240/60000 (70%)]\tDiscriminator Loss: 0.503551\tGenerator Loss: 1.422440\n",
      "Train Epoch: 71 [43520/60000 (72%)]\tDiscriminator Loss: 0.500870\tGenerator Loss: 1.153117\n",
      "Train Epoch: 71 [44800/60000 (75%)]\tDiscriminator Loss: 0.476611\tGenerator Loss: 1.489985\n",
      "Train Epoch: 71 [46080/60000 (77%)]\tDiscriminator Loss: 0.511940\tGenerator Loss: 1.020694\n",
      "Train Epoch: 71 [47360/60000 (79%)]\tDiscriminator Loss: 0.506635\tGenerator Loss: 1.070365\n",
      "Train Epoch: 71 [48640/60000 (81%)]\tDiscriminator Loss: 0.508903\tGenerator Loss: 1.557083\n",
      "Train Epoch: 71 [49920/60000 (83%)]\tDiscriminator Loss: 0.452498\tGenerator Loss: 1.334390\n",
      "Train Epoch: 71 [51200/60000 (85%)]\tDiscriminator Loss: 0.452768\tGenerator Loss: 1.274639\n",
      "Train Epoch: 71 [52480/60000 (87%)]\tDiscriminator Loss: 0.493930\tGenerator Loss: 1.709652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 71 [53760/60000 (90%)]\tDiscriminator Loss: 0.459810\tGenerator Loss: 1.406333\n",
      "Train Epoch: 71 [55040/60000 (92%)]\tDiscriminator Loss: 0.475685\tGenerator Loss: 1.291863\n",
      "Train Epoch: 71 [56320/60000 (94%)]\tDiscriminator Loss: 0.441218\tGenerator Loss: 1.311598\n",
      "Train Epoch: 71 [57600/60000 (96%)]\tDiscriminator Loss: 0.479642\tGenerator Loss: 1.200175\n",
      "Train Epoch: 71 [58880/60000 (98%)]\tDiscriminator Loss: 0.534156\tGenerator Loss: 1.062419\n",
      "Train Epoch: 72 [0/60000 (0%)]\tDiscriminator Loss: 0.534361\tGenerator Loss: 1.159313\n",
      "Train Epoch: 72 [1280/60000 (2%)]\tDiscriminator Loss: 0.518119\tGenerator Loss: 1.060413\n",
      "Train Epoch: 72 [2560/60000 (4%)]\tDiscriminator Loss: 0.477344\tGenerator Loss: 1.669613\n",
      "Train Epoch: 72 [3840/60000 (6%)]\tDiscriminator Loss: 0.514039\tGenerator Loss: 1.620577\n",
      "Train Epoch: 72 [5120/60000 (9%)]\tDiscriminator Loss: 0.539516\tGenerator Loss: 1.855554\n",
      "Train Epoch: 72 [6400/60000 (11%)]\tDiscriminator Loss: 0.569026\tGenerator Loss: 1.501976\n",
      "Train Epoch: 72 [7680/60000 (13%)]\tDiscriminator Loss: 0.489489\tGenerator Loss: 1.375115\n",
      "Train Epoch: 72 [8960/60000 (15%)]\tDiscriminator Loss: 0.515127\tGenerator Loss: 1.043867\n",
      "Train Epoch: 72 [10240/60000 (17%)]\tDiscriminator Loss: 0.481550\tGenerator Loss: 1.371721\n",
      "Train Epoch: 72 [11520/60000 (19%)]\tDiscriminator Loss: 0.472568\tGenerator Loss: 1.112551\n",
      "Train Epoch: 72 [12800/60000 (21%)]\tDiscriminator Loss: 0.462691\tGenerator Loss: 1.327304\n",
      "Train Epoch: 72 [14080/60000 (23%)]\tDiscriminator Loss: 0.499027\tGenerator Loss: 1.362429\n",
      "Train Epoch: 72 [15360/60000 (26%)]\tDiscriminator Loss: 0.504071\tGenerator Loss: 1.416167\n",
      "Train Epoch: 72 [16640/60000 (28%)]\tDiscriminator Loss: 0.471072\tGenerator Loss: 1.228601\n",
      "Train Epoch: 72 [17920/60000 (30%)]\tDiscriminator Loss: 0.486791\tGenerator Loss: 1.600097\n",
      "Train Epoch: 72 [19200/60000 (32%)]\tDiscriminator Loss: 0.444897\tGenerator Loss: 1.609539\n",
      "Train Epoch: 72 [20480/60000 (34%)]\tDiscriminator Loss: 0.452327\tGenerator Loss: 1.584738\n",
      "Train Epoch: 72 [21760/60000 (36%)]\tDiscriminator Loss: 0.519871\tGenerator Loss: 1.536723\n",
      "Train Epoch: 72 [23040/60000 (38%)]\tDiscriminator Loss: 0.483433\tGenerator Loss: 1.339818\n",
      "Train Epoch: 72 [24320/60000 (41%)]\tDiscriminator Loss: 0.506862\tGenerator Loss: 1.571400\n",
      "Train Epoch: 72 [25600/60000 (43%)]\tDiscriminator Loss: 0.541953\tGenerator Loss: 1.137842\n",
      "Train Epoch: 72 [26880/60000 (45%)]\tDiscriminator Loss: 0.478053\tGenerator Loss: 1.400463\n",
      "Train Epoch: 72 [28160/60000 (47%)]\tDiscriminator Loss: 0.557506\tGenerator Loss: 1.372398\n",
      "Train Epoch: 72 [29440/60000 (49%)]\tDiscriminator Loss: 0.434193\tGenerator Loss: 1.370322\n",
      "Train Epoch: 72 [30720/60000 (51%)]\tDiscriminator Loss: 0.486304\tGenerator Loss: 1.715400\n",
      "Train Epoch: 72 [32000/60000 (53%)]\tDiscriminator Loss: 0.485145\tGenerator Loss: 1.451503\n",
      "Train Epoch: 72 [33280/60000 (55%)]\tDiscriminator Loss: 0.458438\tGenerator Loss: 1.375706\n",
      "Train Epoch: 72 [34560/60000 (58%)]\tDiscriminator Loss: 0.491813\tGenerator Loss: 1.133959\n",
      "Train Epoch: 72 [35840/60000 (60%)]\tDiscriminator Loss: 0.453213\tGenerator Loss: 1.447785\n",
      "Train Epoch: 72 [37120/60000 (62%)]\tDiscriminator Loss: 0.467667\tGenerator Loss: 1.253458\n",
      "Train Epoch: 72 [38400/60000 (64%)]\tDiscriminator Loss: 0.465924\tGenerator Loss: 1.184567\n",
      "Train Epoch: 72 [39680/60000 (66%)]\tDiscriminator Loss: 0.490838\tGenerator Loss: 1.217632\n",
      "Train Epoch: 72 [40960/60000 (68%)]\tDiscriminator Loss: 0.461678\tGenerator Loss: 1.480201\n",
      "Train Epoch: 72 [42240/60000 (70%)]\tDiscriminator Loss: 0.515629\tGenerator Loss: 1.295315\n",
      "Train Epoch: 72 [43520/60000 (72%)]\tDiscriminator Loss: 0.486692\tGenerator Loss: 1.123936\n",
      "Train Epoch: 72 [44800/60000 (75%)]\tDiscriminator Loss: 0.532560\tGenerator Loss: 1.715864\n",
      "Train Epoch: 72 [46080/60000 (77%)]\tDiscriminator Loss: 0.428074\tGenerator Loss: 1.297197\n",
      "Train Epoch: 72 [47360/60000 (79%)]\tDiscriminator Loss: 0.543148\tGenerator Loss: 1.251841\n",
      "Train Epoch: 72 [48640/60000 (81%)]\tDiscriminator Loss: 0.493215\tGenerator Loss: 1.323175\n",
      "Train Epoch: 72 [49920/60000 (83%)]\tDiscriminator Loss: 0.491846\tGenerator Loss: 1.405900\n",
      "Train Epoch: 72 [51200/60000 (85%)]\tDiscriminator Loss: 0.476880\tGenerator Loss: 1.615116\n",
      "Train Epoch: 72 [52480/60000 (87%)]\tDiscriminator Loss: 0.515320\tGenerator Loss: 1.819917\n",
      "Train Epoch: 72 [53760/60000 (90%)]\tDiscriminator Loss: 0.658092\tGenerator Loss: 0.717585\n",
      "Train Epoch: 72 [55040/60000 (92%)]\tDiscriminator Loss: 0.475722\tGenerator Loss: 1.511804\n",
      "Train Epoch: 72 [56320/60000 (94%)]\tDiscriminator Loss: 0.596050\tGenerator Loss: 1.260398\n",
      "Train Epoch: 72 [57600/60000 (96%)]\tDiscriminator Loss: 0.505040\tGenerator Loss: 1.394816\n",
      "Train Epoch: 72 [58880/60000 (98%)]\tDiscriminator Loss: 0.436553\tGenerator Loss: 1.824817\n",
      "Train Epoch: 73 [0/60000 (0%)]\tDiscriminator Loss: 0.582488\tGenerator Loss: 0.717020\n",
      "Train Epoch: 73 [1280/60000 (2%)]\tDiscriminator Loss: 0.451189\tGenerator Loss: 1.276963\n",
      "Train Epoch: 73 [2560/60000 (4%)]\tDiscriminator Loss: 0.506784\tGenerator Loss: 1.264018\n",
      "Train Epoch: 73 [3840/60000 (6%)]\tDiscriminator Loss: 0.480228\tGenerator Loss: 1.304806\n",
      "Train Epoch: 73 [5120/60000 (9%)]\tDiscriminator Loss: 0.449758\tGenerator Loss: 1.295089\n",
      "Train Epoch: 73 [6400/60000 (11%)]\tDiscriminator Loss: 0.501621\tGenerator Loss: 1.616748\n",
      "Train Epoch: 73 [7680/60000 (13%)]\tDiscriminator Loss: 0.484201\tGenerator Loss: 1.361222\n",
      "Train Epoch: 73 [8960/60000 (15%)]\tDiscriminator Loss: 0.531630\tGenerator Loss: 1.131146\n",
      "Train Epoch: 73 [10240/60000 (17%)]\tDiscriminator Loss: 0.521708\tGenerator Loss: 1.310897\n",
      "Train Epoch: 73 [11520/60000 (19%)]\tDiscriminator Loss: 0.526016\tGenerator Loss: 1.719175\n",
      "Train Epoch: 73 [12800/60000 (21%)]\tDiscriminator Loss: 0.475276\tGenerator Loss: 1.590869\n",
      "Train Epoch: 73 [14080/60000 (23%)]\tDiscriminator Loss: 0.478657\tGenerator Loss: 1.415292\n",
      "Train Epoch: 73 [15360/60000 (26%)]\tDiscriminator Loss: 0.431637\tGenerator Loss: 1.595384\n",
      "Train Epoch: 73 [16640/60000 (28%)]\tDiscriminator Loss: 0.530970\tGenerator Loss: 1.198916\n",
      "Train Epoch: 73 [17920/60000 (30%)]\tDiscriminator Loss: 0.476863\tGenerator Loss: 1.267745\n",
      "Train Epoch: 73 [19200/60000 (32%)]\tDiscriminator Loss: 0.524023\tGenerator Loss: 1.370246\n",
      "Train Epoch: 73 [20480/60000 (34%)]\tDiscriminator Loss: 0.460177\tGenerator Loss: 1.599571\n",
      "Train Epoch: 73 [21760/60000 (36%)]\tDiscriminator Loss: 0.511944\tGenerator Loss: 1.386142\n",
      "Train Epoch: 73 [23040/60000 (38%)]\tDiscriminator Loss: 0.484902\tGenerator Loss: 1.812315\n",
      "Train Epoch: 73 [24320/60000 (41%)]\tDiscriminator Loss: 0.495881\tGenerator Loss: 1.436290\n",
      "Train Epoch: 73 [25600/60000 (43%)]\tDiscriminator Loss: 0.501538\tGenerator Loss: 1.200229\n",
      "Train Epoch: 73 [26880/60000 (45%)]\tDiscriminator Loss: 0.485284\tGenerator Loss: 1.418925\n",
      "Train Epoch: 73 [28160/60000 (47%)]\tDiscriminator Loss: 0.462528\tGenerator Loss: 1.283167\n",
      "Train Epoch: 73 [29440/60000 (49%)]\tDiscriminator Loss: 0.530515\tGenerator Loss: 1.640910\n",
      "Train Epoch: 73 [30720/60000 (51%)]\tDiscriminator Loss: 0.468229\tGenerator Loss: 1.524751\n",
      "Train Epoch: 73 [32000/60000 (53%)]\tDiscriminator Loss: 0.498683\tGenerator Loss: 1.211883\n",
      "Train Epoch: 73 [33280/60000 (55%)]\tDiscriminator Loss: 0.524203\tGenerator Loss: 1.232081\n",
      "Train Epoch: 73 [34560/60000 (58%)]\tDiscriminator Loss: 0.496421\tGenerator Loss: 1.418948\n",
      "Train Epoch: 73 [35840/60000 (60%)]\tDiscriminator Loss: 0.499321\tGenerator Loss: 1.187531\n",
      "Train Epoch: 73 [37120/60000 (62%)]\tDiscriminator Loss: 0.555329\tGenerator Loss: 1.107385\n",
      "Train Epoch: 73 [38400/60000 (64%)]\tDiscriminator Loss: 0.573520\tGenerator Loss: 1.407276\n",
      "Train Epoch: 73 [39680/60000 (66%)]\tDiscriminator Loss: 0.538260\tGenerator Loss: 1.013025\n",
      "Train Epoch: 73 [40960/60000 (68%)]\tDiscriminator Loss: 0.483399\tGenerator Loss: 1.344861\n",
      "Train Epoch: 73 [42240/60000 (70%)]\tDiscriminator Loss: 0.530419\tGenerator Loss: 1.315317\n",
      "Train Epoch: 73 [43520/60000 (72%)]\tDiscriminator Loss: 0.461491\tGenerator Loss: 1.419822\n",
      "Train Epoch: 73 [44800/60000 (75%)]\tDiscriminator Loss: 0.471655\tGenerator Loss: 1.508840\n",
      "Train Epoch: 73 [46080/60000 (77%)]\tDiscriminator Loss: 0.470676\tGenerator Loss: 1.482028\n",
      "Train Epoch: 73 [47360/60000 (79%)]\tDiscriminator Loss: 0.454492\tGenerator Loss: 1.284345\n",
      "Train Epoch: 73 [48640/60000 (81%)]\tDiscriminator Loss: 0.484850\tGenerator Loss: 1.480089\n",
      "Train Epoch: 73 [49920/60000 (83%)]\tDiscriminator Loss: 0.553386\tGenerator Loss: 1.600380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 73 [51200/60000 (85%)]\tDiscriminator Loss: 0.606560\tGenerator Loss: 0.873196\n",
      "Train Epoch: 73 [52480/60000 (87%)]\tDiscriminator Loss: 0.539032\tGenerator Loss: 0.956585\n",
      "Train Epoch: 73 [53760/60000 (90%)]\tDiscriminator Loss: 0.466376\tGenerator Loss: 1.502443\n",
      "Train Epoch: 73 [55040/60000 (92%)]\tDiscriminator Loss: 0.540816\tGenerator Loss: 1.686197\n",
      "Train Epoch: 73 [56320/60000 (94%)]\tDiscriminator Loss: 0.532330\tGenerator Loss: 1.446967\n",
      "Train Epoch: 73 [57600/60000 (96%)]\tDiscriminator Loss: 0.492713\tGenerator Loss: 1.424870\n",
      "Train Epoch: 73 [58880/60000 (98%)]\tDiscriminator Loss: 0.551207\tGenerator Loss: 1.355567\n",
      "Train Epoch: 74 [0/60000 (0%)]\tDiscriminator Loss: 0.515842\tGenerator Loss: 1.686683\n",
      "Train Epoch: 74 [1280/60000 (2%)]\tDiscriminator Loss: 0.504667\tGenerator Loss: 1.540811\n",
      "Train Epoch: 74 [2560/60000 (4%)]\tDiscriminator Loss: 0.433963\tGenerator Loss: 1.349375\n",
      "Train Epoch: 74 [3840/60000 (6%)]\tDiscriminator Loss: 0.515337\tGenerator Loss: 1.434540\n",
      "Train Epoch: 74 [5120/60000 (9%)]\tDiscriminator Loss: 0.631928\tGenerator Loss: 0.746625\n",
      "Train Epoch: 74 [6400/60000 (11%)]\tDiscriminator Loss: 0.479778\tGenerator Loss: 1.427974\n",
      "Train Epoch: 74 [7680/60000 (13%)]\tDiscriminator Loss: 0.487944\tGenerator Loss: 1.510246\n",
      "Train Epoch: 74 [8960/60000 (15%)]\tDiscriminator Loss: 0.504210\tGenerator Loss: 1.463085\n",
      "Train Epoch: 74 [10240/60000 (17%)]\tDiscriminator Loss: 0.564407\tGenerator Loss: 1.944576\n",
      "Train Epoch: 74 [11520/60000 (19%)]\tDiscriminator Loss: 0.483168\tGenerator Loss: 1.684584\n",
      "Train Epoch: 74 [12800/60000 (21%)]\tDiscriminator Loss: 0.449268\tGenerator Loss: 1.345170\n",
      "Train Epoch: 74 [14080/60000 (23%)]\tDiscriminator Loss: 0.460917\tGenerator Loss: 1.315008\n",
      "Train Epoch: 74 [15360/60000 (26%)]\tDiscriminator Loss: 0.476603\tGenerator Loss: 1.409638\n",
      "Train Epoch: 74 [16640/60000 (28%)]\tDiscriminator Loss: 0.531540\tGenerator Loss: 1.567334\n",
      "Train Epoch: 74 [17920/60000 (30%)]\tDiscriminator Loss: 0.458805\tGenerator Loss: 1.306533\n",
      "Train Epoch: 74 [19200/60000 (32%)]\tDiscriminator Loss: 0.451317\tGenerator Loss: 1.383235\n",
      "Train Epoch: 74 [20480/60000 (34%)]\tDiscriminator Loss: 0.457056\tGenerator Loss: 1.485275\n",
      "Train Epoch: 74 [21760/60000 (36%)]\tDiscriminator Loss: 0.460262\tGenerator Loss: 1.186115\n",
      "Train Epoch: 74 [23040/60000 (38%)]\tDiscriminator Loss: 0.448180\tGenerator Loss: 1.171420\n",
      "Train Epoch: 74 [24320/60000 (41%)]\tDiscriminator Loss: 0.464473\tGenerator Loss: 1.639217\n",
      "Train Epoch: 74 [25600/60000 (43%)]\tDiscriminator Loss: 0.507373\tGenerator Loss: 1.463314\n",
      "Train Epoch: 74 [26880/60000 (45%)]\tDiscriminator Loss: 0.486197\tGenerator Loss: 1.558517\n",
      "Train Epoch: 74 [28160/60000 (47%)]\tDiscriminator Loss: 0.554221\tGenerator Loss: 1.042079\n",
      "Train Epoch: 74 [29440/60000 (49%)]\tDiscriminator Loss: 0.438821\tGenerator Loss: 1.635190\n",
      "Train Epoch: 74 [30720/60000 (51%)]\tDiscriminator Loss: 0.482775\tGenerator Loss: 1.760318\n",
      "Train Epoch: 74 [32000/60000 (53%)]\tDiscriminator Loss: 0.473074\tGenerator Loss: 1.116249\n",
      "Train Epoch: 74 [33280/60000 (55%)]\tDiscriminator Loss: 0.481125\tGenerator Loss: 1.331471\n",
      "Train Epoch: 74 [34560/60000 (58%)]\tDiscriminator Loss: 0.510750\tGenerator Loss: 1.185237\n",
      "Train Epoch: 74 [35840/60000 (60%)]\tDiscriminator Loss: 0.463392\tGenerator Loss: 1.492801\n",
      "Train Epoch: 74 [37120/60000 (62%)]\tDiscriminator Loss: 0.524099\tGenerator Loss: 1.209268\n",
      "Train Epoch: 74 [38400/60000 (64%)]\tDiscriminator Loss: 0.460542\tGenerator Loss: 1.445909\n",
      "Train Epoch: 74 [39680/60000 (66%)]\tDiscriminator Loss: 0.449332\tGenerator Loss: 1.395064\n",
      "Train Epoch: 74 [40960/60000 (68%)]\tDiscriminator Loss: 0.479450\tGenerator Loss: 1.495419\n",
      "Train Epoch: 74 [42240/60000 (70%)]\tDiscriminator Loss: 0.481337\tGenerator Loss: 1.570089\n",
      "Train Epoch: 74 [43520/60000 (72%)]\tDiscriminator Loss: 0.523850\tGenerator Loss: 1.357074\n",
      "Train Epoch: 74 [44800/60000 (75%)]\tDiscriminator Loss: 0.469775\tGenerator Loss: 1.287575\n",
      "Train Epoch: 74 [46080/60000 (77%)]\tDiscriminator Loss: 0.521068\tGenerator Loss: 1.519827\n",
      "Train Epoch: 74 [47360/60000 (79%)]\tDiscriminator Loss: 0.500149\tGenerator Loss: 1.142971\n",
      "Train Epoch: 74 [48640/60000 (81%)]\tDiscriminator Loss: 0.519037\tGenerator Loss: 1.653109\n",
      "Train Epoch: 74 [49920/60000 (83%)]\tDiscriminator Loss: 0.437584\tGenerator Loss: 1.500173\n",
      "Train Epoch: 74 [51200/60000 (85%)]\tDiscriminator Loss: 0.519827\tGenerator Loss: 1.274235\n",
      "Train Epoch: 74 [52480/60000 (87%)]\tDiscriminator Loss: 0.509634\tGenerator Loss: 1.132542\n",
      "Train Epoch: 74 [53760/60000 (90%)]\tDiscriminator Loss: 0.485478\tGenerator Loss: 1.715253\n",
      "Train Epoch: 74 [55040/60000 (92%)]\tDiscriminator Loss: 0.503557\tGenerator Loss: 1.165129\n",
      "Train Epoch: 74 [56320/60000 (94%)]\tDiscriminator Loss: 0.489805\tGenerator Loss: 1.593697\n",
      "Train Epoch: 74 [57600/60000 (96%)]\tDiscriminator Loss: 0.545795\tGenerator Loss: 1.097484\n",
      "Train Epoch: 74 [58880/60000 (98%)]\tDiscriminator Loss: 0.476493\tGenerator Loss: 1.115942\n",
      "Train Epoch: 75 [0/60000 (0%)]\tDiscriminator Loss: 0.498581\tGenerator Loss: 1.385522\n",
      "Train Epoch: 75 [1280/60000 (2%)]\tDiscriminator Loss: 0.488354\tGenerator Loss: 1.158829\n",
      "Train Epoch: 75 [2560/60000 (4%)]\tDiscriminator Loss: 0.511668\tGenerator Loss: 1.140618\n",
      "Train Epoch: 75 [3840/60000 (6%)]\tDiscriminator Loss: 0.524962\tGenerator Loss: 1.620328\n",
      "Train Epoch: 75 [5120/60000 (9%)]\tDiscriminator Loss: 0.482254\tGenerator Loss: 1.168871\n",
      "Train Epoch: 75 [6400/60000 (11%)]\tDiscriminator Loss: 0.466272\tGenerator Loss: 1.408857\n",
      "Train Epoch: 75 [7680/60000 (13%)]\tDiscriminator Loss: 0.473065\tGenerator Loss: 1.478361\n",
      "Train Epoch: 75 [8960/60000 (15%)]\tDiscriminator Loss: 0.527730\tGenerator Loss: 1.641891\n",
      "Train Epoch: 75 [10240/60000 (17%)]\tDiscriminator Loss: 0.506929\tGenerator Loss: 1.501858\n",
      "Train Epoch: 75 [11520/60000 (19%)]\tDiscriminator Loss: 0.491482\tGenerator Loss: 1.680956\n",
      "Train Epoch: 75 [12800/60000 (21%)]\tDiscriminator Loss: 0.444143\tGenerator Loss: 1.304452\n",
      "Train Epoch: 75 [14080/60000 (23%)]\tDiscriminator Loss: 0.474764\tGenerator Loss: 1.208132\n",
      "Train Epoch: 75 [15360/60000 (26%)]\tDiscriminator Loss: 0.458024\tGenerator Loss: 1.505579\n",
      "Train Epoch: 75 [16640/60000 (28%)]\tDiscriminator Loss: 0.472713\tGenerator Loss: 1.358669\n",
      "Train Epoch: 75 [17920/60000 (30%)]\tDiscriminator Loss: 0.465846\tGenerator Loss: 1.634136\n",
      "Train Epoch: 75 [19200/60000 (32%)]\tDiscriminator Loss: 0.544994\tGenerator Loss: 1.388053\n",
      "Train Epoch: 75 [20480/60000 (34%)]\tDiscriminator Loss: 0.520427\tGenerator Loss: 1.199470\n",
      "Train Epoch: 75 [21760/60000 (36%)]\tDiscriminator Loss: 0.529774\tGenerator Loss: 1.224324\n",
      "Train Epoch: 75 [23040/60000 (38%)]\tDiscriminator Loss: 0.555886\tGenerator Loss: 0.938609\n",
      "Train Epoch: 75 [24320/60000 (41%)]\tDiscriminator Loss: 0.470033\tGenerator Loss: 1.224408\n",
      "Train Epoch: 75 [25600/60000 (43%)]\tDiscriminator Loss: 0.469655\tGenerator Loss: 1.516231\n",
      "Train Epoch: 75 [26880/60000 (45%)]\tDiscriminator Loss: 0.548671\tGenerator Loss: 0.858544\n",
      "Train Epoch: 75 [28160/60000 (47%)]\tDiscriminator Loss: 0.525445\tGenerator Loss: 1.622789\n",
      "Train Epoch: 75 [29440/60000 (49%)]\tDiscriminator Loss: 0.531270\tGenerator Loss: 1.090206\n",
      "Train Epoch: 75 [30720/60000 (51%)]\tDiscriminator Loss: 0.491088\tGenerator Loss: 1.174055\n",
      "Train Epoch: 75 [32000/60000 (53%)]\tDiscriminator Loss: 0.549963\tGenerator Loss: 1.829220\n",
      "Train Epoch: 75 [33280/60000 (55%)]\tDiscriminator Loss: 0.539255\tGenerator Loss: 1.520623\n",
      "Train Epoch: 75 [34560/60000 (58%)]\tDiscriminator Loss: 0.490368\tGenerator Loss: 1.540707\n",
      "Train Epoch: 75 [35840/60000 (60%)]\tDiscriminator Loss: 0.464180\tGenerator Loss: 1.290089\n",
      "Train Epoch: 75 [37120/60000 (62%)]\tDiscriminator Loss: 0.484309\tGenerator Loss: 1.826577\n",
      "Train Epoch: 75 [38400/60000 (64%)]\tDiscriminator Loss: 0.519743\tGenerator Loss: 1.387600\n",
      "Train Epoch: 75 [39680/60000 (66%)]\tDiscriminator Loss: 0.502289\tGenerator Loss: 1.529604\n",
      "Train Epoch: 75 [40960/60000 (68%)]\tDiscriminator Loss: 0.492573\tGenerator Loss: 1.040641\n",
      "Train Epoch: 75 [42240/60000 (70%)]\tDiscriminator Loss: 0.473123\tGenerator Loss: 1.297556\n",
      "Train Epoch: 75 [43520/60000 (72%)]\tDiscriminator Loss: 0.465841\tGenerator Loss: 1.263472\n",
      "Train Epoch: 75 [44800/60000 (75%)]\tDiscriminator Loss: 0.437780\tGenerator Loss: 1.288050\n",
      "Train Epoch: 75 [46080/60000 (77%)]\tDiscriminator Loss: 0.459921\tGenerator Loss: 1.293650\n",
      "Train Epoch: 75 [47360/60000 (79%)]\tDiscriminator Loss: 0.521807\tGenerator Loss: 1.295499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 75 [48640/60000 (81%)]\tDiscriminator Loss: 0.490147\tGenerator Loss: 1.291056\n",
      "Train Epoch: 75 [49920/60000 (83%)]\tDiscriminator Loss: 0.528323\tGenerator Loss: 1.377979\n",
      "Train Epoch: 75 [51200/60000 (85%)]\tDiscriminator Loss: 0.540420\tGenerator Loss: 1.898107\n",
      "Train Epoch: 75 [52480/60000 (87%)]\tDiscriminator Loss: 0.485508\tGenerator Loss: 1.644637\n",
      "Train Epoch: 75 [53760/60000 (90%)]\tDiscriminator Loss: 0.448489\tGenerator Loss: 1.329314\n",
      "Train Epoch: 75 [55040/60000 (92%)]\tDiscriminator Loss: 0.451103\tGenerator Loss: 1.269177\n",
      "Train Epoch: 75 [56320/60000 (94%)]\tDiscriminator Loss: 0.457715\tGenerator Loss: 1.600280\n",
      "Train Epoch: 75 [57600/60000 (96%)]\tDiscriminator Loss: 0.457870\tGenerator Loss: 1.498724\n",
      "Train Epoch: 75 [58880/60000 (98%)]\tDiscriminator Loss: 0.510456\tGenerator Loss: 1.101912\n",
      "Train Epoch: 76 [0/60000 (0%)]\tDiscriminator Loss: 0.531764\tGenerator Loss: 0.920354\n",
      "Train Epoch: 76 [1280/60000 (2%)]\tDiscriminator Loss: 0.518803\tGenerator Loss: 1.366570\n",
      "Train Epoch: 76 [2560/60000 (4%)]\tDiscriminator Loss: 0.503838\tGenerator Loss: 1.185540\n",
      "Train Epoch: 76 [3840/60000 (6%)]\tDiscriminator Loss: 0.468895\tGenerator Loss: 1.258592\n",
      "Train Epoch: 76 [5120/60000 (9%)]\tDiscriminator Loss: 0.522760\tGenerator Loss: 1.787295\n",
      "Train Epoch: 76 [6400/60000 (11%)]\tDiscriminator Loss: 0.474589\tGenerator Loss: 1.690914\n",
      "Train Epoch: 76 [7680/60000 (13%)]\tDiscriminator Loss: 0.558762\tGenerator Loss: 1.647318\n",
      "Train Epoch: 76 [8960/60000 (15%)]\tDiscriminator Loss: 0.491640\tGenerator Loss: 1.184620\n",
      "Train Epoch: 76 [10240/60000 (17%)]\tDiscriminator Loss: 0.532868\tGenerator Loss: 1.179756\n",
      "Train Epoch: 76 [11520/60000 (19%)]\tDiscriminator Loss: 0.441929\tGenerator Loss: 2.017118\n",
      "Train Epoch: 76 [12800/60000 (21%)]\tDiscriminator Loss: 0.485432\tGenerator Loss: 1.068413\n",
      "Train Epoch: 76 [14080/60000 (23%)]\tDiscriminator Loss: 0.499417\tGenerator Loss: 1.054718\n",
      "Train Epoch: 76 [15360/60000 (26%)]\tDiscriminator Loss: 0.433668\tGenerator Loss: 1.503226\n",
      "Train Epoch: 76 [16640/60000 (28%)]\tDiscriminator Loss: 0.492142\tGenerator Loss: 1.551825\n",
      "Train Epoch: 76 [17920/60000 (30%)]\tDiscriminator Loss: 0.476194\tGenerator Loss: 1.504878\n",
      "Train Epoch: 76 [19200/60000 (32%)]\tDiscriminator Loss: 0.463563\tGenerator Loss: 1.316621\n",
      "Train Epoch: 76 [20480/60000 (34%)]\tDiscriminator Loss: 0.547736\tGenerator Loss: 1.803015\n",
      "Train Epoch: 76 [21760/60000 (36%)]\tDiscriminator Loss: 0.507121\tGenerator Loss: 1.244312\n",
      "Train Epoch: 76 [23040/60000 (38%)]\tDiscriminator Loss: 0.558953\tGenerator Loss: 1.656330\n",
      "Train Epoch: 76 [24320/60000 (41%)]\tDiscriminator Loss: 0.448653\tGenerator Loss: 1.165089\n",
      "Train Epoch: 76 [25600/60000 (43%)]\tDiscriminator Loss: 0.533194\tGenerator Loss: 1.428300\n",
      "Train Epoch: 76 [26880/60000 (45%)]\tDiscriminator Loss: 0.536111\tGenerator Loss: 1.517136\n",
      "Train Epoch: 76 [28160/60000 (47%)]\tDiscriminator Loss: 0.415458\tGenerator Loss: 1.359489\n",
      "Train Epoch: 76 [29440/60000 (49%)]\tDiscriminator Loss: 0.481872\tGenerator Loss: 1.248020\n",
      "Train Epoch: 76 [30720/60000 (51%)]\tDiscriminator Loss: 0.553104\tGenerator Loss: 0.873858\n",
      "Train Epoch: 76 [32000/60000 (53%)]\tDiscriminator Loss: 0.491648\tGenerator Loss: 1.649953\n",
      "Train Epoch: 76 [33280/60000 (55%)]\tDiscriminator Loss: 0.445263\tGenerator Loss: 1.149703\n",
      "Train Epoch: 76 [34560/60000 (58%)]\tDiscriminator Loss: 0.483662\tGenerator Loss: 1.282477\n",
      "Train Epoch: 76 [35840/60000 (60%)]\tDiscriminator Loss: 0.529496\tGenerator Loss: 0.940449\n",
      "Train Epoch: 76 [37120/60000 (62%)]\tDiscriminator Loss: 0.423300\tGenerator Loss: 1.383314\n",
      "Train Epoch: 76 [38400/60000 (64%)]\tDiscriminator Loss: 0.526832\tGenerator Loss: 1.864589\n",
      "Train Epoch: 76 [39680/60000 (66%)]\tDiscriminator Loss: 0.475253\tGenerator Loss: 1.653910\n",
      "Train Epoch: 76 [40960/60000 (68%)]\tDiscriminator Loss: 0.483122\tGenerator Loss: 1.091968\n",
      "Train Epoch: 76 [42240/60000 (70%)]\tDiscriminator Loss: 0.531064\tGenerator Loss: 1.056782\n",
      "Train Epoch: 76 [43520/60000 (72%)]\tDiscriminator Loss: 0.514036\tGenerator Loss: 1.604968\n",
      "Train Epoch: 76 [44800/60000 (75%)]\tDiscriminator Loss: 0.507991\tGenerator Loss: 1.552684\n",
      "Train Epoch: 76 [46080/60000 (77%)]\tDiscriminator Loss: 0.513430\tGenerator Loss: 1.410566\n",
      "Train Epoch: 76 [47360/60000 (79%)]\tDiscriminator Loss: 0.477075\tGenerator Loss: 1.184231\n",
      "Train Epoch: 76 [48640/60000 (81%)]\tDiscriminator Loss: 0.448238\tGenerator Loss: 1.824391\n",
      "Train Epoch: 76 [49920/60000 (83%)]\tDiscriminator Loss: 0.485628\tGenerator Loss: 1.547384\n",
      "Train Epoch: 76 [51200/60000 (85%)]\tDiscriminator Loss: 0.465338\tGenerator Loss: 1.792762\n",
      "Train Epoch: 76 [52480/60000 (87%)]\tDiscriminator Loss: 0.567593\tGenerator Loss: 1.455364\n",
      "Train Epoch: 76 [53760/60000 (90%)]\tDiscriminator Loss: 0.439483\tGenerator Loss: 1.601199\n",
      "Train Epoch: 76 [55040/60000 (92%)]\tDiscriminator Loss: 0.508165\tGenerator Loss: 1.446588\n",
      "Train Epoch: 76 [56320/60000 (94%)]\tDiscriminator Loss: 0.456090\tGenerator Loss: 1.629494\n",
      "Train Epoch: 76 [57600/60000 (96%)]\tDiscriminator Loss: 0.522557\tGenerator Loss: 1.222519\n",
      "Train Epoch: 76 [58880/60000 (98%)]\tDiscriminator Loss: 0.509389\tGenerator Loss: 1.047162\n",
      "Train Epoch: 77 [0/60000 (0%)]\tDiscriminator Loss: 0.459562\tGenerator Loss: 1.535880\n",
      "Train Epoch: 77 [1280/60000 (2%)]\tDiscriminator Loss: 0.458596\tGenerator Loss: 1.628930\n",
      "Train Epoch: 77 [2560/60000 (4%)]\tDiscriminator Loss: 0.500893\tGenerator Loss: 1.511331\n",
      "Train Epoch: 77 [3840/60000 (6%)]\tDiscriminator Loss: 0.485781\tGenerator Loss: 1.398618\n",
      "Train Epoch: 77 [5120/60000 (9%)]\tDiscriminator Loss: 0.517053\tGenerator Loss: 1.117209\n",
      "Train Epoch: 77 [6400/60000 (11%)]\tDiscriminator Loss: 0.506232\tGenerator Loss: 1.361169\n",
      "Train Epoch: 77 [7680/60000 (13%)]\tDiscriminator Loss: 0.460146\tGenerator Loss: 1.310751\n",
      "Train Epoch: 77 [8960/60000 (15%)]\tDiscriminator Loss: 0.470608\tGenerator Loss: 1.293421\n",
      "Train Epoch: 77 [10240/60000 (17%)]\tDiscriminator Loss: 0.512954\tGenerator Loss: 1.302451\n",
      "Train Epoch: 77 [11520/60000 (19%)]\tDiscriminator Loss: 0.471403\tGenerator Loss: 1.081784\n",
      "Train Epoch: 77 [12800/60000 (21%)]\tDiscriminator Loss: 0.513637\tGenerator Loss: 1.334239\n",
      "Train Epoch: 77 [14080/60000 (23%)]\tDiscriminator Loss: 0.533991\tGenerator Loss: 1.377862\n",
      "Train Epoch: 77 [15360/60000 (26%)]\tDiscriminator Loss: 0.480410\tGenerator Loss: 1.151860\n",
      "Train Epoch: 77 [16640/60000 (28%)]\tDiscriminator Loss: 0.409483\tGenerator Loss: 1.393168\n",
      "Train Epoch: 77 [17920/60000 (30%)]\tDiscriminator Loss: 0.508961\tGenerator Loss: 1.387569\n",
      "Train Epoch: 77 [19200/60000 (32%)]\tDiscriminator Loss: 0.441465\tGenerator Loss: 1.407540\n",
      "Train Epoch: 77 [20480/60000 (34%)]\tDiscriminator Loss: 0.503306\tGenerator Loss: 1.025577\n",
      "Train Epoch: 77 [21760/60000 (36%)]\tDiscriminator Loss: 0.551237\tGenerator Loss: 1.741865\n",
      "Train Epoch: 77 [23040/60000 (38%)]\tDiscriminator Loss: 0.459368\tGenerator Loss: 1.257634\n",
      "Train Epoch: 77 [24320/60000 (41%)]\tDiscriminator Loss: 0.487073\tGenerator Loss: 1.191951\n",
      "Train Epoch: 77 [25600/60000 (43%)]\tDiscriminator Loss: 0.519711\tGenerator Loss: 1.042666\n",
      "Train Epoch: 77 [26880/60000 (45%)]\tDiscriminator Loss: 0.458923\tGenerator Loss: 1.551993\n",
      "Train Epoch: 77 [28160/60000 (47%)]\tDiscriminator Loss: 0.479000\tGenerator Loss: 1.481790\n",
      "Train Epoch: 77 [29440/60000 (49%)]\tDiscriminator Loss: 0.500146\tGenerator Loss: 1.427186\n",
      "Train Epoch: 77 [30720/60000 (51%)]\tDiscriminator Loss: 0.454172\tGenerator Loss: 1.459698\n",
      "Train Epoch: 77 [32000/60000 (53%)]\tDiscriminator Loss: 0.541263\tGenerator Loss: 1.062371\n",
      "Train Epoch: 77 [33280/60000 (55%)]\tDiscriminator Loss: 0.495599\tGenerator Loss: 1.324676\n",
      "Train Epoch: 77 [34560/60000 (58%)]\tDiscriminator Loss: 0.433904\tGenerator Loss: 1.376886\n",
      "Train Epoch: 77 [35840/60000 (60%)]\tDiscriminator Loss: 0.496103\tGenerator Loss: 1.184393\n",
      "Train Epoch: 77 [37120/60000 (62%)]\tDiscriminator Loss: 0.534295\tGenerator Loss: 1.582708\n",
      "Train Epoch: 77 [38400/60000 (64%)]\tDiscriminator Loss: 0.443060\tGenerator Loss: 1.665806\n",
      "Train Epoch: 77 [39680/60000 (66%)]\tDiscriminator Loss: 0.461690\tGenerator Loss: 1.326762\n",
      "Train Epoch: 77 [40960/60000 (68%)]\tDiscriminator Loss: 0.485338\tGenerator Loss: 1.380726\n",
      "Train Epoch: 77 [42240/60000 (70%)]\tDiscriminator Loss: 0.479040\tGenerator Loss: 1.239951\n",
      "Train Epoch: 77 [43520/60000 (72%)]\tDiscriminator Loss: 0.503332\tGenerator Loss: 1.533576\n",
      "Train Epoch: 77 [44800/60000 (75%)]\tDiscriminator Loss: 0.462106\tGenerator Loss: 1.456112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 77 [46080/60000 (77%)]\tDiscriminator Loss: 0.448241\tGenerator Loss: 1.292689\n",
      "Train Epoch: 77 [47360/60000 (79%)]\tDiscriminator Loss: 0.448793\tGenerator Loss: 1.393584\n",
      "Train Epoch: 77 [48640/60000 (81%)]\tDiscriminator Loss: 0.561210\tGenerator Loss: 1.485861\n",
      "Train Epoch: 77 [49920/60000 (83%)]\tDiscriminator Loss: 0.452010\tGenerator Loss: 1.154590\n",
      "Train Epoch: 77 [51200/60000 (85%)]\tDiscriminator Loss: 0.464633\tGenerator Loss: 1.470448\n",
      "Train Epoch: 77 [52480/60000 (87%)]\tDiscriminator Loss: 0.461581\tGenerator Loss: 1.368752\n",
      "Train Epoch: 77 [53760/60000 (90%)]\tDiscriminator Loss: 0.460651\tGenerator Loss: 1.276063\n",
      "Train Epoch: 77 [55040/60000 (92%)]\tDiscriminator Loss: 0.482212\tGenerator Loss: 1.600044\n",
      "Train Epoch: 77 [56320/60000 (94%)]\tDiscriminator Loss: 0.470455\tGenerator Loss: 1.495325\n",
      "Train Epoch: 77 [57600/60000 (96%)]\tDiscriminator Loss: 0.532412\tGenerator Loss: 1.452660\n",
      "Train Epoch: 77 [58880/60000 (98%)]\tDiscriminator Loss: 0.485477\tGenerator Loss: 1.406296\n",
      "Train Epoch: 78 [0/60000 (0%)]\tDiscriminator Loss: 0.557811\tGenerator Loss: 1.031233\n",
      "Train Epoch: 78 [1280/60000 (2%)]\tDiscriminator Loss: 0.429948\tGenerator Loss: 1.682609\n",
      "Train Epoch: 78 [2560/60000 (4%)]\tDiscriminator Loss: 0.522658\tGenerator Loss: 0.874168\n",
      "Train Epoch: 78 [3840/60000 (6%)]\tDiscriminator Loss: 0.447710\tGenerator Loss: 1.788074\n",
      "Train Epoch: 78 [5120/60000 (9%)]\tDiscriminator Loss: 0.497218\tGenerator Loss: 1.357311\n",
      "Train Epoch: 78 [6400/60000 (11%)]\tDiscriminator Loss: 0.506889\tGenerator Loss: 1.110273\n",
      "Train Epoch: 78 [7680/60000 (13%)]\tDiscriminator Loss: 0.474914\tGenerator Loss: 1.341791\n",
      "Train Epoch: 78 [8960/60000 (15%)]\tDiscriminator Loss: 0.463728\tGenerator Loss: 1.509137\n",
      "Train Epoch: 78 [10240/60000 (17%)]\tDiscriminator Loss: 0.497759\tGenerator Loss: 1.224918\n",
      "Train Epoch: 78 [11520/60000 (19%)]\tDiscriminator Loss: 0.454846\tGenerator Loss: 1.818285\n",
      "Train Epoch: 78 [12800/60000 (21%)]\tDiscriminator Loss: 0.482490\tGenerator Loss: 1.797107\n",
      "Train Epoch: 78 [14080/60000 (23%)]\tDiscriminator Loss: 0.527248\tGenerator Loss: 1.093943\n",
      "Train Epoch: 78 [15360/60000 (26%)]\tDiscriminator Loss: 0.495955\tGenerator Loss: 1.561962\n",
      "Train Epoch: 78 [16640/60000 (28%)]\tDiscriminator Loss: 0.474096\tGenerator Loss: 1.269173\n",
      "Train Epoch: 78 [17920/60000 (30%)]\tDiscriminator Loss: 0.519634\tGenerator Loss: 1.012293\n",
      "Train Epoch: 78 [19200/60000 (32%)]\tDiscriminator Loss: 0.459889\tGenerator Loss: 1.412433\n",
      "Train Epoch: 78 [20480/60000 (34%)]\tDiscriminator Loss: 0.470917\tGenerator Loss: 1.477750\n",
      "Train Epoch: 78 [21760/60000 (36%)]\tDiscriminator Loss: 0.444343\tGenerator Loss: 1.414683\n",
      "Train Epoch: 78 [23040/60000 (38%)]\tDiscriminator Loss: 0.448223\tGenerator Loss: 1.566597\n",
      "Train Epoch: 78 [24320/60000 (41%)]\tDiscriminator Loss: 0.491856\tGenerator Loss: 1.449560\n",
      "Train Epoch: 78 [25600/60000 (43%)]\tDiscriminator Loss: 0.464574\tGenerator Loss: 1.236523\n",
      "Train Epoch: 78 [26880/60000 (45%)]\tDiscriminator Loss: 0.496554\tGenerator Loss: 1.502075\n",
      "Train Epoch: 78 [28160/60000 (47%)]\tDiscriminator Loss: 0.420731\tGenerator Loss: 1.327535\n",
      "Train Epoch: 78 [29440/60000 (49%)]\tDiscriminator Loss: 0.511959\tGenerator Loss: 1.445238\n",
      "Train Epoch: 78 [30720/60000 (51%)]\tDiscriminator Loss: 0.490298\tGenerator Loss: 1.214424\n",
      "Train Epoch: 78 [32000/60000 (53%)]\tDiscriminator Loss: 0.514045\tGenerator Loss: 1.695788\n",
      "Train Epoch: 78 [33280/60000 (55%)]\tDiscriminator Loss: 0.465042\tGenerator Loss: 1.253024\n",
      "Train Epoch: 78 [34560/60000 (58%)]\tDiscriminator Loss: 0.510973\tGenerator Loss: 1.257380\n",
      "Train Epoch: 78 [35840/60000 (60%)]\tDiscriminator Loss: 0.513097\tGenerator Loss: 1.014394\n",
      "Train Epoch: 78 [37120/60000 (62%)]\tDiscriminator Loss: 0.485680\tGenerator Loss: 1.327390\n",
      "Train Epoch: 78 [38400/60000 (64%)]\tDiscriminator Loss: 0.532183\tGenerator Loss: 1.027548\n",
      "Train Epoch: 78 [39680/60000 (66%)]\tDiscriminator Loss: 0.484440\tGenerator Loss: 1.436857\n",
      "Train Epoch: 78 [40960/60000 (68%)]\tDiscriminator Loss: 0.457916\tGenerator Loss: 1.472206\n",
      "Train Epoch: 78 [42240/60000 (70%)]\tDiscriminator Loss: 0.518145\tGenerator Loss: 1.303110\n",
      "Train Epoch: 78 [43520/60000 (72%)]\tDiscriminator Loss: 0.470016\tGenerator Loss: 1.447000\n",
      "Train Epoch: 78 [44800/60000 (75%)]\tDiscriminator Loss: 0.532220\tGenerator Loss: 1.089867\n",
      "Train Epoch: 78 [46080/60000 (77%)]\tDiscriminator Loss: 0.468678\tGenerator Loss: 1.415972\n",
      "Train Epoch: 78 [47360/60000 (79%)]\tDiscriminator Loss: 0.455637\tGenerator Loss: 1.479717\n",
      "Train Epoch: 78 [48640/60000 (81%)]\tDiscriminator Loss: 0.538516\tGenerator Loss: 1.644329\n",
      "Train Epoch: 78 [49920/60000 (83%)]\tDiscriminator Loss: 0.435369\tGenerator Loss: 1.649659\n",
      "Train Epoch: 78 [51200/60000 (85%)]\tDiscriminator Loss: 0.528983\tGenerator Loss: 1.161863\n",
      "Train Epoch: 78 [52480/60000 (87%)]\tDiscriminator Loss: 0.495142\tGenerator Loss: 1.471136\n",
      "Train Epoch: 78 [53760/60000 (90%)]\tDiscriminator Loss: 0.544383\tGenerator Loss: 1.477903\n",
      "Train Epoch: 78 [55040/60000 (92%)]\tDiscriminator Loss: 0.472483\tGenerator Loss: 1.353526\n",
      "Train Epoch: 78 [56320/60000 (94%)]\tDiscriminator Loss: 0.479989\tGenerator Loss: 1.279382\n",
      "Train Epoch: 78 [57600/60000 (96%)]\tDiscriminator Loss: 0.496065\tGenerator Loss: 1.242554\n",
      "Train Epoch: 78 [58880/60000 (98%)]\tDiscriminator Loss: 0.560587\tGenerator Loss: 1.158084\n",
      "Train Epoch: 79 [0/60000 (0%)]\tDiscriminator Loss: 0.451696\tGenerator Loss: 1.636525\n",
      "Train Epoch: 79 [1280/60000 (2%)]\tDiscriminator Loss: 0.461910\tGenerator Loss: 1.213907\n",
      "Train Epoch: 79 [2560/60000 (4%)]\tDiscriminator Loss: 0.454510\tGenerator Loss: 1.502969\n",
      "Train Epoch: 79 [3840/60000 (6%)]\tDiscriminator Loss: 0.474877\tGenerator Loss: 1.476670\n",
      "Train Epoch: 79 [5120/60000 (9%)]\tDiscriminator Loss: 0.470225\tGenerator Loss: 1.479417\n",
      "Train Epoch: 79 [6400/60000 (11%)]\tDiscriminator Loss: 0.413768\tGenerator Loss: 1.606289\n",
      "Train Epoch: 79 [7680/60000 (13%)]\tDiscriminator Loss: 0.502876\tGenerator Loss: 1.155893\n",
      "Train Epoch: 79 [8960/60000 (15%)]\tDiscriminator Loss: 0.492432\tGenerator Loss: 1.346642\n",
      "Train Epoch: 79 [10240/60000 (17%)]\tDiscriminator Loss: 0.523747\tGenerator Loss: 1.168968\n",
      "Train Epoch: 79 [11520/60000 (19%)]\tDiscriminator Loss: 0.452567\tGenerator Loss: 1.364077\n",
      "Train Epoch: 79 [12800/60000 (21%)]\tDiscriminator Loss: 0.530500\tGenerator Loss: 1.156613\n",
      "Train Epoch: 79 [14080/60000 (23%)]\tDiscriminator Loss: 0.510188\tGenerator Loss: 1.731889\n",
      "Train Epoch: 79 [15360/60000 (26%)]\tDiscriminator Loss: 0.462086\tGenerator Loss: 1.071545\n",
      "Train Epoch: 79 [16640/60000 (28%)]\tDiscriminator Loss: 0.458648\tGenerator Loss: 1.317822\n",
      "Train Epoch: 79 [17920/60000 (30%)]\tDiscriminator Loss: 0.469136\tGenerator Loss: 1.431416\n",
      "Train Epoch: 79 [19200/60000 (32%)]\tDiscriminator Loss: 0.482721\tGenerator Loss: 1.442674\n",
      "Train Epoch: 79 [20480/60000 (34%)]\tDiscriminator Loss: 0.471195\tGenerator Loss: 1.223038\n",
      "Train Epoch: 79 [21760/60000 (36%)]\tDiscriminator Loss: 0.547142\tGenerator Loss: 1.497715\n",
      "Train Epoch: 79 [23040/60000 (38%)]\tDiscriminator Loss: 0.492938\tGenerator Loss: 1.116009\n",
      "Train Epoch: 79 [24320/60000 (41%)]\tDiscriminator Loss: 0.443529\tGenerator Loss: 1.784884\n",
      "Train Epoch: 79 [25600/60000 (43%)]\tDiscriminator Loss: 0.536124\tGenerator Loss: 1.268921\n",
      "Train Epoch: 79 [26880/60000 (45%)]\tDiscriminator Loss: 0.449219\tGenerator Loss: 1.356414\n",
      "Train Epoch: 79 [28160/60000 (47%)]\tDiscriminator Loss: 0.494214\tGenerator Loss: 1.871926\n",
      "Train Epoch: 79 [29440/60000 (49%)]\tDiscriminator Loss: 0.497837\tGenerator Loss: 1.381944\n",
      "Train Epoch: 79 [30720/60000 (51%)]\tDiscriminator Loss: 0.485441\tGenerator Loss: 0.934721\n",
      "Train Epoch: 79 [32000/60000 (53%)]\tDiscriminator Loss: 0.498715\tGenerator Loss: 1.174613\n",
      "Train Epoch: 79 [33280/60000 (55%)]\tDiscriminator Loss: 0.462995\tGenerator Loss: 1.410792\n",
      "Train Epoch: 79 [34560/60000 (58%)]\tDiscriminator Loss: 0.469723\tGenerator Loss: 1.446724\n",
      "Train Epoch: 79 [35840/60000 (60%)]\tDiscriminator Loss: 0.487447\tGenerator Loss: 1.403065\n",
      "Train Epoch: 79 [37120/60000 (62%)]\tDiscriminator Loss: 0.455947\tGenerator Loss: 1.379686\n",
      "Train Epoch: 79 [38400/60000 (64%)]\tDiscriminator Loss: 0.465735\tGenerator Loss: 1.556887\n",
      "Train Epoch: 79 [39680/60000 (66%)]\tDiscriminator Loss: 0.494156\tGenerator Loss: 1.435059\n",
      "Train Epoch: 79 [40960/60000 (68%)]\tDiscriminator Loss: 0.485103\tGenerator Loss: 1.330347\n",
      "Train Epoch: 79 [42240/60000 (70%)]\tDiscriminator Loss: 0.523445\tGenerator Loss: 0.961788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 79 [43520/60000 (72%)]\tDiscriminator Loss: 0.482197\tGenerator Loss: 1.490577\n",
      "Train Epoch: 79 [44800/60000 (75%)]\tDiscriminator Loss: 0.496502\tGenerator Loss: 1.482107\n",
      "Train Epoch: 79 [46080/60000 (77%)]\tDiscriminator Loss: 0.482665\tGenerator Loss: 1.415049\n",
      "Train Epoch: 79 [47360/60000 (79%)]\tDiscriminator Loss: 0.476708\tGenerator Loss: 1.567925\n",
      "Train Epoch: 79 [48640/60000 (81%)]\tDiscriminator Loss: 0.467161\tGenerator Loss: 1.658776\n",
      "Train Epoch: 79 [49920/60000 (83%)]\tDiscriminator Loss: 0.512893\tGenerator Loss: 1.270748\n",
      "Train Epoch: 79 [51200/60000 (85%)]\tDiscriminator Loss: 0.520573\tGenerator Loss: 1.083366\n",
      "Train Epoch: 79 [52480/60000 (87%)]\tDiscriminator Loss: 0.477932\tGenerator Loss: 1.410701\n",
      "Train Epoch: 79 [53760/60000 (90%)]\tDiscriminator Loss: 0.522367\tGenerator Loss: 1.833233\n",
      "Train Epoch: 79 [55040/60000 (92%)]\tDiscriminator Loss: 0.534062\tGenerator Loss: 1.216636\n",
      "Train Epoch: 79 [56320/60000 (94%)]\tDiscriminator Loss: 0.562684\tGenerator Loss: 1.271724\n",
      "Train Epoch: 79 [57600/60000 (96%)]\tDiscriminator Loss: 0.554906\tGenerator Loss: 1.398587\n",
      "Train Epoch: 79 [58880/60000 (98%)]\tDiscriminator Loss: 0.441852\tGenerator Loss: 1.423435\n",
      "Train Epoch: 80 [0/60000 (0%)]\tDiscriminator Loss: 0.468604\tGenerator Loss: 1.522029\n",
      "Train Epoch: 80 [1280/60000 (2%)]\tDiscriminator Loss: 0.453763\tGenerator Loss: 1.386403\n",
      "Train Epoch: 80 [2560/60000 (4%)]\tDiscriminator Loss: 0.447528\tGenerator Loss: 1.392379\n",
      "Train Epoch: 80 [3840/60000 (6%)]\tDiscriminator Loss: 0.466041\tGenerator Loss: 1.628382\n",
      "Train Epoch: 80 [5120/60000 (9%)]\tDiscriminator Loss: 0.525932\tGenerator Loss: 1.319580\n",
      "Train Epoch: 80 [6400/60000 (11%)]\tDiscriminator Loss: 0.513057\tGenerator Loss: 1.469610\n",
      "Train Epoch: 80 [7680/60000 (13%)]\tDiscriminator Loss: 0.444112\tGenerator Loss: 1.306579\n",
      "Train Epoch: 80 [8960/60000 (15%)]\tDiscriminator Loss: 0.497725\tGenerator Loss: 1.696322\n",
      "Train Epoch: 80 [10240/60000 (17%)]\tDiscriminator Loss: 0.508581\tGenerator Loss: 1.139068\n",
      "Train Epoch: 80 [11520/60000 (19%)]\tDiscriminator Loss: 0.484562\tGenerator Loss: 1.458535\n",
      "Train Epoch: 80 [12800/60000 (21%)]\tDiscriminator Loss: 0.492451\tGenerator Loss: 1.274617\n",
      "Train Epoch: 80 [14080/60000 (23%)]\tDiscriminator Loss: 0.483608\tGenerator Loss: 1.160824\n",
      "Train Epoch: 80 [15360/60000 (26%)]\tDiscriminator Loss: 0.505696\tGenerator Loss: 1.887237\n",
      "Train Epoch: 80 [16640/60000 (28%)]\tDiscriminator Loss: 0.475998\tGenerator Loss: 1.281008\n",
      "Train Epoch: 80 [17920/60000 (30%)]\tDiscriminator Loss: 0.554243\tGenerator Loss: 1.838503\n",
      "Train Epoch: 80 [19200/60000 (32%)]\tDiscriminator Loss: 0.447657\tGenerator Loss: 1.201465\n",
      "Train Epoch: 80 [20480/60000 (34%)]\tDiscriminator Loss: 0.518656\tGenerator Loss: 1.021211\n",
      "Train Epoch: 80 [21760/60000 (36%)]\tDiscriminator Loss: 0.520123\tGenerator Loss: 1.097647\n",
      "Train Epoch: 80 [23040/60000 (38%)]\tDiscriminator Loss: 0.482405\tGenerator Loss: 1.352350\n",
      "Train Epoch: 80 [24320/60000 (41%)]\tDiscriminator Loss: 0.505705\tGenerator Loss: 1.476130\n",
      "Train Epoch: 80 [25600/60000 (43%)]\tDiscriminator Loss: 0.549371\tGenerator Loss: 1.029999\n",
      "Train Epoch: 80 [26880/60000 (45%)]\tDiscriminator Loss: 0.532360\tGenerator Loss: 1.131194\n",
      "Train Epoch: 80 [28160/60000 (47%)]\tDiscriminator Loss: 0.447813\tGenerator Loss: 1.626437\n",
      "Train Epoch: 80 [29440/60000 (49%)]\tDiscriminator Loss: 0.514731\tGenerator Loss: 1.140280\n",
      "Train Epoch: 80 [30720/60000 (51%)]\tDiscriminator Loss: 0.489213\tGenerator Loss: 1.445762\n",
      "Train Epoch: 80 [32000/60000 (53%)]\tDiscriminator Loss: 0.497397\tGenerator Loss: 1.441881\n",
      "Train Epoch: 80 [33280/60000 (55%)]\tDiscriminator Loss: 0.465272\tGenerator Loss: 1.212456\n",
      "Train Epoch: 80 [34560/60000 (58%)]\tDiscriminator Loss: 0.490246\tGenerator Loss: 0.930619\n",
      "Train Epoch: 80 [35840/60000 (60%)]\tDiscriminator Loss: 0.509057\tGenerator Loss: 1.572463\n",
      "Train Epoch: 80 [37120/60000 (62%)]\tDiscriminator Loss: 0.489481\tGenerator Loss: 1.481832\n",
      "Train Epoch: 80 [38400/60000 (64%)]\tDiscriminator Loss: 0.517977\tGenerator Loss: 1.769090\n",
      "Train Epoch: 80 [39680/60000 (66%)]\tDiscriminator Loss: 0.552143\tGenerator Loss: 1.654353\n",
      "Train Epoch: 80 [40960/60000 (68%)]\tDiscriminator Loss: 0.537607\tGenerator Loss: 0.798430\n",
      "Train Epoch: 80 [42240/60000 (70%)]\tDiscriminator Loss: 0.451489\tGenerator Loss: 1.336957\n",
      "Train Epoch: 80 [43520/60000 (72%)]\tDiscriminator Loss: 0.454074\tGenerator Loss: 1.390638\n",
      "Train Epoch: 80 [44800/60000 (75%)]\tDiscriminator Loss: 0.566709\tGenerator Loss: 1.152147\n",
      "Train Epoch: 80 [46080/60000 (77%)]\tDiscriminator Loss: 0.485292\tGenerator Loss: 1.084474\n",
      "Train Epoch: 80 [47360/60000 (79%)]\tDiscriminator Loss: 0.462793\tGenerator Loss: 1.549373\n",
      "Train Epoch: 80 [48640/60000 (81%)]\tDiscriminator Loss: 0.524357\tGenerator Loss: 1.475887\n",
      "Train Epoch: 80 [49920/60000 (83%)]\tDiscriminator Loss: 0.497300\tGenerator Loss: 1.452763\n",
      "Train Epoch: 80 [51200/60000 (85%)]\tDiscriminator Loss: 0.506016\tGenerator Loss: 1.568761\n",
      "Train Epoch: 80 [52480/60000 (87%)]\tDiscriminator Loss: 0.472667\tGenerator Loss: 1.290226\n",
      "Train Epoch: 80 [53760/60000 (90%)]\tDiscriminator Loss: 0.450443\tGenerator Loss: 1.441959\n",
      "Train Epoch: 80 [55040/60000 (92%)]\tDiscriminator Loss: 0.461279\tGenerator Loss: 1.663352\n",
      "Train Epoch: 80 [56320/60000 (94%)]\tDiscriminator Loss: 0.479102\tGenerator Loss: 1.199581\n",
      "Train Epoch: 80 [57600/60000 (96%)]\tDiscriminator Loss: 0.470904\tGenerator Loss: 1.655166\n",
      "Train Epoch: 80 [58880/60000 (98%)]\tDiscriminator Loss: 0.527993\tGenerator Loss: 1.141114\n",
      "Train Epoch: 81 [0/60000 (0%)]\tDiscriminator Loss: 0.468508\tGenerator Loss: 1.424448\n",
      "Train Epoch: 81 [1280/60000 (2%)]\tDiscriminator Loss: 0.478392\tGenerator Loss: 1.638791\n",
      "Train Epoch: 81 [2560/60000 (4%)]\tDiscriminator Loss: 0.478369\tGenerator Loss: 1.676555\n",
      "Train Epoch: 81 [3840/60000 (6%)]\tDiscriminator Loss: 0.482373\tGenerator Loss: 1.448533\n",
      "Train Epoch: 81 [5120/60000 (9%)]\tDiscriminator Loss: 0.506764\tGenerator Loss: 1.256600\n",
      "Train Epoch: 81 [6400/60000 (11%)]\tDiscriminator Loss: 0.436056\tGenerator Loss: 1.390040\n",
      "Train Epoch: 81 [7680/60000 (13%)]\tDiscriminator Loss: 0.489264\tGenerator Loss: 1.488552\n",
      "Train Epoch: 81 [8960/60000 (15%)]\tDiscriminator Loss: 0.446660\tGenerator Loss: 1.567686\n",
      "Train Epoch: 81 [10240/60000 (17%)]\tDiscriminator Loss: 0.520512\tGenerator Loss: 1.302492\n",
      "Train Epoch: 81 [11520/60000 (19%)]\tDiscriminator Loss: 0.463356\tGenerator Loss: 1.513107\n",
      "Train Epoch: 81 [12800/60000 (21%)]\tDiscriminator Loss: 0.494387\tGenerator Loss: 1.653919\n",
      "Train Epoch: 81 [14080/60000 (23%)]\tDiscriminator Loss: 0.462459\tGenerator Loss: 1.239747\n",
      "Train Epoch: 81 [15360/60000 (26%)]\tDiscriminator Loss: 0.478917\tGenerator Loss: 1.341534\n",
      "Train Epoch: 81 [16640/60000 (28%)]\tDiscriminator Loss: 0.494500\tGenerator Loss: 1.202892\n",
      "Train Epoch: 81 [17920/60000 (30%)]\tDiscriminator Loss: 0.482298\tGenerator Loss: 1.792556\n",
      "Train Epoch: 81 [19200/60000 (32%)]\tDiscriminator Loss: 0.576779\tGenerator Loss: 0.996585\n",
      "Train Epoch: 81 [20480/60000 (34%)]\tDiscriminator Loss: 0.500753\tGenerator Loss: 1.134827\n",
      "Train Epoch: 81 [21760/60000 (36%)]\tDiscriminator Loss: 0.520646\tGenerator Loss: 1.214018\n",
      "Train Epoch: 81 [23040/60000 (38%)]\tDiscriminator Loss: 0.515924\tGenerator Loss: 1.263478\n",
      "Train Epoch: 81 [24320/60000 (41%)]\tDiscriminator Loss: 0.493347\tGenerator Loss: 1.542374\n",
      "Train Epoch: 81 [25600/60000 (43%)]\tDiscriminator Loss: 0.533260\tGenerator Loss: 1.738868\n",
      "Train Epoch: 81 [26880/60000 (45%)]\tDiscriminator Loss: 0.533980\tGenerator Loss: 1.049533\n",
      "Train Epoch: 81 [28160/60000 (47%)]\tDiscriminator Loss: 0.436566\tGenerator Loss: 1.240920\n",
      "Train Epoch: 81 [29440/60000 (49%)]\tDiscriminator Loss: 0.545397\tGenerator Loss: 1.631479\n",
      "Train Epoch: 81 [30720/60000 (51%)]\tDiscriminator Loss: 0.438578\tGenerator Loss: 1.337429\n",
      "Train Epoch: 81 [32000/60000 (53%)]\tDiscriminator Loss: 0.511190\tGenerator Loss: 1.287356\n",
      "Train Epoch: 81 [33280/60000 (55%)]\tDiscriminator Loss: 0.514369\tGenerator Loss: 1.329820\n",
      "Train Epoch: 81 [34560/60000 (58%)]\tDiscriminator Loss: 0.458228\tGenerator Loss: 1.294243\n",
      "Train Epoch: 81 [35840/60000 (60%)]\tDiscriminator Loss: 0.492279\tGenerator Loss: 1.495744\n",
      "Train Epoch: 81 [37120/60000 (62%)]\tDiscriminator Loss: 0.515641\tGenerator Loss: 1.212592\n",
      "Train Epoch: 81 [38400/60000 (64%)]\tDiscriminator Loss: 0.496366\tGenerator Loss: 1.480178\n",
      "Train Epoch: 81 [39680/60000 (66%)]\tDiscriminator Loss: 0.464656\tGenerator Loss: 1.406375\n",
      "Train Epoch: 81 [40960/60000 (68%)]\tDiscriminator Loss: 0.484624\tGenerator Loss: 1.273997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 81 [42240/60000 (70%)]\tDiscriminator Loss: 0.508784\tGenerator Loss: 1.496281\n",
      "Train Epoch: 81 [43520/60000 (72%)]\tDiscriminator Loss: 0.479978\tGenerator Loss: 1.966611\n",
      "Train Epoch: 81 [44800/60000 (75%)]\tDiscriminator Loss: 0.571837\tGenerator Loss: 1.166075\n",
      "Train Epoch: 81 [46080/60000 (77%)]\tDiscriminator Loss: 0.523816\tGenerator Loss: 1.213291\n",
      "Train Epoch: 81 [47360/60000 (79%)]\tDiscriminator Loss: 0.478099\tGenerator Loss: 1.389854\n",
      "Train Epoch: 81 [48640/60000 (81%)]\tDiscriminator Loss: 0.480335\tGenerator Loss: 1.385459\n",
      "Train Epoch: 81 [49920/60000 (83%)]\tDiscriminator Loss: 0.472653\tGenerator Loss: 1.403453\n",
      "Train Epoch: 81 [51200/60000 (85%)]\tDiscriminator Loss: 0.561516\tGenerator Loss: 1.384080\n",
      "Train Epoch: 81 [52480/60000 (87%)]\tDiscriminator Loss: 0.454733\tGenerator Loss: 1.421130\n",
      "Train Epoch: 81 [53760/60000 (90%)]\tDiscriminator Loss: 0.502126\tGenerator Loss: 1.702819\n",
      "Train Epoch: 81 [55040/60000 (92%)]\tDiscriminator Loss: 0.521008\tGenerator Loss: 1.319428\n",
      "Train Epoch: 81 [56320/60000 (94%)]\tDiscriminator Loss: 0.513888\tGenerator Loss: 1.614755\n",
      "Train Epoch: 81 [57600/60000 (96%)]\tDiscriminator Loss: 0.506715\tGenerator Loss: 1.683645\n",
      "Train Epoch: 81 [58880/60000 (98%)]\tDiscriminator Loss: 0.425462\tGenerator Loss: 1.262930\n",
      "Train Epoch: 82 [0/60000 (0%)]\tDiscriminator Loss: 0.519576\tGenerator Loss: 1.170741\n",
      "Train Epoch: 82 [1280/60000 (2%)]\tDiscriminator Loss: 0.545159\tGenerator Loss: 1.541309\n",
      "Train Epoch: 82 [2560/60000 (4%)]\tDiscriminator Loss: 0.424327\tGenerator Loss: 1.533971\n",
      "Train Epoch: 82 [3840/60000 (6%)]\tDiscriminator Loss: 0.524832\tGenerator Loss: 1.384245\n",
      "Train Epoch: 82 [5120/60000 (9%)]\tDiscriminator Loss: 0.473477\tGenerator Loss: 1.655700\n",
      "Train Epoch: 82 [6400/60000 (11%)]\tDiscriminator Loss: 0.521422\tGenerator Loss: 1.459846\n",
      "Train Epoch: 82 [7680/60000 (13%)]\tDiscriminator Loss: 0.446785\tGenerator Loss: 1.326216\n",
      "Train Epoch: 82 [8960/60000 (15%)]\tDiscriminator Loss: 0.513588\tGenerator Loss: 1.994321\n",
      "Train Epoch: 82 [10240/60000 (17%)]\tDiscriminator Loss: 0.459163\tGenerator Loss: 1.514069\n",
      "Train Epoch: 82 [11520/60000 (19%)]\tDiscriminator Loss: 0.541678\tGenerator Loss: 2.258200\n",
      "Train Epoch: 82 [12800/60000 (21%)]\tDiscriminator Loss: 0.486838\tGenerator Loss: 1.025712\n",
      "Train Epoch: 82 [14080/60000 (23%)]\tDiscriminator Loss: 0.644954\tGenerator Loss: 0.670412\n",
      "Train Epoch: 82 [15360/60000 (26%)]\tDiscriminator Loss: 0.481331\tGenerator Loss: 1.655398\n",
      "Train Epoch: 82 [16640/60000 (28%)]\tDiscriminator Loss: 0.458803\tGenerator Loss: 1.250122\n",
      "Train Epoch: 82 [17920/60000 (30%)]\tDiscriminator Loss: 0.534721\tGenerator Loss: 1.292668\n",
      "Train Epoch: 82 [19200/60000 (32%)]\tDiscriminator Loss: 0.463998\tGenerator Loss: 1.324678\n",
      "Train Epoch: 82 [20480/60000 (34%)]\tDiscriminator Loss: 0.473469\tGenerator Loss: 1.271116\n",
      "Train Epoch: 82 [21760/60000 (36%)]\tDiscriminator Loss: 0.488551\tGenerator Loss: 1.454232\n",
      "Train Epoch: 82 [23040/60000 (38%)]\tDiscriminator Loss: 0.456381\tGenerator Loss: 1.626682\n",
      "Train Epoch: 82 [24320/60000 (41%)]\tDiscriminator Loss: 0.421157\tGenerator Loss: 1.409828\n",
      "Train Epoch: 82 [25600/60000 (43%)]\tDiscriminator Loss: 0.483767\tGenerator Loss: 1.437391\n",
      "Train Epoch: 82 [26880/60000 (45%)]\tDiscriminator Loss: 0.446669\tGenerator Loss: 1.575672\n",
      "Train Epoch: 82 [28160/60000 (47%)]\tDiscriminator Loss: 0.468978\tGenerator Loss: 1.564433\n",
      "Train Epoch: 82 [29440/60000 (49%)]\tDiscriminator Loss: 0.468309\tGenerator Loss: 1.168190\n",
      "Train Epoch: 82 [30720/60000 (51%)]\tDiscriminator Loss: 0.476784\tGenerator Loss: 1.345664\n",
      "Train Epoch: 82 [32000/60000 (53%)]\tDiscriminator Loss: 0.459045\tGenerator Loss: 1.266406\n",
      "Train Epoch: 82 [33280/60000 (55%)]\tDiscriminator Loss: 0.460484\tGenerator Loss: 1.565720\n",
      "Train Epoch: 82 [34560/60000 (58%)]\tDiscriminator Loss: 0.490223\tGenerator Loss: 1.430335\n",
      "Train Epoch: 82 [35840/60000 (60%)]\tDiscriminator Loss: 0.541030\tGenerator Loss: 1.650028\n",
      "Train Epoch: 82 [37120/60000 (62%)]\tDiscriminator Loss: 0.567136\tGenerator Loss: 0.860166\n",
      "Train Epoch: 82 [38400/60000 (64%)]\tDiscriminator Loss: 0.476782\tGenerator Loss: 1.393636\n",
      "Train Epoch: 82 [39680/60000 (66%)]\tDiscriminator Loss: 0.476228\tGenerator Loss: 1.144976\n",
      "Train Epoch: 82 [40960/60000 (68%)]\tDiscriminator Loss: 0.465282\tGenerator Loss: 1.198731\n",
      "Train Epoch: 82 [42240/60000 (70%)]\tDiscriminator Loss: 0.435570\tGenerator Loss: 1.302799\n",
      "Train Epoch: 82 [43520/60000 (72%)]\tDiscriminator Loss: 0.560919\tGenerator Loss: 1.133956\n",
      "Train Epoch: 82 [44800/60000 (75%)]\tDiscriminator Loss: 0.520469\tGenerator Loss: 1.241072\n",
      "Train Epoch: 82 [46080/60000 (77%)]\tDiscriminator Loss: 0.480321\tGenerator Loss: 1.113200\n",
      "Train Epoch: 82 [47360/60000 (79%)]\tDiscriminator Loss: 0.563326\tGenerator Loss: 0.863812\n",
      "Train Epoch: 82 [48640/60000 (81%)]\tDiscriminator Loss: 0.473309\tGenerator Loss: 1.524341\n",
      "Train Epoch: 82 [49920/60000 (83%)]\tDiscriminator Loss: 0.459908\tGenerator Loss: 1.537965\n",
      "Train Epoch: 82 [51200/60000 (85%)]\tDiscriminator Loss: 0.493986\tGenerator Loss: 1.557858\n",
      "Train Epoch: 82 [52480/60000 (87%)]\tDiscriminator Loss: 0.447494\tGenerator Loss: 1.481429\n",
      "Train Epoch: 82 [53760/60000 (90%)]\tDiscriminator Loss: 0.482046\tGenerator Loss: 1.506013\n",
      "Train Epoch: 82 [55040/60000 (92%)]\tDiscriminator Loss: 0.476468\tGenerator Loss: 0.995556\n",
      "Train Epoch: 82 [56320/60000 (94%)]\tDiscriminator Loss: 0.553979\tGenerator Loss: 1.684878\n",
      "Train Epoch: 82 [57600/60000 (96%)]\tDiscriminator Loss: 0.520477\tGenerator Loss: 1.185210\n",
      "Train Epoch: 82 [58880/60000 (98%)]\tDiscriminator Loss: 0.541312\tGenerator Loss: 1.057768\n",
      "Train Epoch: 83 [0/60000 (0%)]\tDiscriminator Loss: 0.461447\tGenerator Loss: 1.331386\n",
      "Train Epoch: 83 [1280/60000 (2%)]\tDiscriminator Loss: 0.462886\tGenerator Loss: 1.211663\n",
      "Train Epoch: 83 [2560/60000 (4%)]\tDiscriminator Loss: 0.488545\tGenerator Loss: 1.543459\n",
      "Train Epoch: 83 [3840/60000 (6%)]\tDiscriminator Loss: 0.488732\tGenerator Loss: 1.143846\n",
      "Train Epoch: 83 [5120/60000 (9%)]\tDiscriminator Loss: 0.455104\tGenerator Loss: 1.272390\n",
      "Train Epoch: 83 [6400/60000 (11%)]\tDiscriminator Loss: 0.560670\tGenerator Loss: 1.168099\n",
      "Train Epoch: 83 [7680/60000 (13%)]\tDiscriminator Loss: 0.461077\tGenerator Loss: 1.288732\n",
      "Train Epoch: 83 [8960/60000 (15%)]\tDiscriminator Loss: 0.438084\tGenerator Loss: 1.487136\n",
      "Train Epoch: 83 [10240/60000 (17%)]\tDiscriminator Loss: 0.431455\tGenerator Loss: 1.468145\n",
      "Train Epoch: 83 [11520/60000 (19%)]\tDiscriminator Loss: 0.451193\tGenerator Loss: 1.476374\n",
      "Train Epoch: 83 [12800/60000 (21%)]\tDiscriminator Loss: 0.431547\tGenerator Loss: 1.638044\n",
      "Train Epoch: 83 [14080/60000 (23%)]\tDiscriminator Loss: 0.493195\tGenerator Loss: 1.322471\n",
      "Train Epoch: 83 [15360/60000 (26%)]\tDiscriminator Loss: 0.490458\tGenerator Loss: 1.232463\n",
      "Train Epoch: 83 [16640/60000 (28%)]\tDiscriminator Loss: 0.477855\tGenerator Loss: 1.698445\n",
      "Train Epoch: 83 [17920/60000 (30%)]\tDiscriminator Loss: 0.447451\tGenerator Loss: 1.774034\n",
      "Train Epoch: 83 [19200/60000 (32%)]\tDiscriminator Loss: 0.431430\tGenerator Loss: 1.504782\n",
      "Train Epoch: 83 [20480/60000 (34%)]\tDiscriminator Loss: 0.497307\tGenerator Loss: 1.304538\n",
      "Train Epoch: 83 [21760/60000 (36%)]\tDiscriminator Loss: 0.462487\tGenerator Loss: 1.425115\n",
      "Train Epoch: 83 [23040/60000 (38%)]\tDiscriminator Loss: 0.457702\tGenerator Loss: 1.452910\n",
      "Train Epoch: 83 [24320/60000 (41%)]\tDiscriminator Loss: 0.531790\tGenerator Loss: 1.695720\n",
      "Train Epoch: 83 [25600/60000 (43%)]\tDiscriminator Loss: 0.514524\tGenerator Loss: 0.963278\n",
      "Train Epoch: 83 [26880/60000 (45%)]\tDiscriminator Loss: 0.534000\tGenerator Loss: 1.450857\n",
      "Train Epoch: 83 [28160/60000 (47%)]\tDiscriminator Loss: 0.495468\tGenerator Loss: 1.356427\n",
      "Train Epoch: 83 [29440/60000 (49%)]\tDiscriminator Loss: 0.563195\tGenerator Loss: 1.217605\n",
      "Train Epoch: 83 [30720/60000 (51%)]\tDiscriminator Loss: 0.487979\tGenerator Loss: 1.491951\n",
      "Train Epoch: 83 [32000/60000 (53%)]\tDiscriminator Loss: 0.482915\tGenerator Loss: 1.136374\n",
      "Train Epoch: 83 [33280/60000 (55%)]\tDiscriminator Loss: 0.509202\tGenerator Loss: 1.221046\n",
      "Train Epoch: 83 [34560/60000 (58%)]\tDiscriminator Loss: 0.517534\tGenerator Loss: 1.244478\n",
      "Train Epoch: 83 [35840/60000 (60%)]\tDiscriminator Loss: 0.518545\tGenerator Loss: 1.122711\n",
      "Train Epoch: 83 [37120/60000 (62%)]\tDiscriminator Loss: 0.435361\tGenerator Loss: 1.474086\n",
      "Train Epoch: 83 [38400/60000 (64%)]\tDiscriminator Loss: 0.562486\tGenerator Loss: 1.760162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 83 [39680/60000 (66%)]\tDiscriminator Loss: 0.481428\tGenerator Loss: 2.009056\n",
      "Train Epoch: 83 [40960/60000 (68%)]\tDiscriminator Loss: 0.480732\tGenerator Loss: 1.500746\n",
      "Train Epoch: 83 [42240/60000 (70%)]\tDiscriminator Loss: 0.475461\tGenerator Loss: 1.334761\n",
      "Train Epoch: 83 [43520/60000 (72%)]\tDiscriminator Loss: 0.572449\tGenerator Loss: 2.111758\n",
      "Train Epoch: 83 [44800/60000 (75%)]\tDiscriminator Loss: 0.518896\tGenerator Loss: 1.016929\n",
      "Train Epoch: 83 [46080/60000 (77%)]\tDiscriminator Loss: 0.569198\tGenerator Loss: 1.892917\n",
      "Train Epoch: 83 [47360/60000 (79%)]\tDiscriminator Loss: 0.514971\tGenerator Loss: 1.400734\n",
      "Train Epoch: 83 [48640/60000 (81%)]\tDiscriminator Loss: 0.513009\tGenerator Loss: 1.278238\n",
      "Train Epoch: 83 [49920/60000 (83%)]\tDiscriminator Loss: 0.439262\tGenerator Loss: 1.547677\n",
      "Train Epoch: 83 [51200/60000 (85%)]\tDiscriminator Loss: 0.498424\tGenerator Loss: 1.610367\n",
      "Train Epoch: 83 [52480/60000 (87%)]\tDiscriminator Loss: 0.502005\tGenerator Loss: 1.482930\n",
      "Train Epoch: 83 [53760/60000 (90%)]\tDiscriminator Loss: 0.485873\tGenerator Loss: 1.537083\n",
      "Train Epoch: 83 [55040/60000 (92%)]\tDiscriminator Loss: 0.485133\tGenerator Loss: 1.168839\n",
      "Train Epoch: 83 [56320/60000 (94%)]\tDiscriminator Loss: 0.478035\tGenerator Loss: 1.516111\n",
      "Train Epoch: 83 [57600/60000 (96%)]\tDiscriminator Loss: 0.488319\tGenerator Loss: 1.330054\n",
      "Train Epoch: 83 [58880/60000 (98%)]\tDiscriminator Loss: 0.509245\tGenerator Loss: 1.411667\n",
      "Train Epoch: 84 [0/60000 (0%)]\tDiscriminator Loss: 0.410493\tGenerator Loss: 1.386074\n",
      "Train Epoch: 84 [1280/60000 (2%)]\tDiscriminator Loss: 0.510146\tGenerator Loss: 1.252406\n",
      "Train Epoch: 84 [2560/60000 (4%)]\tDiscriminator Loss: 0.487103\tGenerator Loss: 1.744271\n",
      "Train Epoch: 84 [3840/60000 (6%)]\tDiscriminator Loss: 0.417474\tGenerator Loss: 1.506424\n",
      "Train Epoch: 84 [5120/60000 (9%)]\tDiscriminator Loss: 0.439986\tGenerator Loss: 1.342101\n",
      "Train Epoch: 84 [6400/60000 (11%)]\tDiscriminator Loss: 0.471069\tGenerator Loss: 1.126677\n",
      "Train Epoch: 84 [7680/60000 (13%)]\tDiscriminator Loss: 0.507359\tGenerator Loss: 1.535873\n",
      "Train Epoch: 84 [8960/60000 (15%)]\tDiscriminator Loss: 0.469558\tGenerator Loss: 1.619901\n",
      "Train Epoch: 84 [10240/60000 (17%)]\tDiscriminator Loss: 0.494389\tGenerator Loss: 1.092811\n",
      "Train Epoch: 84 [11520/60000 (19%)]\tDiscriminator Loss: 0.495143\tGenerator Loss: 1.912031\n",
      "Train Epoch: 84 [12800/60000 (21%)]\tDiscriminator Loss: 0.499443\tGenerator Loss: 1.419006\n",
      "Train Epoch: 84 [14080/60000 (23%)]\tDiscriminator Loss: 0.536881\tGenerator Loss: 1.533892\n",
      "Train Epoch: 84 [15360/60000 (26%)]\tDiscriminator Loss: 0.482089\tGenerator Loss: 1.505375\n",
      "Train Epoch: 84 [16640/60000 (28%)]\tDiscriminator Loss: 0.486109\tGenerator Loss: 1.195451\n",
      "Train Epoch: 84 [17920/60000 (30%)]\tDiscriminator Loss: 0.502642\tGenerator Loss: 1.137034\n",
      "Train Epoch: 84 [19200/60000 (32%)]\tDiscriminator Loss: 0.494635\tGenerator Loss: 1.495335\n",
      "Train Epoch: 84 [20480/60000 (34%)]\tDiscriminator Loss: 0.499271\tGenerator Loss: 1.012731\n",
      "Train Epoch: 84 [21760/60000 (36%)]\tDiscriminator Loss: 0.512972\tGenerator Loss: 1.316113\n",
      "Train Epoch: 84 [23040/60000 (38%)]\tDiscriminator Loss: 0.442057\tGenerator Loss: 1.676715\n",
      "Train Epoch: 84 [24320/60000 (41%)]\tDiscriminator Loss: 0.454584\tGenerator Loss: 1.402847\n",
      "Train Epoch: 84 [25600/60000 (43%)]\tDiscriminator Loss: 0.472381\tGenerator Loss: 1.384760\n",
      "Train Epoch: 84 [26880/60000 (45%)]\tDiscriminator Loss: 0.456171\tGenerator Loss: 1.369981\n",
      "Train Epoch: 84 [28160/60000 (47%)]\tDiscriminator Loss: 0.441250\tGenerator Loss: 1.315484\n",
      "Train Epoch: 84 [29440/60000 (49%)]\tDiscriminator Loss: 0.504008\tGenerator Loss: 1.492588\n",
      "Train Epoch: 84 [30720/60000 (51%)]\tDiscriminator Loss: 0.511476\tGenerator Loss: 1.483900\n",
      "Train Epoch: 84 [32000/60000 (53%)]\tDiscriminator Loss: 0.519638\tGenerator Loss: 1.122257\n",
      "Train Epoch: 84 [33280/60000 (55%)]\tDiscriminator Loss: 0.438994\tGenerator Loss: 1.383043\n",
      "Train Epoch: 84 [34560/60000 (58%)]\tDiscriminator Loss: 0.514645\tGenerator Loss: 1.046752\n",
      "Train Epoch: 84 [35840/60000 (60%)]\tDiscriminator Loss: 0.476909\tGenerator Loss: 1.572021\n",
      "Train Epoch: 84 [37120/60000 (62%)]\tDiscriminator Loss: 0.467270\tGenerator Loss: 1.202775\n",
      "Train Epoch: 84 [38400/60000 (64%)]\tDiscriminator Loss: 0.451816\tGenerator Loss: 1.460243\n",
      "Train Epoch: 84 [39680/60000 (66%)]\tDiscriminator Loss: 0.548370\tGenerator Loss: 1.875329\n",
      "Train Epoch: 84 [40960/60000 (68%)]\tDiscriminator Loss: 0.482118\tGenerator Loss: 1.313299\n",
      "Train Epoch: 84 [42240/60000 (70%)]\tDiscriminator Loss: 0.487616\tGenerator Loss: 1.370826\n",
      "Train Epoch: 84 [43520/60000 (72%)]\tDiscriminator Loss: 0.493015\tGenerator Loss: 1.611394\n",
      "Train Epoch: 84 [44800/60000 (75%)]\tDiscriminator Loss: 0.479106\tGenerator Loss: 1.186346\n",
      "Train Epoch: 84 [46080/60000 (77%)]\tDiscriminator Loss: 0.533398\tGenerator Loss: 1.406722\n",
      "Train Epoch: 84 [47360/60000 (79%)]\tDiscriminator Loss: 0.449910\tGenerator Loss: 1.581532\n",
      "Train Epoch: 84 [48640/60000 (81%)]\tDiscriminator Loss: 0.453957\tGenerator Loss: 1.743668\n",
      "Train Epoch: 84 [49920/60000 (83%)]\tDiscriminator Loss: 0.457061\tGenerator Loss: 1.514599\n",
      "Train Epoch: 84 [51200/60000 (85%)]\tDiscriminator Loss: 0.445246\tGenerator Loss: 1.245335\n",
      "Train Epoch: 84 [52480/60000 (87%)]\tDiscriminator Loss: 0.484812\tGenerator Loss: 1.089829\n",
      "Train Epoch: 84 [53760/60000 (90%)]\tDiscriminator Loss: 0.486186\tGenerator Loss: 1.291476\n",
      "Train Epoch: 84 [55040/60000 (92%)]\tDiscriminator Loss: 0.556017\tGenerator Loss: 1.445795\n",
      "Train Epoch: 84 [56320/60000 (94%)]\tDiscriminator Loss: 0.482921\tGenerator Loss: 1.159743\n",
      "Train Epoch: 84 [57600/60000 (96%)]\tDiscriminator Loss: 0.450284\tGenerator Loss: 1.285329\n",
      "Train Epoch: 84 [58880/60000 (98%)]\tDiscriminator Loss: 0.476731\tGenerator Loss: 1.305268\n",
      "Train Epoch: 85 [0/60000 (0%)]\tDiscriminator Loss: 0.495066\tGenerator Loss: 1.144335\n",
      "Train Epoch: 85 [1280/60000 (2%)]\tDiscriminator Loss: 0.490628\tGenerator Loss: 1.750712\n",
      "Train Epoch: 85 [2560/60000 (4%)]\tDiscriminator Loss: 0.453021\tGenerator Loss: 1.009731\n",
      "Train Epoch: 85 [3840/60000 (6%)]\tDiscriminator Loss: 0.432710\tGenerator Loss: 1.182465\n",
      "Train Epoch: 85 [5120/60000 (9%)]\tDiscriminator Loss: 0.460514\tGenerator Loss: 1.248993\n",
      "Train Epoch: 85 [6400/60000 (11%)]\tDiscriminator Loss: 0.516815\tGenerator Loss: 1.638673\n",
      "Train Epoch: 85 [7680/60000 (13%)]\tDiscriminator Loss: 0.498172\tGenerator Loss: 1.065084\n",
      "Train Epoch: 85 [8960/60000 (15%)]\tDiscriminator Loss: 0.497419\tGenerator Loss: 1.651320\n",
      "Train Epoch: 85 [10240/60000 (17%)]\tDiscriminator Loss: 0.483506\tGenerator Loss: 1.343860\n",
      "Train Epoch: 85 [11520/60000 (19%)]\tDiscriminator Loss: 0.492169\tGenerator Loss: 1.423860\n",
      "Train Epoch: 85 [12800/60000 (21%)]\tDiscriminator Loss: 0.482910\tGenerator Loss: 1.336086\n",
      "Train Epoch: 85 [14080/60000 (23%)]\tDiscriminator Loss: 0.501720\tGenerator Loss: 1.340438\n",
      "Train Epoch: 85 [15360/60000 (26%)]\tDiscriminator Loss: 0.498382\tGenerator Loss: 1.268293\n",
      "Train Epoch: 85 [16640/60000 (28%)]\tDiscriminator Loss: 0.480746\tGenerator Loss: 1.760568\n",
      "Train Epoch: 85 [17920/60000 (30%)]\tDiscriminator Loss: 0.483054\tGenerator Loss: 1.427357\n",
      "Train Epoch: 85 [19200/60000 (32%)]\tDiscriminator Loss: 0.456398\tGenerator Loss: 2.046661\n",
      "Train Epoch: 85 [20480/60000 (34%)]\tDiscriminator Loss: 0.508852\tGenerator Loss: 1.315130\n",
      "Train Epoch: 85 [21760/60000 (36%)]\tDiscriminator Loss: 0.501319\tGenerator Loss: 1.449687\n",
      "Train Epoch: 85 [23040/60000 (38%)]\tDiscriminator Loss: 0.496423\tGenerator Loss: 1.419963\n",
      "Train Epoch: 85 [24320/60000 (41%)]\tDiscriminator Loss: 0.494363\tGenerator Loss: 1.540154\n",
      "Train Epoch: 85 [25600/60000 (43%)]\tDiscriminator Loss: 0.527022\tGenerator Loss: 1.621418\n",
      "Train Epoch: 85 [26880/60000 (45%)]\tDiscriminator Loss: 0.546992\tGenerator Loss: 1.518685\n",
      "Train Epoch: 85 [28160/60000 (47%)]\tDiscriminator Loss: 0.501287\tGenerator Loss: 1.563646\n",
      "Train Epoch: 85 [29440/60000 (49%)]\tDiscriminator Loss: 0.462584\tGenerator Loss: 1.578450\n",
      "Train Epoch: 85 [30720/60000 (51%)]\tDiscriminator Loss: 0.559705\tGenerator Loss: 1.670400\n",
      "Train Epoch: 85 [32000/60000 (53%)]\tDiscriminator Loss: 0.516540\tGenerator Loss: 1.458126\n",
      "Train Epoch: 85 [33280/60000 (55%)]\tDiscriminator Loss: 0.488817\tGenerator Loss: 1.496427\n",
      "Train Epoch: 85 [34560/60000 (58%)]\tDiscriminator Loss: 0.461187\tGenerator Loss: 1.876940\n",
      "Train Epoch: 85 [35840/60000 (60%)]\tDiscriminator Loss: 0.413164\tGenerator Loss: 1.412418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 85 [37120/60000 (62%)]\tDiscriminator Loss: 0.541449\tGenerator Loss: 1.567543\n",
      "Train Epoch: 85 [38400/60000 (64%)]\tDiscriminator Loss: 0.414603\tGenerator Loss: 1.630522\n",
      "Train Epoch: 85 [39680/60000 (66%)]\tDiscriminator Loss: 0.465922\tGenerator Loss: 1.545937\n",
      "Train Epoch: 85 [40960/60000 (68%)]\tDiscriminator Loss: 0.446018\tGenerator Loss: 1.303920\n",
      "Train Epoch: 85 [42240/60000 (70%)]\tDiscriminator Loss: 0.446231\tGenerator Loss: 1.303927\n",
      "Train Epoch: 85 [43520/60000 (72%)]\tDiscriminator Loss: 0.525465\tGenerator Loss: 1.389797\n",
      "Train Epoch: 85 [44800/60000 (75%)]\tDiscriminator Loss: 0.463984\tGenerator Loss: 1.419855\n",
      "Train Epoch: 85 [46080/60000 (77%)]\tDiscriminator Loss: 0.488612\tGenerator Loss: 1.311958\n",
      "Train Epoch: 85 [47360/60000 (79%)]\tDiscriminator Loss: 0.483043\tGenerator Loss: 1.615402\n",
      "Train Epoch: 85 [48640/60000 (81%)]\tDiscriminator Loss: 0.515508\tGenerator Loss: 1.425577\n",
      "Train Epoch: 85 [49920/60000 (83%)]\tDiscriminator Loss: 0.424832\tGenerator Loss: 1.475694\n",
      "Train Epoch: 85 [51200/60000 (85%)]\tDiscriminator Loss: 0.456407\tGenerator Loss: 1.751928\n",
      "Train Epoch: 85 [52480/60000 (87%)]\tDiscriminator Loss: 0.504769\tGenerator Loss: 1.345397\n",
      "Train Epoch: 85 [53760/60000 (90%)]\tDiscriminator Loss: 0.517544\tGenerator Loss: 1.244736\n",
      "Train Epoch: 85 [55040/60000 (92%)]\tDiscriminator Loss: 0.494304\tGenerator Loss: 1.436087\n",
      "Train Epoch: 85 [56320/60000 (94%)]\tDiscriminator Loss: 0.512698\tGenerator Loss: 1.762343\n",
      "Train Epoch: 85 [57600/60000 (96%)]\tDiscriminator Loss: 0.431474\tGenerator Loss: 1.275742\n",
      "Train Epoch: 85 [58880/60000 (98%)]\tDiscriminator Loss: 0.482360\tGenerator Loss: 1.115522\n",
      "Train Epoch: 86 [0/60000 (0%)]\tDiscriminator Loss: 0.525753\tGenerator Loss: 1.124125\n",
      "Train Epoch: 86 [1280/60000 (2%)]\tDiscriminator Loss: 0.449145\tGenerator Loss: 1.706862\n",
      "Train Epoch: 86 [2560/60000 (4%)]\tDiscriminator Loss: 0.436972\tGenerator Loss: 1.455136\n",
      "Train Epoch: 86 [3840/60000 (6%)]\tDiscriminator Loss: 0.489775\tGenerator Loss: 1.371917\n",
      "Train Epoch: 86 [5120/60000 (9%)]\tDiscriminator Loss: 0.489861\tGenerator Loss: 1.287729\n",
      "Train Epoch: 86 [6400/60000 (11%)]\tDiscriminator Loss: 0.495821\tGenerator Loss: 1.119366\n",
      "Train Epoch: 86 [7680/60000 (13%)]\tDiscriminator Loss: 0.474786\tGenerator Loss: 1.210147\n",
      "Train Epoch: 86 [8960/60000 (15%)]\tDiscriminator Loss: 0.484208\tGenerator Loss: 1.538622\n",
      "Train Epoch: 86 [10240/60000 (17%)]\tDiscriminator Loss: 0.455193\tGenerator Loss: 1.348602\n",
      "Train Epoch: 86 [11520/60000 (19%)]\tDiscriminator Loss: 0.469908\tGenerator Loss: 1.793435\n",
      "Train Epoch: 86 [12800/60000 (21%)]\tDiscriminator Loss: 0.482293\tGenerator Loss: 1.517398\n",
      "Train Epoch: 86 [14080/60000 (23%)]\tDiscriminator Loss: 0.474252\tGenerator Loss: 1.143655\n",
      "Train Epoch: 86 [15360/60000 (26%)]\tDiscriminator Loss: 0.513006\tGenerator Loss: 1.716405\n",
      "Train Epoch: 86 [16640/60000 (28%)]\tDiscriminator Loss: 0.427896\tGenerator Loss: 1.331917\n",
      "Train Epoch: 86 [17920/60000 (30%)]\tDiscriminator Loss: 0.413649\tGenerator Loss: 1.417985\n",
      "Train Epoch: 86 [19200/60000 (32%)]\tDiscriminator Loss: 0.418892\tGenerator Loss: 1.483500\n",
      "Train Epoch: 86 [20480/60000 (34%)]\tDiscriminator Loss: 0.468508\tGenerator Loss: 1.506774\n",
      "Train Epoch: 86 [21760/60000 (36%)]\tDiscriminator Loss: 0.475104\tGenerator Loss: 1.730629\n",
      "Train Epoch: 86 [23040/60000 (38%)]\tDiscriminator Loss: 0.442101\tGenerator Loss: 1.287949\n",
      "Train Epoch: 86 [24320/60000 (41%)]\tDiscriminator Loss: 0.468275\tGenerator Loss: 1.149425\n",
      "Train Epoch: 86 [25600/60000 (43%)]\tDiscriminator Loss: 0.473349\tGenerator Loss: 1.809175\n",
      "Train Epoch: 86 [26880/60000 (45%)]\tDiscriminator Loss: 0.490731\tGenerator Loss: 1.364919\n",
      "Train Epoch: 86 [28160/60000 (47%)]\tDiscriminator Loss: 0.462789\tGenerator Loss: 1.693718\n",
      "Train Epoch: 86 [29440/60000 (49%)]\tDiscriminator Loss: 0.478064\tGenerator Loss: 1.271006\n",
      "Train Epoch: 86 [30720/60000 (51%)]\tDiscriminator Loss: 0.480576\tGenerator Loss: 1.671513\n",
      "Train Epoch: 86 [32000/60000 (53%)]\tDiscriminator Loss: 0.472560\tGenerator Loss: 1.207371\n",
      "Train Epoch: 86 [33280/60000 (55%)]\tDiscriminator Loss: 0.477142\tGenerator Loss: 1.294959\n",
      "Train Epoch: 86 [34560/60000 (58%)]\tDiscriminator Loss: 0.534408\tGenerator Loss: 1.220816\n",
      "Train Epoch: 86 [35840/60000 (60%)]\tDiscriminator Loss: 0.489888\tGenerator Loss: 1.471990\n",
      "Train Epoch: 86 [37120/60000 (62%)]\tDiscriminator Loss: 0.455232\tGenerator Loss: 1.355592\n",
      "Train Epoch: 86 [38400/60000 (64%)]\tDiscriminator Loss: 0.475124\tGenerator Loss: 1.170774\n",
      "Train Epoch: 86 [39680/60000 (66%)]\tDiscriminator Loss: 0.492476\tGenerator Loss: 1.531782\n",
      "Train Epoch: 86 [40960/60000 (68%)]\tDiscriminator Loss: 0.461542\tGenerator Loss: 1.354740\n",
      "Train Epoch: 86 [42240/60000 (70%)]\tDiscriminator Loss: 0.501981\tGenerator Loss: 1.153523\n",
      "Train Epoch: 86 [43520/60000 (72%)]\tDiscriminator Loss: 0.478944\tGenerator Loss: 1.573514\n",
      "Train Epoch: 86 [44800/60000 (75%)]\tDiscriminator Loss: 0.445856\tGenerator Loss: 1.498199\n",
      "Train Epoch: 86 [46080/60000 (77%)]\tDiscriminator Loss: 0.466675\tGenerator Loss: 1.473465\n",
      "Train Epoch: 86 [47360/60000 (79%)]\tDiscriminator Loss: 0.509736\tGenerator Loss: 1.150665\n",
      "Train Epoch: 86 [48640/60000 (81%)]\tDiscriminator Loss: 0.492827\tGenerator Loss: 1.424020\n",
      "Train Epoch: 86 [49920/60000 (83%)]\tDiscriminator Loss: 0.470897\tGenerator Loss: 1.596821\n",
      "Train Epoch: 86 [51200/60000 (85%)]\tDiscriminator Loss: 0.470474\tGenerator Loss: 1.324378\n",
      "Train Epoch: 86 [52480/60000 (87%)]\tDiscriminator Loss: 0.529182\tGenerator Loss: 1.344666\n",
      "Train Epoch: 86 [53760/60000 (90%)]\tDiscriminator Loss: 0.446395\tGenerator Loss: 1.393126\n",
      "Train Epoch: 86 [55040/60000 (92%)]\tDiscriminator Loss: 0.538023\tGenerator Loss: 1.426058\n",
      "Train Epoch: 86 [56320/60000 (94%)]\tDiscriminator Loss: 0.542695\tGenerator Loss: 0.975418\n",
      "Train Epoch: 86 [57600/60000 (96%)]\tDiscriminator Loss: 0.512499\tGenerator Loss: 1.366724\n",
      "Train Epoch: 86 [58880/60000 (98%)]\tDiscriminator Loss: 0.472040\tGenerator Loss: 1.338669\n",
      "Train Epoch: 87 [0/60000 (0%)]\tDiscriminator Loss: 0.541217\tGenerator Loss: 1.221154\n",
      "Train Epoch: 87 [1280/60000 (2%)]\tDiscriminator Loss: 0.499266\tGenerator Loss: 1.159905\n",
      "Train Epoch: 87 [2560/60000 (4%)]\tDiscriminator Loss: 0.438561\tGenerator Loss: 1.575932\n",
      "Train Epoch: 87 [3840/60000 (6%)]\tDiscriminator Loss: 0.516523\tGenerator Loss: 1.277415\n",
      "Train Epoch: 87 [5120/60000 (9%)]\tDiscriminator Loss: 0.491758\tGenerator Loss: 1.375053\n",
      "Train Epoch: 87 [6400/60000 (11%)]\tDiscriminator Loss: 0.467315\tGenerator Loss: 1.510573\n",
      "Train Epoch: 87 [7680/60000 (13%)]\tDiscriminator Loss: 0.444532\tGenerator Loss: 1.607255\n",
      "Train Epoch: 87 [8960/60000 (15%)]\tDiscriminator Loss: 0.424343\tGenerator Loss: 1.612537\n",
      "Train Epoch: 87 [10240/60000 (17%)]\tDiscriminator Loss: 0.571329\tGenerator Loss: 1.003422\n",
      "Train Epoch: 87 [11520/60000 (19%)]\tDiscriminator Loss: 0.481251\tGenerator Loss: 1.352686\n",
      "Train Epoch: 87 [12800/60000 (21%)]\tDiscriminator Loss: 0.429220\tGenerator Loss: 1.468285\n",
      "Train Epoch: 87 [14080/60000 (23%)]\tDiscriminator Loss: 0.498001\tGenerator Loss: 1.555880\n",
      "Train Epoch: 87 [15360/60000 (26%)]\tDiscriminator Loss: 0.456893\tGenerator Loss: 1.380470\n",
      "Train Epoch: 87 [16640/60000 (28%)]\tDiscriminator Loss: 0.543602\tGenerator Loss: 1.180426\n",
      "Train Epoch: 87 [17920/60000 (30%)]\tDiscriminator Loss: 0.540663\tGenerator Loss: 1.262492\n",
      "Train Epoch: 87 [19200/60000 (32%)]\tDiscriminator Loss: 0.463898\tGenerator Loss: 1.618443\n",
      "Train Epoch: 87 [20480/60000 (34%)]\tDiscriminator Loss: 0.500250\tGenerator Loss: 1.336896\n",
      "Train Epoch: 87 [21760/60000 (36%)]\tDiscriminator Loss: 0.423202\tGenerator Loss: 1.529505\n",
      "Train Epoch: 87 [23040/60000 (38%)]\tDiscriminator Loss: 0.505048\tGenerator Loss: 0.982652\n",
      "Train Epoch: 87 [24320/60000 (41%)]\tDiscriminator Loss: 0.431884\tGenerator Loss: 1.591544\n",
      "Train Epoch: 87 [25600/60000 (43%)]\tDiscriminator Loss: 0.501354\tGenerator Loss: 1.990117\n",
      "Train Epoch: 87 [26880/60000 (45%)]\tDiscriminator Loss: 0.487385\tGenerator Loss: 1.739881\n",
      "Train Epoch: 87 [28160/60000 (47%)]\tDiscriminator Loss: 0.489477\tGenerator Loss: 1.307676\n",
      "Train Epoch: 87 [29440/60000 (49%)]\tDiscriminator Loss: 0.469534\tGenerator Loss: 1.348350\n",
      "Train Epoch: 87 [30720/60000 (51%)]\tDiscriminator Loss: 0.493449\tGenerator Loss: 1.339293\n",
      "Train Epoch: 87 [32000/60000 (53%)]\tDiscriminator Loss: 0.469054\tGenerator Loss: 1.826573\n",
      "Train Epoch: 87 [33280/60000 (55%)]\tDiscriminator Loss: 0.495792\tGenerator Loss: 1.764246\n",
      "Train Epoch: 87 [34560/60000 (58%)]\tDiscriminator Loss: 0.459394\tGenerator Loss: 1.832925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 87 [35840/60000 (60%)]\tDiscriminator Loss: 0.460145\tGenerator Loss: 1.795252\n",
      "Train Epoch: 87 [37120/60000 (62%)]\tDiscriminator Loss: 0.477892\tGenerator Loss: 1.205940\n",
      "Train Epoch: 87 [38400/60000 (64%)]\tDiscriminator Loss: 0.462058\tGenerator Loss: 1.456979\n",
      "Train Epoch: 87 [39680/60000 (66%)]\tDiscriminator Loss: 0.497645\tGenerator Loss: 1.197611\n",
      "Train Epoch: 87 [40960/60000 (68%)]\tDiscriminator Loss: 0.506538\tGenerator Loss: 1.286095\n",
      "Train Epoch: 87 [42240/60000 (70%)]\tDiscriminator Loss: 0.489109\tGenerator Loss: 1.588880\n",
      "Train Epoch: 87 [43520/60000 (72%)]\tDiscriminator Loss: 0.432010\tGenerator Loss: 1.285795\n",
      "Train Epoch: 87 [44800/60000 (75%)]\tDiscriminator Loss: 0.467848\tGenerator Loss: 1.291724\n",
      "Train Epoch: 87 [46080/60000 (77%)]\tDiscriminator Loss: 0.529799\tGenerator Loss: 1.315111\n",
      "Train Epoch: 87 [47360/60000 (79%)]\tDiscriminator Loss: 0.490578\tGenerator Loss: 1.762926\n",
      "Train Epoch: 87 [48640/60000 (81%)]\tDiscriminator Loss: 0.458101\tGenerator Loss: 1.685366\n",
      "Train Epoch: 87 [49920/60000 (83%)]\tDiscriminator Loss: 0.504281\tGenerator Loss: 0.991613\n",
      "Train Epoch: 87 [51200/60000 (85%)]\tDiscriminator Loss: 0.507836\tGenerator Loss: 1.267702\n",
      "Train Epoch: 87 [52480/60000 (87%)]\tDiscriminator Loss: 0.444933\tGenerator Loss: 1.703727\n",
      "Train Epoch: 87 [53760/60000 (90%)]\tDiscriminator Loss: 0.424496\tGenerator Loss: 1.727577\n",
      "Train Epoch: 87 [55040/60000 (92%)]\tDiscriminator Loss: 0.429468\tGenerator Loss: 1.462016\n",
      "Train Epoch: 87 [56320/60000 (94%)]\tDiscriminator Loss: 0.431529\tGenerator Loss: 1.543603\n",
      "Train Epoch: 87 [57600/60000 (96%)]\tDiscriminator Loss: 0.459562\tGenerator Loss: 1.457221\n",
      "Train Epoch: 87 [58880/60000 (98%)]\tDiscriminator Loss: 0.497172\tGenerator Loss: 1.362406\n",
      "Train Epoch: 88 [0/60000 (0%)]\tDiscriminator Loss: 0.482390\tGenerator Loss: 1.310205\n",
      "Train Epoch: 88 [1280/60000 (2%)]\tDiscriminator Loss: 0.452422\tGenerator Loss: 1.571495\n",
      "Train Epoch: 88 [2560/60000 (4%)]\tDiscriminator Loss: 0.479783\tGenerator Loss: 1.704347\n",
      "Train Epoch: 88 [3840/60000 (6%)]\tDiscriminator Loss: 0.497504\tGenerator Loss: 1.295268\n",
      "Train Epoch: 88 [5120/60000 (9%)]\tDiscriminator Loss: 0.513900\tGenerator Loss: 1.943649\n",
      "Train Epoch: 88 [6400/60000 (11%)]\tDiscriminator Loss: 0.463566\tGenerator Loss: 1.463324\n",
      "Train Epoch: 88 [7680/60000 (13%)]\tDiscriminator Loss: 0.531156\tGenerator Loss: 1.110653\n",
      "Train Epoch: 88 [8960/60000 (15%)]\tDiscriminator Loss: 0.464290\tGenerator Loss: 1.162829\n",
      "Train Epoch: 88 [10240/60000 (17%)]\tDiscriminator Loss: 0.487567\tGenerator Loss: 1.297060\n",
      "Train Epoch: 88 [11520/60000 (19%)]\tDiscriminator Loss: 0.525390\tGenerator Loss: 2.246369\n",
      "Train Epoch: 88 [12800/60000 (21%)]\tDiscriminator Loss: 0.478602\tGenerator Loss: 1.422705\n",
      "Train Epoch: 88 [14080/60000 (23%)]\tDiscriminator Loss: 0.471593\tGenerator Loss: 1.208415\n",
      "Train Epoch: 88 [15360/60000 (26%)]\tDiscriminator Loss: 0.502979\tGenerator Loss: 1.222440\n",
      "Train Epoch: 88 [16640/60000 (28%)]\tDiscriminator Loss: 0.501415\tGenerator Loss: 1.473170\n",
      "Train Epoch: 88 [17920/60000 (30%)]\tDiscriminator Loss: 0.533122\tGenerator Loss: 1.932549\n",
      "Train Epoch: 88 [19200/60000 (32%)]\tDiscriminator Loss: 0.548946\tGenerator Loss: 1.040344\n",
      "Train Epoch: 88 [20480/60000 (34%)]\tDiscriminator Loss: 0.460995\tGenerator Loss: 1.222496\n",
      "Train Epoch: 88 [21760/60000 (36%)]\tDiscriminator Loss: 0.453036\tGenerator Loss: 1.341198\n",
      "Train Epoch: 88 [23040/60000 (38%)]\tDiscriminator Loss: 0.505747\tGenerator Loss: 1.664369\n",
      "Train Epoch: 88 [24320/60000 (41%)]\tDiscriminator Loss: 0.485931\tGenerator Loss: 1.556170\n",
      "Train Epoch: 88 [25600/60000 (43%)]\tDiscriminator Loss: 0.490265\tGenerator Loss: 1.339507\n",
      "Train Epoch: 88 [26880/60000 (45%)]\tDiscriminator Loss: 0.501021\tGenerator Loss: 1.260172\n",
      "Train Epoch: 88 [28160/60000 (47%)]\tDiscriminator Loss: 0.480795\tGenerator Loss: 1.426437\n",
      "Train Epoch: 88 [29440/60000 (49%)]\tDiscriminator Loss: 0.464796\tGenerator Loss: 1.685791\n",
      "Train Epoch: 88 [30720/60000 (51%)]\tDiscriminator Loss: 0.471604\tGenerator Loss: 1.397991\n",
      "Train Epoch: 88 [32000/60000 (53%)]\tDiscriminator Loss: 0.553604\tGenerator Loss: 1.037505\n",
      "Train Epoch: 88 [33280/60000 (55%)]\tDiscriminator Loss: 0.475854\tGenerator Loss: 1.265648\n",
      "Train Epoch: 88 [34560/60000 (58%)]\tDiscriminator Loss: 0.540504\tGenerator Loss: 1.576739\n",
      "Train Epoch: 88 [35840/60000 (60%)]\tDiscriminator Loss: 0.451379\tGenerator Loss: 1.713109\n",
      "Train Epoch: 88 [37120/60000 (62%)]\tDiscriminator Loss: 0.556179\tGenerator Loss: 1.698971\n",
      "Train Epoch: 88 [38400/60000 (64%)]\tDiscriminator Loss: 0.450883\tGenerator Loss: 1.768064\n",
      "Train Epoch: 88 [39680/60000 (66%)]\tDiscriminator Loss: 0.495457\tGenerator Loss: 1.405668\n",
      "Train Epoch: 88 [40960/60000 (68%)]\tDiscriminator Loss: 0.447352\tGenerator Loss: 1.313001\n",
      "Train Epoch: 88 [42240/60000 (70%)]\tDiscriminator Loss: 0.450388\tGenerator Loss: 1.514482\n",
      "Train Epoch: 88 [43520/60000 (72%)]\tDiscriminator Loss: 0.462432\tGenerator Loss: 1.423010\n",
      "Train Epoch: 88 [44800/60000 (75%)]\tDiscriminator Loss: 0.504940\tGenerator Loss: 1.525877\n",
      "Train Epoch: 88 [46080/60000 (77%)]\tDiscriminator Loss: 0.514382\tGenerator Loss: 1.541934\n",
      "Train Epoch: 88 [47360/60000 (79%)]\tDiscriminator Loss: 0.443331\tGenerator Loss: 1.362098\n",
      "Train Epoch: 88 [48640/60000 (81%)]\tDiscriminator Loss: 0.480216\tGenerator Loss: 1.361076\n",
      "Train Epoch: 88 [49920/60000 (83%)]\tDiscriminator Loss: 0.489062\tGenerator Loss: 1.471448\n",
      "Train Epoch: 88 [51200/60000 (85%)]\tDiscriminator Loss: 0.466653\tGenerator Loss: 1.321329\n",
      "Train Epoch: 88 [52480/60000 (87%)]\tDiscriminator Loss: 0.522337\tGenerator Loss: 1.612721\n",
      "Train Epoch: 88 [53760/60000 (90%)]\tDiscriminator Loss: 0.467158\tGenerator Loss: 1.366805\n",
      "Train Epoch: 88 [55040/60000 (92%)]\tDiscriminator Loss: 0.508617\tGenerator Loss: 1.111830\n",
      "Train Epoch: 88 [56320/60000 (94%)]\tDiscriminator Loss: 0.480794\tGenerator Loss: 1.569144\n",
      "Train Epoch: 88 [57600/60000 (96%)]\tDiscriminator Loss: 0.485010\tGenerator Loss: 1.306286\n",
      "Train Epoch: 88 [58880/60000 (98%)]\tDiscriminator Loss: 0.459535\tGenerator Loss: 1.671063\n",
      "Train Epoch: 89 [0/60000 (0%)]\tDiscriminator Loss: 0.444943\tGenerator Loss: 1.392070\n",
      "Train Epoch: 89 [1280/60000 (2%)]\tDiscriminator Loss: 0.464577\tGenerator Loss: 1.333623\n",
      "Train Epoch: 89 [2560/60000 (4%)]\tDiscriminator Loss: 0.475268\tGenerator Loss: 1.360511\n",
      "Train Epoch: 89 [3840/60000 (6%)]\tDiscriminator Loss: 0.420459\tGenerator Loss: 1.542225\n",
      "Train Epoch: 89 [5120/60000 (9%)]\tDiscriminator Loss: 0.440357\tGenerator Loss: 1.534520\n",
      "Train Epoch: 89 [6400/60000 (11%)]\tDiscriminator Loss: 0.412963\tGenerator Loss: 1.529260\n",
      "Train Epoch: 89 [7680/60000 (13%)]\tDiscriminator Loss: 0.487402\tGenerator Loss: 1.267850\n",
      "Train Epoch: 89 [8960/60000 (15%)]\tDiscriminator Loss: 0.555147\tGenerator Loss: 1.502471\n",
      "Train Epoch: 89 [10240/60000 (17%)]\tDiscriminator Loss: 0.471896\tGenerator Loss: 1.348460\n",
      "Train Epoch: 89 [11520/60000 (19%)]\tDiscriminator Loss: 0.432424\tGenerator Loss: 1.480931\n",
      "Train Epoch: 89 [12800/60000 (21%)]\tDiscriminator Loss: 0.449861\tGenerator Loss: 1.417148\n",
      "Train Epoch: 89 [14080/60000 (23%)]\tDiscriminator Loss: 0.451391\tGenerator Loss: 1.345586\n",
      "Train Epoch: 89 [15360/60000 (26%)]\tDiscriminator Loss: 0.481671\tGenerator Loss: 1.219730\n",
      "Train Epoch: 89 [16640/60000 (28%)]\tDiscriminator Loss: 0.484204\tGenerator Loss: 1.030351\n",
      "Train Epoch: 89 [17920/60000 (30%)]\tDiscriminator Loss: 0.488257\tGenerator Loss: 1.536032\n",
      "Train Epoch: 89 [19200/60000 (32%)]\tDiscriminator Loss: 0.501697\tGenerator Loss: 1.235205\n",
      "Train Epoch: 89 [20480/60000 (34%)]\tDiscriminator Loss: 0.504954\tGenerator Loss: 1.766850\n",
      "Train Epoch: 89 [21760/60000 (36%)]\tDiscriminator Loss: 0.431938\tGenerator Loss: 1.536927\n",
      "Train Epoch: 89 [23040/60000 (38%)]\tDiscriminator Loss: 0.441306\tGenerator Loss: 1.220566\n",
      "Train Epoch: 89 [24320/60000 (41%)]\tDiscriminator Loss: 0.565201\tGenerator Loss: 1.095162\n",
      "Train Epoch: 89 [25600/60000 (43%)]\tDiscriminator Loss: 0.474365\tGenerator Loss: 1.344984\n",
      "Train Epoch: 89 [26880/60000 (45%)]\tDiscriminator Loss: 0.489325\tGenerator Loss: 1.861355\n",
      "Train Epoch: 89 [28160/60000 (47%)]\tDiscriminator Loss: 0.448589\tGenerator Loss: 1.323266\n",
      "Train Epoch: 89 [29440/60000 (49%)]\tDiscriminator Loss: 0.434327\tGenerator Loss: 1.516670\n",
      "Train Epoch: 89 [30720/60000 (51%)]\tDiscriminator Loss: 0.497886\tGenerator Loss: 1.442305\n",
      "Train Epoch: 89 [32000/60000 (53%)]\tDiscriminator Loss: 0.447054\tGenerator Loss: 1.201325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 89 [33280/60000 (55%)]\tDiscriminator Loss: 0.450184\tGenerator Loss: 1.386228\n",
      "Train Epoch: 89 [34560/60000 (58%)]\tDiscriminator Loss: 0.553927\tGenerator Loss: 1.148161\n",
      "Train Epoch: 89 [35840/60000 (60%)]\tDiscriminator Loss: 0.508945\tGenerator Loss: 1.447456\n",
      "Train Epoch: 89 [37120/60000 (62%)]\tDiscriminator Loss: 0.459081\tGenerator Loss: 1.506897\n",
      "Train Epoch: 89 [38400/60000 (64%)]\tDiscriminator Loss: 0.507851\tGenerator Loss: 1.245729\n",
      "Train Epoch: 89 [39680/60000 (66%)]\tDiscriminator Loss: 0.489811\tGenerator Loss: 1.191449\n",
      "Train Epoch: 89 [40960/60000 (68%)]\tDiscriminator Loss: 0.539228\tGenerator Loss: 1.088334\n",
      "Train Epoch: 89 [42240/60000 (70%)]\tDiscriminator Loss: 0.447576\tGenerator Loss: 1.405732\n",
      "Train Epoch: 89 [43520/60000 (72%)]\tDiscriminator Loss: 0.476527\tGenerator Loss: 1.408070\n",
      "Train Epoch: 89 [44800/60000 (75%)]\tDiscriminator Loss: 0.447360\tGenerator Loss: 1.349725\n",
      "Train Epoch: 89 [46080/60000 (77%)]\tDiscriminator Loss: 0.430957\tGenerator Loss: 1.288679\n",
      "Train Epoch: 89 [47360/60000 (79%)]\tDiscriminator Loss: 0.427584\tGenerator Loss: 1.402817\n",
      "Train Epoch: 89 [48640/60000 (81%)]\tDiscriminator Loss: 0.433020\tGenerator Loss: 1.580786\n",
      "Train Epoch: 89 [49920/60000 (83%)]\tDiscriminator Loss: 0.496402\tGenerator Loss: 1.149867\n",
      "Train Epoch: 89 [51200/60000 (85%)]\tDiscriminator Loss: 0.533720\tGenerator Loss: 0.994445\n",
      "Train Epoch: 89 [52480/60000 (87%)]\tDiscriminator Loss: 0.492920\tGenerator Loss: 1.516724\n",
      "Train Epoch: 89 [53760/60000 (90%)]\tDiscriminator Loss: 0.501918\tGenerator Loss: 1.241785\n",
      "Train Epoch: 89 [55040/60000 (92%)]\tDiscriminator Loss: 0.448274\tGenerator Loss: 1.047471\n",
      "Train Epoch: 89 [56320/60000 (94%)]\tDiscriminator Loss: 0.447871\tGenerator Loss: 1.348054\n",
      "Train Epoch: 89 [57600/60000 (96%)]\tDiscriminator Loss: 0.411441\tGenerator Loss: 1.563797\n",
      "Train Epoch: 89 [58880/60000 (98%)]\tDiscriminator Loss: 0.483518\tGenerator Loss: 1.393504\n",
      "Train Epoch: 90 [0/60000 (0%)]\tDiscriminator Loss: 0.498256\tGenerator Loss: 1.965071\n",
      "Train Epoch: 90 [1280/60000 (2%)]\tDiscriminator Loss: 0.481160\tGenerator Loss: 1.504563\n",
      "Train Epoch: 90 [2560/60000 (4%)]\tDiscriminator Loss: 0.481807\tGenerator Loss: 1.556399\n",
      "Train Epoch: 90 [3840/60000 (6%)]\tDiscriminator Loss: 0.457470\tGenerator Loss: 1.583728\n",
      "Train Epoch: 90 [5120/60000 (9%)]\tDiscriminator Loss: 0.535932\tGenerator Loss: 1.385551\n",
      "Train Epoch: 90 [6400/60000 (11%)]\tDiscriminator Loss: 0.541945\tGenerator Loss: 1.090088\n",
      "Train Epoch: 90 [7680/60000 (13%)]\tDiscriminator Loss: 0.482264\tGenerator Loss: 1.270494\n",
      "Train Epoch: 90 [8960/60000 (15%)]\tDiscriminator Loss: 0.529719\tGenerator Loss: 1.729043\n",
      "Train Epoch: 90 [10240/60000 (17%)]\tDiscriminator Loss: 0.483310\tGenerator Loss: 1.681849\n",
      "Train Epoch: 90 [11520/60000 (19%)]\tDiscriminator Loss: 0.471361\tGenerator Loss: 1.284528\n",
      "Train Epoch: 90 [12800/60000 (21%)]\tDiscriminator Loss: 0.495410\tGenerator Loss: 1.273723\n",
      "Train Epoch: 90 [14080/60000 (23%)]\tDiscriminator Loss: 0.451125\tGenerator Loss: 1.524368\n",
      "Train Epoch: 90 [15360/60000 (26%)]\tDiscriminator Loss: 0.456909\tGenerator Loss: 1.463775\n",
      "Train Epoch: 90 [16640/60000 (28%)]\tDiscriminator Loss: 0.492525\tGenerator Loss: 1.809557\n",
      "Train Epoch: 90 [17920/60000 (30%)]\tDiscriminator Loss: 0.479176\tGenerator Loss: 1.140301\n",
      "Train Epoch: 90 [19200/60000 (32%)]\tDiscriminator Loss: 0.485883\tGenerator Loss: 1.442145\n",
      "Train Epoch: 90 [20480/60000 (34%)]\tDiscriminator Loss: 0.498526\tGenerator Loss: 1.818154\n",
      "Train Epoch: 90 [21760/60000 (36%)]\tDiscriminator Loss: 0.502303\tGenerator Loss: 1.219876\n",
      "Train Epoch: 90 [23040/60000 (38%)]\tDiscriminator Loss: 0.493606\tGenerator Loss: 1.571093\n",
      "Train Epoch: 90 [24320/60000 (41%)]\tDiscriminator Loss: 0.536820\tGenerator Loss: 1.205323\n",
      "Train Epoch: 90 [25600/60000 (43%)]\tDiscriminator Loss: 0.475381\tGenerator Loss: 1.227642\n",
      "Train Epoch: 90 [26880/60000 (45%)]\tDiscriminator Loss: 0.523647\tGenerator Loss: 1.350155\n",
      "Train Epoch: 90 [28160/60000 (47%)]\tDiscriminator Loss: 0.474946\tGenerator Loss: 1.429866\n",
      "Train Epoch: 90 [29440/60000 (49%)]\tDiscriminator Loss: 0.482448\tGenerator Loss: 1.832188\n",
      "Train Epoch: 90 [30720/60000 (51%)]\tDiscriminator Loss: 0.474542\tGenerator Loss: 1.221063\n",
      "Train Epoch: 90 [32000/60000 (53%)]\tDiscriminator Loss: 0.497768\tGenerator Loss: 1.651581\n",
      "Train Epoch: 90 [33280/60000 (55%)]\tDiscriminator Loss: 0.479246\tGenerator Loss: 1.546301\n",
      "Train Epoch: 90 [34560/60000 (58%)]\tDiscriminator Loss: 0.496324\tGenerator Loss: 1.166932\n",
      "Train Epoch: 90 [35840/60000 (60%)]\tDiscriminator Loss: 0.508368\tGenerator Loss: 1.061420\n",
      "Train Epoch: 90 [37120/60000 (62%)]\tDiscriminator Loss: 0.515611\tGenerator Loss: 1.371444\n",
      "Train Epoch: 90 [38400/60000 (64%)]\tDiscriminator Loss: 0.465102\tGenerator Loss: 1.494635\n",
      "Train Epoch: 90 [39680/60000 (66%)]\tDiscriminator Loss: 0.517183\tGenerator Loss: 1.551151\n",
      "Train Epoch: 90 [40960/60000 (68%)]\tDiscriminator Loss: 0.416635\tGenerator Loss: 1.630969\n",
      "Train Epoch: 90 [42240/60000 (70%)]\tDiscriminator Loss: 0.488862\tGenerator Loss: 1.192559\n",
      "Train Epoch: 90 [43520/60000 (72%)]\tDiscriminator Loss: 0.483815\tGenerator Loss: 1.377552\n",
      "Train Epoch: 90 [44800/60000 (75%)]\tDiscriminator Loss: 0.442554\tGenerator Loss: 1.452163\n",
      "Train Epoch: 90 [46080/60000 (77%)]\tDiscriminator Loss: 0.467975\tGenerator Loss: 1.588020\n",
      "Train Epoch: 90 [47360/60000 (79%)]\tDiscriminator Loss: 0.457246\tGenerator Loss: 1.405557\n",
      "Train Epoch: 90 [48640/60000 (81%)]\tDiscriminator Loss: 0.520078\tGenerator Loss: 1.176076\n",
      "Train Epoch: 90 [49920/60000 (83%)]\tDiscriminator Loss: 0.445195\tGenerator Loss: 1.328661\n",
      "Train Epoch: 90 [51200/60000 (85%)]\tDiscriminator Loss: 0.439535\tGenerator Loss: 1.655159\n",
      "Train Epoch: 90 [52480/60000 (87%)]\tDiscriminator Loss: 0.490576\tGenerator Loss: 1.610158\n",
      "Train Epoch: 90 [53760/60000 (90%)]\tDiscriminator Loss: 0.508161\tGenerator Loss: 1.247425\n",
      "Train Epoch: 90 [55040/60000 (92%)]\tDiscriminator Loss: 0.547130\tGenerator Loss: 1.615966\n",
      "Train Epoch: 90 [56320/60000 (94%)]\tDiscriminator Loss: 0.557758\tGenerator Loss: 1.498561\n",
      "Train Epoch: 90 [57600/60000 (96%)]\tDiscriminator Loss: 0.518133\tGenerator Loss: 1.330583\n",
      "Train Epoch: 90 [58880/60000 (98%)]\tDiscriminator Loss: 0.455833\tGenerator Loss: 1.300039\n",
      "Train Epoch: 91 [0/60000 (0%)]\tDiscriminator Loss: 0.418252\tGenerator Loss: 1.379264\n",
      "Train Epoch: 91 [1280/60000 (2%)]\tDiscriminator Loss: 0.457838\tGenerator Loss: 1.461016\n",
      "Train Epoch: 91 [2560/60000 (4%)]\tDiscriminator Loss: 0.535149\tGenerator Loss: 1.790972\n",
      "Train Epoch: 91 [3840/60000 (6%)]\tDiscriminator Loss: 0.529426\tGenerator Loss: 2.058271\n",
      "Train Epoch: 91 [5120/60000 (9%)]\tDiscriminator Loss: 0.444566\tGenerator Loss: 1.370815\n",
      "Train Epoch: 91 [6400/60000 (11%)]\tDiscriminator Loss: 0.395739\tGenerator Loss: 1.766319\n",
      "Train Epoch: 91 [7680/60000 (13%)]\tDiscriminator Loss: 0.419366\tGenerator Loss: 1.483435\n",
      "Train Epoch: 91 [8960/60000 (15%)]\tDiscriminator Loss: 0.459885\tGenerator Loss: 1.565442\n",
      "Train Epoch: 91 [10240/60000 (17%)]\tDiscriminator Loss: 0.445139\tGenerator Loss: 1.411345\n",
      "Train Epoch: 91 [11520/60000 (19%)]\tDiscriminator Loss: 0.511454\tGenerator Loss: 1.884418\n",
      "Train Epoch: 91 [12800/60000 (21%)]\tDiscriminator Loss: 0.485984\tGenerator Loss: 1.457208\n",
      "Train Epoch: 91 [14080/60000 (23%)]\tDiscriminator Loss: 0.527754\tGenerator Loss: 1.021770\n",
      "Train Epoch: 91 [15360/60000 (26%)]\tDiscriminator Loss: 0.520144\tGenerator Loss: 1.686706\n",
      "Train Epoch: 91 [16640/60000 (28%)]\tDiscriminator Loss: 0.515952\tGenerator Loss: 1.962924\n",
      "Train Epoch: 91 [17920/60000 (30%)]\tDiscriminator Loss: 0.497200\tGenerator Loss: 1.232548\n",
      "Train Epoch: 91 [19200/60000 (32%)]\tDiscriminator Loss: 0.424299\tGenerator Loss: 1.735288\n",
      "Train Epoch: 91 [20480/60000 (34%)]\tDiscriminator Loss: 0.503730\tGenerator Loss: 1.384079\n",
      "Train Epoch: 91 [21760/60000 (36%)]\tDiscriminator Loss: 0.442426\tGenerator Loss: 1.258767\n",
      "Train Epoch: 91 [23040/60000 (38%)]\tDiscriminator Loss: 0.496956\tGenerator Loss: 1.507007\n",
      "Train Epoch: 91 [24320/60000 (41%)]\tDiscriminator Loss: 0.465709\tGenerator Loss: 1.628857\n",
      "Train Epoch: 91 [25600/60000 (43%)]\tDiscriminator Loss: 0.516057\tGenerator Loss: 1.379400\n",
      "Train Epoch: 91 [26880/60000 (45%)]\tDiscriminator Loss: 0.506495\tGenerator Loss: 1.604788\n",
      "Train Epoch: 91 [28160/60000 (47%)]\tDiscriminator Loss: 0.508339\tGenerator Loss: 1.033427\n",
      "Train Epoch: 91 [29440/60000 (49%)]\tDiscriminator Loss: 0.557613\tGenerator Loss: 1.568691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 91 [30720/60000 (51%)]\tDiscriminator Loss: 0.504642\tGenerator Loss: 1.333652\n",
      "Train Epoch: 91 [32000/60000 (53%)]\tDiscriminator Loss: 0.512197\tGenerator Loss: 1.867036\n",
      "Train Epoch: 91 [33280/60000 (55%)]\tDiscriminator Loss: 0.449980\tGenerator Loss: 1.488728\n",
      "Train Epoch: 91 [34560/60000 (58%)]\tDiscriminator Loss: 0.476531\tGenerator Loss: 1.441682\n",
      "Train Epoch: 91 [35840/60000 (60%)]\tDiscriminator Loss: 0.473909\tGenerator Loss: 1.489497\n",
      "Train Epoch: 91 [37120/60000 (62%)]\tDiscriminator Loss: 0.418179\tGenerator Loss: 1.409055\n",
      "Train Epoch: 91 [38400/60000 (64%)]\tDiscriminator Loss: 0.472053\tGenerator Loss: 1.348317\n",
      "Train Epoch: 91 [39680/60000 (66%)]\tDiscriminator Loss: 0.508151\tGenerator Loss: 1.758428\n",
      "Train Epoch: 91 [40960/60000 (68%)]\tDiscriminator Loss: 0.463756\tGenerator Loss: 1.890778\n",
      "Train Epoch: 91 [42240/60000 (70%)]\tDiscriminator Loss: 0.567202\tGenerator Loss: 1.359165\n",
      "Train Epoch: 91 [43520/60000 (72%)]\tDiscriminator Loss: 0.470267\tGenerator Loss: 1.139982\n",
      "Train Epoch: 91 [44800/60000 (75%)]\tDiscriminator Loss: 0.489832\tGenerator Loss: 1.436211\n",
      "Train Epoch: 91 [46080/60000 (77%)]\tDiscriminator Loss: 0.444387\tGenerator Loss: 1.769989\n",
      "Train Epoch: 91 [47360/60000 (79%)]\tDiscriminator Loss: 0.451474\tGenerator Loss: 1.349462\n",
      "Train Epoch: 91 [48640/60000 (81%)]\tDiscriminator Loss: 0.431217\tGenerator Loss: 1.509990\n",
      "Train Epoch: 91 [49920/60000 (83%)]\tDiscriminator Loss: 0.466063\tGenerator Loss: 1.551622\n",
      "Train Epoch: 91 [51200/60000 (85%)]\tDiscriminator Loss: 0.492287\tGenerator Loss: 1.281639\n",
      "Train Epoch: 91 [52480/60000 (87%)]\tDiscriminator Loss: 0.531728\tGenerator Loss: 1.755949\n",
      "Train Epoch: 91 [53760/60000 (90%)]\tDiscriminator Loss: 0.473125\tGenerator Loss: 1.511338\n",
      "Train Epoch: 91 [55040/60000 (92%)]\tDiscriminator Loss: 0.531887\tGenerator Loss: 1.506184\n",
      "Train Epoch: 91 [56320/60000 (94%)]\tDiscriminator Loss: 0.486247\tGenerator Loss: 1.371162\n",
      "Train Epoch: 91 [57600/60000 (96%)]\tDiscriminator Loss: 0.488939\tGenerator Loss: 1.833031\n",
      "Train Epoch: 91 [58880/60000 (98%)]\tDiscriminator Loss: 0.473656\tGenerator Loss: 1.462325\n",
      "Train Epoch: 92 [0/60000 (0%)]\tDiscriminator Loss: 0.436606\tGenerator Loss: 1.165158\n",
      "Train Epoch: 92 [1280/60000 (2%)]\tDiscriminator Loss: 0.480072\tGenerator Loss: 1.311932\n",
      "Train Epoch: 92 [2560/60000 (4%)]\tDiscriminator Loss: 0.448819\tGenerator Loss: 1.482715\n",
      "Train Epoch: 92 [3840/60000 (6%)]\tDiscriminator Loss: 0.468233\tGenerator Loss: 1.194772\n",
      "Train Epoch: 92 [5120/60000 (9%)]\tDiscriminator Loss: 0.467914\tGenerator Loss: 1.161392\n",
      "Train Epoch: 92 [6400/60000 (11%)]\tDiscriminator Loss: 0.476243\tGenerator Loss: 1.092086\n",
      "Train Epoch: 92 [7680/60000 (13%)]\tDiscriminator Loss: 0.454265\tGenerator Loss: 1.493052\n",
      "Train Epoch: 92 [8960/60000 (15%)]\tDiscriminator Loss: 0.477376\tGenerator Loss: 1.463236\n",
      "Train Epoch: 92 [10240/60000 (17%)]\tDiscriminator Loss: 0.450735\tGenerator Loss: 1.633098\n",
      "Train Epoch: 92 [11520/60000 (19%)]\tDiscriminator Loss: 0.459460\tGenerator Loss: 1.269582\n",
      "Train Epoch: 92 [12800/60000 (21%)]\tDiscriminator Loss: 0.442686\tGenerator Loss: 1.095370\n",
      "Train Epoch: 92 [14080/60000 (23%)]\tDiscriminator Loss: 0.478422\tGenerator Loss: 1.308202\n",
      "Train Epoch: 92 [15360/60000 (26%)]\tDiscriminator Loss: 0.435276\tGenerator Loss: 1.607332\n",
      "Train Epoch: 92 [16640/60000 (28%)]\tDiscriminator Loss: 0.481686\tGenerator Loss: 1.570044\n",
      "Train Epoch: 92 [17920/60000 (30%)]\tDiscriminator Loss: 0.449660\tGenerator Loss: 1.391161\n",
      "Train Epoch: 92 [19200/60000 (32%)]\tDiscriminator Loss: 0.462712\tGenerator Loss: 1.468156\n",
      "Train Epoch: 92 [20480/60000 (34%)]\tDiscriminator Loss: 0.494213\tGenerator Loss: 1.587695\n",
      "Train Epoch: 92 [21760/60000 (36%)]\tDiscriminator Loss: 0.451837\tGenerator Loss: 1.570589\n",
      "Train Epoch: 92 [23040/60000 (38%)]\tDiscriminator Loss: 0.484180\tGenerator Loss: 1.518935\n",
      "Train Epoch: 92 [24320/60000 (41%)]\tDiscriminator Loss: 0.481063\tGenerator Loss: 1.204776\n",
      "Train Epoch: 92 [25600/60000 (43%)]\tDiscriminator Loss: 0.500773\tGenerator Loss: 1.425849\n",
      "Train Epoch: 92 [26880/60000 (45%)]\tDiscriminator Loss: 0.497691\tGenerator Loss: 1.933164\n",
      "Train Epoch: 92 [28160/60000 (47%)]\tDiscriminator Loss: 0.448844\tGenerator Loss: 1.578279\n",
      "Train Epoch: 92 [29440/60000 (49%)]\tDiscriminator Loss: 0.475551\tGenerator Loss: 1.730545\n",
      "Train Epoch: 92 [30720/60000 (51%)]\tDiscriminator Loss: 0.484964\tGenerator Loss: 1.285739\n",
      "Train Epoch: 92 [32000/60000 (53%)]\tDiscriminator Loss: 0.459162\tGenerator Loss: 1.345557\n",
      "Train Epoch: 92 [33280/60000 (55%)]\tDiscriminator Loss: 0.501949\tGenerator Loss: 1.201478\n",
      "Train Epoch: 92 [34560/60000 (58%)]\tDiscriminator Loss: 0.406292\tGenerator Loss: 1.766231\n",
      "Train Epoch: 92 [35840/60000 (60%)]\tDiscriminator Loss: 0.530327\tGenerator Loss: 1.248397\n",
      "Train Epoch: 92 [37120/60000 (62%)]\tDiscriminator Loss: 0.493303\tGenerator Loss: 1.704952\n",
      "Train Epoch: 92 [38400/60000 (64%)]\tDiscriminator Loss: 0.536826\tGenerator Loss: 1.307786\n",
      "Train Epoch: 92 [39680/60000 (66%)]\tDiscriminator Loss: 0.468902\tGenerator Loss: 1.334145\n",
      "Train Epoch: 92 [40960/60000 (68%)]\tDiscriminator Loss: 0.483228\tGenerator Loss: 1.345392\n",
      "Train Epoch: 92 [42240/60000 (70%)]\tDiscriminator Loss: 0.461726\tGenerator Loss: 1.247498\n",
      "Train Epoch: 92 [43520/60000 (72%)]\tDiscriminator Loss: 0.401463\tGenerator Loss: 1.738370\n",
      "Train Epoch: 92 [44800/60000 (75%)]\tDiscriminator Loss: 0.525431\tGenerator Loss: 1.406736\n",
      "Train Epoch: 92 [46080/60000 (77%)]\tDiscriminator Loss: 0.521695\tGenerator Loss: 1.736310\n",
      "Train Epoch: 92 [47360/60000 (79%)]\tDiscriminator Loss: 0.538560\tGenerator Loss: 0.970319\n",
      "Train Epoch: 92 [48640/60000 (81%)]\tDiscriminator Loss: 0.460271\tGenerator Loss: 1.066550\n",
      "Train Epoch: 92 [49920/60000 (83%)]\tDiscriminator Loss: 0.455797\tGenerator Loss: 1.757363\n",
      "Train Epoch: 92 [51200/60000 (85%)]\tDiscriminator Loss: 0.494626\tGenerator Loss: 1.669588\n",
      "Train Epoch: 92 [52480/60000 (87%)]\tDiscriminator Loss: 0.492953\tGenerator Loss: 1.410453\n",
      "Train Epoch: 92 [53760/60000 (90%)]\tDiscriminator Loss: 0.505081\tGenerator Loss: 1.353746\n",
      "Train Epoch: 92 [55040/60000 (92%)]\tDiscriminator Loss: 0.502668\tGenerator Loss: 1.118993\n",
      "Train Epoch: 92 [56320/60000 (94%)]\tDiscriminator Loss: 0.458271\tGenerator Loss: 1.455166\n",
      "Train Epoch: 92 [57600/60000 (96%)]\tDiscriminator Loss: 0.481144\tGenerator Loss: 1.428620\n",
      "Train Epoch: 92 [58880/60000 (98%)]\tDiscriminator Loss: 0.548414\tGenerator Loss: 1.569106\n",
      "Train Epoch: 93 [0/60000 (0%)]\tDiscriminator Loss: 0.413751\tGenerator Loss: 1.764373\n",
      "Train Epoch: 93 [1280/60000 (2%)]\tDiscriminator Loss: 0.524632\tGenerator Loss: 1.521844\n",
      "Train Epoch: 93 [2560/60000 (4%)]\tDiscriminator Loss: 0.420933\tGenerator Loss: 1.647698\n",
      "Train Epoch: 93 [3840/60000 (6%)]\tDiscriminator Loss: 0.472231\tGenerator Loss: 1.370175\n",
      "Train Epoch: 93 [5120/60000 (9%)]\tDiscriminator Loss: 0.467208\tGenerator Loss: 1.506959\n",
      "Train Epoch: 93 [6400/60000 (11%)]\tDiscriminator Loss: 0.497140\tGenerator Loss: 1.098378\n",
      "Train Epoch: 93 [7680/60000 (13%)]\tDiscriminator Loss: 0.509371\tGenerator Loss: 1.229269\n",
      "Train Epoch: 93 [8960/60000 (15%)]\tDiscriminator Loss: 0.543755\tGenerator Loss: 1.080107\n",
      "Train Epoch: 93 [10240/60000 (17%)]\tDiscriminator Loss: 0.493172\tGenerator Loss: 1.467813\n",
      "Train Epoch: 93 [11520/60000 (19%)]\tDiscriminator Loss: 0.520925\tGenerator Loss: 1.485938\n",
      "Train Epoch: 93 [12800/60000 (21%)]\tDiscriminator Loss: 0.486361\tGenerator Loss: 1.448809\n",
      "Train Epoch: 93 [14080/60000 (23%)]\tDiscriminator Loss: 0.523054\tGenerator Loss: 1.045783\n",
      "Train Epoch: 93 [15360/60000 (26%)]\tDiscriminator Loss: 0.464470\tGenerator Loss: 1.440290\n",
      "Train Epoch: 93 [16640/60000 (28%)]\tDiscriminator Loss: 0.496806\tGenerator Loss: 1.573054\n",
      "Train Epoch: 93 [17920/60000 (30%)]\tDiscriminator Loss: 0.455491\tGenerator Loss: 1.765019\n",
      "Train Epoch: 93 [19200/60000 (32%)]\tDiscriminator Loss: 0.481827\tGenerator Loss: 1.459421\n",
      "Train Epoch: 93 [20480/60000 (34%)]\tDiscriminator Loss: 0.442595\tGenerator Loss: 1.479934\n",
      "Train Epoch: 93 [21760/60000 (36%)]\tDiscriminator Loss: 0.491139\tGenerator Loss: 1.707046\n",
      "Train Epoch: 93 [23040/60000 (38%)]\tDiscriminator Loss: 0.436329\tGenerator Loss: 1.174875\n",
      "Train Epoch: 93 [24320/60000 (41%)]\tDiscriminator Loss: 0.483166\tGenerator Loss: 1.340886\n",
      "Train Epoch: 93 [25600/60000 (43%)]\tDiscriminator Loss: 0.499243\tGenerator Loss: 1.146607\n",
      "Train Epoch: 93 [26880/60000 (45%)]\tDiscriminator Loss: 0.455963\tGenerator Loss: 1.262052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 93 [28160/60000 (47%)]\tDiscriminator Loss: 0.448048\tGenerator Loss: 1.592416\n",
      "Train Epoch: 93 [29440/60000 (49%)]\tDiscriminator Loss: 0.430273\tGenerator Loss: 1.611101\n",
      "Train Epoch: 93 [30720/60000 (51%)]\tDiscriminator Loss: 0.513174\tGenerator Loss: 1.829241\n",
      "Train Epoch: 93 [32000/60000 (53%)]\tDiscriminator Loss: 0.537170\tGenerator Loss: 0.854954\n",
      "Train Epoch: 93 [33280/60000 (55%)]\tDiscriminator Loss: 0.473847\tGenerator Loss: 1.763679\n",
      "Train Epoch: 93 [34560/60000 (58%)]\tDiscriminator Loss: 0.425825\tGenerator Loss: 1.521626\n",
      "Train Epoch: 93 [35840/60000 (60%)]\tDiscriminator Loss: 0.534898\tGenerator Loss: 2.081179\n",
      "Train Epoch: 93 [37120/60000 (62%)]\tDiscriminator Loss: 0.464500\tGenerator Loss: 1.422319\n",
      "Train Epoch: 93 [38400/60000 (64%)]\tDiscriminator Loss: 0.499634\tGenerator Loss: 1.343246\n",
      "Train Epoch: 93 [39680/60000 (66%)]\tDiscriminator Loss: 0.461524\tGenerator Loss: 1.548985\n",
      "Train Epoch: 93 [40960/60000 (68%)]\tDiscriminator Loss: 0.465413\tGenerator Loss: 1.355369\n",
      "Train Epoch: 93 [42240/60000 (70%)]\tDiscriminator Loss: 0.529924\tGenerator Loss: 2.080951\n",
      "Train Epoch: 93 [43520/60000 (72%)]\tDiscriminator Loss: 0.470317\tGenerator Loss: 1.702819\n",
      "Train Epoch: 93 [44800/60000 (75%)]\tDiscriminator Loss: 0.415707\tGenerator Loss: 1.386967\n",
      "Train Epoch: 93 [46080/60000 (77%)]\tDiscriminator Loss: 0.436764\tGenerator Loss: 1.613172\n",
      "Train Epoch: 93 [47360/60000 (79%)]\tDiscriminator Loss: 0.483040\tGenerator Loss: 1.618098\n",
      "Train Epoch: 93 [48640/60000 (81%)]\tDiscriminator Loss: 0.510475\tGenerator Loss: 1.438390\n",
      "Train Epoch: 93 [49920/60000 (83%)]\tDiscriminator Loss: 0.510951\tGenerator Loss: 1.687116\n",
      "Train Epoch: 93 [51200/60000 (85%)]\tDiscriminator Loss: 0.471381\tGenerator Loss: 1.537002\n",
      "Train Epoch: 93 [52480/60000 (87%)]\tDiscriminator Loss: 0.490160\tGenerator Loss: 1.516498\n",
      "Train Epoch: 93 [53760/60000 (90%)]\tDiscriminator Loss: 0.499434\tGenerator Loss: 1.462625\n",
      "Train Epoch: 93 [55040/60000 (92%)]\tDiscriminator Loss: 0.520529\tGenerator Loss: 1.437995\n",
      "Train Epoch: 93 [56320/60000 (94%)]\tDiscriminator Loss: 0.495587\tGenerator Loss: 1.337857\n",
      "Train Epoch: 93 [57600/60000 (96%)]\tDiscriminator Loss: 0.496860\tGenerator Loss: 1.478846\n",
      "Train Epoch: 93 [58880/60000 (98%)]\tDiscriminator Loss: 0.534057\tGenerator Loss: 1.842544\n",
      "Train Epoch: 94 [0/60000 (0%)]\tDiscriminator Loss: 0.492134\tGenerator Loss: 2.004821\n",
      "Train Epoch: 94 [1280/60000 (2%)]\tDiscriminator Loss: 0.448465\tGenerator Loss: 1.566575\n",
      "Train Epoch: 94 [2560/60000 (4%)]\tDiscriminator Loss: 0.498354\tGenerator Loss: 1.337149\n",
      "Train Epoch: 94 [3840/60000 (6%)]\tDiscriminator Loss: 0.470438\tGenerator Loss: 1.290733\n",
      "Train Epoch: 94 [5120/60000 (9%)]\tDiscriminator Loss: 0.447977\tGenerator Loss: 1.688062\n",
      "Train Epoch: 94 [6400/60000 (11%)]\tDiscriminator Loss: 0.428270\tGenerator Loss: 1.806932\n",
      "Train Epoch: 94 [7680/60000 (13%)]\tDiscriminator Loss: 0.474660\tGenerator Loss: 1.864109\n",
      "Train Epoch: 94 [8960/60000 (15%)]\tDiscriminator Loss: 0.494437\tGenerator Loss: 1.503019\n",
      "Train Epoch: 94 [10240/60000 (17%)]\tDiscriminator Loss: 0.487236\tGenerator Loss: 1.266684\n",
      "Train Epoch: 94 [11520/60000 (19%)]\tDiscriminator Loss: 0.506472\tGenerator Loss: 1.673970\n",
      "Train Epoch: 94 [12800/60000 (21%)]\tDiscriminator Loss: 0.473266\tGenerator Loss: 1.566890\n",
      "Train Epoch: 94 [14080/60000 (23%)]\tDiscriminator Loss: 0.453787\tGenerator Loss: 1.666930\n",
      "Train Epoch: 94 [15360/60000 (26%)]\tDiscriminator Loss: 0.459768\tGenerator Loss: 1.252852\n",
      "Train Epoch: 94 [16640/60000 (28%)]\tDiscriminator Loss: 0.515456\tGenerator Loss: 1.251203\n",
      "Train Epoch: 94 [17920/60000 (30%)]\tDiscriminator Loss: 0.456664\tGenerator Loss: 1.631210\n",
      "Train Epoch: 94 [19200/60000 (32%)]\tDiscriminator Loss: 0.514117\tGenerator Loss: 2.007386\n",
      "Train Epoch: 94 [20480/60000 (34%)]\tDiscriminator Loss: 0.490481\tGenerator Loss: 1.276968\n",
      "Train Epoch: 94 [21760/60000 (36%)]\tDiscriminator Loss: 0.455461\tGenerator Loss: 1.450675\n",
      "Train Epoch: 94 [23040/60000 (38%)]\tDiscriminator Loss: 0.422641\tGenerator Loss: 1.558887\n",
      "Train Epoch: 94 [24320/60000 (41%)]\tDiscriminator Loss: 0.508827\tGenerator Loss: 1.679307\n",
      "Train Epoch: 94 [25600/60000 (43%)]\tDiscriminator Loss: 0.463618\tGenerator Loss: 1.822384\n",
      "Train Epoch: 94 [26880/60000 (45%)]\tDiscriminator Loss: 0.419873\tGenerator Loss: 1.257772\n",
      "Train Epoch: 94 [28160/60000 (47%)]\tDiscriminator Loss: 0.452876\tGenerator Loss: 1.954888\n",
      "Train Epoch: 94 [29440/60000 (49%)]\tDiscriminator Loss: 0.472270\tGenerator Loss: 1.434123\n",
      "Train Epoch: 94 [30720/60000 (51%)]\tDiscriminator Loss: 0.438210\tGenerator Loss: 1.612118\n",
      "Train Epoch: 94 [32000/60000 (53%)]\tDiscriminator Loss: 0.484282\tGenerator Loss: 1.756358\n",
      "Train Epoch: 94 [33280/60000 (55%)]\tDiscriminator Loss: 0.479249\tGenerator Loss: 1.209773\n",
      "Train Epoch: 94 [34560/60000 (58%)]\tDiscriminator Loss: 0.446683\tGenerator Loss: 1.295571\n",
      "Train Epoch: 94 [35840/60000 (60%)]\tDiscriminator Loss: 0.431741\tGenerator Loss: 1.425583\n",
      "Train Epoch: 94 [37120/60000 (62%)]\tDiscriminator Loss: 0.484872\tGenerator Loss: 1.504474\n",
      "Train Epoch: 94 [38400/60000 (64%)]\tDiscriminator Loss: 0.416006\tGenerator Loss: 1.668891\n",
      "Train Epoch: 94 [39680/60000 (66%)]\tDiscriminator Loss: 0.483863\tGenerator Loss: 1.274297\n",
      "Train Epoch: 94 [40960/60000 (68%)]\tDiscriminator Loss: 0.477887\tGenerator Loss: 1.786997\n",
      "Train Epoch: 94 [42240/60000 (70%)]\tDiscriminator Loss: 0.498567\tGenerator Loss: 1.068227\n",
      "Train Epoch: 94 [43520/60000 (72%)]\tDiscriminator Loss: 0.431875\tGenerator Loss: 1.490310\n",
      "Train Epoch: 94 [44800/60000 (75%)]\tDiscriminator Loss: 0.455367\tGenerator Loss: 1.269464\n",
      "Train Epoch: 94 [46080/60000 (77%)]\tDiscriminator Loss: 0.559077\tGenerator Loss: 1.534440\n",
      "Train Epoch: 94 [47360/60000 (79%)]\tDiscriminator Loss: 0.514773\tGenerator Loss: 1.841062\n",
      "Train Epoch: 94 [48640/60000 (81%)]\tDiscriminator Loss: 0.422751\tGenerator Loss: 1.592916\n",
      "Train Epoch: 94 [49920/60000 (83%)]\tDiscriminator Loss: 0.514851\tGenerator Loss: 1.982618\n",
      "Train Epoch: 94 [51200/60000 (85%)]\tDiscriminator Loss: 0.505071\tGenerator Loss: 1.208194\n",
      "Train Epoch: 94 [52480/60000 (87%)]\tDiscriminator Loss: 0.455407\tGenerator Loss: 1.433877\n",
      "Train Epoch: 94 [53760/60000 (90%)]\tDiscriminator Loss: 0.503912\tGenerator Loss: 1.658335\n",
      "Train Epoch: 94 [55040/60000 (92%)]\tDiscriminator Loss: 0.502559\tGenerator Loss: 1.336366\n",
      "Train Epoch: 94 [56320/60000 (94%)]\tDiscriminator Loss: 0.452946\tGenerator Loss: 1.373365\n",
      "Train Epoch: 94 [57600/60000 (96%)]\tDiscriminator Loss: 0.493223\tGenerator Loss: 1.051822\n",
      "Train Epoch: 94 [58880/60000 (98%)]\tDiscriminator Loss: 0.464787\tGenerator Loss: 1.418108\n",
      "Train Epoch: 95 [0/60000 (0%)]\tDiscriminator Loss: 0.411908\tGenerator Loss: 1.336698\n",
      "Train Epoch: 95 [1280/60000 (2%)]\tDiscriminator Loss: 0.468618\tGenerator Loss: 1.485021\n",
      "Train Epoch: 95 [2560/60000 (4%)]\tDiscriminator Loss: 0.384415\tGenerator Loss: 1.510603\n",
      "Train Epoch: 95 [3840/60000 (6%)]\tDiscriminator Loss: 0.485245\tGenerator Loss: 1.512811\n",
      "Train Epoch: 95 [5120/60000 (9%)]\tDiscriminator Loss: 0.444898\tGenerator Loss: 1.695781\n",
      "Train Epoch: 95 [6400/60000 (11%)]\tDiscriminator Loss: 0.458769\tGenerator Loss: 1.660249\n",
      "Train Epoch: 95 [7680/60000 (13%)]\tDiscriminator Loss: 0.559345\tGenerator Loss: 1.160196\n",
      "Train Epoch: 95 [8960/60000 (15%)]\tDiscriminator Loss: 0.429356\tGenerator Loss: 1.554241\n",
      "Train Epoch: 95 [10240/60000 (17%)]\tDiscriminator Loss: 0.494297\tGenerator Loss: 1.367574\n",
      "Train Epoch: 95 [11520/60000 (19%)]\tDiscriminator Loss: 0.461361\tGenerator Loss: 1.301820\n",
      "Train Epoch: 95 [12800/60000 (21%)]\tDiscriminator Loss: 0.461224\tGenerator Loss: 1.598049\n",
      "Train Epoch: 95 [14080/60000 (23%)]\tDiscriminator Loss: 0.486715\tGenerator Loss: 0.906235\n",
      "Train Epoch: 95 [15360/60000 (26%)]\tDiscriminator Loss: 0.438239\tGenerator Loss: 1.628749\n",
      "Train Epoch: 95 [16640/60000 (28%)]\tDiscriminator Loss: 0.491803\tGenerator Loss: 1.835244\n",
      "Train Epoch: 95 [17920/60000 (30%)]\tDiscriminator Loss: 0.491431\tGenerator Loss: 1.594426\n",
      "Train Epoch: 95 [19200/60000 (32%)]\tDiscriminator Loss: 0.428728\tGenerator Loss: 1.474122\n",
      "Train Epoch: 95 [20480/60000 (34%)]\tDiscriminator Loss: 0.462528\tGenerator Loss: 1.279683\n",
      "Train Epoch: 95 [21760/60000 (36%)]\tDiscriminator Loss: 0.429809\tGenerator Loss: 1.547223\n",
      "Train Epoch: 95 [23040/60000 (38%)]\tDiscriminator Loss: 0.501610\tGenerator Loss: 1.452031\n",
      "Train Epoch: 95 [24320/60000 (41%)]\tDiscriminator Loss: 0.473482\tGenerator Loss: 2.004876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 95 [25600/60000 (43%)]\tDiscriminator Loss: 0.483098\tGenerator Loss: 1.191185\n",
      "Train Epoch: 95 [26880/60000 (45%)]\tDiscriminator Loss: 0.421096\tGenerator Loss: 1.452060\n",
      "Train Epoch: 95 [28160/60000 (47%)]\tDiscriminator Loss: 0.453719\tGenerator Loss: 1.658335\n",
      "Train Epoch: 95 [29440/60000 (49%)]\tDiscriminator Loss: 0.481530\tGenerator Loss: 1.367491\n",
      "Train Epoch: 95 [30720/60000 (51%)]\tDiscriminator Loss: 0.417732\tGenerator Loss: 1.440825\n",
      "Train Epoch: 95 [32000/60000 (53%)]\tDiscriminator Loss: 0.483474\tGenerator Loss: 1.501018\n",
      "Train Epoch: 95 [33280/60000 (55%)]\tDiscriminator Loss: 0.495522\tGenerator Loss: 1.817492\n",
      "Train Epoch: 95 [34560/60000 (58%)]\tDiscriminator Loss: 0.426767\tGenerator Loss: 1.426988\n",
      "Train Epoch: 95 [35840/60000 (60%)]\tDiscriminator Loss: 0.469190\tGenerator Loss: 1.411378\n",
      "Train Epoch: 95 [37120/60000 (62%)]\tDiscriminator Loss: 0.471464\tGenerator Loss: 1.315479\n",
      "Train Epoch: 95 [38400/60000 (64%)]\tDiscriminator Loss: 0.444414\tGenerator Loss: 1.439939\n",
      "Train Epoch: 95 [39680/60000 (66%)]\tDiscriminator Loss: 0.422753\tGenerator Loss: 1.588314\n",
      "Train Epoch: 95 [40960/60000 (68%)]\tDiscriminator Loss: 0.461522\tGenerator Loss: 1.550400\n",
      "Train Epoch: 95 [42240/60000 (70%)]\tDiscriminator Loss: 0.463049\tGenerator Loss: 1.443794\n",
      "Train Epoch: 95 [43520/60000 (72%)]\tDiscriminator Loss: 0.458950\tGenerator Loss: 1.509065\n",
      "Train Epoch: 95 [44800/60000 (75%)]\tDiscriminator Loss: 0.465772\tGenerator Loss: 1.364050\n",
      "Train Epoch: 95 [46080/60000 (77%)]\tDiscriminator Loss: 0.508436\tGenerator Loss: 1.841279\n",
      "Train Epoch: 95 [47360/60000 (79%)]\tDiscriminator Loss: 0.526792\tGenerator Loss: 1.165622\n",
      "Train Epoch: 95 [48640/60000 (81%)]\tDiscriminator Loss: 0.535711\tGenerator Loss: 1.808707\n",
      "Train Epoch: 95 [49920/60000 (83%)]\tDiscriminator Loss: 0.507098\tGenerator Loss: 1.133713\n",
      "Train Epoch: 95 [51200/60000 (85%)]\tDiscriminator Loss: 0.458666\tGenerator Loss: 1.374197\n",
      "Train Epoch: 95 [52480/60000 (87%)]\tDiscriminator Loss: 0.430546\tGenerator Loss: 1.385010\n",
      "Train Epoch: 95 [53760/60000 (90%)]\tDiscriminator Loss: 0.451657\tGenerator Loss: 1.556930\n",
      "Train Epoch: 95 [55040/60000 (92%)]\tDiscriminator Loss: 0.513023\tGenerator Loss: 1.088398\n",
      "Train Epoch: 95 [56320/60000 (94%)]\tDiscriminator Loss: 0.445538\tGenerator Loss: 1.504514\n",
      "Train Epoch: 95 [57600/60000 (96%)]\tDiscriminator Loss: 0.468639\tGenerator Loss: 1.366094\n",
      "Train Epoch: 95 [58880/60000 (98%)]\tDiscriminator Loss: 0.422432\tGenerator Loss: 1.261721\n",
      "Train Epoch: 96 [0/60000 (0%)]\tDiscriminator Loss: 0.439235\tGenerator Loss: 1.197613\n",
      "Train Epoch: 96 [1280/60000 (2%)]\tDiscriminator Loss: 0.476424\tGenerator Loss: 1.244811\n",
      "Train Epoch: 96 [2560/60000 (4%)]\tDiscriminator Loss: 0.432219\tGenerator Loss: 1.597055\n",
      "Train Epoch: 96 [3840/60000 (6%)]\tDiscriminator Loss: 0.476425\tGenerator Loss: 1.312296\n",
      "Train Epoch: 96 [5120/60000 (9%)]\tDiscriminator Loss: 0.478796\tGenerator Loss: 1.639363\n",
      "Train Epoch: 96 [6400/60000 (11%)]\tDiscriminator Loss: 0.488651\tGenerator Loss: 1.481116\n",
      "Train Epoch: 96 [7680/60000 (13%)]\tDiscriminator Loss: 0.426087\tGenerator Loss: 1.257479\n",
      "Train Epoch: 96 [8960/60000 (15%)]\tDiscriminator Loss: 0.439681\tGenerator Loss: 1.672591\n",
      "Train Epoch: 96 [10240/60000 (17%)]\tDiscriminator Loss: 0.470059\tGenerator Loss: 1.820977\n",
      "Train Epoch: 96 [11520/60000 (19%)]\tDiscriminator Loss: 0.432902\tGenerator Loss: 1.402838\n",
      "Train Epoch: 96 [12800/60000 (21%)]\tDiscriminator Loss: 0.433330\tGenerator Loss: 1.439212\n",
      "Train Epoch: 96 [14080/60000 (23%)]\tDiscriminator Loss: 0.474788\tGenerator Loss: 1.574867\n",
      "Train Epoch: 96 [15360/60000 (26%)]\tDiscriminator Loss: 0.577288\tGenerator Loss: 0.967110\n",
      "Train Epoch: 96 [16640/60000 (28%)]\tDiscriminator Loss: 0.487726\tGenerator Loss: 1.339314\n",
      "Train Epoch: 96 [17920/60000 (30%)]\tDiscriminator Loss: 0.454900\tGenerator Loss: 1.334260\n",
      "Train Epoch: 96 [19200/60000 (32%)]\tDiscriminator Loss: 0.454359\tGenerator Loss: 1.484303\n",
      "Train Epoch: 96 [20480/60000 (34%)]\tDiscriminator Loss: 0.491479\tGenerator Loss: 1.452272\n",
      "Train Epoch: 96 [21760/60000 (36%)]\tDiscriminator Loss: 0.495982\tGenerator Loss: 1.315880\n",
      "Train Epoch: 96 [23040/60000 (38%)]\tDiscriminator Loss: 0.493148\tGenerator Loss: 1.028608\n",
      "Train Epoch: 96 [24320/60000 (41%)]\tDiscriminator Loss: 0.476132\tGenerator Loss: 1.477398\n",
      "Train Epoch: 96 [25600/60000 (43%)]\tDiscriminator Loss: 0.485021\tGenerator Loss: 1.155796\n",
      "Train Epoch: 96 [26880/60000 (45%)]\tDiscriminator Loss: 0.454841\tGenerator Loss: 1.929483\n",
      "Train Epoch: 96 [28160/60000 (47%)]\tDiscriminator Loss: 0.488568\tGenerator Loss: 1.372029\n",
      "Train Epoch: 96 [29440/60000 (49%)]\tDiscriminator Loss: 0.459004\tGenerator Loss: 1.963400\n",
      "Train Epoch: 96 [30720/60000 (51%)]\tDiscriminator Loss: 0.496899\tGenerator Loss: 1.440129\n",
      "Train Epoch: 96 [32000/60000 (53%)]\tDiscriminator Loss: 0.458844\tGenerator Loss: 1.817587\n",
      "Train Epoch: 96 [33280/60000 (55%)]\tDiscriminator Loss: 0.496305\tGenerator Loss: 1.087182\n",
      "Train Epoch: 96 [34560/60000 (58%)]\tDiscriminator Loss: 0.507917\tGenerator Loss: 1.185692\n",
      "Train Epoch: 96 [35840/60000 (60%)]\tDiscriminator Loss: 0.499022\tGenerator Loss: 1.628690\n",
      "Train Epoch: 96 [37120/60000 (62%)]\tDiscriminator Loss: 0.464723\tGenerator Loss: 1.817794\n",
      "Train Epoch: 96 [38400/60000 (64%)]\tDiscriminator Loss: 0.489112\tGenerator Loss: 1.432486\n",
      "Train Epoch: 96 [39680/60000 (66%)]\tDiscriminator Loss: 0.532309\tGenerator Loss: 1.597627\n",
      "Train Epoch: 96 [40960/60000 (68%)]\tDiscriminator Loss: 0.474053\tGenerator Loss: 1.538201\n",
      "Train Epoch: 96 [42240/60000 (70%)]\tDiscriminator Loss: 0.497747\tGenerator Loss: 1.306099\n",
      "Train Epoch: 96 [43520/60000 (72%)]\tDiscriminator Loss: 0.470464\tGenerator Loss: 1.629535\n",
      "Train Epoch: 96 [44800/60000 (75%)]\tDiscriminator Loss: 0.443391\tGenerator Loss: 1.523321\n",
      "Train Epoch: 96 [46080/60000 (77%)]\tDiscriminator Loss: 0.460562\tGenerator Loss: 1.383882\n",
      "Train Epoch: 96 [47360/60000 (79%)]\tDiscriminator Loss: 0.460138\tGenerator Loss: 1.327577\n",
      "Train Epoch: 96 [48640/60000 (81%)]\tDiscriminator Loss: 0.447741\tGenerator Loss: 1.386315\n",
      "Train Epoch: 96 [49920/60000 (83%)]\tDiscriminator Loss: 0.444369\tGenerator Loss: 1.864260\n",
      "Train Epoch: 96 [51200/60000 (85%)]\tDiscriminator Loss: 0.536083\tGenerator Loss: 2.034168\n",
      "Train Epoch: 96 [52480/60000 (87%)]\tDiscriminator Loss: 0.508319\tGenerator Loss: 1.385975\n",
      "Train Epoch: 96 [53760/60000 (90%)]\tDiscriminator Loss: 0.443529\tGenerator Loss: 1.696588\n",
      "Train Epoch: 96 [55040/60000 (92%)]\tDiscriminator Loss: 0.468038\tGenerator Loss: 1.548625\n",
      "Train Epoch: 96 [56320/60000 (94%)]\tDiscriminator Loss: 0.445954\tGenerator Loss: 1.604750\n",
      "Train Epoch: 96 [57600/60000 (96%)]\tDiscriminator Loss: 0.497456\tGenerator Loss: 1.220751\n",
      "Train Epoch: 96 [58880/60000 (98%)]\tDiscriminator Loss: 0.478724\tGenerator Loss: 0.981665\n",
      "Train Epoch: 97 [0/60000 (0%)]\tDiscriminator Loss: 0.462706\tGenerator Loss: 1.374362\n",
      "Train Epoch: 97 [1280/60000 (2%)]\tDiscriminator Loss: 0.431229\tGenerator Loss: 1.394457\n",
      "Train Epoch: 97 [2560/60000 (4%)]\tDiscriminator Loss: 0.477400\tGenerator Loss: 1.292075\n",
      "Train Epoch: 97 [3840/60000 (6%)]\tDiscriminator Loss: 0.480294\tGenerator Loss: 1.453099\n",
      "Train Epoch: 97 [5120/60000 (9%)]\tDiscriminator Loss: 0.514421\tGenerator Loss: 1.476379\n",
      "Train Epoch: 97 [6400/60000 (11%)]\tDiscriminator Loss: 0.473124\tGenerator Loss: 1.329595\n",
      "Train Epoch: 97 [7680/60000 (13%)]\tDiscriminator Loss: 0.452638\tGenerator Loss: 1.356032\n",
      "Train Epoch: 97 [8960/60000 (15%)]\tDiscriminator Loss: 0.430950\tGenerator Loss: 1.453905\n",
      "Train Epoch: 97 [10240/60000 (17%)]\tDiscriminator Loss: 0.507428\tGenerator Loss: 1.788970\n",
      "Train Epoch: 97 [11520/60000 (19%)]\tDiscriminator Loss: 0.421296\tGenerator Loss: 1.253984\n",
      "Train Epoch: 97 [12800/60000 (21%)]\tDiscriminator Loss: 0.488114\tGenerator Loss: 1.426617\n",
      "Train Epoch: 97 [14080/60000 (23%)]\tDiscriminator Loss: 0.413658\tGenerator Loss: 1.772128\n",
      "Train Epoch: 97 [15360/60000 (26%)]\tDiscriminator Loss: 0.417408\tGenerator Loss: 1.742074\n",
      "Train Epoch: 97 [16640/60000 (28%)]\tDiscriminator Loss: 0.490118\tGenerator Loss: 1.474029\n",
      "Train Epoch: 97 [17920/60000 (30%)]\tDiscriminator Loss: 0.459363\tGenerator Loss: 1.548140\n",
      "Train Epoch: 97 [19200/60000 (32%)]\tDiscriminator Loss: 0.413615\tGenerator Loss: 1.649454\n",
      "Train Epoch: 97 [20480/60000 (34%)]\tDiscriminator Loss: 0.451299\tGenerator Loss: 1.367475\n",
      "Train Epoch: 97 [21760/60000 (36%)]\tDiscriminator Loss: 0.473867\tGenerator Loss: 1.603000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 97 [23040/60000 (38%)]\tDiscriminator Loss: 0.409886\tGenerator Loss: 1.832827\n",
      "Train Epoch: 97 [24320/60000 (41%)]\tDiscriminator Loss: 0.526616\tGenerator Loss: 2.031808\n",
      "Train Epoch: 97 [25600/60000 (43%)]\tDiscriminator Loss: 0.482871\tGenerator Loss: 1.059787\n",
      "Train Epoch: 97 [26880/60000 (45%)]\tDiscriminator Loss: 0.480294\tGenerator Loss: 1.573972\n",
      "Train Epoch: 97 [28160/60000 (47%)]\tDiscriminator Loss: 0.455044\tGenerator Loss: 1.567767\n",
      "Train Epoch: 97 [29440/60000 (49%)]\tDiscriminator Loss: 0.473629\tGenerator Loss: 1.170183\n",
      "Train Epoch: 97 [30720/60000 (51%)]\tDiscriminator Loss: 0.524479\tGenerator Loss: 0.951996\n",
      "Train Epoch: 97 [32000/60000 (53%)]\tDiscriminator Loss: 0.484446\tGenerator Loss: 1.414931\n",
      "Train Epoch: 97 [33280/60000 (55%)]\tDiscriminator Loss: 0.496149\tGenerator Loss: 1.248586\n",
      "Train Epoch: 97 [34560/60000 (58%)]\tDiscriminator Loss: 0.514658\tGenerator Loss: 1.210408\n",
      "Train Epoch: 97 [35840/60000 (60%)]\tDiscriminator Loss: 0.442250\tGenerator Loss: 1.276619\n",
      "Train Epoch: 97 [37120/60000 (62%)]\tDiscriminator Loss: 0.441590\tGenerator Loss: 1.560968\n",
      "Train Epoch: 97 [38400/60000 (64%)]\tDiscriminator Loss: 0.488777\tGenerator Loss: 1.720761\n",
      "Train Epoch: 97 [39680/60000 (66%)]\tDiscriminator Loss: 0.442071\tGenerator Loss: 1.417541\n",
      "Train Epoch: 97 [40960/60000 (68%)]\tDiscriminator Loss: 0.438419\tGenerator Loss: 1.648306\n",
      "Train Epoch: 97 [42240/60000 (70%)]\tDiscriminator Loss: 0.503481\tGenerator Loss: 1.492002\n",
      "Train Epoch: 97 [43520/60000 (72%)]\tDiscriminator Loss: 0.479489\tGenerator Loss: 1.075179\n",
      "Train Epoch: 97 [44800/60000 (75%)]\tDiscriminator Loss: 0.458967\tGenerator Loss: 1.562943\n",
      "Train Epoch: 97 [46080/60000 (77%)]\tDiscriminator Loss: 0.455098\tGenerator Loss: 1.318455\n",
      "Train Epoch: 97 [47360/60000 (79%)]\tDiscriminator Loss: 0.446468\tGenerator Loss: 1.382039\n",
      "Train Epoch: 97 [48640/60000 (81%)]\tDiscriminator Loss: 0.531847\tGenerator Loss: 1.665041\n",
      "Train Epoch: 97 [49920/60000 (83%)]\tDiscriminator Loss: 0.475105\tGenerator Loss: 1.279885\n",
      "Train Epoch: 97 [51200/60000 (85%)]\tDiscriminator Loss: 0.442581\tGenerator Loss: 1.329167\n",
      "Train Epoch: 97 [52480/60000 (87%)]\tDiscriminator Loss: 0.486479\tGenerator Loss: 1.222383\n",
      "Train Epoch: 97 [53760/60000 (90%)]\tDiscriminator Loss: 0.428375\tGenerator Loss: 1.548900\n",
      "Train Epoch: 97 [55040/60000 (92%)]\tDiscriminator Loss: 0.459634\tGenerator Loss: 1.629140\n",
      "Train Epoch: 97 [56320/60000 (94%)]\tDiscriminator Loss: 0.434810\tGenerator Loss: 1.774061\n",
      "Train Epoch: 97 [57600/60000 (96%)]\tDiscriminator Loss: 0.503516\tGenerator Loss: 1.361852\n",
      "Train Epoch: 97 [58880/60000 (98%)]\tDiscriminator Loss: 0.473601\tGenerator Loss: 1.621190\n",
      "Train Epoch: 98 [0/60000 (0%)]\tDiscriminator Loss: 0.442556\tGenerator Loss: 1.639913\n",
      "Train Epoch: 98 [1280/60000 (2%)]\tDiscriminator Loss: 0.412158\tGenerator Loss: 1.680094\n",
      "Train Epoch: 98 [2560/60000 (4%)]\tDiscriminator Loss: 0.431476\tGenerator Loss: 1.604234\n",
      "Train Epoch: 98 [3840/60000 (6%)]\tDiscriminator Loss: 0.485904\tGenerator Loss: 1.443559\n",
      "Train Epoch: 98 [5120/60000 (9%)]\tDiscriminator Loss: 0.422809\tGenerator Loss: 1.515376\n",
      "Train Epoch: 98 [6400/60000 (11%)]\tDiscriminator Loss: 0.467360\tGenerator Loss: 1.261166\n",
      "Train Epoch: 98 [7680/60000 (13%)]\tDiscriminator Loss: 0.492228\tGenerator Loss: 1.044535\n",
      "Train Epoch: 98 [8960/60000 (15%)]\tDiscriminator Loss: 0.506277\tGenerator Loss: 1.528961\n",
      "Train Epoch: 98 [10240/60000 (17%)]\tDiscriminator Loss: 0.497333\tGenerator Loss: 1.627039\n",
      "Train Epoch: 98 [11520/60000 (19%)]\tDiscriminator Loss: 0.459908\tGenerator Loss: 1.361307\n",
      "Train Epoch: 98 [12800/60000 (21%)]\tDiscriminator Loss: 0.451264\tGenerator Loss: 1.567387\n",
      "Train Epoch: 98 [14080/60000 (23%)]\tDiscriminator Loss: 0.453505\tGenerator Loss: 1.536091\n",
      "Train Epoch: 98 [15360/60000 (26%)]\tDiscriminator Loss: 0.461006\tGenerator Loss: 1.610352\n",
      "Train Epoch: 98 [16640/60000 (28%)]\tDiscriminator Loss: 0.474599\tGenerator Loss: 1.109077\n",
      "Train Epoch: 98 [17920/60000 (30%)]\tDiscriminator Loss: 0.414148\tGenerator Loss: 2.003182\n",
      "Train Epoch: 98 [19200/60000 (32%)]\tDiscriminator Loss: 0.530320\tGenerator Loss: 1.960629\n",
      "Train Epoch: 98 [20480/60000 (34%)]\tDiscriminator Loss: 0.513418\tGenerator Loss: 1.570135\n",
      "Train Epoch: 98 [21760/60000 (36%)]\tDiscriminator Loss: 0.442980\tGenerator Loss: 1.940983\n",
      "Train Epoch: 98 [23040/60000 (38%)]\tDiscriminator Loss: 0.524238\tGenerator Loss: 1.630666\n",
      "Train Epoch: 98 [24320/60000 (41%)]\tDiscriminator Loss: 0.440320\tGenerator Loss: 1.230464\n",
      "Train Epoch: 98 [25600/60000 (43%)]\tDiscriminator Loss: 0.497312\tGenerator Loss: 1.409240\n",
      "Train Epoch: 98 [26880/60000 (45%)]\tDiscriminator Loss: 0.412158\tGenerator Loss: 1.382855\n",
      "Train Epoch: 98 [28160/60000 (47%)]\tDiscriminator Loss: 0.449769\tGenerator Loss: 1.896044\n",
      "Train Epoch: 98 [29440/60000 (49%)]\tDiscriminator Loss: 0.447450\tGenerator Loss: 1.953704\n",
      "Train Epoch: 98 [30720/60000 (51%)]\tDiscriminator Loss: 0.504867\tGenerator Loss: 1.356316\n",
      "Train Epoch: 98 [32000/60000 (53%)]\tDiscriminator Loss: 0.420487\tGenerator Loss: 1.571033\n",
      "Train Epoch: 98 [33280/60000 (55%)]\tDiscriminator Loss: 0.420519\tGenerator Loss: 1.491262\n",
      "Train Epoch: 98 [34560/60000 (58%)]\tDiscriminator Loss: 0.444298\tGenerator Loss: 1.351061\n",
      "Train Epoch: 98 [35840/60000 (60%)]\tDiscriminator Loss: 0.472153\tGenerator Loss: 1.455211\n",
      "Train Epoch: 98 [37120/60000 (62%)]\tDiscriminator Loss: 0.483602\tGenerator Loss: 1.448335\n",
      "Train Epoch: 98 [38400/60000 (64%)]\tDiscriminator Loss: 0.503891\tGenerator Loss: 1.107732\n",
      "Train Epoch: 98 [39680/60000 (66%)]\tDiscriminator Loss: 0.449892\tGenerator Loss: 1.501587\n",
      "Train Epoch: 98 [40960/60000 (68%)]\tDiscriminator Loss: 0.450230\tGenerator Loss: 1.057634\n",
      "Train Epoch: 98 [42240/60000 (70%)]\tDiscriminator Loss: 0.455230\tGenerator Loss: 1.330600\n",
      "Train Epoch: 98 [43520/60000 (72%)]\tDiscriminator Loss: 0.419641\tGenerator Loss: 1.535152\n",
      "Train Epoch: 98 [44800/60000 (75%)]\tDiscriminator Loss: 0.453400\tGenerator Loss: 1.800488\n",
      "Train Epoch: 98 [46080/60000 (77%)]\tDiscriminator Loss: 0.488693\tGenerator Loss: 1.400369\n",
      "Train Epoch: 98 [47360/60000 (79%)]\tDiscriminator Loss: 0.518468\tGenerator Loss: 1.570415\n",
      "Train Epoch: 98 [48640/60000 (81%)]\tDiscriminator Loss: 0.495425\tGenerator Loss: 1.167139\n",
      "Train Epoch: 98 [49920/60000 (83%)]\tDiscriminator Loss: 0.451580\tGenerator Loss: 1.432825\n",
      "Train Epoch: 98 [51200/60000 (85%)]\tDiscriminator Loss: 0.441452\tGenerator Loss: 1.490341\n",
      "Train Epoch: 98 [52480/60000 (87%)]\tDiscriminator Loss: 0.484379\tGenerator Loss: 1.206439\n",
      "Train Epoch: 98 [53760/60000 (90%)]\tDiscriminator Loss: 0.509885\tGenerator Loss: 1.489748\n",
      "Train Epoch: 98 [55040/60000 (92%)]\tDiscriminator Loss: 0.442598\tGenerator Loss: 1.540746\n",
      "Train Epoch: 98 [56320/60000 (94%)]\tDiscriminator Loss: 0.465561\tGenerator Loss: 1.772640\n",
      "Train Epoch: 98 [57600/60000 (96%)]\tDiscriminator Loss: 0.497567\tGenerator Loss: 1.497416\n",
      "Train Epoch: 98 [58880/60000 (98%)]\tDiscriminator Loss: 0.588346\tGenerator Loss: 0.673957\n",
      "Train Epoch: 99 [0/60000 (0%)]\tDiscriminator Loss: 0.446224\tGenerator Loss: 1.535015\n",
      "Train Epoch: 99 [1280/60000 (2%)]\tDiscriminator Loss: 0.476181\tGenerator Loss: 1.953302\n",
      "Train Epoch: 99 [2560/60000 (4%)]\tDiscriminator Loss: 0.422987\tGenerator Loss: 1.363274\n",
      "Train Epoch: 99 [3840/60000 (6%)]\tDiscriminator Loss: 0.514308\tGenerator Loss: 1.696755\n",
      "Train Epoch: 99 [5120/60000 (9%)]\tDiscriminator Loss: 0.471018\tGenerator Loss: 1.402002\n",
      "Train Epoch: 99 [6400/60000 (11%)]\tDiscriminator Loss: 0.414476\tGenerator Loss: 1.710705\n",
      "Train Epoch: 99 [7680/60000 (13%)]\tDiscriminator Loss: 0.439508\tGenerator Loss: 1.493638\n",
      "Train Epoch: 99 [8960/60000 (15%)]\tDiscriminator Loss: 0.420181\tGenerator Loss: 1.402976\n",
      "Train Epoch: 99 [10240/60000 (17%)]\tDiscriminator Loss: 0.480020\tGenerator Loss: 1.087683\n",
      "Train Epoch: 99 [11520/60000 (19%)]\tDiscriminator Loss: 0.462121\tGenerator Loss: 1.559507\n",
      "Train Epoch: 99 [12800/60000 (21%)]\tDiscriminator Loss: 0.523269\tGenerator Loss: 1.371503\n",
      "Train Epoch: 99 [14080/60000 (23%)]\tDiscriminator Loss: 0.531755\tGenerator Loss: 1.520041\n",
      "Train Epoch: 99 [15360/60000 (26%)]\tDiscriminator Loss: 0.476475\tGenerator Loss: 1.274938\n",
      "Train Epoch: 99 [16640/60000 (28%)]\tDiscriminator Loss: 0.510039\tGenerator Loss: 1.280677\n",
      "Train Epoch: 99 [17920/60000 (30%)]\tDiscriminator Loss: 0.420810\tGenerator Loss: 1.666691\n",
      "Train Epoch: 99 [19200/60000 (32%)]\tDiscriminator Loss: 0.471982\tGenerator Loss: 1.238635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 99 [20480/60000 (34%)]\tDiscriminator Loss: 0.514808\tGenerator Loss: 1.091461\n",
      "Train Epoch: 99 [21760/60000 (36%)]\tDiscriminator Loss: 0.502300\tGenerator Loss: 1.596757\n",
      "Train Epoch: 99 [23040/60000 (38%)]\tDiscriminator Loss: 0.503841\tGenerator Loss: 1.566959\n",
      "Train Epoch: 99 [24320/60000 (41%)]\tDiscriminator Loss: 0.428670\tGenerator Loss: 1.321933\n",
      "Train Epoch: 99 [25600/60000 (43%)]\tDiscriminator Loss: 0.462202\tGenerator Loss: 1.335462\n",
      "Train Epoch: 99 [26880/60000 (45%)]\tDiscriminator Loss: 0.478141\tGenerator Loss: 1.419387\n",
      "Train Epoch: 99 [28160/60000 (47%)]\tDiscriminator Loss: 0.472203\tGenerator Loss: 1.496172\n",
      "Train Epoch: 99 [29440/60000 (49%)]\tDiscriminator Loss: 0.431572\tGenerator Loss: 1.402409\n",
      "Train Epoch: 99 [30720/60000 (51%)]\tDiscriminator Loss: 0.482354\tGenerator Loss: 1.376529\n",
      "Train Epoch: 99 [32000/60000 (53%)]\tDiscriminator Loss: 0.456234\tGenerator Loss: 1.412529\n",
      "Train Epoch: 99 [33280/60000 (55%)]\tDiscriminator Loss: 0.507162\tGenerator Loss: 1.486593\n",
      "Train Epoch: 99 [34560/60000 (58%)]\tDiscriminator Loss: 0.477338\tGenerator Loss: 1.515582\n",
      "Train Epoch: 99 [35840/60000 (60%)]\tDiscriminator Loss: 0.453187\tGenerator Loss: 1.065104\n",
      "Train Epoch: 99 [37120/60000 (62%)]\tDiscriminator Loss: 0.468109\tGenerator Loss: 1.704264\n",
      "Train Epoch: 99 [38400/60000 (64%)]\tDiscriminator Loss: 0.455574\tGenerator Loss: 1.502057\n",
      "Train Epoch: 99 [39680/60000 (66%)]\tDiscriminator Loss: 0.535116\tGenerator Loss: 1.240830\n",
      "Train Epoch: 99 [40960/60000 (68%)]\tDiscriminator Loss: 0.463817\tGenerator Loss: 1.339036\n",
      "Train Epoch: 99 [42240/60000 (70%)]\tDiscriminator Loss: 0.474847\tGenerator Loss: 1.352743\n",
      "Train Epoch: 99 [43520/60000 (72%)]\tDiscriminator Loss: 0.482519\tGenerator Loss: 1.200002\n",
      "Train Epoch: 99 [44800/60000 (75%)]\tDiscriminator Loss: 0.470318\tGenerator Loss: 1.322076\n",
      "Train Epoch: 99 [46080/60000 (77%)]\tDiscriminator Loss: 0.540060\tGenerator Loss: 0.856540\n",
      "Train Epoch: 99 [47360/60000 (79%)]\tDiscriminator Loss: 0.450983\tGenerator Loss: 1.637930\n",
      "Train Epoch: 99 [48640/60000 (81%)]\tDiscriminator Loss: 0.486964\tGenerator Loss: 2.197518\n",
      "Train Epoch: 99 [49920/60000 (83%)]\tDiscriminator Loss: 0.479935\tGenerator Loss: 1.749617\n",
      "Train Epoch: 99 [51200/60000 (85%)]\tDiscriminator Loss: 0.433289\tGenerator Loss: 1.591480\n",
      "Train Epoch: 99 [52480/60000 (87%)]\tDiscriminator Loss: 0.449033\tGenerator Loss: 1.528051\n",
      "Train Epoch: 99 [53760/60000 (90%)]\tDiscriminator Loss: 0.451576\tGenerator Loss: 1.919640\n",
      "Train Epoch: 99 [55040/60000 (92%)]\tDiscriminator Loss: 0.437661\tGenerator Loss: 1.452017\n",
      "Train Epoch: 99 [56320/60000 (94%)]\tDiscriminator Loss: 0.452066\tGenerator Loss: 1.516568\n",
      "Train Epoch: 99 [57600/60000 (96%)]\tDiscriminator Loss: 0.468896\tGenerator Loss: 2.224421\n",
      "Train Epoch: 99 [58880/60000 (98%)]\tDiscriminator Loss: 0.477641\tGenerator Loss: 1.192402\n",
      "Train Epoch: 100 [0/60000 (0%)]\tDiscriminator Loss: 0.520108\tGenerator Loss: 1.325844\n",
      "Train Epoch: 100 [1280/60000 (2%)]\tDiscriminator Loss: 0.469022\tGenerator Loss: 1.399668\n",
      "Train Epoch: 100 [2560/60000 (4%)]\tDiscriminator Loss: 0.502488\tGenerator Loss: 1.698152\n",
      "Train Epoch: 100 [3840/60000 (6%)]\tDiscriminator Loss: 0.487155\tGenerator Loss: 1.472257\n",
      "Train Epoch: 100 [5120/60000 (9%)]\tDiscriminator Loss: 0.463492\tGenerator Loss: 1.245586\n",
      "Train Epoch: 100 [6400/60000 (11%)]\tDiscriminator Loss: 0.437762\tGenerator Loss: 1.537824\n",
      "Train Epoch: 100 [7680/60000 (13%)]\tDiscriminator Loss: 0.514888\tGenerator Loss: 1.111057\n",
      "Train Epoch: 100 [8960/60000 (15%)]\tDiscriminator Loss: 0.441663\tGenerator Loss: 1.447884\n",
      "Train Epoch: 100 [10240/60000 (17%)]\tDiscriminator Loss: 0.449538\tGenerator Loss: 1.390230\n",
      "Train Epoch: 100 [11520/60000 (19%)]\tDiscriminator Loss: 0.460453\tGenerator Loss: 1.379697\n",
      "Train Epoch: 100 [12800/60000 (21%)]\tDiscriminator Loss: 0.478662\tGenerator Loss: 1.672266\n",
      "Train Epoch: 100 [14080/60000 (23%)]\tDiscriminator Loss: 0.464692\tGenerator Loss: 1.299459\n",
      "Train Epoch: 100 [15360/60000 (26%)]\tDiscriminator Loss: 0.477554\tGenerator Loss: 1.738904\n",
      "Train Epoch: 100 [16640/60000 (28%)]\tDiscriminator Loss: 0.469798\tGenerator Loss: 1.355367\n",
      "Train Epoch: 100 [17920/60000 (30%)]\tDiscriminator Loss: 0.490877\tGenerator Loss: 1.294511\n",
      "Train Epoch: 100 [19200/60000 (32%)]\tDiscriminator Loss: 0.500890\tGenerator Loss: 1.997463\n",
      "Train Epoch: 100 [20480/60000 (34%)]\tDiscriminator Loss: 0.465221\tGenerator Loss: 1.192082\n",
      "Train Epoch: 100 [21760/60000 (36%)]\tDiscriminator Loss: 0.455320\tGenerator Loss: 1.469211\n",
      "Train Epoch: 100 [23040/60000 (38%)]\tDiscriminator Loss: 0.471371\tGenerator Loss: 1.461633\n",
      "Train Epoch: 100 [24320/60000 (41%)]\tDiscriminator Loss: 0.495369\tGenerator Loss: 1.170716\n",
      "Train Epoch: 100 [25600/60000 (43%)]\tDiscriminator Loss: 0.457601\tGenerator Loss: 1.875445\n",
      "Train Epoch: 100 [26880/60000 (45%)]\tDiscriminator Loss: 0.456156\tGenerator Loss: 1.668754\n",
      "Train Epoch: 100 [28160/60000 (47%)]\tDiscriminator Loss: 0.468355\tGenerator Loss: 1.614481\n",
      "Train Epoch: 100 [29440/60000 (49%)]\tDiscriminator Loss: 0.451873\tGenerator Loss: 1.243264\n",
      "Train Epoch: 100 [30720/60000 (51%)]\tDiscriminator Loss: 0.513223\tGenerator Loss: 1.383064\n",
      "Train Epoch: 100 [32000/60000 (53%)]\tDiscriminator Loss: 0.510165\tGenerator Loss: 1.113956\n",
      "Train Epoch: 100 [33280/60000 (55%)]\tDiscriminator Loss: 0.429513\tGenerator Loss: 1.623055\n",
      "Train Epoch: 100 [34560/60000 (58%)]\tDiscriminator Loss: 0.429187\tGenerator Loss: 1.533985\n",
      "Train Epoch: 100 [35840/60000 (60%)]\tDiscriminator Loss: 0.468638\tGenerator Loss: 1.511303\n",
      "Train Epoch: 100 [37120/60000 (62%)]\tDiscriminator Loss: 0.403360\tGenerator Loss: 1.646120\n",
      "Train Epoch: 100 [38400/60000 (64%)]\tDiscriminator Loss: 0.503508\tGenerator Loss: 1.553719\n",
      "Train Epoch: 100 [39680/60000 (66%)]\tDiscriminator Loss: 0.462665\tGenerator Loss: 1.377459\n",
      "Train Epoch: 100 [40960/60000 (68%)]\tDiscriminator Loss: 0.491791\tGenerator Loss: 1.375077\n",
      "Train Epoch: 100 [42240/60000 (70%)]\tDiscriminator Loss: 0.461439\tGenerator Loss: 1.152217\n",
      "Train Epoch: 100 [43520/60000 (72%)]\tDiscriminator Loss: 0.454922\tGenerator Loss: 1.739425\n",
      "Train Epoch: 100 [44800/60000 (75%)]\tDiscriminator Loss: 0.480238\tGenerator Loss: 2.103886\n",
      "Train Epoch: 100 [46080/60000 (77%)]\tDiscriminator Loss: 0.439971\tGenerator Loss: 1.247592\n",
      "Train Epoch: 100 [47360/60000 (79%)]\tDiscriminator Loss: 0.478687\tGenerator Loss: 1.638284\n",
      "Train Epoch: 100 [48640/60000 (81%)]\tDiscriminator Loss: 0.506746\tGenerator Loss: 1.013668\n",
      "Train Epoch: 100 [49920/60000 (83%)]\tDiscriminator Loss: 0.491050\tGenerator Loss: 1.167619\n",
      "Train Epoch: 100 [51200/60000 (85%)]\tDiscriminator Loss: 0.536652\tGenerator Loss: 1.411837\n",
      "Train Epoch: 100 [52480/60000 (87%)]\tDiscriminator Loss: 0.436460\tGenerator Loss: 1.678014\n",
      "Train Epoch: 100 [53760/60000 (90%)]\tDiscriminator Loss: 0.494093\tGenerator Loss: 1.549600\n",
      "Train Epoch: 100 [55040/60000 (92%)]\tDiscriminator Loss: 0.497500\tGenerator Loss: 1.425912\n",
      "Train Epoch: 100 [56320/60000 (94%)]\tDiscriminator Loss: 0.410973\tGenerator Loss: 1.703423\n",
      "Train Epoch: 100 [57600/60000 (96%)]\tDiscriminator Loss: 0.450983\tGenerator Loss: 1.595517\n",
      "Train Epoch: 100 [58880/60000 (98%)]\tDiscriminator Loss: 0.484378\tGenerator Loss: 1.351293\n"
     ]
    }
   ],
   "source": [
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "gan_losses = []\n",
    "for epoch in range(1, epochs+1):\n",
    "    for i, (real_imgs, _) in enumerate(dataloader):\n",
    "        real_imgs = real_imgs.to(device)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = torch.ones([real_imgs.size(0), 1], device=device)\n",
    "        fake = torch.zeros([real_imgs.size(0), 1], device=device)\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_generator.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = torch.randn(real_imgs.shape[0], nz, device=device)\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "\n",
    "        # classify images using discriminator\n",
    "        classifications = discriminator(gen_imgs)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(classifications, valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_generator.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_discriminator.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "        # detaching since the backprop does not need to run on the generator\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        # take the average of loss against generated images and loss against real images\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_discriminator.step()\n",
    "\n",
    "        if i % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tDiscriminator Loss: {:.6f}\\tGenerator Loss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    i * len(real_imgs),\n",
    "                    len(dataloader.dataset),\n",
    "                    100.0 * i / len(dataloader),\n",
    "                    d_loss.item(),\n",
    "                    g_loss.item()\n",
    "                )\n",
    "            )\n",
    "    gan_losses.append((g_loss, real_loss, fake_loss, d_loss))\n",
    "    with torch.no_grad():\n",
    "        n_images = 64\n",
    "        sample = torch.randn(n_images, nz).to(device)\n",
    "        sample = generator(sample).cpu()\n",
    "        save_image(\n",
    "            sample.view(n_images, *img_shape),\n",
    "            gan_results_directory + \"/sample_\" + str(epoch) + \".png\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ecfd493",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(generator.state_dict(), os.path.join(curr_dirname, \"gan_generator.pt\"))\n",
    "torch.save(discriminator.state_dict(), os.path.join(curr_dirname, \"gan_discriminator.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7328e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_results_directory_name = \"results/vae\"\n",
    "os.makedirs(vae_results_directory_name, exist_ok=True)\n",
    "vae_results_directory = os.path.join(curr_dirname, vae_results_directory_name)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(data_directory, train=True, download=False, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize([0],[1])])),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(data_directory, train=False, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize([0],[1])])),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66bd2032",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc2 = nn.Linear(400, 200)\n",
    "        self.fc31 = nn.Linear(200, nz)\n",
    "        self.fc32 = nn.Linear(200, nz)\n",
    "        self.fc4 = nn.Linear(nz, 200)\n",
    "        self.fc5 = nn.Linear(200, 400)\n",
    "        self.fc6 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        return self.fc31(h2), self.fc32(h2)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc4(z))\n",
    "        h4 = F.relu(self.fc5(h3))\n",
    "        return torch.sigmoid(self.fc6(h4))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f93df782",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE().to(device)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1411faaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_train_losses = []\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def vae_loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction=\"sum\")\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD\n",
    "\n",
    "\n",
    "def train_vae(epoch):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = vae(data)\n",
    "        loss = vae_loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item() / len(data),\n",
    "                )\n",
    "            )\n",
    "    vae_train_losses.append(train_loss)\n",
    "    print(\n",
    "        \"====> Epoch: {} Average loss: {:.4f}\".format(\n",
    "            epoch, train_loss / len(train_loader.dataset)\n",
    "        )\n",
    "    )\n",
    "\n",
    "vae_test_losses = []\n",
    "def test_vae(epoch):\n",
    "    vae.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = vae(data)\n",
    "            test_loss += vae_loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat(\n",
    "                    [data[:n], recon_batch.view(batch_size, 1, 28, 28)[:n]]\n",
    "                )\n",
    "                save_image(\n",
    "                    comparison.cpu(),\n",
    "                    vae_results_directory + \"/reconstruction_\" + str(epoch) + \".png\",\n",
    "                    nrow=n,\n",
    "                )\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    vae_test_losses.append(test_loss)\n",
    "    print(\"====> Test set loss: {:.4f}\".format(test_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db860795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 164.534271\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 151.879227\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 158.449860\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 156.897308\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 156.128281\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 143.756760\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 153.472031\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 153.711761\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 158.372467\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 151.100250\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 147.430435\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 148.927658\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 149.445618\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 144.608200\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 143.897034\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 148.307846\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 142.856522\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 147.206833\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 141.436920\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 149.676117\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 138.460907\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 146.900101\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 150.463318\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 143.736633\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 139.143295\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 135.076859\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 142.536743\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 136.382309\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 142.467987\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 139.573196\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 132.017075\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 133.846497\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 138.751175\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 142.023666\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 136.038910\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 135.414795\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 136.239685\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 142.517242\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 133.462357\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 130.322189\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 134.993713\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 135.204712\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 140.347397\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 139.199600\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 133.786118\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 130.214371\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 132.917908\n",
      "====> Epoch: 1 Average loss: 142.1480\n",
      "====> Test set loss: 131.1435\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 128.527084\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 132.308624\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 125.366249\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 130.455811\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 126.126816\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 132.262665\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 125.802246\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 131.348083\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 127.497940\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 131.521942\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 125.445312\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 127.938248\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 133.573059\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 126.477074\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 128.987167\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 125.006630\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 127.657295\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 126.780396\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 128.415131\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 123.970772\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 125.851799\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 129.072403\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 123.036438\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 122.724701\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 120.858734\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 125.127998\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 122.796333\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 127.814392\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 127.758179\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 120.885529\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 119.492615\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 123.185837\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 119.147194\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 125.602882\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 122.046471\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 127.974915\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 115.810081\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 120.129028\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 115.802971\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 126.637589\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 122.044838\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 121.444893\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 121.220688\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 119.679596\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 115.660706\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 117.371635\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 118.654045\n",
      "====> Epoch: 2 Average loss: 125.2136\n",
      "====> Test set loss: 119.2233\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 121.904549\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 119.170975\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 117.895699\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 113.243698\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 119.792618\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 118.002617\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 119.563446\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 117.847366\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 119.439758\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 115.056541\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 119.338196\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 115.395424\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 123.251587\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 120.525963\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 116.431099\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 115.743164\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 113.937134\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 115.677208\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 121.153366\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 117.630013\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 119.470169\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 118.094482\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 123.759712\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 117.360535\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 118.621819\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 120.120926\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 118.857933\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 120.394913\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 118.246933\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 120.004150\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 119.977966\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 115.625061\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 117.516289\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 117.251717\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 110.482635\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 117.860657\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 117.291260\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 113.941200\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 111.856979\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 116.054169\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 115.645386\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 117.630463\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 114.128693\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 115.503365\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 121.892197\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 117.608803\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 112.694214\n",
      "====> Epoch: 3 Average loss: 117.7575\n",
      "====> Test set loss: 114.7346\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 115.776917\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 117.208466\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 118.792435\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 116.035950\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 114.783249\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 118.918137\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 115.343338\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 119.256607\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 115.109146\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 111.594193\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 115.975510\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 108.436264\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 117.064552\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 114.518486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 115.787766\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 118.492271\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 120.503105\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 107.272041\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 111.983772\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 115.054970\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 116.693985\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 115.994415\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 110.948334\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 108.479050\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 113.459991\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 113.308937\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 112.304733\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 111.699631\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 116.783623\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 107.802628\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 118.160538\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 112.191162\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 110.438461\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 115.694740\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 113.329132\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 111.620064\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 117.932449\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 115.072838\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 110.596710\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 112.233185\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 111.214638\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 111.619553\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 108.865791\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 112.410774\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 112.705383\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 111.916962\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 111.940323\n",
      "====> Epoch: 4 Average loss: 114.1963\n",
      "====> Test set loss: 112.0254\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 116.458237\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 112.641678\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 115.762032\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 114.057190\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 115.741249\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 111.006622\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 114.176788\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 110.496964\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 119.079666\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 113.190887\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 120.272240\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 109.076126\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 109.550377\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 115.417450\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 111.065262\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 107.719749\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 109.081139\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 113.532288\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 109.880722\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 116.666214\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 110.344231\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 115.921173\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 105.183594\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 112.600449\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 115.225449\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 112.268234\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 109.122192\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 107.478661\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 107.498947\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 109.029892\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 106.764984\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 112.182640\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 111.188278\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 110.598007\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 109.505066\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 116.195740\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 116.941788\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 117.295013\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 109.063606\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 109.379402\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 113.662148\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 113.103554\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 113.158478\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 113.673355\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 107.428406\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 110.200447\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 111.645691\n",
      "====> Epoch: 5 Average loss: 111.5654\n",
      "====> Test set loss: 109.7251\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 106.937210\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 111.718697\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 107.617996\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 107.123695\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 107.397202\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 105.695068\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 111.616974\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 109.482903\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 109.927773\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 114.225494\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 108.969719\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 108.287460\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 106.896667\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 114.040604\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 110.309708\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 111.234901\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 108.933235\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 108.630005\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 105.331665\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 107.849213\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 112.745216\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 106.478195\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 112.500664\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 112.032310\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 112.113480\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 109.662033\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 108.207664\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 109.542587\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 108.545372\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 111.804703\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 108.981003\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 110.640732\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 112.253586\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 105.092499\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 110.168266\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 108.713730\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 104.378311\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 112.621811\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 111.188766\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 111.462181\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 109.951828\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 106.340927\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 113.435944\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 106.236351\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 107.173187\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 110.456810\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 108.211609\n",
      "====> Epoch: 6 Average loss: 109.5441\n",
      "====> Test set loss: 107.9999\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 106.114883\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 111.840637\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 106.378151\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 107.670563\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 103.468781\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 110.473404\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 109.520988\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 110.511711\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 108.552376\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 107.782196\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 108.259460\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 105.866516\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 111.187256\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 106.751541\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 108.597916\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 111.294052\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 107.547989\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 107.590942\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 107.021820\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 107.084747\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 109.202652\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 109.417961\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 109.978874\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 110.762489\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 103.796974\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 107.915382\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 104.578156\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 111.695724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 106.081970\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 108.463257\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 110.356308\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 102.516724\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 103.572060\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 105.790627\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 103.297485\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 106.296791\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 105.799194\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 108.920036\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 106.664688\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 109.977417\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 106.545357\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 112.130524\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 104.663116\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 102.776764\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 105.804474\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 109.608528\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 109.376633\n",
      "====> Epoch: 7 Average loss: 107.8994\n",
      "====> Test set loss: 106.8758\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 106.279984\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 109.140900\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 106.328156\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 106.692734\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 104.093895\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 108.608788\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 110.009140\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 102.713661\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 104.321251\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 105.865997\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 106.611748\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 111.462708\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 106.227211\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 103.228149\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 104.392075\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 107.820473\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 104.514290\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 106.345970\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 108.755920\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 110.094467\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 103.515739\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 108.122963\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 106.789474\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 108.793953\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 108.546570\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 107.582169\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 108.637825\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 108.758591\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 107.704620\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 104.176445\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 108.357086\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 109.257515\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 107.475906\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 108.581604\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 108.543823\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 107.612770\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 103.379654\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 104.965546\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 109.072311\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 100.789536\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 105.963562\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 106.389099\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 106.009224\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 110.705505\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 105.366486\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 103.688835\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 106.572823\n",
      "====> Epoch: 8 Average loss: 106.6681\n",
      "====> Test set loss: 105.6139\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 107.362244\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 106.480705\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 109.968178\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 104.152679\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 104.206055\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 105.097763\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 105.907028\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 104.620789\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 109.397766\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 106.634247\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 106.918793\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 105.395279\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 108.171417\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 105.178711\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 104.200531\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 107.461067\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 111.848457\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 107.704330\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 102.144638\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 102.982918\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 103.408203\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 105.122963\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 105.686768\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 107.141815\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 105.123291\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 105.421219\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 104.452545\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 106.215958\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 111.118912\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 105.969597\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 104.303185\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 103.952637\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 102.337845\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 106.571175\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 102.471375\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 104.685776\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 104.004776\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 105.617874\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 104.533234\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 100.059944\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 103.279076\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 101.355530\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 104.230347\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 100.584808\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 106.208038\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 101.806145\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 106.004776\n",
      "====> Epoch: 9 Average loss: 105.6663\n",
      "====> Test set loss: 104.9686\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 105.799156\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 103.855896\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 108.841034\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 106.753036\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 105.290260\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 105.199791\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 103.975937\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 106.470490\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 100.704025\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 106.398926\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 103.315033\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 102.323265\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 105.688232\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 105.833176\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 107.767471\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 102.813324\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 101.742340\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 103.416687\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 105.842079\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 105.536697\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 109.344437\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 104.824638\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 106.216972\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 102.729607\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 103.875427\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 108.070969\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 105.798088\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 104.520508\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 111.602585\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 103.244278\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 104.073463\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 106.594299\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 108.941444\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 101.001846\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 103.083694\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 106.698242\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 107.599121\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 104.709679\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 107.360046\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 102.185898\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 108.825676\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 105.765709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 105.142151\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 103.736862\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 102.440491\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 99.942574\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 106.608597\n",
      "====> Epoch: 10 Average loss: 104.8578\n",
      "====> Test set loss: 104.6448\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 108.132080\n",
      "Train Epoch: 11 [1280/60000 (2%)]\tLoss: 109.065994\n",
      "Train Epoch: 11 [2560/60000 (4%)]\tLoss: 104.118317\n",
      "Train Epoch: 11 [3840/60000 (6%)]\tLoss: 97.448792\n",
      "Train Epoch: 11 [5120/60000 (9%)]\tLoss: 98.433228\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 106.042046\n",
      "Train Epoch: 11 [7680/60000 (13%)]\tLoss: 108.447945\n",
      "Train Epoch: 11 [8960/60000 (15%)]\tLoss: 106.849838\n",
      "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 107.127174\n",
      "Train Epoch: 11 [11520/60000 (19%)]\tLoss: 105.411324\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 99.535217\n",
      "Train Epoch: 11 [14080/60000 (23%)]\tLoss: 106.622543\n",
      "Train Epoch: 11 [15360/60000 (26%)]\tLoss: 101.939377\n",
      "Train Epoch: 11 [16640/60000 (28%)]\tLoss: 108.410095\n",
      "Train Epoch: 11 [17920/60000 (30%)]\tLoss: 104.785233\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 103.929497\n",
      "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 107.368057\n",
      "Train Epoch: 11 [21760/60000 (36%)]\tLoss: 105.180382\n",
      "Train Epoch: 11 [23040/60000 (38%)]\tLoss: 104.845680\n",
      "Train Epoch: 11 [24320/60000 (41%)]\tLoss: 103.346786\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 104.666542\n",
      "Train Epoch: 11 [26880/60000 (45%)]\tLoss: 101.569901\n",
      "Train Epoch: 11 [28160/60000 (47%)]\tLoss: 100.447021\n",
      "Train Epoch: 11 [29440/60000 (49%)]\tLoss: 109.177261\n",
      "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 104.822800\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 103.870049\n",
      "Train Epoch: 11 [33280/60000 (55%)]\tLoss: 105.119362\n",
      "Train Epoch: 11 [34560/60000 (58%)]\tLoss: 109.594681\n",
      "Train Epoch: 11 [35840/60000 (60%)]\tLoss: 100.565056\n",
      "Train Epoch: 11 [37120/60000 (62%)]\tLoss: 100.994400\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 100.551437\n",
      "Train Epoch: 11 [39680/60000 (66%)]\tLoss: 105.551971\n",
      "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 105.089287\n",
      "Train Epoch: 11 [42240/60000 (70%)]\tLoss: 107.195038\n",
      "Train Epoch: 11 [43520/60000 (72%)]\tLoss: 103.676704\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 101.720337\n",
      "Train Epoch: 11 [46080/60000 (77%)]\tLoss: 103.017761\n",
      "Train Epoch: 11 [47360/60000 (79%)]\tLoss: 100.842400\n",
      "Train Epoch: 11 [48640/60000 (81%)]\tLoss: 104.173004\n",
      "Train Epoch: 11 [49920/60000 (83%)]\tLoss: 99.736351\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 102.639984\n",
      "Train Epoch: 11 [52480/60000 (87%)]\tLoss: 108.554352\n",
      "Train Epoch: 11 [53760/60000 (90%)]\tLoss: 102.263641\n",
      "Train Epoch: 11 [55040/60000 (92%)]\tLoss: 104.001389\n",
      "Train Epoch: 11 [56320/60000 (94%)]\tLoss: 102.397987\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 103.202278\n",
      "Train Epoch: 11 [58880/60000 (98%)]\tLoss: 101.806473\n",
      "====> Epoch: 11 Average loss: 104.2126\n",
      "====> Test set loss: 103.3818\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 104.798546\n",
      "Train Epoch: 12 [1280/60000 (2%)]\tLoss: 100.868515\n",
      "Train Epoch: 12 [2560/60000 (4%)]\tLoss: 103.247322\n",
      "Train Epoch: 12 [3840/60000 (6%)]\tLoss: 105.787659\n",
      "Train Epoch: 12 [5120/60000 (9%)]\tLoss: 100.685577\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 101.846970\n",
      "Train Epoch: 12 [7680/60000 (13%)]\tLoss: 105.085693\n",
      "Train Epoch: 12 [8960/60000 (15%)]\tLoss: 103.405411\n",
      "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 101.358437\n",
      "Train Epoch: 12 [11520/60000 (19%)]\tLoss: 105.900459\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 105.111450\n",
      "Train Epoch: 12 [14080/60000 (23%)]\tLoss: 106.111816\n",
      "Train Epoch: 12 [15360/60000 (26%)]\tLoss: 101.412598\n",
      "Train Epoch: 12 [16640/60000 (28%)]\tLoss: 101.823853\n",
      "Train Epoch: 12 [17920/60000 (30%)]\tLoss: 101.395844\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 102.209778\n",
      "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 104.459747\n",
      "Train Epoch: 12 [21760/60000 (36%)]\tLoss: 108.386353\n",
      "Train Epoch: 12 [23040/60000 (38%)]\tLoss: 108.721024\n",
      "Train Epoch: 12 [24320/60000 (41%)]\tLoss: 102.919418\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 101.313812\n",
      "Train Epoch: 12 [26880/60000 (45%)]\tLoss: 96.256042\n",
      "Train Epoch: 12 [28160/60000 (47%)]\tLoss: 105.677582\n",
      "Train Epoch: 12 [29440/60000 (49%)]\tLoss: 106.653503\n",
      "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 106.113113\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 99.711891\n",
      "Train Epoch: 12 [33280/60000 (55%)]\tLoss: 103.337036\n",
      "Train Epoch: 12 [34560/60000 (58%)]\tLoss: 102.396790\n",
      "Train Epoch: 12 [35840/60000 (60%)]\tLoss: 103.437462\n",
      "Train Epoch: 12 [37120/60000 (62%)]\tLoss: 104.318367\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 103.206680\n",
      "Train Epoch: 12 [39680/60000 (66%)]\tLoss: 101.709366\n",
      "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 103.988708\n",
      "Train Epoch: 12 [42240/60000 (70%)]\tLoss: 99.919312\n",
      "Train Epoch: 12 [43520/60000 (72%)]\tLoss: 101.172134\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 101.713348\n",
      "Train Epoch: 12 [46080/60000 (77%)]\tLoss: 102.552063\n",
      "Train Epoch: 12 [47360/60000 (79%)]\tLoss: 102.240753\n",
      "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 103.363274\n",
      "Train Epoch: 12 [49920/60000 (83%)]\tLoss: 100.387329\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 101.437538\n",
      "Train Epoch: 12 [52480/60000 (87%)]\tLoss: 103.406265\n",
      "Train Epoch: 12 [53760/60000 (90%)]\tLoss: 99.513489\n",
      "Train Epoch: 12 [55040/60000 (92%)]\tLoss: 103.813232\n",
      "Train Epoch: 12 [56320/60000 (94%)]\tLoss: 100.391693\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 96.278412\n",
      "Train Epoch: 12 [58880/60000 (98%)]\tLoss: 109.446571\n",
      "====> Epoch: 12 Average loss: 103.6321\n",
      "====> Test set loss: 103.4323\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 107.414192\n",
      "Train Epoch: 13 [1280/60000 (2%)]\tLoss: 103.554855\n",
      "Train Epoch: 13 [2560/60000 (4%)]\tLoss: 104.640915\n",
      "Train Epoch: 13 [3840/60000 (6%)]\tLoss: 109.132263\n",
      "Train Epoch: 13 [5120/60000 (9%)]\tLoss: 107.738327\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 100.506142\n",
      "Train Epoch: 13 [7680/60000 (13%)]\tLoss: 106.382866\n",
      "Train Epoch: 13 [8960/60000 (15%)]\tLoss: 104.928101\n",
      "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 102.862198\n",
      "Train Epoch: 13 [11520/60000 (19%)]\tLoss: 107.093552\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 104.803925\n",
      "Train Epoch: 13 [14080/60000 (23%)]\tLoss: 103.512253\n",
      "Train Epoch: 13 [15360/60000 (26%)]\tLoss: 101.801559\n",
      "Train Epoch: 13 [16640/60000 (28%)]\tLoss: 99.716110\n",
      "Train Epoch: 13 [17920/60000 (30%)]\tLoss: 102.402916\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 104.552284\n",
      "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 107.124260\n",
      "Train Epoch: 13 [21760/60000 (36%)]\tLoss: 105.097229\n",
      "Train Epoch: 13 [23040/60000 (38%)]\tLoss: 104.030724\n",
      "Train Epoch: 13 [24320/60000 (41%)]\tLoss: 99.312759\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 102.078247\n",
      "Train Epoch: 13 [26880/60000 (45%)]\tLoss: 103.822762\n",
      "Train Epoch: 13 [28160/60000 (47%)]\tLoss: 101.407646\n",
      "Train Epoch: 13 [29440/60000 (49%)]\tLoss: 104.294678\n",
      "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 99.621025\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 99.054855\n",
      "Train Epoch: 13 [33280/60000 (55%)]\tLoss: 101.903214\n",
      "Train Epoch: 13 [34560/60000 (58%)]\tLoss: 103.521790\n",
      "Train Epoch: 13 [35840/60000 (60%)]\tLoss: 106.302727\n",
      "Train Epoch: 13 [37120/60000 (62%)]\tLoss: 99.959389\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 102.878792\n",
      "Train Epoch: 13 [39680/60000 (66%)]\tLoss: 105.981613\n",
      "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 108.289352\n",
      "Train Epoch: 13 [42240/60000 (70%)]\tLoss: 104.043175\n",
      "Train Epoch: 13 [43520/60000 (72%)]\tLoss: 104.983505\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 107.446747\n",
      "Train Epoch: 13 [46080/60000 (77%)]\tLoss: 102.157867\n",
      "Train Epoch: 13 [47360/60000 (79%)]\tLoss: 104.931885\n",
      "Train Epoch: 13 [48640/60000 (81%)]\tLoss: 102.286461\n",
      "Train Epoch: 13 [49920/60000 (83%)]\tLoss: 102.741470\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 103.385849\n",
      "Train Epoch: 13 [52480/60000 (87%)]\tLoss: 101.437881\n",
      "Train Epoch: 13 [53760/60000 (90%)]\tLoss: 107.210205\n",
      "Train Epoch: 13 [55040/60000 (92%)]\tLoss: 102.639725\n",
      "Train Epoch: 13 [56320/60000 (94%)]\tLoss: 102.820900\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 104.641068\n",
      "Train Epoch: 13 [58880/60000 (98%)]\tLoss: 106.770340\n",
      "====> Epoch: 13 Average loss: 103.2575\n",
      "====> Test set loss: 103.0581\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 103.052902\n",
      "Train Epoch: 14 [1280/60000 (2%)]\tLoss: 104.686707\n",
      "Train Epoch: 14 [2560/60000 (4%)]\tLoss: 101.363556\n",
      "Train Epoch: 14 [3840/60000 (6%)]\tLoss: 107.371803\n",
      "Train Epoch: 14 [5120/60000 (9%)]\tLoss: 100.393738\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 100.259552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 14 [7680/60000 (13%)]\tLoss: 99.508133\n",
      "Train Epoch: 14 [8960/60000 (15%)]\tLoss: 103.754417\n",
      "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 101.941360\n",
      "Train Epoch: 14 [11520/60000 (19%)]\tLoss: 102.316734\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 104.790558\n",
      "Train Epoch: 14 [14080/60000 (23%)]\tLoss: 96.620834\n",
      "Train Epoch: 14 [15360/60000 (26%)]\tLoss: 102.935997\n",
      "Train Epoch: 14 [16640/60000 (28%)]\tLoss: 103.869598\n",
      "Train Epoch: 14 [17920/60000 (30%)]\tLoss: 104.549133\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 99.852158\n",
      "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 102.483414\n",
      "Train Epoch: 14 [21760/60000 (36%)]\tLoss: 103.182541\n",
      "Train Epoch: 14 [23040/60000 (38%)]\tLoss: 109.684204\n",
      "Train Epoch: 14 [24320/60000 (41%)]\tLoss: 104.214417\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 104.128036\n",
      "Train Epoch: 14 [26880/60000 (45%)]\tLoss: 102.861282\n",
      "Train Epoch: 14 [28160/60000 (47%)]\tLoss: 105.996048\n",
      "Train Epoch: 14 [29440/60000 (49%)]\tLoss: 106.202682\n",
      "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 102.012024\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 108.534523\n",
      "Train Epoch: 14 [33280/60000 (55%)]\tLoss: 102.693779\n",
      "Train Epoch: 14 [34560/60000 (58%)]\tLoss: 104.360268\n",
      "Train Epoch: 14 [35840/60000 (60%)]\tLoss: 103.884674\n",
      "Train Epoch: 14 [37120/60000 (62%)]\tLoss: 100.483597\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 103.678436\n",
      "Train Epoch: 14 [39680/60000 (66%)]\tLoss: 98.027100\n",
      "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 102.524742\n",
      "Train Epoch: 14 [42240/60000 (70%)]\tLoss: 106.120674\n",
      "Train Epoch: 14 [43520/60000 (72%)]\tLoss: 102.497261\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 105.603317\n",
      "Train Epoch: 14 [46080/60000 (77%)]\tLoss: 103.810242\n",
      "Train Epoch: 14 [47360/60000 (79%)]\tLoss: 105.462334\n",
      "Train Epoch: 14 [48640/60000 (81%)]\tLoss: 95.071152\n",
      "Train Epoch: 14 [49920/60000 (83%)]\tLoss: 106.825150\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 102.269249\n",
      "Train Epoch: 14 [52480/60000 (87%)]\tLoss: 102.494446\n",
      "Train Epoch: 14 [53760/60000 (90%)]\tLoss: 102.795395\n",
      "Train Epoch: 14 [55040/60000 (92%)]\tLoss: 100.937668\n",
      "Train Epoch: 14 [56320/60000 (94%)]\tLoss: 105.784637\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 104.506622\n",
      "Train Epoch: 14 [58880/60000 (98%)]\tLoss: 100.615845\n",
      "====> Epoch: 14 Average loss: 102.7961\n",
      "====> Test set loss: 102.8155\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 105.574493\n",
      "Train Epoch: 15 [1280/60000 (2%)]\tLoss: 106.093254\n",
      "Train Epoch: 15 [2560/60000 (4%)]\tLoss: 108.874878\n",
      "Train Epoch: 15 [3840/60000 (6%)]\tLoss: 102.438652\n",
      "Train Epoch: 15 [5120/60000 (9%)]\tLoss: 102.402283\n",
      "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 102.019562\n",
      "Train Epoch: 15 [7680/60000 (13%)]\tLoss: 102.170891\n",
      "Train Epoch: 15 [8960/60000 (15%)]\tLoss: 102.748825\n",
      "Train Epoch: 15 [10240/60000 (17%)]\tLoss: 103.965317\n",
      "Train Epoch: 15 [11520/60000 (19%)]\tLoss: 100.592934\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 101.064034\n",
      "Train Epoch: 15 [14080/60000 (23%)]\tLoss: 100.035355\n",
      "Train Epoch: 15 [15360/60000 (26%)]\tLoss: 99.188293\n",
      "Train Epoch: 15 [16640/60000 (28%)]\tLoss: 104.210297\n",
      "Train Epoch: 15 [17920/60000 (30%)]\tLoss: 100.933075\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 102.982063\n",
      "Train Epoch: 15 [20480/60000 (34%)]\tLoss: 101.211884\n",
      "Train Epoch: 15 [21760/60000 (36%)]\tLoss: 106.549889\n",
      "Train Epoch: 15 [23040/60000 (38%)]\tLoss: 100.725861\n",
      "Train Epoch: 15 [24320/60000 (41%)]\tLoss: 102.461075\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 99.600754\n",
      "Train Epoch: 15 [26880/60000 (45%)]\tLoss: 100.950279\n",
      "Train Epoch: 15 [28160/60000 (47%)]\tLoss: 104.109573\n",
      "Train Epoch: 15 [29440/60000 (49%)]\tLoss: 103.546021\n",
      "Train Epoch: 15 [30720/60000 (51%)]\tLoss: 103.123550\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 105.594788\n",
      "Train Epoch: 15 [33280/60000 (55%)]\tLoss: 104.804207\n",
      "Train Epoch: 15 [34560/60000 (58%)]\tLoss: 105.805695\n",
      "Train Epoch: 15 [35840/60000 (60%)]\tLoss: 101.991745\n",
      "Train Epoch: 15 [37120/60000 (62%)]\tLoss: 106.259079\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 107.887474\n",
      "Train Epoch: 15 [39680/60000 (66%)]\tLoss: 101.815399\n",
      "Train Epoch: 15 [40960/60000 (68%)]\tLoss: 101.447464\n",
      "Train Epoch: 15 [42240/60000 (70%)]\tLoss: 104.123138\n",
      "Train Epoch: 15 [43520/60000 (72%)]\tLoss: 105.462280\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 101.698013\n",
      "Train Epoch: 15 [46080/60000 (77%)]\tLoss: 100.707069\n",
      "Train Epoch: 15 [47360/60000 (79%)]\tLoss: 102.699463\n",
      "Train Epoch: 15 [48640/60000 (81%)]\tLoss: 103.991302\n",
      "Train Epoch: 15 [49920/60000 (83%)]\tLoss: 98.826202\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 105.227234\n",
      "Train Epoch: 15 [52480/60000 (87%)]\tLoss: 101.903633\n",
      "Train Epoch: 15 [53760/60000 (90%)]\tLoss: 102.801239\n",
      "Train Epoch: 15 [55040/60000 (92%)]\tLoss: 103.330826\n",
      "Train Epoch: 15 [56320/60000 (94%)]\tLoss: 102.786751\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 101.333588\n",
      "Train Epoch: 15 [58880/60000 (98%)]\tLoss: 104.414078\n",
      "====> Epoch: 15 Average loss: 102.4266\n",
      "====> Test set loss: 102.5111\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 102.848122\n",
      "Train Epoch: 16 [1280/60000 (2%)]\tLoss: 97.045280\n",
      "Train Epoch: 16 [2560/60000 (4%)]\tLoss: 101.561234\n",
      "Train Epoch: 16 [3840/60000 (6%)]\tLoss: 102.790909\n",
      "Train Epoch: 16 [5120/60000 (9%)]\tLoss: 99.486595\n",
      "Train Epoch: 16 [6400/60000 (11%)]\tLoss: 102.650810\n",
      "Train Epoch: 16 [7680/60000 (13%)]\tLoss: 103.641441\n",
      "Train Epoch: 16 [8960/60000 (15%)]\tLoss: 106.209587\n",
      "Train Epoch: 16 [10240/60000 (17%)]\tLoss: 101.334259\n",
      "Train Epoch: 16 [11520/60000 (19%)]\tLoss: 104.300842\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 100.927765\n",
      "Train Epoch: 16 [14080/60000 (23%)]\tLoss: 101.763412\n",
      "Train Epoch: 16 [15360/60000 (26%)]\tLoss: 99.538147\n",
      "Train Epoch: 16 [16640/60000 (28%)]\tLoss: 101.766464\n",
      "Train Epoch: 16 [17920/60000 (30%)]\tLoss: 105.080917\n",
      "Train Epoch: 16 [19200/60000 (32%)]\tLoss: 98.904404\n",
      "Train Epoch: 16 [20480/60000 (34%)]\tLoss: 101.712997\n",
      "Train Epoch: 16 [21760/60000 (36%)]\tLoss: 104.062347\n",
      "Train Epoch: 16 [23040/60000 (38%)]\tLoss: 104.021362\n",
      "Train Epoch: 16 [24320/60000 (41%)]\tLoss: 98.117737\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 100.914932\n",
      "Train Epoch: 16 [26880/60000 (45%)]\tLoss: 102.676659\n",
      "Train Epoch: 16 [28160/60000 (47%)]\tLoss: 99.241562\n",
      "Train Epoch: 16 [29440/60000 (49%)]\tLoss: 99.508987\n",
      "Train Epoch: 16 [30720/60000 (51%)]\tLoss: 100.780861\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 99.566444\n",
      "Train Epoch: 16 [33280/60000 (55%)]\tLoss: 97.539459\n",
      "Train Epoch: 16 [34560/60000 (58%)]\tLoss: 96.910095\n",
      "Train Epoch: 16 [35840/60000 (60%)]\tLoss: 101.110107\n",
      "Train Epoch: 16 [37120/60000 (62%)]\tLoss: 102.921684\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 97.632965\n",
      "Train Epoch: 16 [39680/60000 (66%)]\tLoss: 104.996796\n",
      "Train Epoch: 16 [40960/60000 (68%)]\tLoss: 103.070457\n",
      "Train Epoch: 16 [42240/60000 (70%)]\tLoss: 98.124466\n",
      "Train Epoch: 16 [43520/60000 (72%)]\tLoss: 102.001694\n",
      "Train Epoch: 16 [44800/60000 (75%)]\tLoss: 97.653671\n",
      "Train Epoch: 16 [46080/60000 (77%)]\tLoss: 102.508896\n",
      "Train Epoch: 16 [47360/60000 (79%)]\tLoss: 107.643463\n",
      "Train Epoch: 16 [48640/60000 (81%)]\tLoss: 97.715767\n",
      "Train Epoch: 16 [49920/60000 (83%)]\tLoss: 97.774673\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 101.113121\n",
      "Train Epoch: 16 [52480/60000 (87%)]\tLoss: 98.243942\n",
      "Train Epoch: 16 [53760/60000 (90%)]\tLoss: 102.247513\n",
      "Train Epoch: 16 [55040/60000 (92%)]\tLoss: 103.438187\n",
      "Train Epoch: 16 [56320/60000 (94%)]\tLoss: 99.581024\n",
      "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 101.479248\n",
      "Train Epoch: 16 [58880/60000 (98%)]\tLoss: 102.441490\n",
      "====> Epoch: 16 Average loss: 102.1265\n",
      "====> Test set loss: 102.1825\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 103.240051\n",
      "Train Epoch: 17 [1280/60000 (2%)]\tLoss: 100.494034\n",
      "Train Epoch: 17 [2560/60000 (4%)]\tLoss: 101.729256\n",
      "Train Epoch: 17 [3840/60000 (6%)]\tLoss: 102.258026\n",
      "Train Epoch: 17 [5120/60000 (9%)]\tLoss: 103.369072\n",
      "Train Epoch: 17 [6400/60000 (11%)]\tLoss: 100.418900\n",
      "Train Epoch: 17 [7680/60000 (13%)]\tLoss: 99.878708\n",
      "Train Epoch: 17 [8960/60000 (15%)]\tLoss: 104.912659\n",
      "Train Epoch: 17 [10240/60000 (17%)]\tLoss: 99.001068\n",
      "Train Epoch: 17 [11520/60000 (19%)]\tLoss: 101.271561\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 107.657341\n",
      "Train Epoch: 17 [14080/60000 (23%)]\tLoss: 100.836655\n",
      "Train Epoch: 17 [15360/60000 (26%)]\tLoss: 100.187126\n",
      "Train Epoch: 17 [16640/60000 (28%)]\tLoss: 99.507103\n",
      "Train Epoch: 17 [17920/60000 (30%)]\tLoss: 99.185921\n",
      "Train Epoch: 17 [19200/60000 (32%)]\tLoss: 101.937607\n",
      "Train Epoch: 17 [20480/60000 (34%)]\tLoss: 102.515671\n",
      "Train Epoch: 17 [21760/60000 (36%)]\tLoss: 104.183395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 17 [23040/60000 (38%)]\tLoss: 100.523216\n",
      "Train Epoch: 17 [24320/60000 (41%)]\tLoss: 107.269226\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 100.358086\n",
      "Train Epoch: 17 [26880/60000 (45%)]\tLoss: 103.657936\n",
      "Train Epoch: 17 [28160/60000 (47%)]\tLoss: 101.767044\n",
      "Train Epoch: 17 [29440/60000 (49%)]\tLoss: 105.257629\n",
      "Train Epoch: 17 [30720/60000 (51%)]\tLoss: 103.210197\n",
      "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 100.911102\n",
      "Train Epoch: 17 [33280/60000 (55%)]\tLoss: 100.606766\n",
      "Train Epoch: 17 [34560/60000 (58%)]\tLoss: 95.682838\n",
      "Train Epoch: 17 [35840/60000 (60%)]\tLoss: 99.629807\n",
      "Train Epoch: 17 [37120/60000 (62%)]\tLoss: 100.939781\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 97.757767\n",
      "Train Epoch: 17 [39680/60000 (66%)]\tLoss: 101.947235\n",
      "Train Epoch: 17 [40960/60000 (68%)]\tLoss: 105.538506\n",
      "Train Epoch: 17 [42240/60000 (70%)]\tLoss: 103.951569\n",
      "Train Epoch: 17 [43520/60000 (72%)]\tLoss: 100.484207\n",
      "Train Epoch: 17 [44800/60000 (75%)]\tLoss: 102.293884\n",
      "Train Epoch: 17 [46080/60000 (77%)]\tLoss: 105.319626\n",
      "Train Epoch: 17 [47360/60000 (79%)]\tLoss: 98.850700\n",
      "Train Epoch: 17 [48640/60000 (81%)]\tLoss: 96.385391\n",
      "Train Epoch: 17 [49920/60000 (83%)]\tLoss: 103.838425\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 102.516319\n",
      "Train Epoch: 17 [52480/60000 (87%)]\tLoss: 103.329857\n",
      "Train Epoch: 17 [53760/60000 (90%)]\tLoss: 105.325089\n",
      "Train Epoch: 17 [55040/60000 (92%)]\tLoss: 103.557236\n",
      "Train Epoch: 17 [56320/60000 (94%)]\tLoss: 101.752243\n",
      "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 101.773338\n",
      "Train Epoch: 17 [58880/60000 (98%)]\tLoss: 100.169319\n",
      "====> Epoch: 17 Average loss: 101.8560\n",
      "====> Test set loss: 102.1841\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 105.857506\n",
      "Train Epoch: 18 [1280/60000 (2%)]\tLoss: 105.143311\n",
      "Train Epoch: 18 [2560/60000 (4%)]\tLoss: 105.809868\n",
      "Train Epoch: 18 [3840/60000 (6%)]\tLoss: 99.843735\n",
      "Train Epoch: 18 [5120/60000 (9%)]\tLoss: 100.330612\n",
      "Train Epoch: 18 [6400/60000 (11%)]\tLoss: 98.989502\n",
      "Train Epoch: 18 [7680/60000 (13%)]\tLoss: 105.533081\n",
      "Train Epoch: 18 [8960/60000 (15%)]\tLoss: 102.162186\n",
      "Train Epoch: 18 [10240/60000 (17%)]\tLoss: 103.569427\n",
      "Train Epoch: 18 [11520/60000 (19%)]\tLoss: 102.366409\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 103.372345\n",
      "Train Epoch: 18 [14080/60000 (23%)]\tLoss: 100.393677\n",
      "Train Epoch: 18 [15360/60000 (26%)]\tLoss: 97.641220\n",
      "Train Epoch: 18 [16640/60000 (28%)]\tLoss: 101.853729\n",
      "Train Epoch: 18 [17920/60000 (30%)]\tLoss: 102.377151\n",
      "Train Epoch: 18 [19200/60000 (32%)]\tLoss: 103.067772\n",
      "Train Epoch: 18 [20480/60000 (34%)]\tLoss: 101.683914\n",
      "Train Epoch: 18 [21760/60000 (36%)]\tLoss: 96.788330\n",
      "Train Epoch: 18 [23040/60000 (38%)]\tLoss: 102.881165\n",
      "Train Epoch: 18 [24320/60000 (41%)]\tLoss: 107.686905\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 100.976212\n",
      "Train Epoch: 18 [26880/60000 (45%)]\tLoss: 102.026711\n",
      "Train Epoch: 18 [28160/60000 (47%)]\tLoss: 100.097427\n",
      "Train Epoch: 18 [29440/60000 (49%)]\tLoss: 106.808990\n",
      "Train Epoch: 18 [30720/60000 (51%)]\tLoss: 98.711090\n",
      "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 101.000290\n",
      "Train Epoch: 18 [33280/60000 (55%)]\tLoss: 99.168030\n",
      "Train Epoch: 18 [34560/60000 (58%)]\tLoss: 100.708908\n",
      "Train Epoch: 18 [35840/60000 (60%)]\tLoss: 104.443710\n",
      "Train Epoch: 18 [37120/60000 (62%)]\tLoss: 100.874702\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 95.344437\n",
      "Train Epoch: 18 [39680/60000 (66%)]\tLoss: 101.273819\n",
      "Train Epoch: 18 [40960/60000 (68%)]\tLoss: 104.005272\n",
      "Train Epoch: 18 [42240/60000 (70%)]\tLoss: 102.356110\n",
      "Train Epoch: 18 [43520/60000 (72%)]\tLoss: 103.029861\n",
      "Train Epoch: 18 [44800/60000 (75%)]\tLoss: 104.014229\n",
      "Train Epoch: 18 [46080/60000 (77%)]\tLoss: 104.180771\n",
      "Train Epoch: 18 [47360/60000 (79%)]\tLoss: 103.773239\n",
      "Train Epoch: 18 [48640/60000 (81%)]\tLoss: 101.492081\n",
      "Train Epoch: 18 [49920/60000 (83%)]\tLoss: 101.268875\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 101.766838\n",
      "Train Epoch: 18 [52480/60000 (87%)]\tLoss: 101.107330\n",
      "Train Epoch: 18 [53760/60000 (90%)]\tLoss: 99.866096\n",
      "Train Epoch: 18 [55040/60000 (92%)]\tLoss: 102.404152\n",
      "Train Epoch: 18 [56320/60000 (94%)]\tLoss: 103.885376\n",
      "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 102.111595\n",
      "Train Epoch: 18 [58880/60000 (98%)]\tLoss: 105.323784\n",
      "====> Epoch: 18 Average loss: 101.6130\n",
      "====> Test set loss: 102.0249\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 99.163857\n",
      "Train Epoch: 19 [1280/60000 (2%)]\tLoss: 103.181709\n",
      "Train Epoch: 19 [2560/60000 (4%)]\tLoss: 101.375259\n",
      "Train Epoch: 19 [3840/60000 (6%)]\tLoss: 97.231308\n",
      "Train Epoch: 19 [5120/60000 (9%)]\tLoss: 104.568619\n",
      "Train Epoch: 19 [6400/60000 (11%)]\tLoss: 103.585167\n",
      "Train Epoch: 19 [7680/60000 (13%)]\tLoss: 103.606583\n",
      "Train Epoch: 19 [8960/60000 (15%)]\tLoss: 102.431137\n",
      "Train Epoch: 19 [10240/60000 (17%)]\tLoss: 102.121056\n",
      "Train Epoch: 19 [11520/60000 (19%)]\tLoss: 101.286133\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 103.878014\n",
      "Train Epoch: 19 [14080/60000 (23%)]\tLoss: 103.127625\n",
      "Train Epoch: 19 [15360/60000 (26%)]\tLoss: 99.446701\n",
      "Train Epoch: 19 [16640/60000 (28%)]\tLoss: 98.491486\n",
      "Train Epoch: 19 [17920/60000 (30%)]\tLoss: 101.232773\n",
      "Train Epoch: 19 [19200/60000 (32%)]\tLoss: 107.489029\n",
      "Train Epoch: 19 [20480/60000 (34%)]\tLoss: 104.735931\n",
      "Train Epoch: 19 [21760/60000 (36%)]\tLoss: 96.474884\n",
      "Train Epoch: 19 [23040/60000 (38%)]\tLoss: 101.097473\n",
      "Train Epoch: 19 [24320/60000 (41%)]\tLoss: 99.870880\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 100.738174\n",
      "Train Epoch: 19 [26880/60000 (45%)]\tLoss: 97.420868\n",
      "Train Epoch: 19 [28160/60000 (47%)]\tLoss: 102.455070\n",
      "Train Epoch: 19 [29440/60000 (49%)]\tLoss: 100.999466\n",
      "Train Epoch: 19 [30720/60000 (51%)]\tLoss: 103.833817\n",
      "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 104.986061\n",
      "Train Epoch: 19 [33280/60000 (55%)]\tLoss: 106.093170\n",
      "Train Epoch: 19 [34560/60000 (58%)]\tLoss: 96.551781\n",
      "Train Epoch: 19 [35840/60000 (60%)]\tLoss: 98.489571\n",
      "Train Epoch: 19 [37120/60000 (62%)]\tLoss: 101.492950\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 104.294525\n",
      "Train Epoch: 19 [39680/60000 (66%)]\tLoss: 102.603546\n",
      "Train Epoch: 19 [40960/60000 (68%)]\tLoss: 104.770622\n",
      "Train Epoch: 19 [42240/60000 (70%)]\tLoss: 99.052361\n",
      "Train Epoch: 19 [43520/60000 (72%)]\tLoss: 102.248688\n",
      "Train Epoch: 19 [44800/60000 (75%)]\tLoss: 97.352722\n",
      "Train Epoch: 19 [46080/60000 (77%)]\tLoss: 102.722198\n",
      "Train Epoch: 19 [47360/60000 (79%)]\tLoss: 102.413551\n",
      "Train Epoch: 19 [48640/60000 (81%)]\tLoss: 100.307594\n",
      "Train Epoch: 19 [49920/60000 (83%)]\tLoss: 100.676781\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 100.885925\n",
      "Train Epoch: 19 [52480/60000 (87%)]\tLoss: 99.354790\n",
      "Train Epoch: 19 [53760/60000 (90%)]\tLoss: 100.764793\n",
      "Train Epoch: 19 [55040/60000 (92%)]\tLoss: 108.466309\n",
      "Train Epoch: 19 [56320/60000 (94%)]\tLoss: 104.001793\n",
      "Train Epoch: 19 [57600/60000 (96%)]\tLoss: 103.142548\n",
      "Train Epoch: 19 [58880/60000 (98%)]\tLoss: 98.676125\n",
      "====> Epoch: 19 Average loss: 101.3736\n",
      "====> Test set loss: 101.8819\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 104.081757\n",
      "Train Epoch: 20 [1280/60000 (2%)]\tLoss: 102.084114\n",
      "Train Epoch: 20 [2560/60000 (4%)]\tLoss: 98.214844\n",
      "Train Epoch: 20 [3840/60000 (6%)]\tLoss: 103.111557\n",
      "Train Epoch: 20 [5120/60000 (9%)]\tLoss: 102.187630\n",
      "Train Epoch: 20 [6400/60000 (11%)]\tLoss: 99.901596\n",
      "Train Epoch: 20 [7680/60000 (13%)]\tLoss: 99.199051\n",
      "Train Epoch: 20 [8960/60000 (15%)]\tLoss: 101.013329\n",
      "Train Epoch: 20 [10240/60000 (17%)]\tLoss: 98.824837\n",
      "Train Epoch: 20 [11520/60000 (19%)]\tLoss: 100.705887\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 105.397766\n",
      "Train Epoch: 20 [14080/60000 (23%)]\tLoss: 98.200851\n",
      "Train Epoch: 20 [15360/60000 (26%)]\tLoss: 94.455109\n",
      "Train Epoch: 20 [16640/60000 (28%)]\tLoss: 105.485466\n",
      "Train Epoch: 20 [17920/60000 (30%)]\tLoss: 97.246902\n",
      "Train Epoch: 20 [19200/60000 (32%)]\tLoss: 103.289047\n",
      "Train Epoch: 20 [20480/60000 (34%)]\tLoss: 104.446358\n",
      "Train Epoch: 20 [21760/60000 (36%)]\tLoss: 101.239273\n",
      "Train Epoch: 20 [23040/60000 (38%)]\tLoss: 102.661255\n",
      "Train Epoch: 20 [24320/60000 (41%)]\tLoss: 100.141624\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 103.244827\n",
      "Train Epoch: 20 [26880/60000 (45%)]\tLoss: 101.963974\n",
      "Train Epoch: 20 [28160/60000 (47%)]\tLoss: 102.168465\n",
      "Train Epoch: 20 [29440/60000 (49%)]\tLoss: 99.592041\n",
      "Train Epoch: 20 [30720/60000 (51%)]\tLoss: 100.213730\n",
      "Train Epoch: 20 [32000/60000 (53%)]\tLoss: 103.211884\n",
      "Train Epoch: 20 [33280/60000 (55%)]\tLoss: 102.929146\n",
      "Train Epoch: 20 [34560/60000 (58%)]\tLoss: 104.586281\n",
      "Train Epoch: 20 [35840/60000 (60%)]\tLoss: 97.162323\n",
      "Train Epoch: 20 [37120/60000 (62%)]\tLoss: 101.993141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 99.791328\n",
      "Train Epoch: 20 [39680/60000 (66%)]\tLoss: 104.024475\n",
      "Train Epoch: 20 [40960/60000 (68%)]\tLoss: 100.856339\n",
      "Train Epoch: 20 [42240/60000 (70%)]\tLoss: 103.891754\n",
      "Train Epoch: 20 [43520/60000 (72%)]\tLoss: 101.775421\n",
      "Train Epoch: 20 [44800/60000 (75%)]\tLoss: 100.167763\n",
      "Train Epoch: 20 [46080/60000 (77%)]\tLoss: 100.453621\n",
      "Train Epoch: 20 [47360/60000 (79%)]\tLoss: 104.081810\n",
      "Train Epoch: 20 [48640/60000 (81%)]\tLoss: 100.902054\n",
      "Train Epoch: 20 [49920/60000 (83%)]\tLoss: 100.037689\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 100.946304\n",
      "Train Epoch: 20 [52480/60000 (87%)]\tLoss: 105.030930\n",
      "Train Epoch: 20 [53760/60000 (90%)]\tLoss: 100.335098\n",
      "Train Epoch: 20 [55040/60000 (92%)]\tLoss: 104.782837\n",
      "Train Epoch: 20 [56320/60000 (94%)]\tLoss: 97.967888\n",
      "Train Epoch: 20 [57600/60000 (96%)]\tLoss: 101.099762\n",
      "Train Epoch: 20 [58880/60000 (98%)]\tLoss: 100.784073\n",
      "====> Epoch: 20 Average loss: 101.1383\n",
      "====> Test set loss: 101.9012\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 101.441513\n",
      "Train Epoch: 21 [1280/60000 (2%)]\tLoss: 96.950905\n",
      "Train Epoch: 21 [2560/60000 (4%)]\tLoss: 103.648788\n",
      "Train Epoch: 21 [3840/60000 (6%)]\tLoss: 103.703430\n",
      "Train Epoch: 21 [5120/60000 (9%)]\tLoss: 102.272072\n",
      "Train Epoch: 21 [6400/60000 (11%)]\tLoss: 97.492050\n",
      "Train Epoch: 21 [7680/60000 (13%)]\tLoss: 99.513329\n",
      "Train Epoch: 21 [8960/60000 (15%)]\tLoss: 103.261467\n",
      "Train Epoch: 21 [10240/60000 (17%)]\tLoss: 98.323387\n",
      "Train Epoch: 21 [11520/60000 (19%)]\tLoss: 102.433548\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 99.395493\n",
      "Train Epoch: 21 [14080/60000 (23%)]\tLoss: 99.863861\n",
      "Train Epoch: 21 [15360/60000 (26%)]\tLoss: 99.793045\n",
      "Train Epoch: 21 [16640/60000 (28%)]\tLoss: 102.523819\n",
      "Train Epoch: 21 [17920/60000 (30%)]\tLoss: 103.559036\n",
      "Train Epoch: 21 [19200/60000 (32%)]\tLoss: 97.980835\n",
      "Train Epoch: 21 [20480/60000 (34%)]\tLoss: 102.778587\n",
      "Train Epoch: 21 [21760/60000 (36%)]\tLoss: 101.595993\n",
      "Train Epoch: 21 [23040/60000 (38%)]\tLoss: 100.337677\n",
      "Train Epoch: 21 [24320/60000 (41%)]\tLoss: 100.214294\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 103.888107\n",
      "Train Epoch: 21 [26880/60000 (45%)]\tLoss: 100.550644\n",
      "Train Epoch: 21 [28160/60000 (47%)]\tLoss: 98.627655\n",
      "Train Epoch: 21 [29440/60000 (49%)]\tLoss: 101.975861\n",
      "Train Epoch: 21 [30720/60000 (51%)]\tLoss: 100.049423\n",
      "Train Epoch: 21 [32000/60000 (53%)]\tLoss: 101.097397\n",
      "Train Epoch: 21 [33280/60000 (55%)]\tLoss: 97.953392\n",
      "Train Epoch: 21 [34560/60000 (58%)]\tLoss: 97.441048\n",
      "Train Epoch: 21 [35840/60000 (60%)]\tLoss: 100.681290\n",
      "Train Epoch: 21 [37120/60000 (62%)]\tLoss: 95.906242\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 100.802994\n",
      "Train Epoch: 21 [39680/60000 (66%)]\tLoss: 103.804596\n",
      "Train Epoch: 21 [40960/60000 (68%)]\tLoss: 99.030495\n",
      "Train Epoch: 21 [42240/60000 (70%)]\tLoss: 99.563728\n",
      "Train Epoch: 21 [43520/60000 (72%)]\tLoss: 99.765198\n",
      "Train Epoch: 21 [44800/60000 (75%)]\tLoss: 102.458504\n",
      "Train Epoch: 21 [46080/60000 (77%)]\tLoss: 102.461143\n",
      "Train Epoch: 21 [47360/60000 (79%)]\tLoss: 104.053017\n",
      "Train Epoch: 21 [48640/60000 (81%)]\tLoss: 104.871063\n",
      "Train Epoch: 21 [49920/60000 (83%)]\tLoss: 100.054092\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 100.014160\n",
      "Train Epoch: 21 [52480/60000 (87%)]\tLoss: 100.771851\n",
      "Train Epoch: 21 [53760/60000 (90%)]\tLoss: 101.009369\n",
      "Train Epoch: 21 [55040/60000 (92%)]\tLoss: 101.876793\n",
      "Train Epoch: 21 [56320/60000 (94%)]\tLoss: 103.207733\n",
      "Train Epoch: 21 [57600/60000 (96%)]\tLoss: 101.879753\n",
      "Train Epoch: 21 [58880/60000 (98%)]\tLoss: 100.398933\n",
      "====> Epoch: 21 Average loss: 101.0019\n",
      "====> Test set loss: 101.4864\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 102.592529\n",
      "Train Epoch: 22 [1280/60000 (2%)]\tLoss: 105.391899\n",
      "Train Epoch: 22 [2560/60000 (4%)]\tLoss: 97.884415\n",
      "Train Epoch: 22 [3840/60000 (6%)]\tLoss: 100.831154\n",
      "Train Epoch: 22 [5120/60000 (9%)]\tLoss: 96.447166\n",
      "Train Epoch: 22 [6400/60000 (11%)]\tLoss: 101.850243\n",
      "Train Epoch: 22 [7680/60000 (13%)]\tLoss: 99.747177\n",
      "Train Epoch: 22 [8960/60000 (15%)]\tLoss: 98.276726\n",
      "Train Epoch: 22 [10240/60000 (17%)]\tLoss: 99.140274\n",
      "Train Epoch: 22 [11520/60000 (19%)]\tLoss: 100.991653\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 100.059189\n",
      "Train Epoch: 22 [14080/60000 (23%)]\tLoss: 98.556297\n",
      "Train Epoch: 22 [15360/60000 (26%)]\tLoss: 97.775360\n",
      "Train Epoch: 22 [16640/60000 (28%)]\tLoss: 101.032478\n",
      "Train Epoch: 22 [17920/60000 (30%)]\tLoss: 100.827179\n",
      "Train Epoch: 22 [19200/60000 (32%)]\tLoss: 102.565552\n",
      "Train Epoch: 22 [20480/60000 (34%)]\tLoss: 100.053970\n",
      "Train Epoch: 22 [21760/60000 (36%)]\tLoss: 102.889618\n",
      "Train Epoch: 22 [23040/60000 (38%)]\tLoss: 104.565117\n",
      "Train Epoch: 22 [24320/60000 (41%)]\tLoss: 99.461006\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 105.703941\n",
      "Train Epoch: 22 [26880/60000 (45%)]\tLoss: 103.670822\n",
      "Train Epoch: 22 [28160/60000 (47%)]\tLoss: 100.167656\n",
      "Train Epoch: 22 [29440/60000 (49%)]\tLoss: 98.100372\n",
      "Train Epoch: 22 [30720/60000 (51%)]\tLoss: 103.604446\n",
      "Train Epoch: 22 [32000/60000 (53%)]\tLoss: 100.118149\n",
      "Train Epoch: 22 [33280/60000 (55%)]\tLoss: 99.139030\n",
      "Train Epoch: 22 [34560/60000 (58%)]\tLoss: 97.854141\n",
      "Train Epoch: 22 [35840/60000 (60%)]\tLoss: 103.473930\n",
      "Train Epoch: 22 [37120/60000 (62%)]\tLoss: 103.949074\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 98.939499\n",
      "Train Epoch: 22 [39680/60000 (66%)]\tLoss: 101.660675\n",
      "Train Epoch: 22 [40960/60000 (68%)]\tLoss: 102.267426\n",
      "Train Epoch: 22 [42240/60000 (70%)]\tLoss: 100.156067\n",
      "Train Epoch: 22 [43520/60000 (72%)]\tLoss: 98.770660\n",
      "Train Epoch: 22 [44800/60000 (75%)]\tLoss: 100.785492\n",
      "Train Epoch: 22 [46080/60000 (77%)]\tLoss: 102.720146\n",
      "Train Epoch: 22 [47360/60000 (79%)]\tLoss: 100.518402\n",
      "Train Epoch: 22 [48640/60000 (81%)]\tLoss: 102.651482\n",
      "Train Epoch: 22 [49920/60000 (83%)]\tLoss: 96.310738\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 98.681961\n",
      "Train Epoch: 22 [52480/60000 (87%)]\tLoss: 105.729866\n",
      "Train Epoch: 22 [53760/60000 (90%)]\tLoss: 102.759689\n",
      "Train Epoch: 22 [55040/60000 (92%)]\tLoss: 97.717102\n",
      "Train Epoch: 22 [56320/60000 (94%)]\tLoss: 98.932785\n",
      "Train Epoch: 22 [57600/60000 (96%)]\tLoss: 96.811790\n",
      "Train Epoch: 22 [58880/60000 (98%)]\tLoss: 103.088745\n",
      "====> Epoch: 22 Average loss: 100.7807\n",
      "====> Test set loss: 101.4435\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 104.938698\n",
      "Train Epoch: 23 [1280/60000 (2%)]\tLoss: 100.066040\n",
      "Train Epoch: 23 [2560/60000 (4%)]\tLoss: 99.164711\n",
      "Train Epoch: 23 [3840/60000 (6%)]\tLoss: 101.974899\n",
      "Train Epoch: 23 [5120/60000 (9%)]\tLoss: 99.343758\n",
      "Train Epoch: 23 [6400/60000 (11%)]\tLoss: 101.800568\n",
      "Train Epoch: 23 [7680/60000 (13%)]\tLoss: 101.827438\n",
      "Train Epoch: 23 [8960/60000 (15%)]\tLoss: 103.701523\n",
      "Train Epoch: 23 [10240/60000 (17%)]\tLoss: 98.714340\n",
      "Train Epoch: 23 [11520/60000 (19%)]\tLoss: 102.835831\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 99.239273\n",
      "Train Epoch: 23 [14080/60000 (23%)]\tLoss: 97.916199\n",
      "Train Epoch: 23 [15360/60000 (26%)]\tLoss: 103.125084\n",
      "Train Epoch: 23 [16640/60000 (28%)]\tLoss: 103.209770\n",
      "Train Epoch: 23 [17920/60000 (30%)]\tLoss: 99.288040\n",
      "Train Epoch: 23 [19200/60000 (32%)]\tLoss: 98.478302\n",
      "Train Epoch: 23 [20480/60000 (34%)]\tLoss: 97.973312\n",
      "Train Epoch: 23 [21760/60000 (36%)]\tLoss: 98.542870\n",
      "Train Epoch: 23 [23040/60000 (38%)]\tLoss: 103.760574\n",
      "Train Epoch: 23 [24320/60000 (41%)]\tLoss: 101.485558\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 99.047348\n",
      "Train Epoch: 23 [26880/60000 (45%)]\tLoss: 98.769913\n",
      "Train Epoch: 23 [28160/60000 (47%)]\tLoss: 101.452538\n",
      "Train Epoch: 23 [29440/60000 (49%)]\tLoss: 98.655716\n",
      "Train Epoch: 23 [30720/60000 (51%)]\tLoss: 101.582283\n",
      "Train Epoch: 23 [32000/60000 (53%)]\tLoss: 100.503311\n",
      "Train Epoch: 23 [33280/60000 (55%)]\tLoss: 103.747177\n",
      "Train Epoch: 23 [34560/60000 (58%)]\tLoss: 102.104927\n",
      "Train Epoch: 23 [35840/60000 (60%)]\tLoss: 105.069946\n",
      "Train Epoch: 23 [37120/60000 (62%)]\tLoss: 101.813705\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 99.293747\n",
      "Train Epoch: 23 [39680/60000 (66%)]\tLoss: 100.467041\n",
      "Train Epoch: 23 [40960/60000 (68%)]\tLoss: 99.770439\n",
      "Train Epoch: 23 [42240/60000 (70%)]\tLoss: 99.838409\n",
      "Train Epoch: 23 [43520/60000 (72%)]\tLoss: 98.765846\n",
      "Train Epoch: 23 [44800/60000 (75%)]\tLoss: 102.152664\n",
      "Train Epoch: 23 [46080/60000 (77%)]\tLoss: 106.663788\n",
      "Train Epoch: 23 [47360/60000 (79%)]\tLoss: 99.442917\n",
      "Train Epoch: 23 [48640/60000 (81%)]\tLoss: 95.120880\n",
      "Train Epoch: 23 [49920/60000 (83%)]\tLoss: 101.477585\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 100.244240\n",
      "Train Epoch: 23 [52480/60000 (87%)]\tLoss: 100.695290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 23 [53760/60000 (90%)]\tLoss: 102.184883\n",
      "Train Epoch: 23 [55040/60000 (92%)]\tLoss: 99.870743\n",
      "Train Epoch: 23 [56320/60000 (94%)]\tLoss: 97.678543\n",
      "Train Epoch: 23 [57600/60000 (96%)]\tLoss: 99.264030\n",
      "Train Epoch: 23 [58880/60000 (98%)]\tLoss: 101.398148\n",
      "====> Epoch: 23 Average loss: 100.6046\n",
      "====> Test set loss: 101.2037\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 101.426392\n",
      "Train Epoch: 24 [1280/60000 (2%)]\tLoss: 95.103767\n",
      "Train Epoch: 24 [2560/60000 (4%)]\tLoss: 102.180817\n",
      "Train Epoch: 24 [3840/60000 (6%)]\tLoss: 97.856422\n",
      "Train Epoch: 24 [5120/60000 (9%)]\tLoss: 98.703735\n",
      "Train Epoch: 24 [6400/60000 (11%)]\tLoss: 100.000626\n",
      "Train Epoch: 24 [7680/60000 (13%)]\tLoss: 102.466942\n",
      "Train Epoch: 24 [8960/60000 (15%)]\tLoss: 101.273628\n",
      "Train Epoch: 24 [10240/60000 (17%)]\tLoss: 94.707672\n",
      "Train Epoch: 24 [11520/60000 (19%)]\tLoss: 98.448639\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 100.462006\n",
      "Train Epoch: 24 [14080/60000 (23%)]\tLoss: 100.114120\n",
      "Train Epoch: 24 [15360/60000 (26%)]\tLoss: 100.285355\n",
      "Train Epoch: 24 [16640/60000 (28%)]\tLoss: 100.690399\n",
      "Train Epoch: 24 [17920/60000 (30%)]\tLoss: 99.907173\n",
      "Train Epoch: 24 [19200/60000 (32%)]\tLoss: 99.212364\n",
      "Train Epoch: 24 [20480/60000 (34%)]\tLoss: 99.234924\n",
      "Train Epoch: 24 [21760/60000 (36%)]\tLoss: 101.562119\n",
      "Train Epoch: 24 [23040/60000 (38%)]\tLoss: 101.228745\n",
      "Train Epoch: 24 [24320/60000 (41%)]\tLoss: 97.287895\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 103.273041\n",
      "Train Epoch: 24 [26880/60000 (45%)]\tLoss: 98.810448\n",
      "Train Epoch: 24 [28160/60000 (47%)]\tLoss: 103.343079\n",
      "Train Epoch: 24 [29440/60000 (49%)]\tLoss: 98.920242\n",
      "Train Epoch: 24 [30720/60000 (51%)]\tLoss: 99.921844\n",
      "Train Epoch: 24 [32000/60000 (53%)]\tLoss: 100.296753\n",
      "Train Epoch: 24 [33280/60000 (55%)]\tLoss: 103.304382\n",
      "Train Epoch: 24 [34560/60000 (58%)]\tLoss: 102.557289\n",
      "Train Epoch: 24 [35840/60000 (60%)]\tLoss: 97.866676\n",
      "Train Epoch: 24 [37120/60000 (62%)]\tLoss: 96.230156\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 100.389000\n",
      "Train Epoch: 24 [39680/60000 (66%)]\tLoss: 100.406456\n",
      "Train Epoch: 24 [40960/60000 (68%)]\tLoss: 99.898132\n",
      "Train Epoch: 24 [42240/60000 (70%)]\tLoss: 102.086792\n",
      "Train Epoch: 24 [43520/60000 (72%)]\tLoss: 98.629013\n",
      "Train Epoch: 24 [44800/60000 (75%)]\tLoss: 101.972725\n",
      "Train Epoch: 24 [46080/60000 (77%)]\tLoss: 98.377693\n",
      "Train Epoch: 24 [47360/60000 (79%)]\tLoss: 97.505501\n",
      "Train Epoch: 24 [48640/60000 (81%)]\tLoss: 99.293579\n",
      "Train Epoch: 24 [49920/60000 (83%)]\tLoss: 97.838905\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 101.354874\n",
      "Train Epoch: 24 [52480/60000 (87%)]\tLoss: 101.394241\n",
      "Train Epoch: 24 [53760/60000 (90%)]\tLoss: 100.142418\n",
      "Train Epoch: 24 [55040/60000 (92%)]\tLoss: 101.292786\n",
      "Train Epoch: 24 [56320/60000 (94%)]\tLoss: 98.019165\n",
      "Train Epoch: 24 [57600/60000 (96%)]\tLoss: 103.337540\n",
      "Train Epoch: 24 [58880/60000 (98%)]\tLoss: 101.207809\n",
      "====> Epoch: 24 Average loss: 100.4606\n",
      "====> Test set loss: 101.1397\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 104.167976\n",
      "Train Epoch: 25 [1280/60000 (2%)]\tLoss: 100.869171\n",
      "Train Epoch: 25 [2560/60000 (4%)]\tLoss: 100.680405\n",
      "Train Epoch: 25 [3840/60000 (6%)]\tLoss: 103.636963\n",
      "Train Epoch: 25 [5120/60000 (9%)]\tLoss: 100.767059\n",
      "Train Epoch: 25 [6400/60000 (11%)]\tLoss: 99.887794\n",
      "Train Epoch: 25 [7680/60000 (13%)]\tLoss: 101.335159\n",
      "Train Epoch: 25 [8960/60000 (15%)]\tLoss: 100.370621\n",
      "Train Epoch: 25 [10240/60000 (17%)]\tLoss: 100.615059\n",
      "Train Epoch: 25 [11520/60000 (19%)]\tLoss: 96.579826\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 95.855110\n",
      "Train Epoch: 25 [14080/60000 (23%)]\tLoss: 98.503700\n",
      "Train Epoch: 25 [15360/60000 (26%)]\tLoss: 100.538071\n",
      "Train Epoch: 25 [16640/60000 (28%)]\tLoss: 101.969429\n",
      "Train Epoch: 25 [17920/60000 (30%)]\tLoss: 99.853569\n",
      "Train Epoch: 25 [19200/60000 (32%)]\tLoss: 97.935959\n",
      "Train Epoch: 25 [20480/60000 (34%)]\tLoss: 102.087753\n",
      "Train Epoch: 25 [21760/60000 (36%)]\tLoss: 103.080978\n",
      "Train Epoch: 25 [23040/60000 (38%)]\tLoss: 102.764420\n",
      "Train Epoch: 25 [24320/60000 (41%)]\tLoss: 97.070381\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 103.861076\n",
      "Train Epoch: 25 [26880/60000 (45%)]\tLoss: 98.084152\n",
      "Train Epoch: 25 [28160/60000 (47%)]\tLoss: 100.909241\n",
      "Train Epoch: 25 [29440/60000 (49%)]\tLoss: 103.452797\n",
      "Train Epoch: 25 [30720/60000 (51%)]\tLoss: 97.534248\n",
      "Train Epoch: 25 [32000/60000 (53%)]\tLoss: 99.121017\n",
      "Train Epoch: 25 [33280/60000 (55%)]\tLoss: 97.723991\n",
      "Train Epoch: 25 [34560/60000 (58%)]\tLoss: 101.155884\n",
      "Train Epoch: 25 [35840/60000 (60%)]\tLoss: 101.966507\n",
      "Train Epoch: 25 [37120/60000 (62%)]\tLoss: 102.913750\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 99.203247\n",
      "Train Epoch: 25 [39680/60000 (66%)]\tLoss: 97.621315\n",
      "Train Epoch: 25 [40960/60000 (68%)]\tLoss: 99.534485\n",
      "Train Epoch: 25 [42240/60000 (70%)]\tLoss: 104.700165\n",
      "Train Epoch: 25 [43520/60000 (72%)]\tLoss: 102.792374\n",
      "Train Epoch: 25 [44800/60000 (75%)]\tLoss: 103.902794\n",
      "Train Epoch: 25 [46080/60000 (77%)]\tLoss: 101.182648\n",
      "Train Epoch: 25 [47360/60000 (79%)]\tLoss: 98.273560\n",
      "Train Epoch: 25 [48640/60000 (81%)]\tLoss: 102.124214\n",
      "Train Epoch: 25 [49920/60000 (83%)]\tLoss: 99.075623\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 100.668472\n",
      "Train Epoch: 25 [52480/60000 (87%)]\tLoss: 102.264404\n",
      "Train Epoch: 25 [53760/60000 (90%)]\tLoss: 97.280212\n",
      "Train Epoch: 25 [55040/60000 (92%)]\tLoss: 100.517166\n",
      "Train Epoch: 25 [56320/60000 (94%)]\tLoss: 103.124367\n",
      "Train Epoch: 25 [57600/60000 (96%)]\tLoss: 104.483780\n",
      "Train Epoch: 25 [58880/60000 (98%)]\tLoss: 99.729538\n",
      "====> Epoch: 25 Average loss: 100.3149\n",
      "====> Test set loss: 101.1570\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 100.244057\n",
      "Train Epoch: 26 [1280/60000 (2%)]\tLoss: 102.082451\n",
      "Train Epoch: 26 [2560/60000 (4%)]\tLoss: 104.703453\n",
      "Train Epoch: 26 [3840/60000 (6%)]\tLoss: 101.073921\n",
      "Train Epoch: 26 [5120/60000 (9%)]\tLoss: 99.774200\n",
      "Train Epoch: 26 [6400/60000 (11%)]\tLoss: 97.428917\n",
      "Train Epoch: 26 [7680/60000 (13%)]\tLoss: 102.238380\n",
      "Train Epoch: 26 [8960/60000 (15%)]\tLoss: 99.084732\n",
      "Train Epoch: 26 [10240/60000 (17%)]\tLoss: 104.620346\n",
      "Train Epoch: 26 [11520/60000 (19%)]\tLoss: 100.423203\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 101.565453\n",
      "Train Epoch: 26 [14080/60000 (23%)]\tLoss: 97.459435\n",
      "Train Epoch: 26 [15360/60000 (26%)]\tLoss: 98.664017\n",
      "Train Epoch: 26 [16640/60000 (28%)]\tLoss: 102.670395\n",
      "Train Epoch: 26 [17920/60000 (30%)]\tLoss: 100.598755\n",
      "Train Epoch: 26 [19200/60000 (32%)]\tLoss: 98.555649\n",
      "Train Epoch: 26 [20480/60000 (34%)]\tLoss: 101.985054\n",
      "Train Epoch: 26 [21760/60000 (36%)]\tLoss: 97.309151\n",
      "Train Epoch: 26 [23040/60000 (38%)]\tLoss: 100.214478\n",
      "Train Epoch: 26 [24320/60000 (41%)]\tLoss: 99.933456\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 104.878754\n",
      "Train Epoch: 26 [26880/60000 (45%)]\tLoss: 97.686882\n",
      "Train Epoch: 26 [28160/60000 (47%)]\tLoss: 98.251450\n",
      "Train Epoch: 26 [29440/60000 (49%)]\tLoss: 100.179108\n",
      "Train Epoch: 26 [30720/60000 (51%)]\tLoss: 96.334366\n",
      "Train Epoch: 26 [32000/60000 (53%)]\tLoss: 101.281723\n",
      "Train Epoch: 26 [33280/60000 (55%)]\tLoss: 102.979874\n",
      "Train Epoch: 26 [34560/60000 (58%)]\tLoss: 97.629639\n",
      "Train Epoch: 26 [35840/60000 (60%)]\tLoss: 100.587524\n",
      "Train Epoch: 26 [37120/60000 (62%)]\tLoss: 101.775375\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 101.647774\n",
      "Train Epoch: 26 [39680/60000 (66%)]\tLoss: 102.224899\n",
      "Train Epoch: 26 [40960/60000 (68%)]\tLoss: 98.743271\n",
      "Train Epoch: 26 [42240/60000 (70%)]\tLoss: 100.305656\n",
      "Train Epoch: 26 [43520/60000 (72%)]\tLoss: 102.461884\n",
      "Train Epoch: 26 [44800/60000 (75%)]\tLoss: 96.101440\n",
      "Train Epoch: 26 [46080/60000 (77%)]\tLoss: 101.428574\n",
      "Train Epoch: 26 [47360/60000 (79%)]\tLoss: 101.082352\n",
      "Train Epoch: 26 [48640/60000 (81%)]\tLoss: 104.696823\n",
      "Train Epoch: 26 [49920/60000 (83%)]\tLoss: 98.236961\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 96.223938\n",
      "Train Epoch: 26 [52480/60000 (87%)]\tLoss: 102.606743\n",
      "Train Epoch: 26 [53760/60000 (90%)]\tLoss: 96.434509\n",
      "Train Epoch: 26 [55040/60000 (92%)]\tLoss: 99.707718\n",
      "Train Epoch: 26 [56320/60000 (94%)]\tLoss: 98.579369\n",
      "Train Epoch: 26 [57600/60000 (96%)]\tLoss: 102.751282\n",
      "Train Epoch: 26 [58880/60000 (98%)]\tLoss: 100.659271\n",
      "====> Epoch: 26 Average loss: 100.1753\n",
      "====> Test set loss: 101.2160\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 101.203735\n",
      "Train Epoch: 27 [1280/60000 (2%)]\tLoss: 100.089478\n",
      "Train Epoch: 27 [2560/60000 (4%)]\tLoss: 100.707695\n",
      "Train Epoch: 27 [3840/60000 (6%)]\tLoss: 100.074554\n",
      "Train Epoch: 27 [5120/60000 (9%)]\tLoss: 100.739052\n",
      "Train Epoch: 27 [6400/60000 (11%)]\tLoss: 101.375412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 27 [7680/60000 (13%)]\tLoss: 101.683411\n",
      "Train Epoch: 27 [8960/60000 (15%)]\tLoss: 99.406044\n",
      "Train Epoch: 27 [10240/60000 (17%)]\tLoss: 99.816238\n",
      "Train Epoch: 27 [11520/60000 (19%)]\tLoss: 100.871521\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 100.359024\n",
      "Train Epoch: 27 [14080/60000 (23%)]\tLoss: 101.571304\n",
      "Train Epoch: 27 [15360/60000 (26%)]\tLoss: 101.349861\n",
      "Train Epoch: 27 [16640/60000 (28%)]\tLoss: 98.981857\n",
      "Train Epoch: 27 [17920/60000 (30%)]\tLoss: 100.666321\n",
      "Train Epoch: 27 [19200/60000 (32%)]\tLoss: 99.542419\n",
      "Train Epoch: 27 [20480/60000 (34%)]\tLoss: 101.213806\n",
      "Train Epoch: 27 [21760/60000 (36%)]\tLoss: 104.025169\n",
      "Train Epoch: 27 [23040/60000 (38%)]\tLoss: 98.234268\n",
      "Train Epoch: 27 [24320/60000 (41%)]\tLoss: 100.256310\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 100.310745\n",
      "Train Epoch: 27 [26880/60000 (45%)]\tLoss: 104.743866\n",
      "Train Epoch: 27 [28160/60000 (47%)]\tLoss: 104.579956\n",
      "Train Epoch: 27 [29440/60000 (49%)]\tLoss: 98.241028\n",
      "Train Epoch: 27 [30720/60000 (51%)]\tLoss: 95.089310\n",
      "Train Epoch: 27 [32000/60000 (53%)]\tLoss: 101.442314\n",
      "Train Epoch: 27 [33280/60000 (55%)]\tLoss: 98.898582\n",
      "Train Epoch: 27 [34560/60000 (58%)]\tLoss: 102.198181\n",
      "Train Epoch: 27 [35840/60000 (60%)]\tLoss: 98.824463\n",
      "Train Epoch: 27 [37120/60000 (62%)]\tLoss: 101.891296\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 97.538620\n",
      "Train Epoch: 27 [39680/60000 (66%)]\tLoss: 100.703789\n",
      "Train Epoch: 27 [40960/60000 (68%)]\tLoss: 97.712563\n",
      "Train Epoch: 27 [42240/60000 (70%)]\tLoss: 97.725899\n",
      "Train Epoch: 27 [43520/60000 (72%)]\tLoss: 96.888794\n",
      "Train Epoch: 27 [44800/60000 (75%)]\tLoss: 98.610481\n",
      "Train Epoch: 27 [46080/60000 (77%)]\tLoss: 101.534081\n",
      "Train Epoch: 27 [47360/60000 (79%)]\tLoss: 101.884155\n",
      "Train Epoch: 27 [48640/60000 (81%)]\tLoss: 101.122482\n",
      "Train Epoch: 27 [49920/60000 (83%)]\tLoss: 100.139565\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 99.837303\n",
      "Train Epoch: 27 [52480/60000 (87%)]\tLoss: 98.348373\n",
      "Train Epoch: 27 [53760/60000 (90%)]\tLoss: 104.453468\n",
      "Train Epoch: 27 [55040/60000 (92%)]\tLoss: 100.435982\n",
      "Train Epoch: 27 [56320/60000 (94%)]\tLoss: 99.787933\n",
      "Train Epoch: 27 [57600/60000 (96%)]\tLoss: 99.579208\n",
      "Train Epoch: 27 [58880/60000 (98%)]\tLoss: 101.040604\n",
      "====> Epoch: 27 Average loss: 100.0626\n",
      "====> Test set loss: 100.9570\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 101.534447\n",
      "Train Epoch: 28 [1280/60000 (2%)]\tLoss: 98.962738\n",
      "Train Epoch: 28 [2560/60000 (4%)]\tLoss: 100.043152\n",
      "Train Epoch: 28 [3840/60000 (6%)]\tLoss: 99.148376\n",
      "Train Epoch: 28 [5120/60000 (9%)]\tLoss: 99.062012\n",
      "Train Epoch: 28 [6400/60000 (11%)]\tLoss: 101.484543\n",
      "Train Epoch: 28 [7680/60000 (13%)]\tLoss: 100.129227\n",
      "Train Epoch: 28 [8960/60000 (15%)]\tLoss: 103.334290\n",
      "Train Epoch: 28 [10240/60000 (17%)]\tLoss: 102.319778\n",
      "Train Epoch: 28 [11520/60000 (19%)]\tLoss: 101.668968\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 99.973579\n",
      "Train Epoch: 28 [14080/60000 (23%)]\tLoss: 99.120911\n",
      "Train Epoch: 28 [15360/60000 (26%)]\tLoss: 98.400421\n",
      "Train Epoch: 28 [16640/60000 (28%)]\tLoss: 99.398308\n",
      "Train Epoch: 28 [17920/60000 (30%)]\tLoss: 101.748062\n",
      "Train Epoch: 28 [19200/60000 (32%)]\tLoss: 97.183624\n",
      "Train Epoch: 28 [20480/60000 (34%)]\tLoss: 97.746414\n",
      "Train Epoch: 28 [21760/60000 (36%)]\tLoss: 99.279892\n",
      "Train Epoch: 28 [23040/60000 (38%)]\tLoss: 108.778610\n",
      "Train Epoch: 28 [24320/60000 (41%)]\tLoss: 97.932793\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 97.957542\n",
      "Train Epoch: 28 [26880/60000 (45%)]\tLoss: 100.887764\n",
      "Train Epoch: 28 [28160/60000 (47%)]\tLoss: 99.731834\n",
      "Train Epoch: 28 [29440/60000 (49%)]\tLoss: 98.379807\n",
      "Train Epoch: 28 [30720/60000 (51%)]\tLoss: 104.079750\n",
      "Train Epoch: 28 [32000/60000 (53%)]\tLoss: 100.110336\n",
      "Train Epoch: 28 [33280/60000 (55%)]\tLoss: 104.047417\n",
      "Train Epoch: 28 [34560/60000 (58%)]\tLoss: 100.872215\n",
      "Train Epoch: 28 [35840/60000 (60%)]\tLoss: 103.544586\n",
      "Train Epoch: 28 [37120/60000 (62%)]\tLoss: 98.890312\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 100.988152\n",
      "Train Epoch: 28 [39680/60000 (66%)]\tLoss: 102.127907\n",
      "Train Epoch: 28 [40960/60000 (68%)]\tLoss: 98.963440\n",
      "Train Epoch: 28 [42240/60000 (70%)]\tLoss: 101.073624\n",
      "Train Epoch: 28 [43520/60000 (72%)]\tLoss: 98.809746\n",
      "Train Epoch: 28 [44800/60000 (75%)]\tLoss: 104.169800\n",
      "Train Epoch: 28 [46080/60000 (77%)]\tLoss: 99.866577\n",
      "Train Epoch: 28 [47360/60000 (79%)]\tLoss: 99.849800\n",
      "Train Epoch: 28 [48640/60000 (81%)]\tLoss: 96.402382\n",
      "Train Epoch: 28 [49920/60000 (83%)]\tLoss: 101.214874\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 100.798637\n",
      "Train Epoch: 28 [52480/60000 (87%)]\tLoss: 98.640381\n",
      "Train Epoch: 28 [53760/60000 (90%)]\tLoss: 100.586433\n",
      "Train Epoch: 28 [55040/60000 (92%)]\tLoss: 101.297897\n",
      "Train Epoch: 28 [56320/60000 (94%)]\tLoss: 97.115921\n",
      "Train Epoch: 28 [57600/60000 (96%)]\tLoss: 104.596390\n",
      "Train Epoch: 28 [58880/60000 (98%)]\tLoss: 95.951370\n",
      "====> Epoch: 28 Average loss: 99.9350\n",
      "====> Test set loss: 100.8416\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 99.093918\n",
      "Train Epoch: 29 [1280/60000 (2%)]\tLoss: 101.486061\n",
      "Train Epoch: 29 [2560/60000 (4%)]\tLoss: 101.976051\n",
      "Train Epoch: 29 [3840/60000 (6%)]\tLoss: 99.560783\n",
      "Train Epoch: 29 [5120/60000 (9%)]\tLoss: 100.714523\n",
      "Train Epoch: 29 [6400/60000 (11%)]\tLoss: 99.606010\n",
      "Train Epoch: 29 [7680/60000 (13%)]\tLoss: 98.554031\n",
      "Train Epoch: 29 [8960/60000 (15%)]\tLoss: 97.239380\n",
      "Train Epoch: 29 [10240/60000 (17%)]\tLoss: 104.538254\n",
      "Train Epoch: 29 [11520/60000 (19%)]\tLoss: 101.331467\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 96.038567\n",
      "Train Epoch: 29 [14080/60000 (23%)]\tLoss: 98.115387\n",
      "Train Epoch: 29 [15360/60000 (26%)]\tLoss: 97.334839\n",
      "Train Epoch: 29 [16640/60000 (28%)]\tLoss: 99.122055\n",
      "Train Epoch: 29 [17920/60000 (30%)]\tLoss: 103.285065\n",
      "Train Epoch: 29 [19200/60000 (32%)]\tLoss: 99.886719\n",
      "Train Epoch: 29 [20480/60000 (34%)]\tLoss: 103.318604\n",
      "Train Epoch: 29 [21760/60000 (36%)]\tLoss: 102.831619\n",
      "Train Epoch: 29 [23040/60000 (38%)]\tLoss: 98.149292\n",
      "Train Epoch: 29 [24320/60000 (41%)]\tLoss: 96.042023\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 102.365753\n",
      "Train Epoch: 29 [26880/60000 (45%)]\tLoss: 99.841446\n",
      "Train Epoch: 29 [28160/60000 (47%)]\tLoss: 101.113937\n",
      "Train Epoch: 29 [29440/60000 (49%)]\tLoss: 98.148605\n",
      "Train Epoch: 29 [30720/60000 (51%)]\tLoss: 98.003250\n",
      "Train Epoch: 29 [32000/60000 (53%)]\tLoss: 98.116669\n",
      "Train Epoch: 29 [33280/60000 (55%)]\tLoss: 97.601234\n",
      "Train Epoch: 29 [34560/60000 (58%)]\tLoss: 99.124252\n",
      "Train Epoch: 29 [35840/60000 (60%)]\tLoss: 98.634247\n",
      "Train Epoch: 29 [37120/60000 (62%)]\tLoss: 104.023369\n",
      "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 100.369537\n",
      "Train Epoch: 29 [39680/60000 (66%)]\tLoss: 100.873550\n",
      "Train Epoch: 29 [40960/60000 (68%)]\tLoss: 99.526047\n",
      "Train Epoch: 29 [42240/60000 (70%)]\tLoss: 100.758461\n",
      "Train Epoch: 29 [43520/60000 (72%)]\tLoss: 96.046555\n",
      "Train Epoch: 29 [44800/60000 (75%)]\tLoss: 103.910896\n",
      "Train Epoch: 29 [46080/60000 (77%)]\tLoss: 101.728958\n",
      "Train Epoch: 29 [47360/60000 (79%)]\tLoss: 100.690521\n",
      "Train Epoch: 29 [48640/60000 (81%)]\tLoss: 102.465523\n",
      "Train Epoch: 29 [49920/60000 (83%)]\tLoss: 98.771088\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 99.333794\n",
      "Train Epoch: 29 [52480/60000 (87%)]\tLoss: 96.933037\n",
      "Train Epoch: 29 [53760/60000 (90%)]\tLoss: 100.500381\n",
      "Train Epoch: 29 [55040/60000 (92%)]\tLoss: 100.394791\n",
      "Train Epoch: 29 [56320/60000 (94%)]\tLoss: 97.204506\n",
      "Train Epoch: 29 [57600/60000 (96%)]\tLoss: 100.867119\n",
      "Train Epoch: 29 [58880/60000 (98%)]\tLoss: 98.278717\n",
      "====> Epoch: 29 Average loss: 99.8147\n",
      "====> Test set loss: 100.7697\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 100.680557\n",
      "Train Epoch: 30 [1280/60000 (2%)]\tLoss: 97.673660\n",
      "Train Epoch: 30 [2560/60000 (4%)]\tLoss: 99.254715\n",
      "Train Epoch: 30 [3840/60000 (6%)]\tLoss: 95.540970\n",
      "Train Epoch: 30 [5120/60000 (9%)]\tLoss: 100.716423\n",
      "Train Epoch: 30 [6400/60000 (11%)]\tLoss: 102.588043\n",
      "Train Epoch: 30 [7680/60000 (13%)]\tLoss: 95.789124\n",
      "Train Epoch: 30 [8960/60000 (15%)]\tLoss: 101.680771\n",
      "Train Epoch: 30 [10240/60000 (17%)]\tLoss: 98.160583\n",
      "Train Epoch: 30 [11520/60000 (19%)]\tLoss: 100.512604\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 100.092384\n",
      "Train Epoch: 30 [14080/60000 (23%)]\tLoss: 95.831100\n",
      "Train Epoch: 30 [15360/60000 (26%)]\tLoss: 102.505219\n",
      "Train Epoch: 30 [16640/60000 (28%)]\tLoss: 100.555405\n",
      "Train Epoch: 30 [17920/60000 (30%)]\tLoss: 99.126129\n",
      "Train Epoch: 30 [19200/60000 (32%)]\tLoss: 98.592896\n",
      "Train Epoch: 30 [20480/60000 (34%)]\tLoss: 101.907356\n",
      "Train Epoch: 30 [21760/60000 (36%)]\tLoss: 100.979797\n",
      "Train Epoch: 30 [23040/60000 (38%)]\tLoss: 99.509735\n",
      "Train Epoch: 30 [24320/60000 (41%)]\tLoss: 99.422768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 100.947029\n",
      "Train Epoch: 30 [26880/60000 (45%)]\tLoss: 95.544510\n",
      "Train Epoch: 30 [28160/60000 (47%)]\tLoss: 104.043381\n",
      "Train Epoch: 30 [29440/60000 (49%)]\tLoss: 102.052383\n",
      "Train Epoch: 30 [30720/60000 (51%)]\tLoss: 103.528290\n",
      "Train Epoch: 30 [32000/60000 (53%)]\tLoss: 103.635391\n",
      "Train Epoch: 30 [33280/60000 (55%)]\tLoss: 103.881989\n",
      "Train Epoch: 30 [34560/60000 (58%)]\tLoss: 100.112091\n",
      "Train Epoch: 30 [35840/60000 (60%)]\tLoss: 99.635780\n",
      "Train Epoch: 30 [37120/60000 (62%)]\tLoss: 96.436562\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 98.365685\n",
      "Train Epoch: 30 [39680/60000 (66%)]\tLoss: 93.661713\n",
      "Train Epoch: 30 [40960/60000 (68%)]\tLoss: 98.758858\n",
      "Train Epoch: 30 [42240/60000 (70%)]\tLoss: 101.520561\n",
      "Train Epoch: 30 [43520/60000 (72%)]\tLoss: 102.266663\n",
      "Train Epoch: 30 [44800/60000 (75%)]\tLoss: 99.501343\n",
      "Train Epoch: 30 [46080/60000 (77%)]\tLoss: 98.634689\n",
      "Train Epoch: 30 [47360/60000 (79%)]\tLoss: 97.860947\n",
      "Train Epoch: 30 [48640/60000 (81%)]\tLoss: 98.806564\n",
      "Train Epoch: 30 [49920/60000 (83%)]\tLoss: 96.064835\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 99.792892\n",
      "Train Epoch: 30 [52480/60000 (87%)]\tLoss: 103.508362\n",
      "Train Epoch: 30 [53760/60000 (90%)]\tLoss: 101.960487\n",
      "Train Epoch: 30 [55040/60000 (92%)]\tLoss: 99.272659\n",
      "Train Epoch: 30 [56320/60000 (94%)]\tLoss: 97.628922\n",
      "Train Epoch: 30 [57600/60000 (96%)]\tLoss: 104.312500\n",
      "Train Epoch: 30 [58880/60000 (98%)]\tLoss: 97.651367\n",
      "====> Epoch: 30 Average loss: 99.7131\n",
      "====> Test set loss: 100.7968\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 101.751022\n",
      "Train Epoch: 31 [1280/60000 (2%)]\tLoss: 95.518578\n",
      "Train Epoch: 31 [2560/60000 (4%)]\tLoss: 102.778961\n",
      "Train Epoch: 31 [3840/60000 (6%)]\tLoss: 98.862091\n",
      "Train Epoch: 31 [5120/60000 (9%)]\tLoss: 99.899567\n",
      "Train Epoch: 31 [6400/60000 (11%)]\tLoss: 99.299301\n",
      "Train Epoch: 31 [7680/60000 (13%)]\tLoss: 97.563507\n",
      "Train Epoch: 31 [8960/60000 (15%)]\tLoss: 102.610397\n",
      "Train Epoch: 31 [10240/60000 (17%)]\tLoss: 100.662888\n",
      "Train Epoch: 31 [11520/60000 (19%)]\tLoss: 96.489182\n",
      "Train Epoch: 31 [12800/60000 (21%)]\tLoss: 97.053490\n",
      "Train Epoch: 31 [14080/60000 (23%)]\tLoss: 99.877800\n",
      "Train Epoch: 31 [15360/60000 (26%)]\tLoss: 101.201523\n",
      "Train Epoch: 31 [16640/60000 (28%)]\tLoss: 101.778778\n",
      "Train Epoch: 31 [17920/60000 (30%)]\tLoss: 100.812866\n",
      "Train Epoch: 31 [19200/60000 (32%)]\tLoss: 100.434998\n",
      "Train Epoch: 31 [20480/60000 (34%)]\tLoss: 94.720062\n",
      "Train Epoch: 31 [21760/60000 (36%)]\tLoss: 95.171013\n",
      "Train Epoch: 31 [23040/60000 (38%)]\tLoss: 101.598602\n",
      "Train Epoch: 31 [24320/60000 (41%)]\tLoss: 101.512039\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 98.306129\n",
      "Train Epoch: 31 [26880/60000 (45%)]\tLoss: 100.940277\n",
      "Train Epoch: 31 [28160/60000 (47%)]\tLoss: 103.775940\n",
      "Train Epoch: 31 [29440/60000 (49%)]\tLoss: 96.758888\n",
      "Train Epoch: 31 [30720/60000 (51%)]\tLoss: 97.387177\n",
      "Train Epoch: 31 [32000/60000 (53%)]\tLoss: 103.786804\n",
      "Train Epoch: 31 [33280/60000 (55%)]\tLoss: 100.361641\n",
      "Train Epoch: 31 [34560/60000 (58%)]\tLoss: 101.363823\n",
      "Train Epoch: 31 [35840/60000 (60%)]\tLoss: 100.278641\n",
      "Train Epoch: 31 [37120/60000 (62%)]\tLoss: 102.260193\n",
      "Train Epoch: 31 [38400/60000 (64%)]\tLoss: 100.472633\n",
      "Train Epoch: 31 [39680/60000 (66%)]\tLoss: 98.141945\n",
      "Train Epoch: 31 [40960/60000 (68%)]\tLoss: 102.336884\n",
      "Train Epoch: 31 [42240/60000 (70%)]\tLoss: 99.267609\n",
      "Train Epoch: 31 [43520/60000 (72%)]\tLoss: 106.425110\n",
      "Train Epoch: 31 [44800/60000 (75%)]\tLoss: 99.855103\n",
      "Train Epoch: 31 [46080/60000 (77%)]\tLoss: 103.534454\n",
      "Train Epoch: 31 [47360/60000 (79%)]\tLoss: 99.701874\n",
      "Train Epoch: 31 [48640/60000 (81%)]\tLoss: 102.075981\n",
      "Train Epoch: 31 [49920/60000 (83%)]\tLoss: 98.712006\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 100.573227\n",
      "Train Epoch: 31 [52480/60000 (87%)]\tLoss: 96.022217\n",
      "Train Epoch: 31 [53760/60000 (90%)]\tLoss: 97.894814\n",
      "Train Epoch: 31 [55040/60000 (92%)]\tLoss: 101.717072\n",
      "Train Epoch: 31 [56320/60000 (94%)]\tLoss: 101.091003\n",
      "Train Epoch: 31 [57600/60000 (96%)]\tLoss: 97.260712\n",
      "Train Epoch: 31 [58880/60000 (98%)]\tLoss: 94.333389\n",
      "====> Epoch: 31 Average loss: 99.6359\n",
      "====> Test set loss: 100.8541\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 99.179550\n",
      "Train Epoch: 32 [1280/60000 (2%)]\tLoss: 103.455528\n",
      "Train Epoch: 32 [2560/60000 (4%)]\tLoss: 97.554764\n",
      "Train Epoch: 32 [3840/60000 (6%)]\tLoss: 100.817497\n",
      "Train Epoch: 32 [5120/60000 (9%)]\tLoss: 97.290390\n",
      "Train Epoch: 32 [6400/60000 (11%)]\tLoss: 97.656662\n",
      "Train Epoch: 32 [7680/60000 (13%)]\tLoss: 97.544998\n",
      "Train Epoch: 32 [8960/60000 (15%)]\tLoss: 104.400574\n",
      "Train Epoch: 32 [10240/60000 (17%)]\tLoss: 99.312378\n",
      "Train Epoch: 32 [11520/60000 (19%)]\tLoss: 97.130829\n",
      "Train Epoch: 32 [12800/60000 (21%)]\tLoss: 100.069344\n",
      "Train Epoch: 32 [14080/60000 (23%)]\tLoss: 98.104256\n",
      "Train Epoch: 32 [15360/60000 (26%)]\tLoss: 101.044464\n",
      "Train Epoch: 32 [16640/60000 (28%)]\tLoss: 97.603043\n",
      "Train Epoch: 32 [17920/60000 (30%)]\tLoss: 100.493874\n",
      "Train Epoch: 32 [19200/60000 (32%)]\tLoss: 95.446602\n",
      "Train Epoch: 32 [20480/60000 (34%)]\tLoss: 100.557144\n",
      "Train Epoch: 32 [21760/60000 (36%)]\tLoss: 98.523430\n",
      "Train Epoch: 32 [23040/60000 (38%)]\tLoss: 99.396835\n",
      "Train Epoch: 32 [24320/60000 (41%)]\tLoss: 95.358971\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 101.914360\n",
      "Train Epoch: 32 [26880/60000 (45%)]\tLoss: 99.768143\n",
      "Train Epoch: 32 [28160/60000 (47%)]\tLoss: 99.398819\n",
      "Train Epoch: 32 [29440/60000 (49%)]\tLoss: 94.598648\n",
      "Train Epoch: 32 [30720/60000 (51%)]\tLoss: 101.101746\n",
      "Train Epoch: 32 [32000/60000 (53%)]\tLoss: 97.661621\n",
      "Train Epoch: 32 [33280/60000 (55%)]\tLoss: 103.921646\n",
      "Train Epoch: 32 [34560/60000 (58%)]\tLoss: 102.193398\n",
      "Train Epoch: 32 [35840/60000 (60%)]\tLoss: 102.461121\n",
      "Train Epoch: 32 [37120/60000 (62%)]\tLoss: 97.577316\n",
      "Train Epoch: 32 [38400/60000 (64%)]\tLoss: 100.381889\n",
      "Train Epoch: 32 [39680/60000 (66%)]\tLoss: 101.452858\n",
      "Train Epoch: 32 [40960/60000 (68%)]\tLoss: 98.448364\n",
      "Train Epoch: 32 [42240/60000 (70%)]\tLoss: 100.249092\n",
      "Train Epoch: 32 [43520/60000 (72%)]\tLoss: 97.650505\n",
      "Train Epoch: 32 [44800/60000 (75%)]\tLoss: 103.070816\n",
      "Train Epoch: 32 [46080/60000 (77%)]\tLoss: 100.961288\n",
      "Train Epoch: 32 [47360/60000 (79%)]\tLoss: 100.720139\n",
      "Train Epoch: 32 [48640/60000 (81%)]\tLoss: 99.849602\n",
      "Train Epoch: 32 [49920/60000 (83%)]\tLoss: 103.680130\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 97.820419\n",
      "Train Epoch: 32 [52480/60000 (87%)]\tLoss: 102.934235\n",
      "Train Epoch: 32 [53760/60000 (90%)]\tLoss: 102.703583\n",
      "Train Epoch: 32 [55040/60000 (92%)]\tLoss: 104.238785\n",
      "Train Epoch: 32 [56320/60000 (94%)]\tLoss: 99.506851\n",
      "Train Epoch: 32 [57600/60000 (96%)]\tLoss: 99.148453\n",
      "Train Epoch: 32 [58880/60000 (98%)]\tLoss: 106.543579\n",
      "====> Epoch: 32 Average loss: 99.4969\n",
      "====> Test set loss: 100.5499\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 97.891121\n",
      "Train Epoch: 33 [1280/60000 (2%)]\tLoss: 98.548683\n",
      "Train Epoch: 33 [2560/60000 (4%)]\tLoss: 97.869583\n",
      "Train Epoch: 33 [3840/60000 (6%)]\tLoss: 95.639664\n",
      "Train Epoch: 33 [5120/60000 (9%)]\tLoss: 97.491684\n",
      "Train Epoch: 33 [6400/60000 (11%)]\tLoss: 97.923065\n",
      "Train Epoch: 33 [7680/60000 (13%)]\tLoss: 98.000778\n",
      "Train Epoch: 33 [8960/60000 (15%)]\tLoss: 102.227005\n",
      "Train Epoch: 33 [10240/60000 (17%)]\tLoss: 100.580353\n",
      "Train Epoch: 33 [11520/60000 (19%)]\tLoss: 94.479355\n",
      "Train Epoch: 33 [12800/60000 (21%)]\tLoss: 102.238708\n",
      "Train Epoch: 33 [14080/60000 (23%)]\tLoss: 100.413147\n",
      "Train Epoch: 33 [15360/60000 (26%)]\tLoss: 95.939644\n",
      "Train Epoch: 33 [16640/60000 (28%)]\tLoss: 98.396164\n",
      "Train Epoch: 33 [17920/60000 (30%)]\tLoss: 97.963882\n",
      "Train Epoch: 33 [19200/60000 (32%)]\tLoss: 102.138000\n",
      "Train Epoch: 33 [20480/60000 (34%)]\tLoss: 100.732697\n",
      "Train Epoch: 33 [21760/60000 (36%)]\tLoss: 100.384323\n",
      "Train Epoch: 33 [23040/60000 (38%)]\tLoss: 100.630386\n",
      "Train Epoch: 33 [24320/60000 (41%)]\tLoss: 98.231491\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 96.881561\n",
      "Train Epoch: 33 [26880/60000 (45%)]\tLoss: 99.980812\n",
      "Train Epoch: 33 [28160/60000 (47%)]\tLoss: 95.516663\n",
      "Train Epoch: 33 [29440/60000 (49%)]\tLoss: 95.794235\n",
      "Train Epoch: 33 [30720/60000 (51%)]\tLoss: 96.426254\n",
      "Train Epoch: 33 [32000/60000 (53%)]\tLoss: 98.165840\n",
      "Train Epoch: 33 [33280/60000 (55%)]\tLoss: 99.617348\n",
      "Train Epoch: 33 [34560/60000 (58%)]\tLoss: 100.603340\n",
      "Train Epoch: 33 [35840/60000 (60%)]\tLoss: 98.470146\n",
      "Train Epoch: 33 [37120/60000 (62%)]\tLoss: 99.226028\n",
      "Train Epoch: 33 [38400/60000 (64%)]\tLoss: 93.348618\n",
      "Train Epoch: 33 [39680/60000 (66%)]\tLoss: 100.076782\n",
      "Train Epoch: 33 [40960/60000 (68%)]\tLoss: 102.197357\n",
      "Train Epoch: 33 [42240/60000 (70%)]\tLoss: 101.909180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 33 [43520/60000 (72%)]\tLoss: 98.292511\n",
      "Train Epoch: 33 [44800/60000 (75%)]\tLoss: 96.879257\n",
      "Train Epoch: 33 [46080/60000 (77%)]\tLoss: 99.107986\n",
      "Train Epoch: 33 [47360/60000 (79%)]\tLoss: 101.562096\n",
      "Train Epoch: 33 [48640/60000 (81%)]\tLoss: 99.042542\n",
      "Train Epoch: 33 [49920/60000 (83%)]\tLoss: 101.580399\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 99.351700\n",
      "Train Epoch: 33 [52480/60000 (87%)]\tLoss: 100.314713\n",
      "Train Epoch: 33 [53760/60000 (90%)]\tLoss: 100.640427\n",
      "Train Epoch: 33 [55040/60000 (92%)]\tLoss: 97.921463\n",
      "Train Epoch: 33 [56320/60000 (94%)]\tLoss: 100.328796\n",
      "Train Epoch: 33 [57600/60000 (96%)]\tLoss: 105.567566\n",
      "Train Epoch: 33 [58880/60000 (98%)]\tLoss: 98.192047\n",
      "====> Epoch: 33 Average loss: 99.4028\n",
      "====> Test set loss: 100.5459\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 101.810913\n",
      "Train Epoch: 34 [1280/60000 (2%)]\tLoss: 98.796753\n",
      "Train Epoch: 34 [2560/60000 (4%)]\tLoss: 97.149208\n",
      "Train Epoch: 34 [3840/60000 (6%)]\tLoss: 96.569092\n",
      "Train Epoch: 34 [5120/60000 (9%)]\tLoss: 97.819412\n",
      "Train Epoch: 34 [6400/60000 (11%)]\tLoss: 100.098587\n",
      "Train Epoch: 34 [7680/60000 (13%)]\tLoss: 97.114532\n",
      "Train Epoch: 34 [8960/60000 (15%)]\tLoss: 99.122803\n",
      "Train Epoch: 34 [10240/60000 (17%)]\tLoss: 101.041092\n",
      "Train Epoch: 34 [11520/60000 (19%)]\tLoss: 98.115196\n",
      "Train Epoch: 34 [12800/60000 (21%)]\tLoss: 98.249619\n",
      "Train Epoch: 34 [14080/60000 (23%)]\tLoss: 98.627625\n",
      "Train Epoch: 34 [15360/60000 (26%)]\tLoss: 100.222595\n",
      "Train Epoch: 34 [16640/60000 (28%)]\tLoss: 101.233643\n",
      "Train Epoch: 34 [17920/60000 (30%)]\tLoss: 97.474327\n",
      "Train Epoch: 34 [19200/60000 (32%)]\tLoss: 95.743530\n",
      "Train Epoch: 34 [20480/60000 (34%)]\tLoss: 101.714157\n",
      "Train Epoch: 34 [21760/60000 (36%)]\tLoss: 100.050224\n",
      "Train Epoch: 34 [23040/60000 (38%)]\tLoss: 93.145218\n",
      "Train Epoch: 34 [24320/60000 (41%)]\tLoss: 99.138229\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 100.055252\n",
      "Train Epoch: 34 [26880/60000 (45%)]\tLoss: 98.042816\n",
      "Train Epoch: 34 [28160/60000 (47%)]\tLoss: 98.411491\n",
      "Train Epoch: 34 [29440/60000 (49%)]\tLoss: 96.775589\n",
      "Train Epoch: 34 [30720/60000 (51%)]\tLoss: 101.827553\n",
      "Train Epoch: 34 [32000/60000 (53%)]\tLoss: 96.345711\n",
      "Train Epoch: 34 [33280/60000 (55%)]\tLoss: 98.809937\n",
      "Train Epoch: 34 [34560/60000 (58%)]\tLoss: 97.289047\n",
      "Train Epoch: 34 [35840/60000 (60%)]\tLoss: 97.459824\n",
      "Train Epoch: 34 [37120/60000 (62%)]\tLoss: 97.400742\n",
      "Train Epoch: 34 [38400/60000 (64%)]\tLoss: 98.645561\n",
      "Train Epoch: 34 [39680/60000 (66%)]\tLoss: 102.309334\n",
      "Train Epoch: 34 [40960/60000 (68%)]\tLoss: 96.399185\n",
      "Train Epoch: 34 [42240/60000 (70%)]\tLoss: 94.065475\n",
      "Train Epoch: 34 [43520/60000 (72%)]\tLoss: 93.348213\n",
      "Train Epoch: 34 [44800/60000 (75%)]\tLoss: 97.184982\n",
      "Train Epoch: 34 [46080/60000 (77%)]\tLoss: 101.464417\n",
      "Train Epoch: 34 [47360/60000 (79%)]\tLoss: 102.852165\n",
      "Train Epoch: 34 [48640/60000 (81%)]\tLoss: 100.602715\n",
      "Train Epoch: 34 [49920/60000 (83%)]\tLoss: 95.792389\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 101.014336\n",
      "Train Epoch: 34 [52480/60000 (87%)]\tLoss: 98.142853\n",
      "Train Epoch: 34 [53760/60000 (90%)]\tLoss: 103.959679\n",
      "Train Epoch: 34 [55040/60000 (92%)]\tLoss: 105.301140\n",
      "Train Epoch: 34 [56320/60000 (94%)]\tLoss: 97.134079\n",
      "Train Epoch: 34 [57600/60000 (96%)]\tLoss: 93.940933\n",
      "Train Epoch: 34 [58880/60000 (98%)]\tLoss: 101.877708\n",
      "====> Epoch: 34 Average loss: 99.3427\n",
      "====> Test set loss: 100.5425\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 98.644882\n",
      "Train Epoch: 35 [1280/60000 (2%)]\tLoss: 99.277496\n",
      "Train Epoch: 35 [2560/60000 (4%)]\tLoss: 99.582809\n",
      "Train Epoch: 35 [3840/60000 (6%)]\tLoss: 102.609253\n",
      "Train Epoch: 35 [5120/60000 (9%)]\tLoss: 100.950172\n",
      "Train Epoch: 35 [6400/60000 (11%)]\tLoss: 101.759071\n",
      "Train Epoch: 35 [7680/60000 (13%)]\tLoss: 102.438904\n",
      "Train Epoch: 35 [8960/60000 (15%)]\tLoss: 100.003998\n",
      "Train Epoch: 35 [10240/60000 (17%)]\tLoss: 103.991699\n",
      "Train Epoch: 35 [11520/60000 (19%)]\tLoss: 102.863686\n",
      "Train Epoch: 35 [12800/60000 (21%)]\tLoss: 95.921516\n",
      "Train Epoch: 35 [14080/60000 (23%)]\tLoss: 101.869156\n",
      "Train Epoch: 35 [15360/60000 (26%)]\tLoss: 97.953018\n",
      "Train Epoch: 35 [16640/60000 (28%)]\tLoss: 98.729759\n",
      "Train Epoch: 35 [17920/60000 (30%)]\tLoss: 97.941818\n",
      "Train Epoch: 35 [19200/60000 (32%)]\tLoss: 97.036598\n",
      "Train Epoch: 35 [20480/60000 (34%)]\tLoss: 100.286835\n",
      "Train Epoch: 35 [21760/60000 (36%)]\tLoss: 98.203369\n",
      "Train Epoch: 35 [23040/60000 (38%)]\tLoss: 100.761032\n",
      "Train Epoch: 35 [24320/60000 (41%)]\tLoss: 104.945435\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 96.751137\n",
      "Train Epoch: 35 [26880/60000 (45%)]\tLoss: 99.304047\n",
      "Train Epoch: 35 [28160/60000 (47%)]\tLoss: 98.198685\n",
      "Train Epoch: 35 [29440/60000 (49%)]\tLoss: 99.159653\n",
      "Train Epoch: 35 [30720/60000 (51%)]\tLoss: 97.077896\n",
      "Train Epoch: 35 [32000/60000 (53%)]\tLoss: 99.577179\n",
      "Train Epoch: 35 [33280/60000 (55%)]\tLoss: 101.833626\n",
      "Train Epoch: 35 [34560/60000 (58%)]\tLoss: 95.465446\n",
      "Train Epoch: 35 [35840/60000 (60%)]\tLoss: 101.272064\n",
      "Train Epoch: 35 [37120/60000 (62%)]\tLoss: 99.754585\n",
      "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 97.128784\n",
      "Train Epoch: 35 [39680/60000 (66%)]\tLoss: 99.130928\n",
      "Train Epoch: 35 [40960/60000 (68%)]\tLoss: 95.845146\n",
      "Train Epoch: 35 [42240/60000 (70%)]\tLoss: 99.645706\n",
      "Train Epoch: 35 [43520/60000 (72%)]\tLoss: 100.490608\n",
      "Train Epoch: 35 [44800/60000 (75%)]\tLoss: 98.876190\n",
      "Train Epoch: 35 [46080/60000 (77%)]\tLoss: 101.525925\n",
      "Train Epoch: 35 [47360/60000 (79%)]\tLoss: 99.359352\n",
      "Train Epoch: 35 [48640/60000 (81%)]\tLoss: 98.268135\n",
      "Train Epoch: 35 [49920/60000 (83%)]\tLoss: 97.233047\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 97.062088\n",
      "Train Epoch: 35 [52480/60000 (87%)]\tLoss: 95.309174\n",
      "Train Epoch: 35 [53760/60000 (90%)]\tLoss: 98.045578\n",
      "Train Epoch: 35 [55040/60000 (92%)]\tLoss: 103.710289\n",
      "Train Epoch: 35 [56320/60000 (94%)]\tLoss: 99.949997\n",
      "Train Epoch: 35 [57600/60000 (96%)]\tLoss: 95.518127\n",
      "Train Epoch: 35 [58880/60000 (98%)]\tLoss: 102.529312\n",
      "====> Epoch: 35 Average loss: 99.2496\n",
      "====> Test set loss: 100.5855\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 99.509659\n",
      "Train Epoch: 36 [1280/60000 (2%)]\tLoss: 98.282082\n",
      "Train Epoch: 36 [2560/60000 (4%)]\tLoss: 100.713074\n",
      "Train Epoch: 36 [3840/60000 (6%)]\tLoss: 95.085983\n",
      "Train Epoch: 36 [5120/60000 (9%)]\tLoss: 100.564758\n",
      "Train Epoch: 36 [6400/60000 (11%)]\tLoss: 100.029968\n",
      "Train Epoch: 36 [7680/60000 (13%)]\tLoss: 100.574593\n",
      "Train Epoch: 36 [8960/60000 (15%)]\tLoss: 97.290977\n",
      "Train Epoch: 36 [10240/60000 (17%)]\tLoss: 97.529930\n",
      "Train Epoch: 36 [11520/60000 (19%)]\tLoss: 96.376984\n",
      "Train Epoch: 36 [12800/60000 (21%)]\tLoss: 99.283714\n",
      "Train Epoch: 36 [14080/60000 (23%)]\tLoss: 94.867203\n",
      "Train Epoch: 36 [15360/60000 (26%)]\tLoss: 98.666458\n",
      "Train Epoch: 36 [16640/60000 (28%)]\tLoss: 95.852020\n",
      "Train Epoch: 36 [17920/60000 (30%)]\tLoss: 101.099625\n",
      "Train Epoch: 36 [19200/60000 (32%)]\tLoss: 95.943047\n",
      "Train Epoch: 36 [20480/60000 (34%)]\tLoss: 99.475861\n",
      "Train Epoch: 36 [21760/60000 (36%)]\tLoss: 99.417343\n",
      "Train Epoch: 36 [23040/60000 (38%)]\tLoss: 101.366425\n",
      "Train Epoch: 36 [24320/60000 (41%)]\tLoss: 102.680939\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 98.888184\n",
      "Train Epoch: 36 [26880/60000 (45%)]\tLoss: 97.539497\n",
      "Train Epoch: 36 [28160/60000 (47%)]\tLoss: 104.073723\n",
      "Train Epoch: 36 [29440/60000 (49%)]\tLoss: 101.151260\n",
      "Train Epoch: 36 [30720/60000 (51%)]\tLoss: 100.101036\n",
      "Train Epoch: 36 [32000/60000 (53%)]\tLoss: 107.460213\n",
      "Train Epoch: 36 [33280/60000 (55%)]\tLoss: 100.555130\n",
      "Train Epoch: 36 [34560/60000 (58%)]\tLoss: 99.249306\n",
      "Train Epoch: 36 [35840/60000 (60%)]\tLoss: 100.530495\n",
      "Train Epoch: 36 [37120/60000 (62%)]\tLoss: 98.839363\n",
      "Train Epoch: 36 [38400/60000 (64%)]\tLoss: 98.618538\n",
      "Train Epoch: 36 [39680/60000 (66%)]\tLoss: 97.302841\n",
      "Train Epoch: 36 [40960/60000 (68%)]\tLoss: 98.894501\n",
      "Train Epoch: 36 [42240/60000 (70%)]\tLoss: 96.829834\n",
      "Train Epoch: 36 [43520/60000 (72%)]\tLoss: 100.325493\n",
      "Train Epoch: 36 [44800/60000 (75%)]\tLoss: 100.941795\n",
      "Train Epoch: 36 [46080/60000 (77%)]\tLoss: 99.397705\n",
      "Train Epoch: 36 [47360/60000 (79%)]\tLoss: 99.353790\n",
      "Train Epoch: 36 [48640/60000 (81%)]\tLoss: 98.846786\n",
      "Train Epoch: 36 [49920/60000 (83%)]\tLoss: 98.785179\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 99.339859\n",
      "Train Epoch: 36 [52480/60000 (87%)]\tLoss: 99.425194\n",
      "Train Epoch: 36 [53760/60000 (90%)]\tLoss: 97.408928\n",
      "Train Epoch: 36 [55040/60000 (92%)]\tLoss: 97.227112\n",
      "Train Epoch: 36 [56320/60000 (94%)]\tLoss: 99.118675\n",
      "Train Epoch: 36 [57600/60000 (96%)]\tLoss: 101.462112\n",
      "Train Epoch: 36 [58880/60000 (98%)]\tLoss: 100.738922\n",
      "====> Epoch: 36 Average loss: 99.1794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 100.4709\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 101.130020\n",
      "Train Epoch: 37 [1280/60000 (2%)]\tLoss: 101.471497\n",
      "Train Epoch: 37 [2560/60000 (4%)]\tLoss: 101.733612\n",
      "Train Epoch: 37 [3840/60000 (6%)]\tLoss: 101.204117\n",
      "Train Epoch: 37 [5120/60000 (9%)]\tLoss: 100.431274\n",
      "Train Epoch: 37 [6400/60000 (11%)]\tLoss: 100.301804\n",
      "Train Epoch: 37 [7680/60000 (13%)]\tLoss: 98.024948\n",
      "Train Epoch: 37 [8960/60000 (15%)]\tLoss: 98.511955\n",
      "Train Epoch: 37 [10240/60000 (17%)]\tLoss: 99.706253\n",
      "Train Epoch: 37 [11520/60000 (19%)]\tLoss: 99.855812\n",
      "Train Epoch: 37 [12800/60000 (21%)]\tLoss: 98.901840\n",
      "Train Epoch: 37 [14080/60000 (23%)]\tLoss: 98.607002\n",
      "Train Epoch: 37 [15360/60000 (26%)]\tLoss: 97.681915\n",
      "Train Epoch: 37 [16640/60000 (28%)]\tLoss: 100.989670\n",
      "Train Epoch: 37 [17920/60000 (30%)]\tLoss: 98.816833\n",
      "Train Epoch: 37 [19200/60000 (32%)]\tLoss: 94.492645\n",
      "Train Epoch: 37 [20480/60000 (34%)]\tLoss: 101.007278\n",
      "Train Epoch: 37 [21760/60000 (36%)]\tLoss: 98.036728\n",
      "Train Epoch: 37 [23040/60000 (38%)]\tLoss: 96.956749\n",
      "Train Epoch: 37 [24320/60000 (41%)]\tLoss: 103.704971\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 98.485283\n",
      "Train Epoch: 37 [26880/60000 (45%)]\tLoss: 97.532211\n",
      "Train Epoch: 37 [28160/60000 (47%)]\tLoss: 93.410400\n",
      "Train Epoch: 37 [29440/60000 (49%)]\tLoss: 99.867401\n",
      "Train Epoch: 37 [30720/60000 (51%)]\tLoss: 97.442162\n",
      "Train Epoch: 37 [32000/60000 (53%)]\tLoss: 100.495895\n",
      "Train Epoch: 37 [33280/60000 (55%)]\tLoss: 98.400497\n",
      "Train Epoch: 37 [34560/60000 (58%)]\tLoss: 95.372986\n",
      "Train Epoch: 37 [35840/60000 (60%)]\tLoss: 101.665428\n",
      "Train Epoch: 37 [37120/60000 (62%)]\tLoss: 95.820038\n",
      "Train Epoch: 37 [38400/60000 (64%)]\tLoss: 99.449783\n",
      "Train Epoch: 37 [39680/60000 (66%)]\tLoss: 101.295418\n",
      "Train Epoch: 37 [40960/60000 (68%)]\tLoss: 101.255241\n",
      "Train Epoch: 37 [42240/60000 (70%)]\tLoss: 100.969437\n",
      "Train Epoch: 37 [43520/60000 (72%)]\tLoss: 98.018532\n",
      "Train Epoch: 37 [44800/60000 (75%)]\tLoss: 97.574821\n",
      "Train Epoch: 37 [46080/60000 (77%)]\tLoss: 98.901978\n",
      "Train Epoch: 37 [47360/60000 (79%)]\tLoss: 98.660736\n",
      "Train Epoch: 37 [48640/60000 (81%)]\tLoss: 97.612915\n",
      "Train Epoch: 37 [49920/60000 (83%)]\tLoss: 102.375885\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 98.214104\n",
      "Train Epoch: 37 [52480/60000 (87%)]\tLoss: 98.010391\n",
      "Train Epoch: 37 [53760/60000 (90%)]\tLoss: 96.721062\n",
      "Train Epoch: 37 [55040/60000 (92%)]\tLoss: 102.612762\n",
      "Train Epoch: 37 [56320/60000 (94%)]\tLoss: 96.634827\n",
      "Train Epoch: 37 [57600/60000 (96%)]\tLoss: 99.777954\n",
      "Train Epoch: 37 [58880/60000 (98%)]\tLoss: 104.235352\n",
      "====> Epoch: 37 Average loss: 99.0959\n",
      "====> Test set loss: 100.3382\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 99.715355\n",
      "Train Epoch: 38 [1280/60000 (2%)]\tLoss: 102.107117\n",
      "Train Epoch: 38 [2560/60000 (4%)]\tLoss: 98.556519\n",
      "Train Epoch: 38 [3840/60000 (6%)]\tLoss: 100.368210\n",
      "Train Epoch: 38 [5120/60000 (9%)]\tLoss: 101.971230\n",
      "Train Epoch: 38 [6400/60000 (11%)]\tLoss: 98.831757\n",
      "Train Epoch: 38 [7680/60000 (13%)]\tLoss: 100.018135\n",
      "Train Epoch: 38 [8960/60000 (15%)]\tLoss: 100.514053\n",
      "Train Epoch: 38 [10240/60000 (17%)]\tLoss: 95.345886\n",
      "Train Epoch: 38 [11520/60000 (19%)]\tLoss: 100.345436\n",
      "Train Epoch: 38 [12800/60000 (21%)]\tLoss: 100.651962\n",
      "Train Epoch: 38 [14080/60000 (23%)]\tLoss: 95.237915\n",
      "Train Epoch: 38 [15360/60000 (26%)]\tLoss: 99.290276\n",
      "Train Epoch: 38 [16640/60000 (28%)]\tLoss: 101.901260\n",
      "Train Epoch: 38 [17920/60000 (30%)]\tLoss: 99.284668\n",
      "Train Epoch: 38 [19200/60000 (32%)]\tLoss: 96.449944\n",
      "Train Epoch: 38 [20480/60000 (34%)]\tLoss: 97.480225\n",
      "Train Epoch: 38 [21760/60000 (36%)]\tLoss: 103.689522\n",
      "Train Epoch: 38 [23040/60000 (38%)]\tLoss: 97.111366\n",
      "Train Epoch: 38 [24320/60000 (41%)]\tLoss: 95.402672\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 100.103180\n",
      "Train Epoch: 38 [26880/60000 (45%)]\tLoss: 100.173714\n",
      "Train Epoch: 38 [28160/60000 (47%)]\tLoss: 98.639725\n",
      "Train Epoch: 38 [29440/60000 (49%)]\tLoss: 100.429359\n",
      "Train Epoch: 38 [30720/60000 (51%)]\tLoss: 99.499466\n",
      "Train Epoch: 38 [32000/60000 (53%)]\tLoss: 98.546898\n",
      "Train Epoch: 38 [33280/60000 (55%)]\tLoss: 92.610619\n",
      "Train Epoch: 38 [34560/60000 (58%)]\tLoss: 96.898224\n",
      "Train Epoch: 38 [35840/60000 (60%)]\tLoss: 98.762314\n",
      "Train Epoch: 38 [37120/60000 (62%)]\tLoss: 98.972412\n",
      "Train Epoch: 38 [38400/60000 (64%)]\tLoss: 98.559875\n",
      "Train Epoch: 38 [39680/60000 (66%)]\tLoss: 96.828773\n",
      "Train Epoch: 38 [40960/60000 (68%)]\tLoss: 97.919815\n",
      "Train Epoch: 38 [42240/60000 (70%)]\tLoss: 98.093277\n",
      "Train Epoch: 38 [43520/60000 (72%)]\tLoss: 101.261497\n",
      "Train Epoch: 38 [44800/60000 (75%)]\tLoss: 97.520508\n",
      "Train Epoch: 38 [46080/60000 (77%)]\tLoss: 97.960617\n",
      "Train Epoch: 38 [47360/60000 (79%)]\tLoss: 95.116600\n",
      "Train Epoch: 38 [48640/60000 (81%)]\tLoss: 100.295898\n",
      "Train Epoch: 38 [49920/60000 (83%)]\tLoss: 100.776062\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 100.714622\n",
      "Train Epoch: 38 [52480/60000 (87%)]\tLoss: 98.996529\n",
      "Train Epoch: 38 [53760/60000 (90%)]\tLoss: 99.669716\n",
      "Train Epoch: 38 [55040/60000 (92%)]\tLoss: 101.228973\n",
      "Train Epoch: 38 [56320/60000 (94%)]\tLoss: 101.482178\n",
      "Train Epoch: 38 [57600/60000 (96%)]\tLoss: 98.798111\n",
      "Train Epoch: 38 [58880/60000 (98%)]\tLoss: 100.285568\n",
      "====> Epoch: 38 Average loss: 98.9398\n",
      "====> Test set loss: 100.3857\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 98.379974\n",
      "Train Epoch: 39 [1280/60000 (2%)]\tLoss: 98.546524\n",
      "Train Epoch: 39 [2560/60000 (4%)]\tLoss: 98.099266\n",
      "Train Epoch: 39 [3840/60000 (6%)]\tLoss: 99.582283\n",
      "Train Epoch: 39 [5120/60000 (9%)]\tLoss: 98.581200\n",
      "Train Epoch: 39 [6400/60000 (11%)]\tLoss: 99.017395\n",
      "Train Epoch: 39 [7680/60000 (13%)]\tLoss: 102.739334\n",
      "Train Epoch: 39 [8960/60000 (15%)]\tLoss: 96.001984\n",
      "Train Epoch: 39 [10240/60000 (17%)]\tLoss: 101.761414\n",
      "Train Epoch: 39 [11520/60000 (19%)]\tLoss: 102.437698\n",
      "Train Epoch: 39 [12800/60000 (21%)]\tLoss: 97.099846\n",
      "Train Epoch: 39 [14080/60000 (23%)]\tLoss: 102.437546\n",
      "Train Epoch: 39 [15360/60000 (26%)]\tLoss: 102.614334\n",
      "Train Epoch: 39 [16640/60000 (28%)]\tLoss: 96.609558\n",
      "Train Epoch: 39 [17920/60000 (30%)]\tLoss: 100.297836\n",
      "Train Epoch: 39 [19200/60000 (32%)]\tLoss: 96.708580\n",
      "Train Epoch: 39 [20480/60000 (34%)]\tLoss: 94.806908\n",
      "Train Epoch: 39 [21760/60000 (36%)]\tLoss: 100.683739\n",
      "Train Epoch: 39 [23040/60000 (38%)]\tLoss: 101.185036\n",
      "Train Epoch: 39 [24320/60000 (41%)]\tLoss: 97.613083\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 95.695633\n",
      "Train Epoch: 39 [26880/60000 (45%)]\tLoss: 96.531654\n",
      "Train Epoch: 39 [28160/60000 (47%)]\tLoss: 101.244057\n",
      "Train Epoch: 39 [29440/60000 (49%)]\tLoss: 97.477524\n",
      "Train Epoch: 39 [30720/60000 (51%)]\tLoss: 100.145584\n",
      "Train Epoch: 39 [32000/60000 (53%)]\tLoss: 96.634636\n",
      "Train Epoch: 39 [33280/60000 (55%)]\tLoss: 99.167183\n",
      "Train Epoch: 39 [34560/60000 (58%)]\tLoss: 101.090958\n",
      "Train Epoch: 39 [35840/60000 (60%)]\tLoss: 98.259247\n",
      "Train Epoch: 39 [37120/60000 (62%)]\tLoss: 93.910507\n",
      "Train Epoch: 39 [38400/60000 (64%)]\tLoss: 99.893677\n",
      "Train Epoch: 39 [39680/60000 (66%)]\tLoss: 97.843460\n",
      "Train Epoch: 39 [40960/60000 (68%)]\tLoss: 97.595215\n",
      "Train Epoch: 39 [42240/60000 (70%)]\tLoss: 96.729233\n",
      "Train Epoch: 39 [43520/60000 (72%)]\tLoss: 98.839874\n",
      "Train Epoch: 39 [44800/60000 (75%)]\tLoss: 98.135811\n",
      "Train Epoch: 39 [46080/60000 (77%)]\tLoss: 99.287750\n",
      "Train Epoch: 39 [47360/60000 (79%)]\tLoss: 97.213890\n",
      "Train Epoch: 39 [48640/60000 (81%)]\tLoss: 97.831024\n",
      "Train Epoch: 39 [49920/60000 (83%)]\tLoss: 99.505234\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 98.966087\n",
      "Train Epoch: 39 [52480/60000 (87%)]\tLoss: 100.832878\n",
      "Train Epoch: 39 [53760/60000 (90%)]\tLoss: 97.974716\n",
      "Train Epoch: 39 [55040/60000 (92%)]\tLoss: 95.742554\n",
      "Train Epoch: 39 [56320/60000 (94%)]\tLoss: 101.372963\n",
      "Train Epoch: 39 [57600/60000 (96%)]\tLoss: 101.166885\n",
      "Train Epoch: 39 [58880/60000 (98%)]\tLoss: 101.315704\n",
      "====> Epoch: 39 Average loss: 98.9065\n",
      "====> Test set loss: 100.3399\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 97.808495\n",
      "Train Epoch: 40 [1280/60000 (2%)]\tLoss: 100.308472\n",
      "Train Epoch: 40 [2560/60000 (4%)]\tLoss: 96.890121\n",
      "Train Epoch: 40 [3840/60000 (6%)]\tLoss: 97.973572\n",
      "Train Epoch: 40 [5120/60000 (9%)]\tLoss: 100.542870\n",
      "Train Epoch: 40 [6400/60000 (11%)]\tLoss: 97.019951\n",
      "Train Epoch: 40 [7680/60000 (13%)]\tLoss: 99.657120\n",
      "Train Epoch: 40 [8960/60000 (15%)]\tLoss: 97.136551\n",
      "Train Epoch: 40 [10240/60000 (17%)]\tLoss: 97.363342\n",
      "Train Epoch: 40 [11520/60000 (19%)]\tLoss: 97.694214\n",
      "Train Epoch: 40 [12800/60000 (21%)]\tLoss: 97.249405\n",
      "Train Epoch: 40 [14080/60000 (23%)]\tLoss: 97.409256\n",
      "Train Epoch: 40 [15360/60000 (26%)]\tLoss: 96.330101\n",
      "Train Epoch: 40 [16640/60000 (28%)]\tLoss: 99.776337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 40 [17920/60000 (30%)]\tLoss: 100.116318\n",
      "Train Epoch: 40 [19200/60000 (32%)]\tLoss: 92.437653\n",
      "Train Epoch: 40 [20480/60000 (34%)]\tLoss: 102.336655\n",
      "Train Epoch: 40 [21760/60000 (36%)]\tLoss: 95.698441\n",
      "Train Epoch: 40 [23040/60000 (38%)]\tLoss: 102.207397\n",
      "Train Epoch: 40 [24320/60000 (41%)]\tLoss: 99.268318\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 95.154160\n",
      "Train Epoch: 40 [26880/60000 (45%)]\tLoss: 102.233353\n",
      "Train Epoch: 40 [28160/60000 (47%)]\tLoss: 98.394608\n",
      "Train Epoch: 40 [29440/60000 (49%)]\tLoss: 97.816269\n",
      "Train Epoch: 40 [30720/60000 (51%)]\tLoss: 98.622635\n",
      "Train Epoch: 40 [32000/60000 (53%)]\tLoss: 100.016312\n",
      "Train Epoch: 40 [33280/60000 (55%)]\tLoss: 93.256241\n",
      "Train Epoch: 40 [34560/60000 (58%)]\tLoss: 99.278610\n",
      "Train Epoch: 40 [35840/60000 (60%)]\tLoss: 100.788742\n",
      "Train Epoch: 40 [37120/60000 (62%)]\tLoss: 100.453743\n",
      "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 97.879784\n",
      "Train Epoch: 40 [39680/60000 (66%)]\tLoss: 100.399857\n",
      "Train Epoch: 40 [40960/60000 (68%)]\tLoss: 97.968887\n",
      "Train Epoch: 40 [42240/60000 (70%)]\tLoss: 97.970169\n",
      "Train Epoch: 40 [43520/60000 (72%)]\tLoss: 97.585922\n",
      "Train Epoch: 40 [44800/60000 (75%)]\tLoss: 100.058273\n",
      "Train Epoch: 40 [46080/60000 (77%)]\tLoss: 100.786942\n",
      "Train Epoch: 40 [47360/60000 (79%)]\tLoss: 101.794617\n",
      "Train Epoch: 40 [48640/60000 (81%)]\tLoss: 102.426094\n",
      "Train Epoch: 40 [49920/60000 (83%)]\tLoss: 102.380997\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 101.383476\n",
      "Train Epoch: 40 [52480/60000 (87%)]\tLoss: 97.993439\n",
      "Train Epoch: 40 [53760/60000 (90%)]\tLoss: 96.603996\n",
      "Train Epoch: 40 [55040/60000 (92%)]\tLoss: 98.589966\n",
      "Train Epoch: 40 [56320/60000 (94%)]\tLoss: 94.860970\n",
      "Train Epoch: 40 [57600/60000 (96%)]\tLoss: 99.533318\n",
      "Train Epoch: 40 [58880/60000 (98%)]\tLoss: 100.456009\n",
      "====> Epoch: 40 Average loss: 98.8372\n",
      "====> Test set loss: 100.1973\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 98.151344\n",
      "Train Epoch: 41 [1280/60000 (2%)]\tLoss: 97.445038\n",
      "Train Epoch: 41 [2560/60000 (4%)]\tLoss: 98.891052\n",
      "Train Epoch: 41 [3840/60000 (6%)]\tLoss: 96.850433\n",
      "Train Epoch: 41 [5120/60000 (9%)]\tLoss: 99.212852\n",
      "Train Epoch: 41 [6400/60000 (11%)]\tLoss: 94.589966\n",
      "Train Epoch: 41 [7680/60000 (13%)]\tLoss: 102.771255\n",
      "Train Epoch: 41 [8960/60000 (15%)]\tLoss: 97.675888\n",
      "Train Epoch: 41 [10240/60000 (17%)]\tLoss: 97.870209\n",
      "Train Epoch: 41 [11520/60000 (19%)]\tLoss: 101.213333\n",
      "Train Epoch: 41 [12800/60000 (21%)]\tLoss: 95.697281\n",
      "Train Epoch: 41 [14080/60000 (23%)]\tLoss: 101.404076\n",
      "Train Epoch: 41 [15360/60000 (26%)]\tLoss: 102.865799\n",
      "Train Epoch: 41 [16640/60000 (28%)]\tLoss: 98.206169\n",
      "Train Epoch: 41 [17920/60000 (30%)]\tLoss: 97.393921\n",
      "Train Epoch: 41 [19200/60000 (32%)]\tLoss: 99.521332\n",
      "Train Epoch: 41 [20480/60000 (34%)]\tLoss: 96.773933\n",
      "Train Epoch: 41 [21760/60000 (36%)]\tLoss: 97.595551\n",
      "Train Epoch: 41 [23040/60000 (38%)]\tLoss: 99.667183\n",
      "Train Epoch: 41 [24320/60000 (41%)]\tLoss: 98.525360\n",
      "Train Epoch: 41 [25600/60000 (43%)]\tLoss: 93.641983\n",
      "Train Epoch: 41 [26880/60000 (45%)]\tLoss: 101.865112\n",
      "Train Epoch: 41 [28160/60000 (47%)]\tLoss: 100.927345\n",
      "Train Epoch: 41 [29440/60000 (49%)]\tLoss: 94.938713\n",
      "Train Epoch: 41 [30720/60000 (51%)]\tLoss: 98.943253\n",
      "Train Epoch: 41 [32000/60000 (53%)]\tLoss: 93.189301\n",
      "Train Epoch: 41 [33280/60000 (55%)]\tLoss: 99.515335\n",
      "Train Epoch: 41 [34560/60000 (58%)]\tLoss: 98.453529\n",
      "Train Epoch: 41 [35840/60000 (60%)]\tLoss: 98.669998\n",
      "Train Epoch: 41 [37120/60000 (62%)]\tLoss: 96.632660\n",
      "Train Epoch: 41 [38400/60000 (64%)]\tLoss: 97.481232\n",
      "Train Epoch: 41 [39680/60000 (66%)]\tLoss: 104.194000\n",
      "Train Epoch: 41 [40960/60000 (68%)]\tLoss: 97.985435\n",
      "Train Epoch: 41 [42240/60000 (70%)]\tLoss: 99.936310\n",
      "Train Epoch: 41 [43520/60000 (72%)]\tLoss: 99.292900\n",
      "Train Epoch: 41 [44800/60000 (75%)]\tLoss: 98.479362\n",
      "Train Epoch: 41 [46080/60000 (77%)]\tLoss: 100.809578\n",
      "Train Epoch: 41 [47360/60000 (79%)]\tLoss: 104.281250\n",
      "Train Epoch: 41 [48640/60000 (81%)]\tLoss: 98.556824\n",
      "Train Epoch: 41 [49920/60000 (83%)]\tLoss: 101.735428\n",
      "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 96.717812\n",
      "Train Epoch: 41 [52480/60000 (87%)]\tLoss: 101.410004\n",
      "Train Epoch: 41 [53760/60000 (90%)]\tLoss: 93.609642\n",
      "Train Epoch: 41 [55040/60000 (92%)]\tLoss: 101.382416\n",
      "Train Epoch: 41 [56320/60000 (94%)]\tLoss: 97.332794\n",
      "Train Epoch: 41 [57600/60000 (96%)]\tLoss: 99.525558\n",
      "Train Epoch: 41 [58880/60000 (98%)]\tLoss: 101.628799\n",
      "====> Epoch: 41 Average loss: 98.7671\n",
      "====> Test set loss: 100.0409\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 98.549538\n",
      "Train Epoch: 42 [1280/60000 (2%)]\tLoss: 100.729355\n",
      "Train Epoch: 42 [2560/60000 (4%)]\tLoss: 98.147758\n",
      "Train Epoch: 42 [3840/60000 (6%)]\tLoss: 98.436928\n",
      "Train Epoch: 42 [5120/60000 (9%)]\tLoss: 96.447266\n",
      "Train Epoch: 42 [6400/60000 (11%)]\tLoss: 97.359436\n",
      "Train Epoch: 42 [7680/60000 (13%)]\tLoss: 99.694855\n",
      "Train Epoch: 42 [8960/60000 (15%)]\tLoss: 98.043816\n",
      "Train Epoch: 42 [10240/60000 (17%)]\tLoss: 96.450516\n",
      "Train Epoch: 42 [11520/60000 (19%)]\tLoss: 97.350052\n",
      "Train Epoch: 42 [12800/60000 (21%)]\tLoss: 102.637848\n",
      "Train Epoch: 42 [14080/60000 (23%)]\tLoss: 102.270096\n",
      "Train Epoch: 42 [15360/60000 (26%)]\tLoss: 100.117126\n",
      "Train Epoch: 42 [16640/60000 (28%)]\tLoss: 95.822838\n",
      "Train Epoch: 42 [17920/60000 (30%)]\tLoss: 102.073868\n",
      "Train Epoch: 42 [19200/60000 (32%)]\tLoss: 100.957146\n",
      "Train Epoch: 42 [20480/60000 (34%)]\tLoss: 99.885727\n",
      "Train Epoch: 42 [21760/60000 (36%)]\tLoss: 97.642044\n",
      "Train Epoch: 42 [23040/60000 (38%)]\tLoss: 96.335220\n",
      "Train Epoch: 42 [24320/60000 (41%)]\tLoss: 97.635666\n",
      "Train Epoch: 42 [25600/60000 (43%)]\tLoss: 98.710831\n",
      "Train Epoch: 42 [26880/60000 (45%)]\tLoss: 96.507629\n",
      "Train Epoch: 42 [28160/60000 (47%)]\tLoss: 102.950943\n",
      "Train Epoch: 42 [29440/60000 (49%)]\tLoss: 105.210144\n",
      "Train Epoch: 42 [30720/60000 (51%)]\tLoss: 94.721840\n",
      "Train Epoch: 42 [32000/60000 (53%)]\tLoss: 101.590942\n",
      "Train Epoch: 42 [33280/60000 (55%)]\tLoss: 98.505867\n",
      "Train Epoch: 42 [34560/60000 (58%)]\tLoss: 100.818680\n",
      "Train Epoch: 42 [35840/60000 (60%)]\tLoss: 95.567078\n",
      "Train Epoch: 42 [37120/60000 (62%)]\tLoss: 95.687523\n",
      "Train Epoch: 42 [38400/60000 (64%)]\tLoss: 98.263115\n",
      "Train Epoch: 42 [39680/60000 (66%)]\tLoss: 97.998383\n",
      "Train Epoch: 42 [40960/60000 (68%)]\tLoss: 99.985527\n",
      "Train Epoch: 42 [42240/60000 (70%)]\tLoss: 97.111725\n",
      "Train Epoch: 42 [43520/60000 (72%)]\tLoss: 98.095184\n",
      "Train Epoch: 42 [44800/60000 (75%)]\tLoss: 98.714226\n",
      "Train Epoch: 42 [46080/60000 (77%)]\tLoss: 102.707588\n",
      "Train Epoch: 42 [47360/60000 (79%)]\tLoss: 101.043442\n",
      "Train Epoch: 42 [48640/60000 (81%)]\tLoss: 100.483696\n",
      "Train Epoch: 42 [49920/60000 (83%)]\tLoss: 95.922119\n",
      "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 100.084045\n",
      "Train Epoch: 42 [52480/60000 (87%)]\tLoss: 102.278381\n",
      "Train Epoch: 42 [53760/60000 (90%)]\tLoss: 96.519142\n",
      "Train Epoch: 42 [55040/60000 (92%)]\tLoss: 99.299057\n",
      "Train Epoch: 42 [56320/60000 (94%)]\tLoss: 99.060959\n",
      "Train Epoch: 42 [57600/60000 (96%)]\tLoss: 94.019394\n",
      "Train Epoch: 42 [58880/60000 (98%)]\tLoss: 95.656845\n",
      "====> Epoch: 42 Average loss: 98.7230\n",
      "====> Test set loss: 99.7114\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 102.539047\n",
      "Train Epoch: 43 [1280/60000 (2%)]\tLoss: 99.669632\n",
      "Train Epoch: 43 [2560/60000 (4%)]\tLoss: 94.729355\n",
      "Train Epoch: 43 [3840/60000 (6%)]\tLoss: 96.471169\n",
      "Train Epoch: 43 [5120/60000 (9%)]\tLoss: 97.400276\n",
      "Train Epoch: 43 [6400/60000 (11%)]\tLoss: 101.178436\n",
      "Train Epoch: 43 [7680/60000 (13%)]\tLoss: 96.790146\n",
      "Train Epoch: 43 [8960/60000 (15%)]\tLoss: 97.358688\n",
      "Train Epoch: 43 [10240/60000 (17%)]\tLoss: 98.532150\n",
      "Train Epoch: 43 [11520/60000 (19%)]\tLoss: 99.070488\n",
      "Train Epoch: 43 [12800/60000 (21%)]\tLoss: 99.912308\n",
      "Train Epoch: 43 [14080/60000 (23%)]\tLoss: 100.605659\n",
      "Train Epoch: 43 [15360/60000 (26%)]\tLoss: 96.827415\n",
      "Train Epoch: 43 [16640/60000 (28%)]\tLoss: 101.022957\n",
      "Train Epoch: 43 [17920/60000 (30%)]\tLoss: 98.124283\n",
      "Train Epoch: 43 [19200/60000 (32%)]\tLoss: 98.734840\n",
      "Train Epoch: 43 [20480/60000 (34%)]\tLoss: 97.852783\n",
      "Train Epoch: 43 [21760/60000 (36%)]\tLoss: 98.100296\n",
      "Train Epoch: 43 [23040/60000 (38%)]\tLoss: 99.552719\n",
      "Train Epoch: 43 [24320/60000 (41%)]\tLoss: 97.839951\n",
      "Train Epoch: 43 [25600/60000 (43%)]\tLoss: 99.344398\n",
      "Train Epoch: 43 [26880/60000 (45%)]\tLoss: 104.111557\n",
      "Train Epoch: 43 [28160/60000 (47%)]\tLoss: 99.428299\n",
      "Train Epoch: 43 [29440/60000 (49%)]\tLoss: 98.283569\n",
      "Train Epoch: 43 [30720/60000 (51%)]\tLoss: 93.934906\n",
      "Train Epoch: 43 [32000/60000 (53%)]\tLoss: 98.211082\n",
      "Train Epoch: 43 [33280/60000 (55%)]\tLoss: 99.883255\n",
      "Train Epoch: 43 [34560/60000 (58%)]\tLoss: 99.772934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 43 [35840/60000 (60%)]\tLoss: 96.716293\n",
      "Train Epoch: 43 [37120/60000 (62%)]\tLoss: 102.988983\n",
      "Train Epoch: 43 [38400/60000 (64%)]\tLoss: 97.456589\n",
      "Train Epoch: 43 [39680/60000 (66%)]\tLoss: 98.442047\n",
      "Train Epoch: 43 [40960/60000 (68%)]\tLoss: 99.667282\n",
      "Train Epoch: 43 [42240/60000 (70%)]\tLoss: 94.771957\n",
      "Train Epoch: 43 [43520/60000 (72%)]\tLoss: 97.941589\n",
      "Train Epoch: 43 [44800/60000 (75%)]\tLoss: 96.080292\n",
      "Train Epoch: 43 [46080/60000 (77%)]\tLoss: 100.377945\n",
      "Train Epoch: 43 [47360/60000 (79%)]\tLoss: 102.684387\n",
      "Train Epoch: 43 [48640/60000 (81%)]\tLoss: 97.773804\n",
      "Train Epoch: 43 [49920/60000 (83%)]\tLoss: 101.027374\n",
      "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 95.552109\n",
      "Train Epoch: 43 [52480/60000 (87%)]\tLoss: 101.138100\n",
      "Train Epoch: 43 [53760/60000 (90%)]\tLoss: 99.656822\n",
      "Train Epoch: 43 [55040/60000 (92%)]\tLoss: 97.980209\n",
      "Train Epoch: 43 [56320/60000 (94%)]\tLoss: 98.748795\n",
      "Train Epoch: 43 [57600/60000 (96%)]\tLoss: 101.005775\n",
      "Train Epoch: 43 [58880/60000 (98%)]\tLoss: 95.930794\n",
      "====> Epoch: 43 Average loss: 98.6690\n",
      "====> Test set loss: 100.0089\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 98.871300\n",
      "Train Epoch: 44 [1280/60000 (2%)]\tLoss: 99.582504\n",
      "Train Epoch: 44 [2560/60000 (4%)]\tLoss: 99.154663\n",
      "Train Epoch: 44 [3840/60000 (6%)]\tLoss: 95.792557\n",
      "Train Epoch: 44 [5120/60000 (9%)]\tLoss: 99.718719\n",
      "Train Epoch: 44 [6400/60000 (11%)]\tLoss: 96.754013\n",
      "Train Epoch: 44 [7680/60000 (13%)]\tLoss: 100.917175\n",
      "Train Epoch: 44 [8960/60000 (15%)]\tLoss: 97.508194\n",
      "Train Epoch: 44 [10240/60000 (17%)]\tLoss: 97.448723\n",
      "Train Epoch: 44 [11520/60000 (19%)]\tLoss: 101.104210\n",
      "Train Epoch: 44 [12800/60000 (21%)]\tLoss: 97.065018\n",
      "Train Epoch: 44 [14080/60000 (23%)]\tLoss: 102.907372\n",
      "Train Epoch: 44 [15360/60000 (26%)]\tLoss: 97.070290\n",
      "Train Epoch: 44 [16640/60000 (28%)]\tLoss: 103.548645\n",
      "Train Epoch: 44 [17920/60000 (30%)]\tLoss: 97.178230\n",
      "Train Epoch: 44 [19200/60000 (32%)]\tLoss: 101.719368\n",
      "Train Epoch: 44 [20480/60000 (34%)]\tLoss: 99.391144\n",
      "Train Epoch: 44 [21760/60000 (36%)]\tLoss: 96.498909\n",
      "Train Epoch: 44 [23040/60000 (38%)]\tLoss: 98.358360\n",
      "Train Epoch: 44 [24320/60000 (41%)]\tLoss: 95.892601\n",
      "Train Epoch: 44 [25600/60000 (43%)]\tLoss: 95.661438\n",
      "Train Epoch: 44 [26880/60000 (45%)]\tLoss: 98.929604\n",
      "Train Epoch: 44 [28160/60000 (47%)]\tLoss: 96.604424\n",
      "Train Epoch: 44 [29440/60000 (49%)]\tLoss: 95.795372\n",
      "Train Epoch: 44 [30720/60000 (51%)]\tLoss: 97.976936\n",
      "Train Epoch: 44 [32000/60000 (53%)]\tLoss: 99.165199\n",
      "Train Epoch: 44 [33280/60000 (55%)]\tLoss: 97.120491\n",
      "Train Epoch: 44 [34560/60000 (58%)]\tLoss: 98.009537\n",
      "Train Epoch: 44 [35840/60000 (60%)]\tLoss: 99.405281\n",
      "Train Epoch: 44 [37120/60000 (62%)]\tLoss: 95.947693\n",
      "Train Epoch: 44 [38400/60000 (64%)]\tLoss: 93.392204\n",
      "Train Epoch: 44 [39680/60000 (66%)]\tLoss: 101.436234\n",
      "Train Epoch: 44 [40960/60000 (68%)]\tLoss: 98.911659\n",
      "Train Epoch: 44 [42240/60000 (70%)]\tLoss: 103.052704\n",
      "Train Epoch: 44 [43520/60000 (72%)]\tLoss: 97.110435\n",
      "Train Epoch: 44 [44800/60000 (75%)]\tLoss: 98.609673\n",
      "Train Epoch: 44 [46080/60000 (77%)]\tLoss: 101.647423\n",
      "Train Epoch: 44 [47360/60000 (79%)]\tLoss: 99.540977\n",
      "Train Epoch: 44 [48640/60000 (81%)]\tLoss: 99.433296\n",
      "Train Epoch: 44 [49920/60000 (83%)]\tLoss: 101.171089\n",
      "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 94.105530\n",
      "Train Epoch: 44 [52480/60000 (87%)]\tLoss: 99.236359\n",
      "Train Epoch: 44 [53760/60000 (90%)]\tLoss: 96.786331\n",
      "Train Epoch: 44 [55040/60000 (92%)]\tLoss: 100.013039\n",
      "Train Epoch: 44 [56320/60000 (94%)]\tLoss: 97.930557\n",
      "Train Epoch: 44 [57600/60000 (96%)]\tLoss: 97.909576\n",
      "Train Epoch: 44 [58880/60000 (98%)]\tLoss: 97.328957\n",
      "====> Epoch: 44 Average loss: 98.5773\n",
      "====> Test set loss: 99.9681\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 92.022598\n",
      "Train Epoch: 45 [1280/60000 (2%)]\tLoss: 94.007996\n",
      "Train Epoch: 45 [2560/60000 (4%)]\tLoss: 98.404587\n",
      "Train Epoch: 45 [3840/60000 (6%)]\tLoss: 95.637711\n",
      "Train Epoch: 45 [5120/60000 (9%)]\tLoss: 99.854126\n",
      "Train Epoch: 45 [6400/60000 (11%)]\tLoss: 98.403107\n",
      "Train Epoch: 45 [7680/60000 (13%)]\tLoss: 100.211891\n",
      "Train Epoch: 45 [8960/60000 (15%)]\tLoss: 96.415909\n",
      "Train Epoch: 45 [10240/60000 (17%)]\tLoss: 95.308029\n",
      "Train Epoch: 45 [11520/60000 (19%)]\tLoss: 95.406929\n",
      "Train Epoch: 45 [12800/60000 (21%)]\tLoss: 96.662971\n",
      "Train Epoch: 45 [14080/60000 (23%)]\tLoss: 98.328629\n",
      "Train Epoch: 45 [15360/60000 (26%)]\tLoss: 95.320091\n",
      "Train Epoch: 45 [16640/60000 (28%)]\tLoss: 98.715782\n",
      "Train Epoch: 45 [17920/60000 (30%)]\tLoss: 100.554855\n",
      "Train Epoch: 45 [19200/60000 (32%)]\tLoss: 99.456795\n",
      "Train Epoch: 45 [20480/60000 (34%)]\tLoss: 100.762558\n",
      "Train Epoch: 45 [21760/60000 (36%)]\tLoss: 97.690048\n",
      "Train Epoch: 45 [23040/60000 (38%)]\tLoss: 98.220367\n",
      "Train Epoch: 45 [24320/60000 (41%)]\tLoss: 92.687241\n",
      "Train Epoch: 45 [25600/60000 (43%)]\tLoss: 98.206924\n",
      "Train Epoch: 45 [26880/60000 (45%)]\tLoss: 93.770889\n",
      "Train Epoch: 45 [28160/60000 (47%)]\tLoss: 97.346008\n",
      "Train Epoch: 45 [29440/60000 (49%)]\tLoss: 97.667267\n",
      "Train Epoch: 45 [30720/60000 (51%)]\tLoss: 96.011230\n",
      "Train Epoch: 45 [32000/60000 (53%)]\tLoss: 104.440559\n",
      "Train Epoch: 45 [33280/60000 (55%)]\tLoss: 98.956825\n",
      "Train Epoch: 45 [34560/60000 (58%)]\tLoss: 99.839142\n",
      "Train Epoch: 45 [35840/60000 (60%)]\tLoss: 95.779602\n",
      "Train Epoch: 45 [37120/60000 (62%)]\tLoss: 96.965790\n",
      "Train Epoch: 45 [38400/60000 (64%)]\tLoss: 95.474380\n",
      "Train Epoch: 45 [39680/60000 (66%)]\tLoss: 98.627739\n",
      "Train Epoch: 45 [40960/60000 (68%)]\tLoss: 99.592880\n",
      "Train Epoch: 45 [42240/60000 (70%)]\tLoss: 102.004463\n",
      "Train Epoch: 45 [43520/60000 (72%)]\tLoss: 102.154404\n",
      "Train Epoch: 45 [44800/60000 (75%)]\tLoss: 98.622162\n",
      "Train Epoch: 45 [46080/60000 (77%)]\tLoss: 98.337982\n",
      "Train Epoch: 45 [47360/60000 (79%)]\tLoss: 99.908447\n",
      "Train Epoch: 45 [48640/60000 (81%)]\tLoss: 99.626930\n",
      "Train Epoch: 45 [49920/60000 (83%)]\tLoss: 99.326736\n",
      "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 101.198586\n",
      "Train Epoch: 45 [52480/60000 (87%)]\tLoss: 99.011276\n",
      "Train Epoch: 45 [53760/60000 (90%)]\tLoss: 95.648277\n",
      "Train Epoch: 45 [55040/60000 (92%)]\tLoss: 94.633209\n",
      "Train Epoch: 45 [56320/60000 (94%)]\tLoss: 101.455322\n",
      "Train Epoch: 45 [57600/60000 (96%)]\tLoss: 101.767036\n",
      "Train Epoch: 45 [58880/60000 (98%)]\tLoss: 98.337624\n",
      "====> Epoch: 45 Average loss: 98.5515\n",
      "====> Test set loss: 100.0191\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 96.874825\n",
      "Train Epoch: 46 [1280/60000 (2%)]\tLoss: 99.110504\n",
      "Train Epoch: 46 [2560/60000 (4%)]\tLoss: 93.053864\n",
      "Train Epoch: 46 [3840/60000 (6%)]\tLoss: 96.600410\n",
      "Train Epoch: 46 [5120/60000 (9%)]\tLoss: 97.729279\n",
      "Train Epoch: 46 [6400/60000 (11%)]\tLoss: 97.958466\n",
      "Train Epoch: 46 [7680/60000 (13%)]\tLoss: 96.474640\n",
      "Train Epoch: 46 [8960/60000 (15%)]\tLoss: 97.991196\n",
      "Train Epoch: 46 [10240/60000 (17%)]\tLoss: 97.242630\n",
      "Train Epoch: 46 [11520/60000 (19%)]\tLoss: 99.649719\n",
      "Train Epoch: 46 [12800/60000 (21%)]\tLoss: 98.951340\n",
      "Train Epoch: 46 [14080/60000 (23%)]\tLoss: 98.864365\n",
      "Train Epoch: 46 [15360/60000 (26%)]\tLoss: 95.711090\n",
      "Train Epoch: 46 [16640/60000 (28%)]\tLoss: 100.125145\n",
      "Train Epoch: 46 [17920/60000 (30%)]\tLoss: 96.332870\n",
      "Train Epoch: 46 [19200/60000 (32%)]\tLoss: 102.551651\n",
      "Train Epoch: 46 [20480/60000 (34%)]\tLoss: 100.085503\n",
      "Train Epoch: 46 [21760/60000 (36%)]\tLoss: 98.405884\n",
      "Train Epoch: 46 [23040/60000 (38%)]\tLoss: 95.992828\n",
      "Train Epoch: 46 [24320/60000 (41%)]\tLoss: 97.522713\n",
      "Train Epoch: 46 [25600/60000 (43%)]\tLoss: 97.763275\n",
      "Train Epoch: 46 [26880/60000 (45%)]\tLoss: 100.397690\n",
      "Train Epoch: 46 [28160/60000 (47%)]\tLoss: 97.610229\n",
      "Train Epoch: 46 [29440/60000 (49%)]\tLoss: 96.371048\n",
      "Train Epoch: 46 [30720/60000 (51%)]\tLoss: 95.886177\n",
      "Train Epoch: 46 [32000/60000 (53%)]\tLoss: 98.942078\n",
      "Train Epoch: 46 [33280/60000 (55%)]\tLoss: 100.094482\n",
      "Train Epoch: 46 [34560/60000 (58%)]\tLoss: 96.682594\n",
      "Train Epoch: 46 [35840/60000 (60%)]\tLoss: 103.582932\n",
      "Train Epoch: 46 [37120/60000 (62%)]\tLoss: 99.861298\n",
      "Train Epoch: 46 [38400/60000 (64%)]\tLoss: 98.501289\n",
      "Train Epoch: 46 [39680/60000 (66%)]\tLoss: 98.654083\n",
      "Train Epoch: 46 [40960/60000 (68%)]\tLoss: 100.468094\n",
      "Train Epoch: 46 [42240/60000 (70%)]\tLoss: 100.059845\n",
      "Train Epoch: 46 [43520/60000 (72%)]\tLoss: 102.978867\n",
      "Train Epoch: 46 [44800/60000 (75%)]\tLoss: 97.109375\n",
      "Train Epoch: 46 [46080/60000 (77%)]\tLoss: 98.670982\n",
      "Train Epoch: 46 [47360/60000 (79%)]\tLoss: 94.715202\n",
      "Train Epoch: 46 [48640/60000 (81%)]\tLoss: 99.672272\n",
      "Train Epoch: 46 [49920/60000 (83%)]\tLoss: 101.554184\n",
      "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 101.917854\n",
      "Train Epoch: 46 [52480/60000 (87%)]\tLoss: 103.733566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 46 [53760/60000 (90%)]\tLoss: 96.785492\n",
      "Train Epoch: 46 [55040/60000 (92%)]\tLoss: 102.149971\n",
      "Train Epoch: 46 [56320/60000 (94%)]\tLoss: 95.214737\n",
      "Train Epoch: 46 [57600/60000 (96%)]\tLoss: 99.929871\n",
      "Train Epoch: 46 [58880/60000 (98%)]\tLoss: 98.077621\n",
      "====> Epoch: 46 Average loss: 98.4877\n",
      "====> Test set loss: 100.0435\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 98.194664\n",
      "Train Epoch: 47 [1280/60000 (2%)]\tLoss: 98.955002\n",
      "Train Epoch: 47 [2560/60000 (4%)]\tLoss: 97.701416\n",
      "Train Epoch: 47 [3840/60000 (6%)]\tLoss: 97.720001\n",
      "Train Epoch: 47 [5120/60000 (9%)]\tLoss: 95.726364\n",
      "Train Epoch: 47 [6400/60000 (11%)]\tLoss: 98.204140\n",
      "Train Epoch: 47 [7680/60000 (13%)]\tLoss: 98.554123\n",
      "Train Epoch: 47 [8960/60000 (15%)]\tLoss: 99.295090\n",
      "Train Epoch: 47 [10240/60000 (17%)]\tLoss: 98.351768\n",
      "Train Epoch: 47 [11520/60000 (19%)]\tLoss: 99.581505\n",
      "Train Epoch: 47 [12800/60000 (21%)]\tLoss: 96.049454\n",
      "Train Epoch: 47 [14080/60000 (23%)]\tLoss: 98.246964\n",
      "Train Epoch: 47 [15360/60000 (26%)]\tLoss: 96.105774\n",
      "Train Epoch: 47 [16640/60000 (28%)]\tLoss: 97.082695\n",
      "Train Epoch: 47 [17920/60000 (30%)]\tLoss: 95.940369\n",
      "Train Epoch: 47 [19200/60000 (32%)]\tLoss: 102.148270\n",
      "Train Epoch: 47 [20480/60000 (34%)]\tLoss: 97.865051\n",
      "Train Epoch: 47 [21760/60000 (36%)]\tLoss: 95.085846\n",
      "Train Epoch: 47 [23040/60000 (38%)]\tLoss: 100.490677\n",
      "Train Epoch: 47 [24320/60000 (41%)]\tLoss: 100.544907\n",
      "Train Epoch: 47 [25600/60000 (43%)]\tLoss: 98.623184\n",
      "Train Epoch: 47 [26880/60000 (45%)]\tLoss: 97.032845\n",
      "Train Epoch: 47 [28160/60000 (47%)]\tLoss: 96.771049\n",
      "Train Epoch: 47 [29440/60000 (49%)]\tLoss: 99.540451\n",
      "Train Epoch: 47 [30720/60000 (51%)]\tLoss: 99.390945\n",
      "Train Epoch: 47 [32000/60000 (53%)]\tLoss: 96.274017\n",
      "Train Epoch: 47 [33280/60000 (55%)]\tLoss: 94.636368\n",
      "Train Epoch: 47 [34560/60000 (58%)]\tLoss: 101.189964\n",
      "Train Epoch: 47 [35840/60000 (60%)]\tLoss: 100.132019\n",
      "Train Epoch: 47 [37120/60000 (62%)]\tLoss: 102.017776\n",
      "Train Epoch: 47 [38400/60000 (64%)]\tLoss: 102.812889\n",
      "Train Epoch: 47 [39680/60000 (66%)]\tLoss: 99.020660\n",
      "Train Epoch: 47 [40960/60000 (68%)]\tLoss: 93.831543\n",
      "Train Epoch: 47 [42240/60000 (70%)]\tLoss: 100.885437\n",
      "Train Epoch: 47 [43520/60000 (72%)]\tLoss: 100.385818\n",
      "Train Epoch: 47 [44800/60000 (75%)]\tLoss: 97.903969\n",
      "Train Epoch: 47 [46080/60000 (77%)]\tLoss: 101.883514\n",
      "Train Epoch: 47 [47360/60000 (79%)]\tLoss: 95.968414\n",
      "Train Epoch: 47 [48640/60000 (81%)]\tLoss: 96.920082\n",
      "Train Epoch: 47 [49920/60000 (83%)]\tLoss: 98.374298\n",
      "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 101.971939\n",
      "Train Epoch: 47 [52480/60000 (87%)]\tLoss: 96.606277\n",
      "Train Epoch: 47 [53760/60000 (90%)]\tLoss: 97.431198\n",
      "Train Epoch: 47 [55040/60000 (92%)]\tLoss: 98.718719\n",
      "Train Epoch: 47 [56320/60000 (94%)]\tLoss: 101.808250\n",
      "Train Epoch: 47 [57600/60000 (96%)]\tLoss: 97.009232\n",
      "Train Epoch: 47 [58880/60000 (98%)]\tLoss: 100.428482\n",
      "====> Epoch: 47 Average loss: 98.4565\n",
      "====> Test set loss: 99.9800\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 100.978065\n",
      "Train Epoch: 48 [1280/60000 (2%)]\tLoss: 100.860687\n",
      "Train Epoch: 48 [2560/60000 (4%)]\tLoss: 104.297852\n",
      "Train Epoch: 48 [3840/60000 (6%)]\tLoss: 96.599335\n",
      "Train Epoch: 48 [5120/60000 (9%)]\tLoss: 95.825989\n",
      "Train Epoch: 48 [6400/60000 (11%)]\tLoss: 97.956253\n",
      "Train Epoch: 48 [7680/60000 (13%)]\tLoss: 94.111504\n",
      "Train Epoch: 48 [8960/60000 (15%)]\tLoss: 101.606247\n",
      "Train Epoch: 48 [10240/60000 (17%)]\tLoss: 96.131241\n",
      "Train Epoch: 48 [11520/60000 (19%)]\tLoss: 98.435242\n",
      "Train Epoch: 48 [12800/60000 (21%)]\tLoss: 93.887299\n",
      "Train Epoch: 48 [14080/60000 (23%)]\tLoss: 98.514099\n",
      "Train Epoch: 48 [15360/60000 (26%)]\tLoss: 97.719917\n",
      "Train Epoch: 48 [16640/60000 (28%)]\tLoss: 94.883545\n",
      "Train Epoch: 48 [17920/60000 (30%)]\tLoss: 97.047577\n",
      "Train Epoch: 48 [19200/60000 (32%)]\tLoss: 100.593781\n",
      "Train Epoch: 48 [20480/60000 (34%)]\tLoss: 99.452667\n",
      "Train Epoch: 48 [21760/60000 (36%)]\tLoss: 101.444435\n",
      "Train Epoch: 48 [23040/60000 (38%)]\tLoss: 101.696564\n",
      "Train Epoch: 48 [24320/60000 (41%)]\tLoss: 97.050095\n",
      "Train Epoch: 48 [25600/60000 (43%)]\tLoss: 95.646652\n",
      "Train Epoch: 48 [26880/60000 (45%)]\tLoss: 98.640945\n",
      "Train Epoch: 48 [28160/60000 (47%)]\tLoss: 96.109039\n",
      "Train Epoch: 48 [29440/60000 (49%)]\tLoss: 102.584320\n",
      "Train Epoch: 48 [30720/60000 (51%)]\tLoss: 95.794189\n",
      "Train Epoch: 48 [32000/60000 (53%)]\tLoss: 99.215141\n",
      "Train Epoch: 48 [33280/60000 (55%)]\tLoss: 100.363647\n",
      "Train Epoch: 48 [34560/60000 (58%)]\tLoss: 101.580009\n",
      "Train Epoch: 48 [35840/60000 (60%)]\tLoss: 93.070190\n",
      "Train Epoch: 48 [37120/60000 (62%)]\tLoss: 103.705650\n",
      "Train Epoch: 48 [38400/60000 (64%)]\tLoss: 95.167671\n",
      "Train Epoch: 48 [39680/60000 (66%)]\tLoss: 99.560715\n",
      "Train Epoch: 48 [40960/60000 (68%)]\tLoss: 100.164276\n",
      "Train Epoch: 48 [42240/60000 (70%)]\tLoss: 93.554642\n",
      "Train Epoch: 48 [43520/60000 (72%)]\tLoss: 96.987236\n",
      "Train Epoch: 48 [44800/60000 (75%)]\tLoss: 97.042992\n",
      "Train Epoch: 48 [46080/60000 (77%)]\tLoss: 96.611649\n",
      "Train Epoch: 48 [47360/60000 (79%)]\tLoss: 99.333214\n",
      "Train Epoch: 48 [48640/60000 (81%)]\tLoss: 103.330635\n",
      "Train Epoch: 48 [49920/60000 (83%)]\tLoss: 97.760284\n",
      "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 97.810089\n",
      "Train Epoch: 48 [52480/60000 (87%)]\tLoss: 94.994629\n",
      "Train Epoch: 48 [53760/60000 (90%)]\tLoss: 99.897408\n",
      "Train Epoch: 48 [55040/60000 (92%)]\tLoss: 95.371956\n",
      "Train Epoch: 48 [56320/60000 (94%)]\tLoss: 98.590164\n",
      "Train Epoch: 48 [57600/60000 (96%)]\tLoss: 95.425751\n",
      "Train Epoch: 48 [58880/60000 (98%)]\tLoss: 100.454025\n",
      "====> Epoch: 48 Average loss: 98.3852\n",
      "====> Test set loss: 100.0293\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 99.645844\n",
      "Train Epoch: 49 [1280/60000 (2%)]\tLoss: 99.471062\n",
      "Train Epoch: 49 [2560/60000 (4%)]\tLoss: 99.827080\n",
      "Train Epoch: 49 [3840/60000 (6%)]\tLoss: 99.945160\n",
      "Train Epoch: 49 [5120/60000 (9%)]\tLoss: 97.737473\n",
      "Train Epoch: 49 [6400/60000 (11%)]\tLoss: 99.501610\n",
      "Train Epoch: 49 [7680/60000 (13%)]\tLoss: 99.319977\n",
      "Train Epoch: 49 [8960/60000 (15%)]\tLoss: 94.641556\n",
      "Train Epoch: 49 [10240/60000 (17%)]\tLoss: 102.570618\n",
      "Train Epoch: 49 [11520/60000 (19%)]\tLoss: 97.984810\n",
      "Train Epoch: 49 [12800/60000 (21%)]\tLoss: 100.815437\n",
      "Train Epoch: 49 [14080/60000 (23%)]\tLoss: 101.406265\n",
      "Train Epoch: 49 [15360/60000 (26%)]\tLoss: 98.085846\n",
      "Train Epoch: 49 [16640/60000 (28%)]\tLoss: 97.157570\n",
      "Train Epoch: 49 [17920/60000 (30%)]\tLoss: 92.649231\n",
      "Train Epoch: 49 [19200/60000 (32%)]\tLoss: 93.775299\n",
      "Train Epoch: 49 [20480/60000 (34%)]\tLoss: 97.092575\n",
      "Train Epoch: 49 [21760/60000 (36%)]\tLoss: 98.244179\n",
      "Train Epoch: 49 [23040/60000 (38%)]\tLoss: 98.653328\n",
      "Train Epoch: 49 [24320/60000 (41%)]\tLoss: 97.287033\n",
      "Train Epoch: 49 [25600/60000 (43%)]\tLoss: 100.227493\n",
      "Train Epoch: 49 [26880/60000 (45%)]\tLoss: 95.321312\n",
      "Train Epoch: 49 [28160/60000 (47%)]\tLoss: 102.115082\n",
      "Train Epoch: 49 [29440/60000 (49%)]\tLoss: 98.394043\n",
      "Train Epoch: 49 [30720/60000 (51%)]\tLoss: 97.167267\n",
      "Train Epoch: 49 [32000/60000 (53%)]\tLoss: 95.330254\n",
      "Train Epoch: 49 [33280/60000 (55%)]\tLoss: 102.376587\n",
      "Train Epoch: 49 [34560/60000 (58%)]\tLoss: 98.501556\n",
      "Train Epoch: 49 [35840/60000 (60%)]\tLoss: 100.020233\n",
      "Train Epoch: 49 [37120/60000 (62%)]\tLoss: 98.599152\n",
      "Train Epoch: 49 [38400/60000 (64%)]\tLoss: 94.270798\n",
      "Train Epoch: 49 [39680/60000 (66%)]\tLoss: 97.113152\n",
      "Train Epoch: 49 [40960/60000 (68%)]\tLoss: 94.885162\n",
      "Train Epoch: 49 [42240/60000 (70%)]\tLoss: 102.674843\n",
      "Train Epoch: 49 [43520/60000 (72%)]\tLoss: 98.885544\n",
      "Train Epoch: 49 [44800/60000 (75%)]\tLoss: 96.197235\n",
      "Train Epoch: 49 [46080/60000 (77%)]\tLoss: 101.003120\n",
      "Train Epoch: 49 [47360/60000 (79%)]\tLoss: 100.058701\n",
      "Train Epoch: 49 [48640/60000 (81%)]\tLoss: 96.119827\n",
      "Train Epoch: 49 [49920/60000 (83%)]\tLoss: 100.279068\n",
      "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 100.494629\n",
      "Train Epoch: 49 [52480/60000 (87%)]\tLoss: 100.061882\n",
      "Train Epoch: 49 [53760/60000 (90%)]\tLoss: 96.284134\n",
      "Train Epoch: 49 [55040/60000 (92%)]\tLoss: 98.973892\n",
      "Train Epoch: 49 [56320/60000 (94%)]\tLoss: 98.538162\n",
      "Train Epoch: 49 [57600/60000 (96%)]\tLoss: 100.903534\n",
      "Train Epoch: 49 [58880/60000 (98%)]\tLoss: 98.290665\n",
      "====> Epoch: 49 Average loss: 98.3252\n",
      "====> Test set loss: 99.9252\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 100.879372\n",
      "Train Epoch: 50 [1280/60000 (2%)]\tLoss: 94.133957\n",
      "Train Epoch: 50 [2560/60000 (4%)]\tLoss: 97.410080\n",
      "Train Epoch: 50 [3840/60000 (6%)]\tLoss: 98.638420\n",
      "Train Epoch: 50 [5120/60000 (9%)]\tLoss: 99.047867\n",
      "Train Epoch: 50 [6400/60000 (11%)]\tLoss: 99.667694\n",
      "Train Epoch: 50 [7680/60000 (13%)]\tLoss: 98.458229\n",
      "Train Epoch: 50 [8960/60000 (15%)]\tLoss: 102.298706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 50 [10240/60000 (17%)]\tLoss: 96.256638\n",
      "Train Epoch: 50 [11520/60000 (19%)]\tLoss: 99.802589\n",
      "Train Epoch: 50 [12800/60000 (21%)]\tLoss: 99.322182\n",
      "Train Epoch: 50 [14080/60000 (23%)]\tLoss: 95.846382\n",
      "Train Epoch: 50 [15360/60000 (26%)]\tLoss: 98.725189\n",
      "Train Epoch: 50 [16640/60000 (28%)]\tLoss: 94.055336\n",
      "Train Epoch: 50 [17920/60000 (30%)]\tLoss: 96.529305\n",
      "Train Epoch: 50 [19200/60000 (32%)]\tLoss: 99.626373\n",
      "Train Epoch: 50 [20480/60000 (34%)]\tLoss: 96.597046\n",
      "Train Epoch: 50 [21760/60000 (36%)]\tLoss: 97.326683\n",
      "Train Epoch: 50 [23040/60000 (38%)]\tLoss: 99.313187\n",
      "Train Epoch: 50 [24320/60000 (41%)]\tLoss: 97.952705\n",
      "Train Epoch: 50 [25600/60000 (43%)]\tLoss: 100.276482\n",
      "Train Epoch: 50 [26880/60000 (45%)]\tLoss: 98.335411\n",
      "Train Epoch: 50 [28160/60000 (47%)]\tLoss: 99.419449\n",
      "Train Epoch: 50 [29440/60000 (49%)]\tLoss: 102.005562\n",
      "Train Epoch: 50 [30720/60000 (51%)]\tLoss: 97.478745\n",
      "Train Epoch: 50 [32000/60000 (53%)]\tLoss: 97.781090\n",
      "Train Epoch: 50 [33280/60000 (55%)]\tLoss: 93.924599\n",
      "Train Epoch: 50 [34560/60000 (58%)]\tLoss: 92.394249\n",
      "Train Epoch: 50 [35840/60000 (60%)]\tLoss: 97.787964\n",
      "Train Epoch: 50 [37120/60000 (62%)]\tLoss: 97.783730\n",
      "Train Epoch: 50 [38400/60000 (64%)]\tLoss: 99.197006\n",
      "Train Epoch: 50 [39680/60000 (66%)]\tLoss: 101.008537\n",
      "Train Epoch: 50 [40960/60000 (68%)]\tLoss: 96.921982\n",
      "Train Epoch: 50 [42240/60000 (70%)]\tLoss: 96.686913\n",
      "Train Epoch: 50 [43520/60000 (72%)]\tLoss: 97.124443\n",
      "Train Epoch: 50 [44800/60000 (75%)]\tLoss: 99.919388\n",
      "Train Epoch: 50 [46080/60000 (77%)]\tLoss: 98.749130\n",
      "Train Epoch: 50 [47360/60000 (79%)]\tLoss: 100.746277\n",
      "Train Epoch: 50 [48640/60000 (81%)]\tLoss: 92.006516\n",
      "Train Epoch: 50 [49920/60000 (83%)]\tLoss: 101.025688\n",
      "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 99.070984\n",
      "Train Epoch: 50 [52480/60000 (87%)]\tLoss: 102.093918\n",
      "Train Epoch: 50 [53760/60000 (90%)]\tLoss: 99.460960\n",
      "Train Epoch: 50 [55040/60000 (92%)]\tLoss: 99.353416\n",
      "Train Epoch: 50 [56320/60000 (94%)]\tLoss: 97.552818\n",
      "Train Epoch: 50 [57600/60000 (96%)]\tLoss: 99.723328\n",
      "Train Epoch: 50 [58880/60000 (98%)]\tLoss: 97.634216\n",
      "====> Epoch: 50 Average loss: 98.2882\n",
      "====> Test set loss: 99.9337\n",
      "Train Epoch: 51 [0/60000 (0%)]\tLoss: 93.259521\n",
      "Train Epoch: 51 [1280/60000 (2%)]\tLoss: 97.436584\n",
      "Train Epoch: 51 [2560/60000 (4%)]\tLoss: 96.942719\n",
      "Train Epoch: 51 [3840/60000 (6%)]\tLoss: 102.754387\n",
      "Train Epoch: 51 [5120/60000 (9%)]\tLoss: 96.995445\n",
      "Train Epoch: 51 [6400/60000 (11%)]\tLoss: 99.355637\n",
      "Train Epoch: 51 [7680/60000 (13%)]\tLoss: 98.548431\n",
      "Train Epoch: 51 [8960/60000 (15%)]\tLoss: 99.442719\n",
      "Train Epoch: 51 [10240/60000 (17%)]\tLoss: 99.452599\n",
      "Train Epoch: 51 [11520/60000 (19%)]\tLoss: 94.495987\n",
      "Train Epoch: 51 [12800/60000 (21%)]\tLoss: 100.224632\n",
      "Train Epoch: 51 [14080/60000 (23%)]\tLoss: 96.824394\n",
      "Train Epoch: 51 [15360/60000 (26%)]\tLoss: 97.212952\n",
      "Train Epoch: 51 [16640/60000 (28%)]\tLoss: 95.371582\n",
      "Train Epoch: 51 [17920/60000 (30%)]\tLoss: 97.843445\n",
      "Train Epoch: 51 [19200/60000 (32%)]\tLoss: 98.041336\n",
      "Train Epoch: 51 [20480/60000 (34%)]\tLoss: 96.902756\n",
      "Train Epoch: 51 [21760/60000 (36%)]\tLoss: 100.875313\n",
      "Train Epoch: 51 [23040/60000 (38%)]\tLoss: 100.660721\n",
      "Train Epoch: 51 [24320/60000 (41%)]\tLoss: 98.359146\n",
      "Train Epoch: 51 [25600/60000 (43%)]\tLoss: 102.038376\n",
      "Train Epoch: 51 [26880/60000 (45%)]\tLoss: 93.069305\n",
      "Train Epoch: 51 [28160/60000 (47%)]\tLoss: 99.003532\n",
      "Train Epoch: 51 [29440/60000 (49%)]\tLoss: 98.212387\n",
      "Train Epoch: 51 [30720/60000 (51%)]\tLoss: 98.189880\n",
      "Train Epoch: 51 [32000/60000 (53%)]\tLoss: 98.052521\n",
      "Train Epoch: 51 [33280/60000 (55%)]\tLoss: 97.868240\n",
      "Train Epoch: 51 [34560/60000 (58%)]\tLoss: 98.297737\n",
      "Train Epoch: 51 [35840/60000 (60%)]\tLoss: 95.143295\n",
      "Train Epoch: 51 [37120/60000 (62%)]\tLoss: 99.507217\n",
      "Train Epoch: 51 [38400/60000 (64%)]\tLoss: 101.642883\n",
      "Train Epoch: 51 [39680/60000 (66%)]\tLoss: 99.221466\n",
      "Train Epoch: 51 [40960/60000 (68%)]\tLoss: 98.420464\n",
      "Train Epoch: 51 [42240/60000 (70%)]\tLoss: 97.414764\n",
      "Train Epoch: 51 [43520/60000 (72%)]\tLoss: 99.316025\n",
      "Train Epoch: 51 [44800/60000 (75%)]\tLoss: 97.695938\n",
      "Train Epoch: 51 [46080/60000 (77%)]\tLoss: 94.921585\n",
      "Train Epoch: 51 [47360/60000 (79%)]\tLoss: 100.221733\n",
      "Train Epoch: 51 [48640/60000 (81%)]\tLoss: 100.246414\n",
      "Train Epoch: 51 [49920/60000 (83%)]\tLoss: 98.021774\n",
      "Train Epoch: 51 [51200/60000 (85%)]\tLoss: 97.757935\n",
      "Train Epoch: 51 [52480/60000 (87%)]\tLoss: 98.251068\n",
      "Train Epoch: 51 [53760/60000 (90%)]\tLoss: 99.583290\n",
      "Train Epoch: 51 [55040/60000 (92%)]\tLoss: 98.762650\n",
      "Train Epoch: 51 [56320/60000 (94%)]\tLoss: 95.762619\n",
      "Train Epoch: 51 [57600/60000 (96%)]\tLoss: 95.263489\n",
      "Train Epoch: 51 [58880/60000 (98%)]\tLoss: 99.356857\n",
      "====> Epoch: 51 Average loss: 98.2381\n",
      "====> Test set loss: 100.0044\n",
      "Train Epoch: 52 [0/60000 (0%)]\tLoss: 98.503632\n",
      "Train Epoch: 52 [1280/60000 (2%)]\tLoss: 97.260208\n",
      "Train Epoch: 52 [2560/60000 (4%)]\tLoss: 99.395050\n",
      "Train Epoch: 52 [3840/60000 (6%)]\tLoss: 98.294624\n",
      "Train Epoch: 52 [5120/60000 (9%)]\tLoss: 98.895401\n",
      "Train Epoch: 52 [6400/60000 (11%)]\tLoss: 99.646057\n",
      "Train Epoch: 52 [7680/60000 (13%)]\tLoss: 101.932426\n",
      "Train Epoch: 52 [8960/60000 (15%)]\tLoss: 97.919159\n",
      "Train Epoch: 52 [10240/60000 (17%)]\tLoss: 97.378860\n",
      "Train Epoch: 52 [11520/60000 (19%)]\tLoss: 93.347878\n",
      "Train Epoch: 52 [12800/60000 (21%)]\tLoss: 95.334900\n",
      "Train Epoch: 52 [14080/60000 (23%)]\tLoss: 93.873230\n",
      "Train Epoch: 52 [15360/60000 (26%)]\tLoss: 99.289864\n",
      "Train Epoch: 52 [16640/60000 (28%)]\tLoss: 100.819962\n",
      "Train Epoch: 52 [17920/60000 (30%)]\tLoss: 96.100700\n",
      "Train Epoch: 52 [19200/60000 (32%)]\tLoss: 98.847527\n",
      "Train Epoch: 52 [20480/60000 (34%)]\tLoss: 100.461861\n",
      "Train Epoch: 52 [21760/60000 (36%)]\tLoss: 96.964050\n",
      "Train Epoch: 52 [23040/60000 (38%)]\tLoss: 98.991936\n",
      "Train Epoch: 52 [24320/60000 (41%)]\tLoss: 98.385513\n",
      "Train Epoch: 52 [25600/60000 (43%)]\tLoss: 96.897675\n",
      "Train Epoch: 52 [26880/60000 (45%)]\tLoss: 98.615318\n",
      "Train Epoch: 52 [28160/60000 (47%)]\tLoss: 95.156723\n",
      "Train Epoch: 52 [29440/60000 (49%)]\tLoss: 100.664886\n",
      "Train Epoch: 52 [30720/60000 (51%)]\tLoss: 97.687454\n",
      "Train Epoch: 52 [32000/60000 (53%)]\tLoss: 96.213379\n",
      "Train Epoch: 52 [33280/60000 (55%)]\tLoss: 93.177727\n",
      "Train Epoch: 52 [34560/60000 (58%)]\tLoss: 101.635216\n",
      "Train Epoch: 52 [35840/60000 (60%)]\tLoss: 99.635307\n",
      "Train Epoch: 52 [37120/60000 (62%)]\tLoss: 97.219910\n",
      "Train Epoch: 52 [38400/60000 (64%)]\tLoss: 98.760254\n",
      "Train Epoch: 52 [39680/60000 (66%)]\tLoss: 96.440887\n",
      "Train Epoch: 52 [40960/60000 (68%)]\tLoss: 98.173828\n",
      "Train Epoch: 52 [42240/60000 (70%)]\tLoss: 100.032516\n",
      "Train Epoch: 52 [43520/60000 (72%)]\tLoss: 98.744926\n",
      "Train Epoch: 52 [44800/60000 (75%)]\tLoss: 96.650238\n",
      "Train Epoch: 52 [46080/60000 (77%)]\tLoss: 94.916031\n",
      "Train Epoch: 52 [47360/60000 (79%)]\tLoss: 93.543480\n",
      "Train Epoch: 52 [48640/60000 (81%)]\tLoss: 95.588486\n",
      "Train Epoch: 52 [49920/60000 (83%)]\tLoss: 102.115540\n",
      "Train Epoch: 52 [51200/60000 (85%)]\tLoss: 95.795547\n",
      "Train Epoch: 52 [52480/60000 (87%)]\tLoss: 94.019783\n",
      "Train Epoch: 52 [53760/60000 (90%)]\tLoss: 99.034500\n",
      "Train Epoch: 52 [55040/60000 (92%)]\tLoss: 101.796692\n",
      "Train Epoch: 52 [56320/60000 (94%)]\tLoss: 101.456764\n",
      "Train Epoch: 52 [57600/60000 (96%)]\tLoss: 96.005081\n",
      "Train Epoch: 52 [58880/60000 (98%)]\tLoss: 98.888733\n",
      "====> Epoch: 52 Average loss: 98.1352\n",
      "====> Test set loss: 99.8451\n",
      "Train Epoch: 53 [0/60000 (0%)]\tLoss: 99.674927\n",
      "Train Epoch: 53 [1280/60000 (2%)]\tLoss: 98.827026\n",
      "Train Epoch: 53 [2560/60000 (4%)]\tLoss: 97.316208\n",
      "Train Epoch: 53 [3840/60000 (6%)]\tLoss: 101.627281\n",
      "Train Epoch: 53 [5120/60000 (9%)]\tLoss: 98.240372\n",
      "Train Epoch: 53 [6400/60000 (11%)]\tLoss: 97.300201\n",
      "Train Epoch: 53 [7680/60000 (13%)]\tLoss: 100.258995\n",
      "Train Epoch: 53 [8960/60000 (15%)]\tLoss: 99.554008\n",
      "Train Epoch: 53 [10240/60000 (17%)]\tLoss: 99.299210\n",
      "Train Epoch: 53 [11520/60000 (19%)]\tLoss: 97.139908\n",
      "Train Epoch: 53 [12800/60000 (21%)]\tLoss: 101.982727\n",
      "Train Epoch: 53 [14080/60000 (23%)]\tLoss: 98.635139\n",
      "Train Epoch: 53 [15360/60000 (26%)]\tLoss: 95.495239\n",
      "Train Epoch: 53 [16640/60000 (28%)]\tLoss: 100.023079\n",
      "Train Epoch: 53 [17920/60000 (30%)]\tLoss: 92.364304\n",
      "Train Epoch: 53 [19200/60000 (32%)]\tLoss: 95.001526\n",
      "Train Epoch: 53 [20480/60000 (34%)]\tLoss: 98.541702\n",
      "Train Epoch: 53 [21760/60000 (36%)]\tLoss: 99.212250\n",
      "Train Epoch: 53 [23040/60000 (38%)]\tLoss: 99.226006\n",
      "Train Epoch: 53 [24320/60000 (41%)]\tLoss: 98.075424\n",
      "Train Epoch: 53 [25600/60000 (43%)]\tLoss: 101.256638\n",
      "Train Epoch: 53 [26880/60000 (45%)]\tLoss: 100.886002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 53 [28160/60000 (47%)]\tLoss: 102.864586\n",
      "Train Epoch: 53 [29440/60000 (49%)]\tLoss: 96.657066\n",
      "Train Epoch: 53 [30720/60000 (51%)]\tLoss: 97.700142\n",
      "Train Epoch: 53 [32000/60000 (53%)]\tLoss: 101.048241\n",
      "Train Epoch: 53 [33280/60000 (55%)]\tLoss: 96.358536\n",
      "Train Epoch: 53 [34560/60000 (58%)]\tLoss: 95.019096\n",
      "Train Epoch: 53 [35840/60000 (60%)]\tLoss: 93.887985\n",
      "Train Epoch: 53 [37120/60000 (62%)]\tLoss: 96.326515\n",
      "Train Epoch: 53 [38400/60000 (64%)]\tLoss: 95.860298\n",
      "Train Epoch: 53 [39680/60000 (66%)]\tLoss: 97.884483\n",
      "Train Epoch: 53 [40960/60000 (68%)]\tLoss: 101.387711\n",
      "Train Epoch: 53 [42240/60000 (70%)]\tLoss: 98.300873\n",
      "Train Epoch: 53 [43520/60000 (72%)]\tLoss: 101.351357\n",
      "Train Epoch: 53 [44800/60000 (75%)]\tLoss: 92.566788\n",
      "Train Epoch: 53 [46080/60000 (77%)]\tLoss: 95.664505\n",
      "Train Epoch: 53 [47360/60000 (79%)]\tLoss: 95.885834\n",
      "Train Epoch: 53 [48640/60000 (81%)]\tLoss: 99.415077\n",
      "Train Epoch: 53 [49920/60000 (83%)]\tLoss: 97.275932\n",
      "Train Epoch: 53 [51200/60000 (85%)]\tLoss: 99.356110\n",
      "Train Epoch: 53 [52480/60000 (87%)]\tLoss: 100.373283\n",
      "Train Epoch: 53 [53760/60000 (90%)]\tLoss: 98.539032\n",
      "Train Epoch: 53 [55040/60000 (92%)]\tLoss: 100.567406\n",
      "Train Epoch: 53 [56320/60000 (94%)]\tLoss: 98.332825\n",
      "Train Epoch: 53 [57600/60000 (96%)]\tLoss: 97.100700\n",
      "Train Epoch: 53 [58880/60000 (98%)]\tLoss: 101.269814\n",
      "====> Epoch: 53 Average loss: 98.1202\n",
      "====> Test set loss: 99.9504\n",
      "Train Epoch: 54 [0/60000 (0%)]\tLoss: 94.288513\n",
      "Train Epoch: 54 [1280/60000 (2%)]\tLoss: 97.685577\n",
      "Train Epoch: 54 [2560/60000 (4%)]\tLoss: 96.569901\n",
      "Train Epoch: 54 [3840/60000 (6%)]\tLoss: 96.807205\n",
      "Train Epoch: 54 [5120/60000 (9%)]\tLoss: 97.197456\n",
      "Train Epoch: 54 [6400/60000 (11%)]\tLoss: 98.015526\n",
      "Train Epoch: 54 [7680/60000 (13%)]\tLoss: 97.651230\n",
      "Train Epoch: 54 [8960/60000 (15%)]\tLoss: 98.583435\n",
      "Train Epoch: 54 [10240/60000 (17%)]\tLoss: 93.625381\n",
      "Train Epoch: 54 [11520/60000 (19%)]\tLoss: 95.240685\n",
      "Train Epoch: 54 [12800/60000 (21%)]\tLoss: 99.260719\n",
      "Train Epoch: 54 [14080/60000 (23%)]\tLoss: 97.894081\n",
      "Train Epoch: 54 [15360/60000 (26%)]\tLoss: 98.124161\n",
      "Train Epoch: 54 [16640/60000 (28%)]\tLoss: 101.264587\n",
      "Train Epoch: 54 [17920/60000 (30%)]\tLoss: 103.072670\n",
      "Train Epoch: 54 [19200/60000 (32%)]\tLoss: 96.626762\n",
      "Train Epoch: 54 [20480/60000 (34%)]\tLoss: 95.714828\n",
      "Train Epoch: 54 [21760/60000 (36%)]\tLoss: 94.432014\n",
      "Train Epoch: 54 [23040/60000 (38%)]\tLoss: 102.006317\n",
      "Train Epoch: 54 [24320/60000 (41%)]\tLoss: 97.562866\n",
      "Train Epoch: 54 [25600/60000 (43%)]\tLoss: 97.385330\n",
      "Train Epoch: 54 [26880/60000 (45%)]\tLoss: 95.722687\n",
      "Train Epoch: 54 [28160/60000 (47%)]\tLoss: 97.063622\n",
      "Train Epoch: 54 [29440/60000 (49%)]\tLoss: 97.774208\n",
      "Train Epoch: 54 [30720/60000 (51%)]\tLoss: 97.008018\n",
      "Train Epoch: 54 [32000/60000 (53%)]\tLoss: 100.683281\n",
      "Train Epoch: 54 [33280/60000 (55%)]\tLoss: 98.997787\n",
      "Train Epoch: 54 [34560/60000 (58%)]\tLoss: 94.693100\n",
      "Train Epoch: 54 [35840/60000 (60%)]\tLoss: 95.358475\n",
      "Train Epoch: 54 [37120/60000 (62%)]\tLoss: 98.452980\n",
      "Train Epoch: 54 [38400/60000 (64%)]\tLoss: 98.373344\n",
      "Train Epoch: 54 [39680/60000 (66%)]\tLoss: 97.779121\n",
      "Train Epoch: 54 [40960/60000 (68%)]\tLoss: 96.815735\n",
      "Train Epoch: 54 [42240/60000 (70%)]\tLoss: 99.003838\n",
      "Train Epoch: 54 [43520/60000 (72%)]\tLoss: 100.938728\n",
      "Train Epoch: 54 [44800/60000 (75%)]\tLoss: 95.263306\n",
      "Train Epoch: 54 [46080/60000 (77%)]\tLoss: 103.294846\n",
      "Train Epoch: 54 [47360/60000 (79%)]\tLoss: 98.163269\n",
      "Train Epoch: 54 [48640/60000 (81%)]\tLoss: 101.100105\n",
      "Train Epoch: 54 [49920/60000 (83%)]\tLoss: 94.533646\n",
      "Train Epoch: 54 [51200/60000 (85%)]\tLoss: 96.031212\n",
      "Train Epoch: 54 [52480/60000 (87%)]\tLoss: 101.635201\n",
      "Train Epoch: 54 [53760/60000 (90%)]\tLoss: 99.011108\n",
      "Train Epoch: 54 [55040/60000 (92%)]\tLoss: 95.645706\n",
      "Train Epoch: 54 [56320/60000 (94%)]\tLoss: 96.408775\n",
      "Train Epoch: 54 [57600/60000 (96%)]\tLoss: 98.008118\n",
      "Train Epoch: 54 [58880/60000 (98%)]\tLoss: 102.767807\n",
      "====> Epoch: 54 Average loss: 98.0823\n",
      "====> Test set loss: 99.6501\n",
      "Train Epoch: 55 [0/60000 (0%)]\tLoss: 96.044563\n",
      "Train Epoch: 55 [1280/60000 (2%)]\tLoss: 97.307571\n",
      "Train Epoch: 55 [2560/60000 (4%)]\tLoss: 100.936775\n",
      "Train Epoch: 55 [3840/60000 (6%)]\tLoss: 96.198456\n",
      "Train Epoch: 55 [5120/60000 (9%)]\tLoss: 98.864136\n",
      "Train Epoch: 55 [6400/60000 (11%)]\tLoss: 97.380188\n",
      "Train Epoch: 55 [7680/60000 (13%)]\tLoss: 100.188034\n",
      "Train Epoch: 55 [8960/60000 (15%)]\tLoss: 96.192909\n",
      "Train Epoch: 55 [10240/60000 (17%)]\tLoss: 98.321495\n",
      "Train Epoch: 55 [11520/60000 (19%)]\tLoss: 97.021553\n",
      "Train Epoch: 55 [12800/60000 (21%)]\tLoss: 97.741753\n",
      "Train Epoch: 55 [14080/60000 (23%)]\tLoss: 95.305305\n",
      "Train Epoch: 55 [15360/60000 (26%)]\tLoss: 98.705345\n",
      "Train Epoch: 55 [16640/60000 (28%)]\tLoss: 98.883987\n",
      "Train Epoch: 55 [17920/60000 (30%)]\tLoss: 98.670296\n",
      "Train Epoch: 55 [19200/60000 (32%)]\tLoss: 98.218773\n",
      "Train Epoch: 55 [20480/60000 (34%)]\tLoss: 98.070206\n",
      "Train Epoch: 55 [21760/60000 (36%)]\tLoss: 98.786499\n",
      "Train Epoch: 55 [23040/60000 (38%)]\tLoss: 97.457092\n",
      "Train Epoch: 55 [24320/60000 (41%)]\tLoss: 94.576981\n",
      "Train Epoch: 55 [25600/60000 (43%)]\tLoss: 95.987518\n",
      "Train Epoch: 55 [26880/60000 (45%)]\tLoss: 98.829132\n",
      "Train Epoch: 55 [28160/60000 (47%)]\tLoss: 94.984322\n",
      "Train Epoch: 55 [29440/60000 (49%)]\tLoss: 97.739868\n",
      "Train Epoch: 55 [30720/60000 (51%)]\tLoss: 99.438599\n",
      "Train Epoch: 55 [32000/60000 (53%)]\tLoss: 100.476654\n",
      "Train Epoch: 55 [33280/60000 (55%)]\tLoss: 99.526489\n",
      "Train Epoch: 55 [34560/60000 (58%)]\tLoss: 99.055168\n",
      "Train Epoch: 55 [35840/60000 (60%)]\tLoss: 100.345543\n",
      "Train Epoch: 55 [37120/60000 (62%)]\tLoss: 101.547256\n",
      "Train Epoch: 55 [38400/60000 (64%)]\tLoss: 96.098526\n",
      "Train Epoch: 55 [39680/60000 (66%)]\tLoss: 100.292267\n",
      "Train Epoch: 55 [40960/60000 (68%)]\tLoss: 101.164177\n",
      "Train Epoch: 55 [42240/60000 (70%)]\tLoss: 102.429886\n",
      "Train Epoch: 55 [43520/60000 (72%)]\tLoss: 98.493790\n",
      "Train Epoch: 55 [44800/60000 (75%)]\tLoss: 96.344780\n",
      "Train Epoch: 55 [46080/60000 (77%)]\tLoss: 96.870155\n",
      "Train Epoch: 55 [47360/60000 (79%)]\tLoss: 97.101532\n",
      "Train Epoch: 55 [48640/60000 (81%)]\tLoss: 96.357285\n",
      "Train Epoch: 55 [49920/60000 (83%)]\tLoss: 98.303551\n",
      "Train Epoch: 55 [51200/60000 (85%)]\tLoss: 98.804039\n",
      "Train Epoch: 55 [52480/60000 (87%)]\tLoss: 96.666916\n",
      "Train Epoch: 55 [53760/60000 (90%)]\tLoss: 98.516220\n",
      "Train Epoch: 55 [55040/60000 (92%)]\tLoss: 99.570625\n",
      "Train Epoch: 55 [56320/60000 (94%)]\tLoss: 97.390984\n",
      "Train Epoch: 55 [57600/60000 (96%)]\tLoss: 97.349121\n",
      "Train Epoch: 55 [58880/60000 (98%)]\tLoss: 100.354645\n",
      "====> Epoch: 55 Average loss: 98.0540\n",
      "====> Test set loss: 99.8217\n",
      "Train Epoch: 56 [0/60000 (0%)]\tLoss: 98.149422\n",
      "Train Epoch: 56 [1280/60000 (2%)]\tLoss: 97.185287\n",
      "Train Epoch: 56 [2560/60000 (4%)]\tLoss: 102.268456\n",
      "Train Epoch: 56 [3840/60000 (6%)]\tLoss: 99.754410\n",
      "Train Epoch: 56 [5120/60000 (9%)]\tLoss: 100.720032\n",
      "Train Epoch: 56 [6400/60000 (11%)]\tLoss: 96.531250\n",
      "Train Epoch: 56 [7680/60000 (13%)]\tLoss: 97.055840\n",
      "Train Epoch: 56 [8960/60000 (15%)]\tLoss: 95.091568\n",
      "Train Epoch: 56 [10240/60000 (17%)]\tLoss: 99.072037\n",
      "Train Epoch: 56 [11520/60000 (19%)]\tLoss: 97.615372\n",
      "Train Epoch: 56 [12800/60000 (21%)]\tLoss: 104.460388\n",
      "Train Epoch: 56 [14080/60000 (23%)]\tLoss: 94.697090\n",
      "Train Epoch: 56 [15360/60000 (26%)]\tLoss: 96.362152\n",
      "Train Epoch: 56 [16640/60000 (28%)]\tLoss: 96.539047\n",
      "Train Epoch: 56 [17920/60000 (30%)]\tLoss: 98.753799\n",
      "Train Epoch: 56 [19200/60000 (32%)]\tLoss: 98.790665\n",
      "Train Epoch: 56 [20480/60000 (34%)]\tLoss: 96.479904\n",
      "Train Epoch: 56 [21760/60000 (36%)]\tLoss: 95.386963\n",
      "Train Epoch: 56 [23040/60000 (38%)]\tLoss: 95.639816\n",
      "Train Epoch: 56 [24320/60000 (41%)]\tLoss: 97.877174\n",
      "Train Epoch: 56 [25600/60000 (43%)]\tLoss: 97.263405\n",
      "Train Epoch: 56 [26880/60000 (45%)]\tLoss: 102.337029\n",
      "Train Epoch: 56 [28160/60000 (47%)]\tLoss: 98.829086\n",
      "Train Epoch: 56 [29440/60000 (49%)]\tLoss: 102.001793\n",
      "Train Epoch: 56 [30720/60000 (51%)]\tLoss: 96.721848\n",
      "Train Epoch: 56 [32000/60000 (53%)]\tLoss: 95.416779\n",
      "Train Epoch: 56 [33280/60000 (55%)]\tLoss: 99.508057\n",
      "Train Epoch: 56 [34560/60000 (58%)]\tLoss: 102.967773\n",
      "Train Epoch: 56 [35840/60000 (60%)]\tLoss: 88.707245\n",
      "Train Epoch: 56 [37120/60000 (62%)]\tLoss: 99.081993\n",
      "Train Epoch: 56 [38400/60000 (64%)]\tLoss: 99.425323\n",
      "Train Epoch: 56 [39680/60000 (66%)]\tLoss: 96.859459\n",
      "Train Epoch: 56 [40960/60000 (68%)]\tLoss: 96.416962\n",
      "Train Epoch: 56 [42240/60000 (70%)]\tLoss: 100.009361\n",
      "Train Epoch: 56 [43520/60000 (72%)]\tLoss: 97.887222\n",
      "Train Epoch: 56 [44800/60000 (75%)]\tLoss: 95.746315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 56 [46080/60000 (77%)]\tLoss: 98.076355\n",
      "Train Epoch: 56 [47360/60000 (79%)]\tLoss: 98.307587\n",
      "Train Epoch: 56 [48640/60000 (81%)]\tLoss: 100.788376\n",
      "Train Epoch: 56 [49920/60000 (83%)]\tLoss: 97.583138\n",
      "Train Epoch: 56 [51200/60000 (85%)]\tLoss: 98.415688\n",
      "Train Epoch: 56 [52480/60000 (87%)]\tLoss: 95.264282\n",
      "Train Epoch: 56 [53760/60000 (90%)]\tLoss: 100.697784\n",
      "Train Epoch: 56 [55040/60000 (92%)]\tLoss: 96.392471\n",
      "Train Epoch: 56 [56320/60000 (94%)]\tLoss: 98.263504\n",
      "Train Epoch: 56 [57600/60000 (96%)]\tLoss: 93.923431\n",
      "Train Epoch: 56 [58880/60000 (98%)]\tLoss: 100.460052\n",
      "====> Epoch: 56 Average loss: 98.0181\n",
      "====> Test set loss: 99.6651\n",
      "Train Epoch: 57 [0/60000 (0%)]\tLoss: 100.029205\n",
      "Train Epoch: 57 [1280/60000 (2%)]\tLoss: 96.599907\n",
      "Train Epoch: 57 [2560/60000 (4%)]\tLoss: 97.731537\n",
      "Train Epoch: 57 [3840/60000 (6%)]\tLoss: 97.238380\n",
      "Train Epoch: 57 [5120/60000 (9%)]\tLoss: 97.234344\n",
      "Train Epoch: 57 [6400/60000 (11%)]\tLoss: 98.275848\n",
      "Train Epoch: 57 [7680/60000 (13%)]\tLoss: 97.988289\n",
      "Train Epoch: 57 [8960/60000 (15%)]\tLoss: 97.436958\n",
      "Train Epoch: 57 [10240/60000 (17%)]\tLoss: 97.694901\n",
      "Train Epoch: 57 [11520/60000 (19%)]\tLoss: 100.317596\n",
      "Train Epoch: 57 [12800/60000 (21%)]\tLoss: 96.486656\n",
      "Train Epoch: 57 [14080/60000 (23%)]\tLoss: 95.194321\n",
      "Train Epoch: 57 [15360/60000 (26%)]\tLoss: 96.169540\n",
      "Train Epoch: 57 [16640/60000 (28%)]\tLoss: 99.037193\n",
      "Train Epoch: 57 [17920/60000 (30%)]\tLoss: 98.122154\n",
      "Train Epoch: 57 [19200/60000 (32%)]\tLoss: 100.238785\n",
      "Train Epoch: 57 [20480/60000 (34%)]\tLoss: 99.235512\n",
      "Train Epoch: 57 [21760/60000 (36%)]\tLoss: 98.907745\n",
      "Train Epoch: 57 [23040/60000 (38%)]\tLoss: 96.174278\n",
      "Train Epoch: 57 [24320/60000 (41%)]\tLoss: 96.784592\n",
      "Train Epoch: 57 [25600/60000 (43%)]\tLoss: 98.723923\n",
      "Train Epoch: 57 [26880/60000 (45%)]\tLoss: 97.808006\n",
      "Train Epoch: 57 [28160/60000 (47%)]\tLoss: 97.004089\n",
      "Train Epoch: 57 [29440/60000 (49%)]\tLoss: 97.610085\n",
      "Train Epoch: 57 [30720/60000 (51%)]\tLoss: 98.388443\n",
      "Train Epoch: 57 [32000/60000 (53%)]\tLoss: 94.879211\n",
      "Train Epoch: 57 [33280/60000 (55%)]\tLoss: 98.278252\n",
      "Train Epoch: 57 [34560/60000 (58%)]\tLoss: 95.291946\n",
      "Train Epoch: 57 [35840/60000 (60%)]\tLoss: 96.191803\n",
      "Train Epoch: 57 [37120/60000 (62%)]\tLoss: 96.190277\n",
      "Train Epoch: 57 [38400/60000 (64%)]\tLoss: 97.830238\n",
      "Train Epoch: 57 [39680/60000 (66%)]\tLoss: 98.844147\n",
      "Train Epoch: 57 [40960/60000 (68%)]\tLoss: 95.269592\n",
      "Train Epoch: 57 [42240/60000 (70%)]\tLoss: 99.662918\n",
      "Train Epoch: 57 [43520/60000 (72%)]\tLoss: 99.516602\n",
      "Train Epoch: 57 [44800/60000 (75%)]\tLoss: 97.935501\n",
      "Train Epoch: 57 [46080/60000 (77%)]\tLoss: 96.699234\n",
      "Train Epoch: 57 [47360/60000 (79%)]\tLoss: 99.252853\n",
      "Train Epoch: 57 [48640/60000 (81%)]\tLoss: 99.845634\n",
      "Train Epoch: 57 [49920/60000 (83%)]\tLoss: 96.381744\n",
      "Train Epoch: 57 [51200/60000 (85%)]\tLoss: 97.589096\n",
      "Train Epoch: 57 [52480/60000 (87%)]\tLoss: 93.870529\n",
      "Train Epoch: 57 [53760/60000 (90%)]\tLoss: 100.992355\n",
      "Train Epoch: 57 [55040/60000 (92%)]\tLoss: 94.541916\n",
      "Train Epoch: 57 [56320/60000 (94%)]\tLoss: 97.441055\n",
      "Train Epoch: 57 [57600/60000 (96%)]\tLoss: 93.964096\n",
      "Train Epoch: 57 [58880/60000 (98%)]\tLoss: 100.288528\n",
      "====> Epoch: 57 Average loss: 97.9367\n",
      "====> Test set loss: 99.6775\n",
      "Train Epoch: 58 [0/60000 (0%)]\tLoss: 97.010239\n",
      "Train Epoch: 58 [1280/60000 (2%)]\tLoss: 100.082504\n",
      "Train Epoch: 58 [2560/60000 (4%)]\tLoss: 93.479813\n",
      "Train Epoch: 58 [3840/60000 (6%)]\tLoss: 99.757271\n",
      "Train Epoch: 58 [5120/60000 (9%)]\tLoss: 102.025284\n",
      "Train Epoch: 58 [6400/60000 (11%)]\tLoss: 98.713730\n",
      "Train Epoch: 58 [7680/60000 (13%)]\tLoss: 103.332420\n",
      "Train Epoch: 58 [8960/60000 (15%)]\tLoss: 98.661003\n",
      "Train Epoch: 58 [10240/60000 (17%)]\tLoss: 94.257538\n",
      "Train Epoch: 58 [11520/60000 (19%)]\tLoss: 95.692757\n",
      "Train Epoch: 58 [12800/60000 (21%)]\tLoss: 99.522568\n",
      "Train Epoch: 58 [14080/60000 (23%)]\tLoss: 98.470192\n",
      "Train Epoch: 58 [15360/60000 (26%)]\tLoss: 93.415329\n",
      "Train Epoch: 58 [16640/60000 (28%)]\tLoss: 101.631981\n",
      "Train Epoch: 58 [17920/60000 (30%)]\tLoss: 98.464417\n",
      "Train Epoch: 58 [19200/60000 (32%)]\tLoss: 91.577026\n",
      "Train Epoch: 58 [20480/60000 (34%)]\tLoss: 98.255898\n",
      "Train Epoch: 58 [21760/60000 (36%)]\tLoss: 98.146477\n",
      "Train Epoch: 58 [23040/60000 (38%)]\tLoss: 101.717598\n",
      "Train Epoch: 58 [24320/60000 (41%)]\tLoss: 98.896957\n",
      "Train Epoch: 58 [25600/60000 (43%)]\tLoss: 96.954346\n",
      "Train Epoch: 58 [26880/60000 (45%)]\tLoss: 95.831009\n",
      "Train Epoch: 58 [28160/60000 (47%)]\tLoss: 97.054001\n",
      "Train Epoch: 58 [29440/60000 (49%)]\tLoss: 98.655716\n",
      "Train Epoch: 58 [30720/60000 (51%)]\tLoss: 96.926765\n",
      "Train Epoch: 58 [32000/60000 (53%)]\tLoss: 95.488922\n",
      "Train Epoch: 58 [33280/60000 (55%)]\tLoss: 98.555405\n",
      "Train Epoch: 58 [34560/60000 (58%)]\tLoss: 101.928169\n",
      "Train Epoch: 58 [35840/60000 (60%)]\tLoss: 96.410187\n",
      "Train Epoch: 58 [37120/60000 (62%)]\tLoss: 92.724533\n",
      "Train Epoch: 58 [38400/60000 (64%)]\tLoss: 98.444748\n",
      "Train Epoch: 58 [39680/60000 (66%)]\tLoss: 96.652878\n",
      "Train Epoch: 58 [40960/60000 (68%)]\tLoss: 99.108246\n",
      "Train Epoch: 58 [42240/60000 (70%)]\tLoss: 93.571762\n",
      "Train Epoch: 58 [43520/60000 (72%)]\tLoss: 95.306885\n",
      "Train Epoch: 58 [44800/60000 (75%)]\tLoss: 99.302948\n",
      "Train Epoch: 58 [46080/60000 (77%)]\tLoss: 103.335678\n",
      "Train Epoch: 58 [47360/60000 (79%)]\tLoss: 99.654861\n",
      "Train Epoch: 58 [48640/60000 (81%)]\tLoss: 101.454849\n",
      "Train Epoch: 58 [49920/60000 (83%)]\tLoss: 96.133171\n",
      "Train Epoch: 58 [51200/60000 (85%)]\tLoss: 96.202332\n",
      "Train Epoch: 58 [52480/60000 (87%)]\tLoss: 98.166718\n",
      "Train Epoch: 58 [53760/60000 (90%)]\tLoss: 97.400803\n",
      "Train Epoch: 58 [55040/60000 (92%)]\tLoss: 98.360535\n",
      "Train Epoch: 58 [56320/60000 (94%)]\tLoss: 97.743546\n",
      "Train Epoch: 58 [57600/60000 (96%)]\tLoss: 97.447189\n",
      "Train Epoch: 58 [58880/60000 (98%)]\tLoss: 97.137321\n",
      "====> Epoch: 58 Average loss: 97.9230\n",
      "====> Test set loss: 99.8521\n",
      "Train Epoch: 59 [0/60000 (0%)]\tLoss: 101.574005\n",
      "Train Epoch: 59 [1280/60000 (2%)]\tLoss: 99.169006\n",
      "Train Epoch: 59 [2560/60000 (4%)]\tLoss: 98.317978\n",
      "Train Epoch: 59 [3840/60000 (6%)]\tLoss: 104.188080\n",
      "Train Epoch: 59 [5120/60000 (9%)]\tLoss: 96.383011\n",
      "Train Epoch: 59 [6400/60000 (11%)]\tLoss: 99.451813\n",
      "Train Epoch: 59 [7680/60000 (13%)]\tLoss: 99.664726\n",
      "Train Epoch: 59 [8960/60000 (15%)]\tLoss: 98.304581\n",
      "Train Epoch: 59 [10240/60000 (17%)]\tLoss: 95.336121\n",
      "Train Epoch: 59 [11520/60000 (19%)]\tLoss: 99.128838\n",
      "Train Epoch: 59 [12800/60000 (21%)]\tLoss: 97.708290\n",
      "Train Epoch: 59 [14080/60000 (23%)]\tLoss: 96.513039\n",
      "Train Epoch: 59 [15360/60000 (26%)]\tLoss: 98.839798\n",
      "Train Epoch: 59 [16640/60000 (28%)]\tLoss: 98.482773\n",
      "Train Epoch: 59 [17920/60000 (30%)]\tLoss: 96.100098\n",
      "Train Epoch: 59 [19200/60000 (32%)]\tLoss: 96.380501\n",
      "Train Epoch: 59 [20480/60000 (34%)]\tLoss: 101.418800\n",
      "Train Epoch: 59 [21760/60000 (36%)]\tLoss: 98.764023\n",
      "Train Epoch: 59 [23040/60000 (38%)]\tLoss: 97.795097\n",
      "Train Epoch: 59 [24320/60000 (41%)]\tLoss: 97.158829\n",
      "Train Epoch: 59 [25600/60000 (43%)]\tLoss: 102.368149\n",
      "Train Epoch: 59 [26880/60000 (45%)]\tLoss: 99.082306\n",
      "Train Epoch: 59 [28160/60000 (47%)]\tLoss: 97.563217\n",
      "Train Epoch: 59 [29440/60000 (49%)]\tLoss: 99.061310\n",
      "Train Epoch: 59 [30720/60000 (51%)]\tLoss: 97.388771\n",
      "Train Epoch: 59 [32000/60000 (53%)]\tLoss: 99.669235\n",
      "Train Epoch: 59 [33280/60000 (55%)]\tLoss: 94.254570\n",
      "Train Epoch: 59 [34560/60000 (58%)]\tLoss: 96.108124\n",
      "Train Epoch: 59 [35840/60000 (60%)]\tLoss: 101.327164\n",
      "Train Epoch: 59 [37120/60000 (62%)]\tLoss: 96.466583\n",
      "Train Epoch: 59 [38400/60000 (64%)]\tLoss: 99.159073\n",
      "Train Epoch: 59 [39680/60000 (66%)]\tLoss: 94.752762\n",
      "Train Epoch: 59 [40960/60000 (68%)]\tLoss: 95.861443\n",
      "Train Epoch: 59 [42240/60000 (70%)]\tLoss: 95.953781\n",
      "Train Epoch: 59 [43520/60000 (72%)]\tLoss: 99.000206\n",
      "Train Epoch: 59 [44800/60000 (75%)]\tLoss: 98.015457\n",
      "Train Epoch: 59 [46080/60000 (77%)]\tLoss: 96.596947\n",
      "Train Epoch: 59 [47360/60000 (79%)]\tLoss: 95.975876\n",
      "Train Epoch: 59 [48640/60000 (81%)]\tLoss: 102.326355\n",
      "Train Epoch: 59 [49920/60000 (83%)]\tLoss: 100.534653\n",
      "Train Epoch: 59 [51200/60000 (85%)]\tLoss: 99.971146\n",
      "Train Epoch: 59 [52480/60000 (87%)]\tLoss: 97.925507\n",
      "Train Epoch: 59 [53760/60000 (90%)]\tLoss: 97.852585\n",
      "Train Epoch: 59 [55040/60000 (92%)]\tLoss: 98.416199\n",
      "Train Epoch: 59 [56320/60000 (94%)]\tLoss: 93.432961\n",
      "Train Epoch: 59 [57600/60000 (96%)]\tLoss: 96.067520\n",
      "Train Epoch: 59 [58880/60000 (98%)]\tLoss: 96.855614\n",
      "====> Epoch: 59 Average loss: 97.8751\n",
      "====> Test set loss: 99.7096\n",
      "Train Epoch: 60 [0/60000 (0%)]\tLoss: 96.459503\n",
      "Train Epoch: 60 [1280/60000 (2%)]\tLoss: 93.000168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 60 [2560/60000 (4%)]\tLoss: 97.748688\n",
      "Train Epoch: 60 [3840/60000 (6%)]\tLoss: 97.472672\n",
      "Train Epoch: 60 [5120/60000 (9%)]\tLoss: 98.711464\n",
      "Train Epoch: 60 [6400/60000 (11%)]\tLoss: 95.648399\n",
      "Train Epoch: 60 [7680/60000 (13%)]\tLoss: 97.715141\n",
      "Train Epoch: 60 [8960/60000 (15%)]\tLoss: 98.110916\n",
      "Train Epoch: 60 [10240/60000 (17%)]\tLoss: 98.862228\n",
      "Train Epoch: 60 [11520/60000 (19%)]\tLoss: 98.845345\n",
      "Train Epoch: 60 [12800/60000 (21%)]\tLoss: 98.830833\n",
      "Train Epoch: 60 [14080/60000 (23%)]\tLoss: 99.593529\n",
      "Train Epoch: 60 [15360/60000 (26%)]\tLoss: 98.651688\n",
      "Train Epoch: 60 [16640/60000 (28%)]\tLoss: 98.869675\n",
      "Train Epoch: 60 [17920/60000 (30%)]\tLoss: 101.638626\n",
      "Train Epoch: 60 [19200/60000 (32%)]\tLoss: 96.878220\n",
      "Train Epoch: 60 [20480/60000 (34%)]\tLoss: 100.204704\n",
      "Train Epoch: 60 [21760/60000 (36%)]\tLoss: 98.517044\n",
      "Train Epoch: 60 [23040/60000 (38%)]\tLoss: 96.069519\n",
      "Train Epoch: 60 [24320/60000 (41%)]\tLoss: 101.054558\n",
      "Train Epoch: 60 [25600/60000 (43%)]\tLoss: 100.100479\n",
      "Train Epoch: 60 [26880/60000 (45%)]\tLoss: 97.167465\n",
      "Train Epoch: 60 [28160/60000 (47%)]\tLoss: 98.476379\n",
      "Train Epoch: 60 [29440/60000 (49%)]\tLoss: 99.165443\n",
      "Train Epoch: 60 [30720/60000 (51%)]\tLoss: 100.217361\n",
      "Train Epoch: 60 [32000/60000 (53%)]\tLoss: 101.536140\n",
      "Train Epoch: 60 [33280/60000 (55%)]\tLoss: 97.751419\n",
      "Train Epoch: 60 [34560/60000 (58%)]\tLoss: 98.336319\n",
      "Train Epoch: 60 [35840/60000 (60%)]\tLoss: 99.597031\n",
      "Train Epoch: 60 [37120/60000 (62%)]\tLoss: 96.593445\n",
      "Train Epoch: 60 [38400/60000 (64%)]\tLoss: 95.483871\n",
      "Train Epoch: 60 [39680/60000 (66%)]\tLoss: 98.286041\n",
      "Train Epoch: 60 [40960/60000 (68%)]\tLoss: 95.015869\n",
      "Train Epoch: 60 [42240/60000 (70%)]\tLoss: 96.829727\n",
      "Train Epoch: 60 [43520/60000 (72%)]\tLoss: 102.395424\n",
      "Train Epoch: 60 [44800/60000 (75%)]\tLoss: 98.217369\n",
      "Train Epoch: 60 [46080/60000 (77%)]\tLoss: 98.765953\n",
      "Train Epoch: 60 [47360/60000 (79%)]\tLoss: 97.586288\n",
      "Train Epoch: 60 [48640/60000 (81%)]\tLoss: 95.286469\n",
      "Train Epoch: 60 [49920/60000 (83%)]\tLoss: 102.174187\n",
      "Train Epoch: 60 [51200/60000 (85%)]\tLoss: 94.760040\n",
      "Train Epoch: 60 [52480/60000 (87%)]\tLoss: 100.740906\n",
      "Train Epoch: 60 [53760/60000 (90%)]\tLoss: 96.233932\n",
      "Train Epoch: 60 [55040/60000 (92%)]\tLoss: 98.276199\n",
      "Train Epoch: 60 [56320/60000 (94%)]\tLoss: 98.501396\n",
      "Train Epoch: 60 [57600/60000 (96%)]\tLoss: 97.844139\n",
      "Train Epoch: 60 [58880/60000 (98%)]\tLoss: 97.474121\n",
      "====> Epoch: 60 Average loss: 97.8341\n",
      "====> Test set loss: 99.7084\n",
      "Train Epoch: 61 [0/60000 (0%)]\tLoss: 95.809326\n",
      "Train Epoch: 61 [1280/60000 (2%)]\tLoss: 99.787605\n",
      "Train Epoch: 61 [2560/60000 (4%)]\tLoss: 98.623711\n",
      "Train Epoch: 61 [3840/60000 (6%)]\tLoss: 94.802422\n",
      "Train Epoch: 61 [5120/60000 (9%)]\tLoss: 100.138626\n",
      "Train Epoch: 61 [6400/60000 (11%)]\tLoss: 98.049347\n",
      "Train Epoch: 61 [7680/60000 (13%)]\tLoss: 95.188225\n",
      "Train Epoch: 61 [8960/60000 (15%)]\tLoss: 95.922508\n",
      "Train Epoch: 61 [10240/60000 (17%)]\tLoss: 99.278381\n",
      "Train Epoch: 61 [11520/60000 (19%)]\tLoss: 100.191628\n",
      "Train Epoch: 61 [12800/60000 (21%)]\tLoss: 101.001068\n",
      "Train Epoch: 61 [14080/60000 (23%)]\tLoss: 97.897263\n",
      "Train Epoch: 61 [15360/60000 (26%)]\tLoss: 105.418060\n",
      "Train Epoch: 61 [16640/60000 (28%)]\tLoss: 97.751343\n",
      "Train Epoch: 61 [17920/60000 (30%)]\tLoss: 98.575356\n",
      "Train Epoch: 61 [19200/60000 (32%)]\tLoss: 96.522202\n",
      "Train Epoch: 61 [20480/60000 (34%)]\tLoss: 99.292946\n",
      "Train Epoch: 61 [21760/60000 (36%)]\tLoss: 98.548096\n",
      "Train Epoch: 61 [23040/60000 (38%)]\tLoss: 98.502274\n",
      "Train Epoch: 61 [24320/60000 (41%)]\tLoss: 100.321327\n",
      "Train Epoch: 61 [25600/60000 (43%)]\tLoss: 97.832825\n",
      "Train Epoch: 61 [26880/60000 (45%)]\tLoss: 99.548355\n",
      "Train Epoch: 61 [28160/60000 (47%)]\tLoss: 96.229294\n",
      "Train Epoch: 61 [29440/60000 (49%)]\tLoss: 97.306519\n",
      "Train Epoch: 61 [30720/60000 (51%)]\tLoss: 98.205551\n",
      "Train Epoch: 61 [32000/60000 (53%)]\tLoss: 96.605011\n",
      "Train Epoch: 61 [33280/60000 (55%)]\tLoss: 102.868149\n",
      "Train Epoch: 61 [34560/60000 (58%)]\tLoss: 94.372849\n",
      "Train Epoch: 61 [35840/60000 (60%)]\tLoss: 92.210915\n",
      "Train Epoch: 61 [37120/60000 (62%)]\tLoss: 95.992172\n",
      "Train Epoch: 61 [38400/60000 (64%)]\tLoss: 98.393890\n",
      "Train Epoch: 61 [39680/60000 (66%)]\tLoss: 94.932465\n",
      "Train Epoch: 61 [40960/60000 (68%)]\tLoss: 101.087135\n",
      "Train Epoch: 61 [42240/60000 (70%)]\tLoss: 94.152168\n",
      "Train Epoch: 61 [43520/60000 (72%)]\tLoss: 98.132889\n",
      "Train Epoch: 61 [44800/60000 (75%)]\tLoss: 95.461571\n",
      "Train Epoch: 61 [46080/60000 (77%)]\tLoss: 96.413925\n",
      "Train Epoch: 61 [47360/60000 (79%)]\tLoss: 98.823936\n",
      "Train Epoch: 61 [48640/60000 (81%)]\tLoss: 105.197189\n",
      "Train Epoch: 61 [49920/60000 (83%)]\tLoss: 96.412384\n",
      "Train Epoch: 61 [51200/60000 (85%)]\tLoss: 96.299774\n",
      "Train Epoch: 61 [52480/60000 (87%)]\tLoss: 91.240189\n",
      "Train Epoch: 61 [53760/60000 (90%)]\tLoss: 97.120743\n",
      "Train Epoch: 61 [55040/60000 (92%)]\tLoss: 100.004097\n",
      "Train Epoch: 61 [56320/60000 (94%)]\tLoss: 94.426895\n",
      "Train Epoch: 61 [57600/60000 (96%)]\tLoss: 100.774254\n",
      "Train Epoch: 61 [58880/60000 (98%)]\tLoss: 100.207565\n",
      "====> Epoch: 61 Average loss: 97.8035\n",
      "====> Test set loss: 99.8080\n",
      "Train Epoch: 62 [0/60000 (0%)]\tLoss: 95.059868\n",
      "Train Epoch: 62 [1280/60000 (2%)]\tLoss: 98.911423\n",
      "Train Epoch: 62 [2560/60000 (4%)]\tLoss: 95.477295\n",
      "Train Epoch: 62 [3840/60000 (6%)]\tLoss: 96.853989\n",
      "Train Epoch: 62 [5120/60000 (9%)]\tLoss: 99.817810\n",
      "Train Epoch: 62 [6400/60000 (11%)]\tLoss: 97.360214\n",
      "Train Epoch: 62 [7680/60000 (13%)]\tLoss: 97.187904\n",
      "Train Epoch: 62 [8960/60000 (15%)]\tLoss: 101.570862\n",
      "Train Epoch: 62 [10240/60000 (17%)]\tLoss: 93.991447\n",
      "Train Epoch: 62 [11520/60000 (19%)]\tLoss: 100.397789\n",
      "Train Epoch: 62 [12800/60000 (21%)]\tLoss: 95.085419\n",
      "Train Epoch: 62 [14080/60000 (23%)]\tLoss: 100.715355\n",
      "Train Epoch: 62 [15360/60000 (26%)]\tLoss: 96.480125\n",
      "Train Epoch: 62 [16640/60000 (28%)]\tLoss: 96.147148\n",
      "Train Epoch: 62 [17920/60000 (30%)]\tLoss: 98.314552\n",
      "Train Epoch: 62 [19200/60000 (32%)]\tLoss: 99.744507\n",
      "Train Epoch: 62 [20480/60000 (34%)]\tLoss: 94.762756\n",
      "Train Epoch: 62 [21760/60000 (36%)]\tLoss: 98.287201\n",
      "Train Epoch: 62 [23040/60000 (38%)]\tLoss: 96.004509\n",
      "Train Epoch: 62 [24320/60000 (41%)]\tLoss: 95.947533\n",
      "Train Epoch: 62 [25600/60000 (43%)]\tLoss: 96.627106\n",
      "Train Epoch: 62 [26880/60000 (45%)]\tLoss: 97.841835\n",
      "Train Epoch: 62 [28160/60000 (47%)]\tLoss: 98.923584\n",
      "Train Epoch: 62 [29440/60000 (49%)]\tLoss: 98.700607\n",
      "Train Epoch: 62 [30720/60000 (51%)]\tLoss: 98.960449\n",
      "Train Epoch: 62 [32000/60000 (53%)]\tLoss: 95.362183\n",
      "Train Epoch: 62 [33280/60000 (55%)]\tLoss: 98.974739\n",
      "Train Epoch: 62 [34560/60000 (58%)]\tLoss: 100.215736\n",
      "Train Epoch: 62 [35840/60000 (60%)]\tLoss: 99.869896\n",
      "Train Epoch: 62 [37120/60000 (62%)]\tLoss: 92.746307\n",
      "Train Epoch: 62 [38400/60000 (64%)]\tLoss: 99.889038\n",
      "Train Epoch: 62 [39680/60000 (66%)]\tLoss: 97.685471\n",
      "Train Epoch: 62 [40960/60000 (68%)]\tLoss: 97.983818\n",
      "Train Epoch: 62 [42240/60000 (70%)]\tLoss: 99.398590\n",
      "Train Epoch: 62 [43520/60000 (72%)]\tLoss: 97.329605\n",
      "Train Epoch: 62 [44800/60000 (75%)]\tLoss: 94.636826\n",
      "Train Epoch: 62 [46080/60000 (77%)]\tLoss: 101.652367\n",
      "Train Epoch: 62 [47360/60000 (79%)]\tLoss: 98.357834\n",
      "Train Epoch: 62 [48640/60000 (81%)]\tLoss: 97.801941\n",
      "Train Epoch: 62 [49920/60000 (83%)]\tLoss: 96.963768\n",
      "Train Epoch: 62 [51200/60000 (85%)]\tLoss: 96.428993\n",
      "Train Epoch: 62 [52480/60000 (87%)]\tLoss: 95.514923\n",
      "Train Epoch: 62 [53760/60000 (90%)]\tLoss: 100.540054\n",
      "Train Epoch: 62 [55040/60000 (92%)]\tLoss: 96.542664\n",
      "Train Epoch: 62 [56320/60000 (94%)]\tLoss: 95.009766\n",
      "Train Epoch: 62 [57600/60000 (96%)]\tLoss: 100.974075\n",
      "Train Epoch: 62 [58880/60000 (98%)]\tLoss: 97.337494\n",
      "====> Epoch: 62 Average loss: 97.7938\n",
      "====> Test set loss: 99.8251\n",
      "Train Epoch: 63 [0/60000 (0%)]\tLoss: 92.481300\n",
      "Train Epoch: 63 [1280/60000 (2%)]\tLoss: 98.071144\n",
      "Train Epoch: 63 [2560/60000 (4%)]\tLoss: 100.066391\n",
      "Train Epoch: 63 [3840/60000 (6%)]\tLoss: 99.757126\n",
      "Train Epoch: 63 [5120/60000 (9%)]\tLoss: 95.146721\n",
      "Train Epoch: 63 [6400/60000 (11%)]\tLoss: 95.577423\n",
      "Train Epoch: 63 [7680/60000 (13%)]\tLoss: 94.671066\n",
      "Train Epoch: 63 [8960/60000 (15%)]\tLoss: 98.778648\n",
      "Train Epoch: 63 [10240/60000 (17%)]\tLoss: 94.592453\n",
      "Train Epoch: 63 [11520/60000 (19%)]\tLoss: 96.614006\n",
      "Train Epoch: 63 [12800/60000 (21%)]\tLoss: 98.560783\n",
      "Train Epoch: 63 [14080/60000 (23%)]\tLoss: 100.139069\n",
      "Train Epoch: 63 [15360/60000 (26%)]\tLoss: 98.038574\n",
      "Train Epoch: 63 [16640/60000 (28%)]\tLoss: 99.970795\n",
      "Train Epoch: 63 [17920/60000 (30%)]\tLoss: 98.686531\n",
      "Train Epoch: 63 [19200/60000 (32%)]\tLoss: 103.140648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 63 [20480/60000 (34%)]\tLoss: 98.821640\n",
      "Train Epoch: 63 [21760/60000 (36%)]\tLoss: 93.823166\n",
      "Train Epoch: 63 [23040/60000 (38%)]\tLoss: 102.826431\n",
      "Train Epoch: 63 [24320/60000 (41%)]\tLoss: 96.915985\n",
      "Train Epoch: 63 [25600/60000 (43%)]\tLoss: 98.775482\n",
      "Train Epoch: 63 [26880/60000 (45%)]\tLoss: 101.824348\n",
      "Train Epoch: 63 [28160/60000 (47%)]\tLoss: 101.131706\n",
      "Train Epoch: 63 [29440/60000 (49%)]\tLoss: 96.517662\n",
      "Train Epoch: 63 [30720/60000 (51%)]\tLoss: 98.355537\n",
      "Train Epoch: 63 [32000/60000 (53%)]\tLoss: 97.122749\n",
      "Train Epoch: 63 [33280/60000 (55%)]\tLoss: 98.657043\n",
      "Train Epoch: 63 [34560/60000 (58%)]\tLoss: 94.632141\n",
      "Train Epoch: 63 [35840/60000 (60%)]\tLoss: 98.476242\n",
      "Train Epoch: 63 [37120/60000 (62%)]\tLoss: 99.645767\n",
      "Train Epoch: 63 [38400/60000 (64%)]\tLoss: 101.250023\n",
      "Train Epoch: 63 [39680/60000 (66%)]\tLoss: 100.935799\n",
      "Train Epoch: 63 [40960/60000 (68%)]\tLoss: 98.016663\n",
      "Train Epoch: 63 [42240/60000 (70%)]\tLoss: 96.660728\n",
      "Train Epoch: 63 [43520/60000 (72%)]\tLoss: 98.028320\n",
      "Train Epoch: 63 [44800/60000 (75%)]\tLoss: 99.677429\n",
      "Train Epoch: 63 [46080/60000 (77%)]\tLoss: 100.198608\n",
      "Train Epoch: 63 [47360/60000 (79%)]\tLoss: 98.423950\n",
      "Train Epoch: 63 [48640/60000 (81%)]\tLoss: 95.432144\n",
      "Train Epoch: 63 [49920/60000 (83%)]\tLoss: 99.689362\n",
      "Train Epoch: 63 [51200/60000 (85%)]\tLoss: 98.363647\n",
      "Train Epoch: 63 [52480/60000 (87%)]\tLoss: 96.006706\n",
      "Train Epoch: 63 [53760/60000 (90%)]\tLoss: 97.854225\n",
      "Train Epoch: 63 [55040/60000 (92%)]\tLoss: 99.744087\n",
      "Train Epoch: 63 [56320/60000 (94%)]\tLoss: 103.429375\n",
      "Train Epoch: 63 [57600/60000 (96%)]\tLoss: 95.492447\n",
      "Train Epoch: 63 [58880/60000 (98%)]\tLoss: 96.890945\n",
      "====> Epoch: 63 Average loss: 97.7758\n",
      "====> Test set loss: 99.7492\n",
      "Train Epoch: 64 [0/60000 (0%)]\tLoss: 95.230469\n",
      "Train Epoch: 64 [1280/60000 (2%)]\tLoss: 98.026764\n",
      "Train Epoch: 64 [2560/60000 (4%)]\tLoss: 101.677254\n",
      "Train Epoch: 64 [3840/60000 (6%)]\tLoss: 95.825699\n",
      "Train Epoch: 64 [5120/60000 (9%)]\tLoss: 93.837967\n",
      "Train Epoch: 64 [6400/60000 (11%)]\tLoss: 93.635712\n",
      "Train Epoch: 64 [7680/60000 (13%)]\tLoss: 98.482742\n",
      "Train Epoch: 64 [8960/60000 (15%)]\tLoss: 99.275574\n",
      "Train Epoch: 64 [10240/60000 (17%)]\tLoss: 96.516335\n",
      "Train Epoch: 64 [11520/60000 (19%)]\tLoss: 100.616333\n",
      "Train Epoch: 64 [12800/60000 (21%)]\tLoss: 96.449493\n",
      "Train Epoch: 64 [14080/60000 (23%)]\tLoss: 95.557114\n",
      "Train Epoch: 64 [15360/60000 (26%)]\tLoss: 95.862305\n",
      "Train Epoch: 64 [16640/60000 (28%)]\tLoss: 98.049011\n",
      "Train Epoch: 64 [17920/60000 (30%)]\tLoss: 97.765869\n",
      "Train Epoch: 64 [19200/60000 (32%)]\tLoss: 100.649498\n",
      "Train Epoch: 64 [20480/60000 (34%)]\tLoss: 95.772461\n",
      "Train Epoch: 64 [21760/60000 (36%)]\tLoss: 96.973495\n",
      "Train Epoch: 64 [23040/60000 (38%)]\tLoss: 99.819115\n",
      "Train Epoch: 64 [24320/60000 (41%)]\tLoss: 97.481354\n",
      "Train Epoch: 64 [25600/60000 (43%)]\tLoss: 96.452316\n",
      "Train Epoch: 64 [26880/60000 (45%)]\tLoss: 100.523590\n",
      "Train Epoch: 64 [28160/60000 (47%)]\tLoss: 95.781197\n",
      "Train Epoch: 64 [29440/60000 (49%)]\tLoss: 99.988121\n",
      "Train Epoch: 64 [30720/60000 (51%)]\tLoss: 97.211189\n",
      "Train Epoch: 64 [32000/60000 (53%)]\tLoss: 98.415092\n",
      "Train Epoch: 64 [33280/60000 (55%)]\tLoss: 96.983780\n",
      "Train Epoch: 64 [34560/60000 (58%)]\tLoss: 99.327957\n",
      "Train Epoch: 64 [35840/60000 (60%)]\tLoss: 97.828903\n",
      "Train Epoch: 64 [37120/60000 (62%)]\tLoss: 99.268471\n",
      "Train Epoch: 64 [38400/60000 (64%)]\tLoss: 98.635078\n",
      "Train Epoch: 64 [39680/60000 (66%)]\tLoss: 98.580711\n",
      "Train Epoch: 64 [40960/60000 (68%)]\tLoss: 100.255920\n",
      "Train Epoch: 64 [42240/60000 (70%)]\tLoss: 97.384033\n",
      "Train Epoch: 64 [43520/60000 (72%)]\tLoss: 96.683792\n",
      "Train Epoch: 64 [44800/60000 (75%)]\tLoss: 101.720078\n",
      "Train Epoch: 64 [46080/60000 (77%)]\tLoss: 99.312202\n",
      "Train Epoch: 64 [47360/60000 (79%)]\tLoss: 96.861145\n",
      "Train Epoch: 64 [48640/60000 (81%)]\tLoss: 94.308823\n",
      "Train Epoch: 64 [49920/60000 (83%)]\tLoss: 98.083466\n",
      "Train Epoch: 64 [51200/60000 (85%)]\tLoss: 95.666809\n",
      "Train Epoch: 64 [52480/60000 (87%)]\tLoss: 100.711060\n",
      "Train Epoch: 64 [53760/60000 (90%)]\tLoss: 96.322472\n",
      "Train Epoch: 64 [55040/60000 (92%)]\tLoss: 99.953522\n",
      "Train Epoch: 64 [56320/60000 (94%)]\tLoss: 96.087402\n",
      "Train Epoch: 64 [57600/60000 (96%)]\tLoss: 99.669449\n",
      "Train Epoch: 64 [58880/60000 (98%)]\tLoss: 102.329315\n",
      "====> Epoch: 64 Average loss: 97.6985\n",
      "====> Test set loss: 99.8284\n",
      "Train Epoch: 65 [0/60000 (0%)]\tLoss: 97.334091\n",
      "Train Epoch: 65 [1280/60000 (2%)]\tLoss: 100.204315\n",
      "Train Epoch: 65 [2560/60000 (4%)]\tLoss: 92.815575\n",
      "Train Epoch: 65 [3840/60000 (6%)]\tLoss: 97.682663\n",
      "Train Epoch: 65 [5120/60000 (9%)]\tLoss: 98.337120\n",
      "Train Epoch: 65 [6400/60000 (11%)]\tLoss: 93.851952\n",
      "Train Epoch: 65 [7680/60000 (13%)]\tLoss: 98.030403\n",
      "Train Epoch: 65 [8960/60000 (15%)]\tLoss: 98.290939\n",
      "Train Epoch: 65 [10240/60000 (17%)]\tLoss: 97.094589\n",
      "Train Epoch: 65 [11520/60000 (19%)]\tLoss: 102.687065\n",
      "Train Epoch: 65 [12800/60000 (21%)]\tLoss: 95.749130\n",
      "Train Epoch: 65 [14080/60000 (23%)]\tLoss: 98.648712\n",
      "Train Epoch: 65 [15360/60000 (26%)]\tLoss: 97.977936\n",
      "Train Epoch: 65 [16640/60000 (28%)]\tLoss: 96.926071\n",
      "Train Epoch: 65 [17920/60000 (30%)]\tLoss: 98.956688\n",
      "Train Epoch: 65 [19200/60000 (32%)]\tLoss: 97.066231\n",
      "Train Epoch: 65 [20480/60000 (34%)]\tLoss: 98.145355\n",
      "Train Epoch: 65 [21760/60000 (36%)]\tLoss: 98.474915\n",
      "Train Epoch: 65 [23040/60000 (38%)]\tLoss: 92.137779\n",
      "Train Epoch: 65 [24320/60000 (41%)]\tLoss: 98.642212\n",
      "Train Epoch: 65 [25600/60000 (43%)]\tLoss: 97.790878\n",
      "Train Epoch: 65 [26880/60000 (45%)]\tLoss: 96.957275\n",
      "Train Epoch: 65 [28160/60000 (47%)]\tLoss: 98.711647\n",
      "Train Epoch: 65 [29440/60000 (49%)]\tLoss: 95.179001\n",
      "Train Epoch: 65 [30720/60000 (51%)]\tLoss: 99.417938\n",
      "Train Epoch: 65 [32000/60000 (53%)]\tLoss: 96.163681\n",
      "Train Epoch: 65 [33280/60000 (55%)]\tLoss: 99.123268\n",
      "Train Epoch: 65 [34560/60000 (58%)]\tLoss: 97.480331\n",
      "Train Epoch: 65 [35840/60000 (60%)]\tLoss: 97.169327\n",
      "Train Epoch: 65 [37120/60000 (62%)]\tLoss: 92.917755\n",
      "Train Epoch: 65 [38400/60000 (64%)]\tLoss: 96.656715\n",
      "Train Epoch: 65 [39680/60000 (66%)]\tLoss: 99.199921\n",
      "Train Epoch: 65 [40960/60000 (68%)]\tLoss: 98.257118\n",
      "Train Epoch: 65 [42240/60000 (70%)]\tLoss: 95.697968\n",
      "Train Epoch: 65 [43520/60000 (72%)]\tLoss: 101.621964\n",
      "Train Epoch: 65 [44800/60000 (75%)]\tLoss: 94.322083\n",
      "Train Epoch: 65 [46080/60000 (77%)]\tLoss: 103.788605\n",
      "Train Epoch: 65 [47360/60000 (79%)]\tLoss: 94.081360\n",
      "Train Epoch: 65 [48640/60000 (81%)]\tLoss: 99.913330\n",
      "Train Epoch: 65 [49920/60000 (83%)]\tLoss: 98.248688\n",
      "Train Epoch: 65 [51200/60000 (85%)]\tLoss: 98.880692\n",
      "Train Epoch: 65 [52480/60000 (87%)]\tLoss: 92.334885\n",
      "Train Epoch: 65 [53760/60000 (90%)]\tLoss: 96.664246\n",
      "Train Epoch: 65 [55040/60000 (92%)]\tLoss: 93.983124\n",
      "Train Epoch: 65 [56320/60000 (94%)]\tLoss: 93.986084\n",
      "Train Epoch: 65 [57600/60000 (96%)]\tLoss: 96.511978\n",
      "Train Epoch: 65 [58880/60000 (98%)]\tLoss: 93.043022\n",
      "====> Epoch: 65 Average loss: 97.6843\n",
      "====> Test set loss: 99.5167\n",
      "Train Epoch: 66 [0/60000 (0%)]\tLoss: 98.972160\n",
      "Train Epoch: 66 [1280/60000 (2%)]\tLoss: 94.551208\n",
      "Train Epoch: 66 [2560/60000 (4%)]\tLoss: 94.880440\n",
      "Train Epoch: 66 [3840/60000 (6%)]\tLoss: 97.764519\n",
      "Train Epoch: 66 [5120/60000 (9%)]\tLoss: 96.793007\n",
      "Train Epoch: 66 [6400/60000 (11%)]\tLoss: 96.877686\n",
      "Train Epoch: 66 [7680/60000 (13%)]\tLoss: 98.607727\n",
      "Train Epoch: 66 [8960/60000 (15%)]\tLoss: 99.394127\n",
      "Train Epoch: 66 [10240/60000 (17%)]\tLoss: 96.518471\n",
      "Train Epoch: 66 [11520/60000 (19%)]\tLoss: 99.560555\n",
      "Train Epoch: 66 [12800/60000 (21%)]\tLoss: 96.709656\n",
      "Train Epoch: 66 [14080/60000 (23%)]\tLoss: 99.699120\n",
      "Train Epoch: 66 [15360/60000 (26%)]\tLoss: 100.605865\n",
      "Train Epoch: 66 [16640/60000 (28%)]\tLoss: 95.732834\n",
      "Train Epoch: 66 [17920/60000 (30%)]\tLoss: 95.991318\n",
      "Train Epoch: 66 [19200/60000 (32%)]\tLoss: 97.861404\n",
      "Train Epoch: 66 [20480/60000 (34%)]\tLoss: 99.512070\n",
      "Train Epoch: 66 [21760/60000 (36%)]\tLoss: 97.151382\n",
      "Train Epoch: 66 [23040/60000 (38%)]\tLoss: 98.582687\n",
      "Train Epoch: 66 [24320/60000 (41%)]\tLoss: 95.508461\n",
      "Train Epoch: 66 [25600/60000 (43%)]\tLoss: 97.685349\n",
      "Train Epoch: 66 [26880/60000 (45%)]\tLoss: 97.342064\n",
      "Train Epoch: 66 [28160/60000 (47%)]\tLoss: 95.643860\n",
      "Train Epoch: 66 [29440/60000 (49%)]\tLoss: 97.039970\n",
      "Train Epoch: 66 [30720/60000 (51%)]\tLoss: 97.654137\n",
      "Train Epoch: 66 [32000/60000 (53%)]\tLoss: 97.741905\n",
      "Train Epoch: 66 [33280/60000 (55%)]\tLoss: 97.207001\n",
      "Train Epoch: 66 [34560/60000 (58%)]\tLoss: 96.548721\n",
      "Train Epoch: 66 [35840/60000 (60%)]\tLoss: 99.577972\n",
      "Train Epoch: 66 [37120/60000 (62%)]\tLoss: 97.745750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 66 [38400/60000 (64%)]\tLoss: 102.299332\n",
      "Train Epoch: 66 [39680/60000 (66%)]\tLoss: 97.408211\n",
      "Train Epoch: 66 [40960/60000 (68%)]\tLoss: 97.822357\n",
      "Train Epoch: 66 [42240/60000 (70%)]\tLoss: 98.959366\n",
      "Train Epoch: 66 [43520/60000 (72%)]\tLoss: 98.882164\n",
      "Train Epoch: 66 [44800/60000 (75%)]\tLoss: 98.523735\n",
      "Train Epoch: 66 [46080/60000 (77%)]\tLoss: 98.678032\n",
      "Train Epoch: 66 [47360/60000 (79%)]\tLoss: 102.039368\n",
      "Train Epoch: 66 [48640/60000 (81%)]\tLoss: 95.863403\n",
      "Train Epoch: 66 [49920/60000 (83%)]\tLoss: 95.219788\n",
      "Train Epoch: 66 [51200/60000 (85%)]\tLoss: 98.017548\n",
      "Train Epoch: 66 [52480/60000 (87%)]\tLoss: 97.692139\n",
      "Train Epoch: 66 [53760/60000 (90%)]\tLoss: 98.716507\n",
      "Train Epoch: 66 [55040/60000 (92%)]\tLoss: 98.842354\n",
      "Train Epoch: 66 [56320/60000 (94%)]\tLoss: 98.061523\n",
      "Train Epoch: 66 [57600/60000 (96%)]\tLoss: 98.232086\n",
      "Train Epoch: 66 [58880/60000 (98%)]\tLoss: 102.285492\n",
      "====> Epoch: 66 Average loss: 97.6188\n",
      "====> Test set loss: 99.6312\n",
      "Train Epoch: 67 [0/60000 (0%)]\tLoss: 95.288277\n",
      "Train Epoch: 67 [1280/60000 (2%)]\tLoss: 95.767784\n",
      "Train Epoch: 67 [2560/60000 (4%)]\tLoss: 97.788445\n",
      "Train Epoch: 67 [3840/60000 (6%)]\tLoss: 99.000710\n",
      "Train Epoch: 67 [5120/60000 (9%)]\tLoss: 97.302177\n",
      "Train Epoch: 67 [6400/60000 (11%)]\tLoss: 94.016586\n",
      "Train Epoch: 67 [7680/60000 (13%)]\tLoss: 94.533386\n",
      "Train Epoch: 67 [8960/60000 (15%)]\tLoss: 95.048500\n",
      "Train Epoch: 67 [10240/60000 (17%)]\tLoss: 93.101242\n",
      "Train Epoch: 67 [11520/60000 (19%)]\tLoss: 97.002586\n",
      "Train Epoch: 67 [12800/60000 (21%)]\tLoss: 95.289459\n",
      "Train Epoch: 67 [14080/60000 (23%)]\tLoss: 98.880287\n",
      "Train Epoch: 67 [15360/60000 (26%)]\tLoss: 96.441704\n",
      "Train Epoch: 67 [16640/60000 (28%)]\tLoss: 97.903839\n",
      "Train Epoch: 67 [17920/60000 (30%)]\tLoss: 93.363480\n",
      "Train Epoch: 67 [19200/60000 (32%)]\tLoss: 101.066032\n",
      "Train Epoch: 67 [20480/60000 (34%)]\tLoss: 96.164047\n",
      "Train Epoch: 67 [21760/60000 (36%)]\tLoss: 98.417351\n",
      "Train Epoch: 67 [23040/60000 (38%)]\tLoss: 99.988525\n",
      "Train Epoch: 67 [24320/60000 (41%)]\tLoss: 97.904007\n",
      "Train Epoch: 67 [25600/60000 (43%)]\tLoss: 95.363991\n",
      "Train Epoch: 67 [26880/60000 (45%)]\tLoss: 99.387871\n",
      "Train Epoch: 67 [28160/60000 (47%)]\tLoss: 97.942039\n",
      "Train Epoch: 67 [29440/60000 (49%)]\tLoss: 93.960739\n",
      "Train Epoch: 67 [30720/60000 (51%)]\tLoss: 99.494522\n",
      "Train Epoch: 67 [32000/60000 (53%)]\tLoss: 97.020950\n",
      "Train Epoch: 67 [33280/60000 (55%)]\tLoss: 97.970406\n",
      "Train Epoch: 67 [34560/60000 (58%)]\tLoss: 97.946823\n",
      "Train Epoch: 67 [35840/60000 (60%)]\tLoss: 97.931068\n",
      "Train Epoch: 67 [37120/60000 (62%)]\tLoss: 95.594688\n",
      "Train Epoch: 67 [38400/60000 (64%)]\tLoss: 99.151955\n",
      "Train Epoch: 67 [39680/60000 (66%)]\tLoss: 93.445480\n",
      "Train Epoch: 67 [40960/60000 (68%)]\tLoss: 97.092628\n",
      "Train Epoch: 67 [42240/60000 (70%)]\tLoss: 98.646439\n",
      "Train Epoch: 67 [43520/60000 (72%)]\tLoss: 98.763283\n",
      "Train Epoch: 67 [44800/60000 (75%)]\tLoss: 102.911514\n",
      "Train Epoch: 67 [46080/60000 (77%)]\tLoss: 98.652794\n",
      "Train Epoch: 67 [47360/60000 (79%)]\tLoss: 98.290359\n",
      "Train Epoch: 67 [48640/60000 (81%)]\tLoss: 93.096901\n",
      "Train Epoch: 67 [49920/60000 (83%)]\tLoss: 98.903366\n",
      "Train Epoch: 67 [51200/60000 (85%)]\tLoss: 98.831345\n",
      "Train Epoch: 67 [52480/60000 (87%)]\tLoss: 99.551712\n",
      "Train Epoch: 67 [53760/60000 (90%)]\tLoss: 100.240341\n",
      "Train Epoch: 67 [55040/60000 (92%)]\tLoss: 97.612244\n",
      "Train Epoch: 67 [56320/60000 (94%)]\tLoss: 98.254219\n",
      "Train Epoch: 67 [57600/60000 (96%)]\tLoss: 97.472862\n",
      "Train Epoch: 67 [58880/60000 (98%)]\tLoss: 94.023041\n",
      "====> Epoch: 67 Average loss: 97.5943\n",
      "====> Test set loss: 99.6230\n",
      "Train Epoch: 68 [0/60000 (0%)]\tLoss: 97.578117\n",
      "Train Epoch: 68 [1280/60000 (2%)]\tLoss: 95.032387\n",
      "Train Epoch: 68 [2560/60000 (4%)]\tLoss: 97.623077\n",
      "Train Epoch: 68 [3840/60000 (6%)]\tLoss: 95.479195\n",
      "Train Epoch: 68 [5120/60000 (9%)]\tLoss: 94.297684\n",
      "Train Epoch: 68 [6400/60000 (11%)]\tLoss: 97.359467\n",
      "Train Epoch: 68 [7680/60000 (13%)]\tLoss: 99.110619\n",
      "Train Epoch: 68 [8960/60000 (15%)]\tLoss: 96.860802\n",
      "Train Epoch: 68 [10240/60000 (17%)]\tLoss: 95.117477\n",
      "Train Epoch: 68 [11520/60000 (19%)]\tLoss: 96.137779\n",
      "Train Epoch: 68 [12800/60000 (21%)]\tLoss: 96.932358\n",
      "Train Epoch: 68 [14080/60000 (23%)]\tLoss: 99.950577\n",
      "Train Epoch: 68 [15360/60000 (26%)]\tLoss: 99.267204\n",
      "Train Epoch: 68 [16640/60000 (28%)]\tLoss: 96.287689\n",
      "Train Epoch: 68 [17920/60000 (30%)]\tLoss: 92.252411\n",
      "Train Epoch: 68 [19200/60000 (32%)]\tLoss: 97.368912\n",
      "Train Epoch: 68 [20480/60000 (34%)]\tLoss: 100.372749\n",
      "Train Epoch: 68 [21760/60000 (36%)]\tLoss: 98.502350\n",
      "Train Epoch: 68 [23040/60000 (38%)]\tLoss: 93.963425\n",
      "Train Epoch: 68 [24320/60000 (41%)]\tLoss: 96.741127\n",
      "Train Epoch: 68 [25600/60000 (43%)]\tLoss: 96.997375\n",
      "Train Epoch: 68 [26880/60000 (45%)]\tLoss: 99.869019\n",
      "Train Epoch: 68 [28160/60000 (47%)]\tLoss: 96.165337\n",
      "Train Epoch: 68 [29440/60000 (49%)]\tLoss: 95.648727\n",
      "Train Epoch: 68 [30720/60000 (51%)]\tLoss: 97.181366\n",
      "Train Epoch: 68 [32000/60000 (53%)]\tLoss: 101.667068\n",
      "Train Epoch: 68 [33280/60000 (55%)]\tLoss: 99.717934\n",
      "Train Epoch: 68 [34560/60000 (58%)]\tLoss: 101.002960\n",
      "Train Epoch: 68 [35840/60000 (60%)]\tLoss: 97.682549\n",
      "Train Epoch: 68 [37120/60000 (62%)]\tLoss: 96.234787\n",
      "Train Epoch: 68 [38400/60000 (64%)]\tLoss: 93.781609\n",
      "Train Epoch: 68 [39680/60000 (66%)]\tLoss: 99.548401\n",
      "Train Epoch: 68 [40960/60000 (68%)]\tLoss: 97.080849\n",
      "Train Epoch: 68 [42240/60000 (70%)]\tLoss: 92.907288\n",
      "Train Epoch: 68 [43520/60000 (72%)]\tLoss: 99.047546\n",
      "Train Epoch: 68 [44800/60000 (75%)]\tLoss: 98.232536\n",
      "Train Epoch: 68 [46080/60000 (77%)]\tLoss: 96.461380\n",
      "Train Epoch: 68 [47360/60000 (79%)]\tLoss: 95.173256\n",
      "Train Epoch: 68 [48640/60000 (81%)]\tLoss: 97.398468\n",
      "Train Epoch: 68 [49920/60000 (83%)]\tLoss: 99.137993\n",
      "Train Epoch: 68 [51200/60000 (85%)]\tLoss: 100.684120\n",
      "Train Epoch: 68 [52480/60000 (87%)]\tLoss: 97.065369\n",
      "Train Epoch: 68 [53760/60000 (90%)]\tLoss: 94.922371\n",
      "Train Epoch: 68 [55040/60000 (92%)]\tLoss: 100.150192\n",
      "Train Epoch: 68 [56320/60000 (94%)]\tLoss: 94.270004\n",
      "Train Epoch: 68 [57600/60000 (96%)]\tLoss: 96.704216\n",
      "Train Epoch: 68 [58880/60000 (98%)]\tLoss: 95.142395\n",
      "====> Epoch: 68 Average loss: 97.5548\n",
      "====> Test set loss: 99.5043\n",
      "Train Epoch: 69 [0/60000 (0%)]\tLoss: 98.306938\n",
      "Train Epoch: 69 [1280/60000 (2%)]\tLoss: 96.867020\n",
      "Train Epoch: 69 [2560/60000 (4%)]\tLoss: 101.336594\n",
      "Train Epoch: 69 [3840/60000 (6%)]\tLoss: 99.743690\n",
      "Train Epoch: 69 [5120/60000 (9%)]\tLoss: 96.132347\n",
      "Train Epoch: 69 [6400/60000 (11%)]\tLoss: 100.825241\n",
      "Train Epoch: 69 [7680/60000 (13%)]\tLoss: 101.550316\n",
      "Train Epoch: 69 [8960/60000 (15%)]\tLoss: 98.056961\n",
      "Train Epoch: 69 [10240/60000 (17%)]\tLoss: 96.810715\n",
      "Train Epoch: 69 [11520/60000 (19%)]\tLoss: 99.160400\n",
      "Train Epoch: 69 [12800/60000 (21%)]\tLoss: 98.216309\n",
      "Train Epoch: 69 [14080/60000 (23%)]\tLoss: 92.107780\n",
      "Train Epoch: 69 [15360/60000 (26%)]\tLoss: 96.310455\n",
      "Train Epoch: 69 [16640/60000 (28%)]\tLoss: 98.112549\n",
      "Train Epoch: 69 [17920/60000 (30%)]\tLoss: 99.123642\n",
      "Train Epoch: 69 [19200/60000 (32%)]\tLoss: 97.222610\n",
      "Train Epoch: 69 [20480/60000 (34%)]\tLoss: 96.091339\n",
      "Train Epoch: 69 [21760/60000 (36%)]\tLoss: 99.128777\n",
      "Train Epoch: 69 [23040/60000 (38%)]\tLoss: 102.427612\n",
      "Train Epoch: 69 [24320/60000 (41%)]\tLoss: 93.917992\n",
      "Train Epoch: 69 [25600/60000 (43%)]\tLoss: 98.416153\n",
      "Train Epoch: 69 [26880/60000 (45%)]\tLoss: 96.609146\n",
      "Train Epoch: 69 [28160/60000 (47%)]\tLoss: 99.466354\n",
      "Train Epoch: 69 [29440/60000 (49%)]\tLoss: 93.857178\n",
      "Train Epoch: 69 [30720/60000 (51%)]\tLoss: 90.348267\n",
      "Train Epoch: 69 [32000/60000 (53%)]\tLoss: 98.315300\n",
      "Train Epoch: 69 [33280/60000 (55%)]\tLoss: 94.711777\n",
      "Train Epoch: 69 [34560/60000 (58%)]\tLoss: 101.805679\n",
      "Train Epoch: 69 [35840/60000 (60%)]\tLoss: 98.259384\n",
      "Train Epoch: 69 [37120/60000 (62%)]\tLoss: 94.884605\n",
      "Train Epoch: 69 [38400/60000 (64%)]\tLoss: 98.209167\n",
      "Train Epoch: 69 [39680/60000 (66%)]\tLoss: 102.195602\n",
      "Train Epoch: 69 [40960/60000 (68%)]\tLoss: 98.206978\n",
      "Train Epoch: 69 [42240/60000 (70%)]\tLoss: 98.491524\n",
      "Train Epoch: 69 [43520/60000 (72%)]\tLoss: 96.673386\n",
      "Train Epoch: 69 [44800/60000 (75%)]\tLoss: 96.128677\n",
      "Train Epoch: 69 [46080/60000 (77%)]\tLoss: 99.304298\n",
      "Train Epoch: 69 [47360/60000 (79%)]\tLoss: 102.910843\n",
      "Train Epoch: 69 [48640/60000 (81%)]\tLoss: 98.220627\n",
      "Train Epoch: 69 [49920/60000 (83%)]\tLoss: 98.125725\n",
      "Train Epoch: 69 [51200/60000 (85%)]\tLoss: 97.418610\n",
      "Train Epoch: 69 [52480/60000 (87%)]\tLoss: 99.623337\n",
      "Train Epoch: 69 [53760/60000 (90%)]\tLoss: 97.071976\n",
      "Train Epoch: 69 [55040/60000 (92%)]\tLoss: 102.101379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 69 [56320/60000 (94%)]\tLoss: 97.870178\n",
      "Train Epoch: 69 [57600/60000 (96%)]\tLoss: 97.194313\n",
      "Train Epoch: 69 [58880/60000 (98%)]\tLoss: 101.566620\n",
      "====> Epoch: 69 Average loss: 97.5108\n",
      "====> Test set loss: 99.4490\n",
      "Train Epoch: 70 [0/60000 (0%)]\tLoss: 94.688934\n",
      "Train Epoch: 70 [1280/60000 (2%)]\tLoss: 95.016716\n",
      "Train Epoch: 70 [2560/60000 (4%)]\tLoss: 97.467667\n",
      "Train Epoch: 70 [3840/60000 (6%)]\tLoss: 96.415726\n",
      "Train Epoch: 70 [5120/60000 (9%)]\tLoss: 95.794464\n",
      "Train Epoch: 70 [6400/60000 (11%)]\tLoss: 99.213715\n",
      "Train Epoch: 70 [7680/60000 (13%)]\tLoss: 95.041100\n",
      "Train Epoch: 70 [8960/60000 (15%)]\tLoss: 93.865295\n",
      "Train Epoch: 70 [10240/60000 (17%)]\tLoss: 96.136002\n",
      "Train Epoch: 70 [11520/60000 (19%)]\tLoss: 96.058182\n",
      "Train Epoch: 70 [12800/60000 (21%)]\tLoss: 95.871529\n",
      "Train Epoch: 70 [14080/60000 (23%)]\tLoss: 99.326889\n",
      "Train Epoch: 70 [15360/60000 (26%)]\tLoss: 97.650314\n",
      "Train Epoch: 70 [16640/60000 (28%)]\tLoss: 96.276718\n",
      "Train Epoch: 70 [17920/60000 (30%)]\tLoss: 94.098877\n",
      "Train Epoch: 70 [19200/60000 (32%)]\tLoss: 101.750732\n",
      "Train Epoch: 70 [20480/60000 (34%)]\tLoss: 101.763412\n",
      "Train Epoch: 70 [21760/60000 (36%)]\tLoss: 96.253738\n",
      "Train Epoch: 70 [23040/60000 (38%)]\tLoss: 98.358521\n",
      "Train Epoch: 70 [24320/60000 (41%)]\tLoss: 97.243690\n",
      "Train Epoch: 70 [25600/60000 (43%)]\tLoss: 97.386902\n",
      "Train Epoch: 70 [26880/60000 (45%)]\tLoss: 99.293465\n",
      "Train Epoch: 70 [28160/60000 (47%)]\tLoss: 95.824333\n",
      "Train Epoch: 70 [29440/60000 (49%)]\tLoss: 95.425774\n",
      "Train Epoch: 70 [30720/60000 (51%)]\tLoss: 97.190598\n",
      "Train Epoch: 70 [32000/60000 (53%)]\tLoss: 101.996384\n",
      "Train Epoch: 70 [33280/60000 (55%)]\tLoss: 99.034683\n",
      "Train Epoch: 70 [34560/60000 (58%)]\tLoss: 99.649811\n",
      "Train Epoch: 70 [35840/60000 (60%)]\tLoss: 96.454613\n",
      "Train Epoch: 70 [37120/60000 (62%)]\tLoss: 95.742935\n",
      "Train Epoch: 70 [38400/60000 (64%)]\tLoss: 97.331406\n",
      "Train Epoch: 70 [39680/60000 (66%)]\tLoss: 97.678619\n",
      "Train Epoch: 70 [40960/60000 (68%)]\tLoss: 99.972397\n",
      "Train Epoch: 70 [42240/60000 (70%)]\tLoss: 97.488548\n",
      "Train Epoch: 70 [43520/60000 (72%)]\tLoss: 100.916878\n",
      "Train Epoch: 70 [44800/60000 (75%)]\tLoss: 98.431671\n",
      "Train Epoch: 70 [46080/60000 (77%)]\tLoss: 97.056122\n",
      "Train Epoch: 70 [47360/60000 (79%)]\tLoss: 97.397385\n",
      "Train Epoch: 70 [48640/60000 (81%)]\tLoss: 100.529114\n",
      "Train Epoch: 70 [49920/60000 (83%)]\tLoss: 96.562790\n",
      "Train Epoch: 70 [51200/60000 (85%)]\tLoss: 100.059616\n",
      "Train Epoch: 70 [52480/60000 (87%)]\tLoss: 101.514336\n",
      "Train Epoch: 70 [53760/60000 (90%)]\tLoss: 101.576355\n",
      "Train Epoch: 70 [55040/60000 (92%)]\tLoss: 99.929871\n",
      "Train Epoch: 70 [56320/60000 (94%)]\tLoss: 98.466125\n",
      "Train Epoch: 70 [57600/60000 (96%)]\tLoss: 99.653992\n",
      "Train Epoch: 70 [58880/60000 (98%)]\tLoss: 98.083801\n",
      "====> Epoch: 70 Average loss: 97.4987\n",
      "====> Test set loss: 99.7581\n",
      "Train Epoch: 71 [0/60000 (0%)]\tLoss: 97.898170\n",
      "Train Epoch: 71 [1280/60000 (2%)]\tLoss: 96.684441\n",
      "Train Epoch: 71 [2560/60000 (4%)]\tLoss: 96.613731\n",
      "Train Epoch: 71 [3840/60000 (6%)]\tLoss: 97.283463\n",
      "Train Epoch: 71 [5120/60000 (9%)]\tLoss: 94.076324\n",
      "Train Epoch: 71 [6400/60000 (11%)]\tLoss: 96.395470\n",
      "Train Epoch: 71 [7680/60000 (13%)]\tLoss: 97.628952\n",
      "Train Epoch: 71 [8960/60000 (15%)]\tLoss: 99.473427\n",
      "Train Epoch: 71 [10240/60000 (17%)]\tLoss: 96.864838\n",
      "Train Epoch: 71 [11520/60000 (19%)]\tLoss: 98.057457\n",
      "Train Epoch: 71 [12800/60000 (21%)]\tLoss: 96.930618\n",
      "Train Epoch: 71 [14080/60000 (23%)]\tLoss: 93.591209\n",
      "Train Epoch: 71 [15360/60000 (26%)]\tLoss: 98.636475\n",
      "Train Epoch: 71 [16640/60000 (28%)]\tLoss: 94.980423\n",
      "Train Epoch: 71 [17920/60000 (30%)]\tLoss: 95.511940\n",
      "Train Epoch: 71 [19200/60000 (32%)]\tLoss: 98.080368\n",
      "Train Epoch: 71 [20480/60000 (34%)]\tLoss: 101.152557\n",
      "Train Epoch: 71 [21760/60000 (36%)]\tLoss: 102.929703\n",
      "Train Epoch: 71 [23040/60000 (38%)]\tLoss: 101.727287\n",
      "Train Epoch: 71 [24320/60000 (41%)]\tLoss: 98.282082\n",
      "Train Epoch: 71 [25600/60000 (43%)]\tLoss: 94.478249\n",
      "Train Epoch: 71 [26880/60000 (45%)]\tLoss: 98.569527\n",
      "Train Epoch: 71 [28160/60000 (47%)]\tLoss: 100.650276\n",
      "Train Epoch: 71 [29440/60000 (49%)]\tLoss: 99.845978\n",
      "Train Epoch: 71 [30720/60000 (51%)]\tLoss: 93.571899\n",
      "Train Epoch: 71 [32000/60000 (53%)]\tLoss: 96.745087\n",
      "Train Epoch: 71 [33280/60000 (55%)]\tLoss: 94.895844\n",
      "Train Epoch: 71 [34560/60000 (58%)]\tLoss: 99.088936\n",
      "Train Epoch: 71 [35840/60000 (60%)]\tLoss: 97.850243\n",
      "Train Epoch: 71 [37120/60000 (62%)]\tLoss: 92.643051\n",
      "Train Epoch: 71 [38400/60000 (64%)]\tLoss: 101.445389\n",
      "Train Epoch: 71 [39680/60000 (66%)]\tLoss: 97.355652\n",
      "Train Epoch: 71 [40960/60000 (68%)]\tLoss: 98.104080\n",
      "Train Epoch: 71 [42240/60000 (70%)]\tLoss: 95.148056\n",
      "Train Epoch: 71 [43520/60000 (72%)]\tLoss: 97.250793\n",
      "Train Epoch: 71 [44800/60000 (75%)]\tLoss: 97.983932\n",
      "Train Epoch: 71 [46080/60000 (77%)]\tLoss: 99.656181\n",
      "Train Epoch: 71 [47360/60000 (79%)]\tLoss: 101.237907\n",
      "Train Epoch: 71 [48640/60000 (81%)]\tLoss: 100.888832\n",
      "Train Epoch: 71 [49920/60000 (83%)]\tLoss: 91.587967\n",
      "Train Epoch: 71 [51200/60000 (85%)]\tLoss: 103.075722\n",
      "Train Epoch: 71 [52480/60000 (87%)]\tLoss: 96.740486\n",
      "Train Epoch: 71 [53760/60000 (90%)]\tLoss: 97.538910\n",
      "Train Epoch: 71 [55040/60000 (92%)]\tLoss: 98.314850\n",
      "Train Epoch: 71 [56320/60000 (94%)]\tLoss: 95.413010\n",
      "Train Epoch: 71 [57600/60000 (96%)]\tLoss: 96.387634\n",
      "Train Epoch: 71 [58880/60000 (98%)]\tLoss: 97.857574\n",
      "====> Epoch: 71 Average loss: 97.4724\n",
      "====> Test set loss: 99.2469\n",
      "Train Epoch: 72 [0/60000 (0%)]\tLoss: 93.530090\n",
      "Train Epoch: 72 [1280/60000 (2%)]\tLoss: 95.480743\n",
      "Train Epoch: 72 [2560/60000 (4%)]\tLoss: 99.724762\n",
      "Train Epoch: 72 [3840/60000 (6%)]\tLoss: 93.804611\n",
      "Train Epoch: 72 [5120/60000 (9%)]\tLoss: 97.158485\n",
      "Train Epoch: 72 [6400/60000 (11%)]\tLoss: 100.564865\n",
      "Train Epoch: 72 [7680/60000 (13%)]\tLoss: 95.895767\n",
      "Train Epoch: 72 [8960/60000 (15%)]\tLoss: 97.736626\n",
      "Train Epoch: 72 [10240/60000 (17%)]\tLoss: 96.499130\n",
      "Train Epoch: 72 [11520/60000 (19%)]\tLoss: 97.537010\n",
      "Train Epoch: 72 [12800/60000 (21%)]\tLoss: 99.318863\n",
      "Train Epoch: 72 [14080/60000 (23%)]\tLoss: 98.770111\n",
      "Train Epoch: 72 [15360/60000 (26%)]\tLoss: 97.665733\n",
      "Train Epoch: 72 [16640/60000 (28%)]\tLoss: 101.606079\n",
      "Train Epoch: 72 [17920/60000 (30%)]\tLoss: 97.304375\n",
      "Train Epoch: 72 [19200/60000 (32%)]\tLoss: 97.798172\n",
      "Train Epoch: 72 [20480/60000 (34%)]\tLoss: 101.179169\n",
      "Train Epoch: 72 [21760/60000 (36%)]\tLoss: 100.218674\n",
      "Train Epoch: 72 [23040/60000 (38%)]\tLoss: 98.284988\n",
      "Train Epoch: 72 [24320/60000 (41%)]\tLoss: 95.181152\n",
      "Train Epoch: 72 [25600/60000 (43%)]\tLoss: 100.077713\n",
      "Train Epoch: 72 [26880/60000 (45%)]\tLoss: 100.682228\n",
      "Train Epoch: 72 [28160/60000 (47%)]\tLoss: 97.686409\n",
      "Train Epoch: 72 [29440/60000 (49%)]\tLoss: 94.885254\n",
      "Train Epoch: 72 [30720/60000 (51%)]\tLoss: 101.141518\n",
      "Train Epoch: 72 [32000/60000 (53%)]\tLoss: 98.643448\n",
      "Train Epoch: 72 [33280/60000 (55%)]\tLoss: 98.572784\n",
      "Train Epoch: 72 [34560/60000 (58%)]\tLoss: 97.961647\n",
      "Train Epoch: 72 [35840/60000 (60%)]\tLoss: 102.385696\n",
      "Train Epoch: 72 [37120/60000 (62%)]\tLoss: 95.988884\n",
      "Train Epoch: 72 [38400/60000 (64%)]\tLoss: 101.038765\n",
      "Train Epoch: 72 [39680/60000 (66%)]\tLoss: 99.445320\n",
      "Train Epoch: 72 [40960/60000 (68%)]\tLoss: 97.531448\n",
      "Train Epoch: 72 [42240/60000 (70%)]\tLoss: 97.715111\n",
      "Train Epoch: 72 [43520/60000 (72%)]\tLoss: 94.715012\n",
      "Train Epoch: 72 [44800/60000 (75%)]\tLoss: 100.058830\n",
      "Train Epoch: 72 [46080/60000 (77%)]\tLoss: 98.532227\n",
      "Train Epoch: 72 [47360/60000 (79%)]\tLoss: 97.253944\n",
      "Train Epoch: 72 [48640/60000 (81%)]\tLoss: 95.342880\n",
      "Train Epoch: 72 [49920/60000 (83%)]\tLoss: 96.291794\n",
      "Train Epoch: 72 [51200/60000 (85%)]\tLoss: 94.983505\n",
      "Train Epoch: 72 [52480/60000 (87%)]\tLoss: 98.896935\n",
      "Train Epoch: 72 [53760/60000 (90%)]\tLoss: 99.765678\n",
      "Train Epoch: 72 [55040/60000 (92%)]\tLoss: 102.549347\n",
      "Train Epoch: 72 [56320/60000 (94%)]\tLoss: 100.939705\n",
      "Train Epoch: 72 [57600/60000 (96%)]\tLoss: 99.512924\n",
      "Train Epoch: 72 [58880/60000 (98%)]\tLoss: 94.723534\n",
      "====> Epoch: 72 Average loss: 97.4409\n",
      "====> Test set loss: 99.5610\n",
      "Train Epoch: 73 [0/60000 (0%)]\tLoss: 98.163200\n",
      "Train Epoch: 73 [1280/60000 (2%)]\tLoss: 98.978821\n",
      "Train Epoch: 73 [2560/60000 (4%)]\tLoss: 96.986504\n",
      "Train Epoch: 73 [3840/60000 (6%)]\tLoss: 94.826797\n",
      "Train Epoch: 73 [5120/60000 (9%)]\tLoss: 96.519554\n",
      "Train Epoch: 73 [6400/60000 (11%)]\tLoss: 96.318588\n",
      "Train Epoch: 73 [7680/60000 (13%)]\tLoss: 96.201950\n",
      "Train Epoch: 73 [8960/60000 (15%)]\tLoss: 97.950241\n",
      "Train Epoch: 73 [10240/60000 (17%)]\tLoss: 99.589394\n",
      "Train Epoch: 73 [11520/60000 (19%)]\tLoss: 94.648163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 73 [12800/60000 (21%)]\tLoss: 99.170273\n",
      "Train Epoch: 73 [14080/60000 (23%)]\tLoss: 99.892387\n",
      "Train Epoch: 73 [15360/60000 (26%)]\tLoss: 96.352272\n",
      "Train Epoch: 73 [16640/60000 (28%)]\tLoss: 96.095299\n",
      "Train Epoch: 73 [17920/60000 (30%)]\tLoss: 96.236465\n",
      "Train Epoch: 73 [19200/60000 (32%)]\tLoss: 102.653336\n",
      "Train Epoch: 73 [20480/60000 (34%)]\tLoss: 98.812515\n",
      "Train Epoch: 73 [21760/60000 (36%)]\tLoss: 97.645760\n",
      "Train Epoch: 73 [23040/60000 (38%)]\tLoss: 97.788811\n",
      "Train Epoch: 73 [24320/60000 (41%)]\tLoss: 97.940094\n",
      "Train Epoch: 73 [25600/60000 (43%)]\tLoss: 96.485428\n",
      "Train Epoch: 73 [26880/60000 (45%)]\tLoss: 98.364738\n",
      "Train Epoch: 73 [28160/60000 (47%)]\tLoss: 95.722839\n",
      "Train Epoch: 73 [29440/60000 (49%)]\tLoss: 95.324814\n",
      "Train Epoch: 73 [30720/60000 (51%)]\tLoss: 99.137596\n",
      "Train Epoch: 73 [32000/60000 (53%)]\tLoss: 95.833763\n",
      "Train Epoch: 73 [33280/60000 (55%)]\tLoss: 96.994324\n",
      "Train Epoch: 73 [34560/60000 (58%)]\tLoss: 98.936264\n",
      "Train Epoch: 73 [35840/60000 (60%)]\tLoss: 97.318680\n",
      "Train Epoch: 73 [37120/60000 (62%)]\tLoss: 97.308121\n",
      "Train Epoch: 73 [38400/60000 (64%)]\tLoss: 92.660950\n",
      "Train Epoch: 73 [39680/60000 (66%)]\tLoss: 93.763649\n",
      "Train Epoch: 73 [40960/60000 (68%)]\tLoss: 97.392990\n",
      "Train Epoch: 73 [42240/60000 (70%)]\tLoss: 100.326378\n",
      "Train Epoch: 73 [43520/60000 (72%)]\tLoss: 97.606491\n",
      "Train Epoch: 73 [44800/60000 (75%)]\tLoss: 96.914825\n",
      "Train Epoch: 73 [46080/60000 (77%)]\tLoss: 92.004044\n",
      "Train Epoch: 73 [47360/60000 (79%)]\tLoss: 96.902512\n",
      "Train Epoch: 73 [48640/60000 (81%)]\tLoss: 93.023026\n",
      "Train Epoch: 73 [49920/60000 (83%)]\tLoss: 97.434921\n",
      "Train Epoch: 73 [51200/60000 (85%)]\tLoss: 93.860657\n",
      "Train Epoch: 73 [52480/60000 (87%)]\tLoss: 100.602158\n",
      "Train Epoch: 73 [53760/60000 (90%)]\tLoss: 95.619385\n",
      "Train Epoch: 73 [55040/60000 (92%)]\tLoss: 100.281845\n",
      "Train Epoch: 73 [56320/60000 (94%)]\tLoss: 96.814499\n",
      "Train Epoch: 73 [57600/60000 (96%)]\tLoss: 96.369537\n",
      "Train Epoch: 73 [58880/60000 (98%)]\tLoss: 96.004654\n",
      "====> Epoch: 73 Average loss: 97.4294\n",
      "====> Test set loss: 99.3927\n",
      "Train Epoch: 74 [0/60000 (0%)]\tLoss: 96.716675\n",
      "Train Epoch: 74 [1280/60000 (2%)]\tLoss: 98.080650\n",
      "Train Epoch: 74 [2560/60000 (4%)]\tLoss: 98.656403\n",
      "Train Epoch: 74 [3840/60000 (6%)]\tLoss: 96.093750\n",
      "Train Epoch: 74 [5120/60000 (9%)]\tLoss: 97.480392\n",
      "Train Epoch: 74 [6400/60000 (11%)]\tLoss: 95.980774\n",
      "Train Epoch: 74 [7680/60000 (13%)]\tLoss: 98.068420\n",
      "Train Epoch: 74 [8960/60000 (15%)]\tLoss: 93.478714\n",
      "Train Epoch: 74 [10240/60000 (17%)]\tLoss: 94.213943\n",
      "Train Epoch: 74 [11520/60000 (19%)]\tLoss: 97.041824\n",
      "Train Epoch: 74 [12800/60000 (21%)]\tLoss: 94.725403\n",
      "Train Epoch: 74 [14080/60000 (23%)]\tLoss: 94.200516\n",
      "Train Epoch: 74 [15360/60000 (26%)]\tLoss: 98.951027\n",
      "Train Epoch: 74 [16640/60000 (28%)]\tLoss: 90.008018\n",
      "Train Epoch: 74 [17920/60000 (30%)]\tLoss: 97.625267\n",
      "Train Epoch: 74 [19200/60000 (32%)]\tLoss: 99.090813\n",
      "Train Epoch: 74 [20480/60000 (34%)]\tLoss: 97.095657\n",
      "Train Epoch: 74 [21760/60000 (36%)]\tLoss: 96.632935\n",
      "Train Epoch: 74 [23040/60000 (38%)]\tLoss: 98.093811\n",
      "Train Epoch: 74 [24320/60000 (41%)]\tLoss: 97.106552\n",
      "Train Epoch: 74 [25600/60000 (43%)]\tLoss: 95.759415\n",
      "Train Epoch: 74 [26880/60000 (45%)]\tLoss: 94.808266\n",
      "Train Epoch: 74 [28160/60000 (47%)]\tLoss: 98.799423\n",
      "Train Epoch: 74 [29440/60000 (49%)]\tLoss: 99.854309\n",
      "Train Epoch: 74 [30720/60000 (51%)]\tLoss: 94.613098\n",
      "Train Epoch: 74 [32000/60000 (53%)]\tLoss: 99.994247\n",
      "Train Epoch: 74 [33280/60000 (55%)]\tLoss: 97.997826\n",
      "Train Epoch: 74 [34560/60000 (58%)]\tLoss: 100.718384\n",
      "Train Epoch: 74 [35840/60000 (60%)]\tLoss: 95.328506\n",
      "Train Epoch: 74 [37120/60000 (62%)]\tLoss: 97.077667\n",
      "Train Epoch: 74 [38400/60000 (64%)]\tLoss: 94.092987\n",
      "Train Epoch: 74 [39680/60000 (66%)]\tLoss: 99.903740\n",
      "Train Epoch: 74 [40960/60000 (68%)]\tLoss: 100.329498\n",
      "Train Epoch: 74 [42240/60000 (70%)]\tLoss: 94.045486\n",
      "Train Epoch: 74 [43520/60000 (72%)]\tLoss: 96.391357\n",
      "Train Epoch: 74 [44800/60000 (75%)]\tLoss: 96.158417\n",
      "Train Epoch: 74 [46080/60000 (77%)]\tLoss: 99.159988\n",
      "Train Epoch: 74 [47360/60000 (79%)]\tLoss: 99.376587\n",
      "Train Epoch: 74 [48640/60000 (81%)]\tLoss: 98.384048\n",
      "Train Epoch: 74 [49920/60000 (83%)]\tLoss: 98.024437\n",
      "Train Epoch: 74 [51200/60000 (85%)]\tLoss: 97.987427\n",
      "Train Epoch: 74 [52480/60000 (87%)]\tLoss: 90.615860\n",
      "Train Epoch: 74 [53760/60000 (90%)]\tLoss: 99.189583\n",
      "Train Epoch: 74 [55040/60000 (92%)]\tLoss: 100.297234\n",
      "Train Epoch: 74 [56320/60000 (94%)]\tLoss: 96.036865\n",
      "Train Epoch: 74 [57600/60000 (96%)]\tLoss: 94.486694\n",
      "Train Epoch: 74 [58880/60000 (98%)]\tLoss: 98.055527\n",
      "====> Epoch: 74 Average loss: 97.3899\n",
      "====> Test set loss: 99.4246\n",
      "Train Epoch: 75 [0/60000 (0%)]\tLoss: 96.442001\n",
      "Train Epoch: 75 [1280/60000 (2%)]\tLoss: 98.518692\n",
      "Train Epoch: 75 [2560/60000 (4%)]\tLoss: 95.268478\n",
      "Train Epoch: 75 [3840/60000 (6%)]\tLoss: 96.085556\n",
      "Train Epoch: 75 [5120/60000 (9%)]\tLoss: 94.914230\n",
      "Train Epoch: 75 [6400/60000 (11%)]\tLoss: 96.541824\n",
      "Train Epoch: 75 [7680/60000 (13%)]\tLoss: 96.100845\n",
      "Train Epoch: 75 [8960/60000 (15%)]\tLoss: 99.377136\n",
      "Train Epoch: 75 [10240/60000 (17%)]\tLoss: 97.079590\n",
      "Train Epoch: 75 [11520/60000 (19%)]\tLoss: 99.181679\n",
      "Train Epoch: 75 [12800/60000 (21%)]\tLoss: 97.245445\n",
      "Train Epoch: 75 [14080/60000 (23%)]\tLoss: 93.237679\n",
      "Train Epoch: 75 [15360/60000 (26%)]\tLoss: 103.464294\n",
      "Train Epoch: 75 [16640/60000 (28%)]\tLoss: 97.717804\n",
      "Train Epoch: 75 [17920/60000 (30%)]\tLoss: 96.256607\n",
      "Train Epoch: 75 [19200/60000 (32%)]\tLoss: 96.206581\n",
      "Train Epoch: 75 [20480/60000 (34%)]\tLoss: 101.133057\n",
      "Train Epoch: 75 [21760/60000 (36%)]\tLoss: 100.224846\n",
      "Train Epoch: 75 [23040/60000 (38%)]\tLoss: 96.280838\n",
      "Train Epoch: 75 [24320/60000 (41%)]\tLoss: 96.450607\n",
      "Train Epoch: 75 [25600/60000 (43%)]\tLoss: 99.515556\n",
      "Train Epoch: 75 [26880/60000 (45%)]\tLoss: 100.167709\n",
      "Train Epoch: 75 [28160/60000 (47%)]\tLoss: 97.990730\n",
      "Train Epoch: 75 [29440/60000 (49%)]\tLoss: 97.780983\n",
      "Train Epoch: 75 [30720/60000 (51%)]\tLoss: 93.469543\n",
      "Train Epoch: 75 [32000/60000 (53%)]\tLoss: 95.858269\n",
      "Train Epoch: 75 [33280/60000 (55%)]\tLoss: 99.570801\n",
      "Train Epoch: 75 [34560/60000 (58%)]\tLoss: 98.642204\n",
      "Train Epoch: 75 [35840/60000 (60%)]\tLoss: 95.130066\n",
      "Train Epoch: 75 [37120/60000 (62%)]\tLoss: 101.071625\n",
      "Train Epoch: 75 [38400/60000 (64%)]\tLoss: 96.383087\n",
      "Train Epoch: 75 [39680/60000 (66%)]\tLoss: 96.664001\n",
      "Train Epoch: 75 [40960/60000 (68%)]\tLoss: 100.637512\n",
      "Train Epoch: 75 [42240/60000 (70%)]\tLoss: 96.641846\n",
      "Train Epoch: 75 [43520/60000 (72%)]\tLoss: 99.374718\n",
      "Train Epoch: 75 [44800/60000 (75%)]\tLoss: 98.460953\n",
      "Train Epoch: 75 [46080/60000 (77%)]\tLoss: 96.428780\n",
      "Train Epoch: 75 [47360/60000 (79%)]\tLoss: 98.509926\n",
      "Train Epoch: 75 [48640/60000 (81%)]\tLoss: 96.932854\n",
      "Train Epoch: 75 [49920/60000 (83%)]\tLoss: 95.448479\n",
      "Train Epoch: 75 [51200/60000 (85%)]\tLoss: 96.115623\n",
      "Train Epoch: 75 [52480/60000 (87%)]\tLoss: 96.019150\n",
      "Train Epoch: 75 [53760/60000 (90%)]\tLoss: 96.295013\n",
      "Train Epoch: 75 [55040/60000 (92%)]\tLoss: 96.327805\n",
      "Train Epoch: 75 [56320/60000 (94%)]\tLoss: 99.963692\n",
      "Train Epoch: 75 [57600/60000 (96%)]\tLoss: 99.062759\n",
      "Train Epoch: 75 [58880/60000 (98%)]\tLoss: 100.291077\n",
      "====> Epoch: 75 Average loss: 97.3685\n",
      "====> Test set loss: 99.6340\n",
      "Train Epoch: 76 [0/60000 (0%)]\tLoss: 97.929550\n",
      "Train Epoch: 76 [1280/60000 (2%)]\tLoss: 97.239594\n",
      "Train Epoch: 76 [2560/60000 (4%)]\tLoss: 100.162621\n",
      "Train Epoch: 76 [3840/60000 (6%)]\tLoss: 94.920883\n",
      "Train Epoch: 76 [5120/60000 (9%)]\tLoss: 97.983932\n",
      "Train Epoch: 76 [6400/60000 (11%)]\tLoss: 98.038177\n",
      "Train Epoch: 76 [7680/60000 (13%)]\tLoss: 96.741600\n",
      "Train Epoch: 76 [8960/60000 (15%)]\tLoss: 96.214653\n",
      "Train Epoch: 76 [10240/60000 (17%)]\tLoss: 96.659119\n",
      "Train Epoch: 76 [11520/60000 (19%)]\tLoss: 97.423294\n",
      "Train Epoch: 76 [12800/60000 (21%)]\tLoss: 100.700554\n",
      "Train Epoch: 76 [14080/60000 (23%)]\tLoss: 100.045700\n",
      "Train Epoch: 76 [15360/60000 (26%)]\tLoss: 97.244125\n",
      "Train Epoch: 76 [16640/60000 (28%)]\tLoss: 96.937973\n",
      "Train Epoch: 76 [17920/60000 (30%)]\tLoss: 96.046875\n",
      "Train Epoch: 76 [19200/60000 (32%)]\tLoss: 95.860855\n",
      "Train Epoch: 76 [20480/60000 (34%)]\tLoss: 96.862900\n",
      "Train Epoch: 76 [21760/60000 (36%)]\tLoss: 101.012123\n",
      "Train Epoch: 76 [23040/60000 (38%)]\tLoss: 93.683456\n",
      "Train Epoch: 76 [24320/60000 (41%)]\tLoss: 95.484344\n",
      "Train Epoch: 76 [25600/60000 (43%)]\tLoss: 93.899216\n",
      "Train Epoch: 76 [26880/60000 (45%)]\tLoss: 97.139671\n",
      "Train Epoch: 76 [28160/60000 (47%)]\tLoss: 97.324295\n",
      "Train Epoch: 76 [29440/60000 (49%)]\tLoss: 98.025391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 76 [30720/60000 (51%)]\tLoss: 96.012695\n",
      "Train Epoch: 76 [32000/60000 (53%)]\tLoss: 100.663391\n",
      "Train Epoch: 76 [33280/60000 (55%)]\tLoss: 93.433426\n",
      "Train Epoch: 76 [34560/60000 (58%)]\tLoss: 92.246124\n",
      "Train Epoch: 76 [35840/60000 (60%)]\tLoss: 96.024338\n",
      "Train Epoch: 76 [37120/60000 (62%)]\tLoss: 97.571686\n",
      "Train Epoch: 76 [38400/60000 (64%)]\tLoss: 97.360733\n",
      "Train Epoch: 76 [39680/60000 (66%)]\tLoss: 99.274307\n",
      "Train Epoch: 76 [40960/60000 (68%)]\tLoss: 101.353638\n",
      "Train Epoch: 76 [42240/60000 (70%)]\tLoss: 97.582336\n",
      "Train Epoch: 76 [43520/60000 (72%)]\tLoss: 95.126801\n",
      "Train Epoch: 76 [44800/60000 (75%)]\tLoss: 99.179703\n",
      "Train Epoch: 76 [46080/60000 (77%)]\tLoss: 98.385361\n",
      "Train Epoch: 76 [47360/60000 (79%)]\tLoss: 94.182640\n",
      "Train Epoch: 76 [48640/60000 (81%)]\tLoss: 96.302399\n",
      "Train Epoch: 76 [49920/60000 (83%)]\tLoss: 97.357048\n",
      "Train Epoch: 76 [51200/60000 (85%)]\tLoss: 96.677826\n",
      "Train Epoch: 76 [52480/60000 (87%)]\tLoss: 102.332352\n",
      "Train Epoch: 76 [53760/60000 (90%)]\tLoss: 96.883179\n",
      "Train Epoch: 76 [55040/60000 (92%)]\tLoss: 101.059914\n",
      "Train Epoch: 76 [56320/60000 (94%)]\tLoss: 95.248779\n",
      "Train Epoch: 76 [57600/60000 (96%)]\tLoss: 97.146072\n",
      "Train Epoch: 76 [58880/60000 (98%)]\tLoss: 97.355316\n",
      "====> Epoch: 76 Average loss: 97.3265\n",
      "====> Test set loss: 99.2424\n",
      "Train Epoch: 77 [0/60000 (0%)]\tLoss: 94.997139\n",
      "Train Epoch: 77 [1280/60000 (2%)]\tLoss: 100.735748\n",
      "Train Epoch: 77 [2560/60000 (4%)]\tLoss: 98.910011\n",
      "Train Epoch: 77 [3840/60000 (6%)]\tLoss: 97.753983\n",
      "Train Epoch: 77 [5120/60000 (9%)]\tLoss: 100.664001\n",
      "Train Epoch: 77 [6400/60000 (11%)]\tLoss: 94.847183\n",
      "Train Epoch: 77 [7680/60000 (13%)]\tLoss: 99.417969\n",
      "Train Epoch: 77 [8960/60000 (15%)]\tLoss: 98.228104\n",
      "Train Epoch: 77 [10240/60000 (17%)]\tLoss: 99.049820\n",
      "Train Epoch: 77 [11520/60000 (19%)]\tLoss: 96.041412\n",
      "Train Epoch: 77 [12800/60000 (21%)]\tLoss: 94.627357\n",
      "Train Epoch: 77 [14080/60000 (23%)]\tLoss: 101.045738\n",
      "Train Epoch: 77 [15360/60000 (26%)]\tLoss: 98.180222\n",
      "Train Epoch: 77 [16640/60000 (28%)]\tLoss: 95.118011\n",
      "Train Epoch: 77 [17920/60000 (30%)]\tLoss: 98.962753\n",
      "Train Epoch: 77 [19200/60000 (32%)]\tLoss: 98.233002\n",
      "Train Epoch: 77 [20480/60000 (34%)]\tLoss: 102.042595\n",
      "Train Epoch: 77 [21760/60000 (36%)]\tLoss: 99.336899\n",
      "Train Epoch: 77 [23040/60000 (38%)]\tLoss: 97.543724\n",
      "Train Epoch: 77 [24320/60000 (41%)]\tLoss: 97.381424\n",
      "Train Epoch: 77 [25600/60000 (43%)]\tLoss: 97.519745\n",
      "Train Epoch: 77 [26880/60000 (45%)]\tLoss: 98.401497\n",
      "Train Epoch: 77 [28160/60000 (47%)]\tLoss: 95.609917\n",
      "Train Epoch: 77 [29440/60000 (49%)]\tLoss: 97.292526\n",
      "Train Epoch: 77 [30720/60000 (51%)]\tLoss: 94.628311\n",
      "Train Epoch: 77 [32000/60000 (53%)]\tLoss: 98.148727\n",
      "Train Epoch: 77 [33280/60000 (55%)]\tLoss: 93.746017\n",
      "Train Epoch: 77 [34560/60000 (58%)]\tLoss: 97.251030\n",
      "Train Epoch: 77 [35840/60000 (60%)]\tLoss: 101.012718\n",
      "Train Epoch: 77 [37120/60000 (62%)]\tLoss: 100.904381\n",
      "Train Epoch: 77 [38400/60000 (64%)]\tLoss: 101.571922\n",
      "Train Epoch: 77 [39680/60000 (66%)]\tLoss: 97.219025\n",
      "Train Epoch: 77 [40960/60000 (68%)]\tLoss: 95.444000\n",
      "Train Epoch: 77 [42240/60000 (70%)]\tLoss: 95.938828\n",
      "Train Epoch: 77 [43520/60000 (72%)]\tLoss: 97.658325\n",
      "Train Epoch: 77 [44800/60000 (75%)]\tLoss: 95.885201\n",
      "Train Epoch: 77 [46080/60000 (77%)]\tLoss: 98.294769\n",
      "Train Epoch: 77 [47360/60000 (79%)]\tLoss: 97.938080\n",
      "Train Epoch: 77 [48640/60000 (81%)]\tLoss: 98.620514\n",
      "Train Epoch: 77 [49920/60000 (83%)]\tLoss: 98.731255\n",
      "Train Epoch: 77 [51200/60000 (85%)]\tLoss: 97.031029\n",
      "Train Epoch: 77 [52480/60000 (87%)]\tLoss: 99.442329\n",
      "Train Epoch: 77 [53760/60000 (90%)]\tLoss: 95.705696\n",
      "Train Epoch: 77 [55040/60000 (92%)]\tLoss: 100.272255\n",
      "Train Epoch: 77 [56320/60000 (94%)]\tLoss: 95.437737\n",
      "Train Epoch: 77 [57600/60000 (96%)]\tLoss: 96.578896\n",
      "Train Epoch: 77 [58880/60000 (98%)]\tLoss: 91.234802\n",
      "====> Epoch: 77 Average loss: 97.3312\n",
      "====> Test set loss: 99.3798\n",
      "Train Epoch: 78 [0/60000 (0%)]\tLoss: 96.256165\n",
      "Train Epoch: 78 [1280/60000 (2%)]\tLoss: 95.584229\n",
      "Train Epoch: 78 [2560/60000 (4%)]\tLoss: 97.384377\n",
      "Train Epoch: 78 [3840/60000 (6%)]\tLoss: 97.537193\n",
      "Train Epoch: 78 [5120/60000 (9%)]\tLoss: 99.453918\n",
      "Train Epoch: 78 [6400/60000 (11%)]\tLoss: 99.311340\n",
      "Train Epoch: 78 [7680/60000 (13%)]\tLoss: 96.646996\n",
      "Train Epoch: 78 [8960/60000 (15%)]\tLoss: 95.897614\n",
      "Train Epoch: 78 [10240/60000 (17%)]\tLoss: 98.636673\n",
      "Train Epoch: 78 [11520/60000 (19%)]\tLoss: 97.940430\n",
      "Train Epoch: 78 [12800/60000 (21%)]\tLoss: 99.667648\n",
      "Train Epoch: 78 [14080/60000 (23%)]\tLoss: 101.033005\n",
      "Train Epoch: 78 [15360/60000 (26%)]\tLoss: 98.400337\n",
      "Train Epoch: 78 [16640/60000 (28%)]\tLoss: 98.008286\n",
      "Train Epoch: 78 [17920/60000 (30%)]\tLoss: 98.490692\n",
      "Train Epoch: 78 [19200/60000 (32%)]\tLoss: 98.343872\n",
      "Train Epoch: 78 [20480/60000 (34%)]\tLoss: 95.206940\n",
      "Train Epoch: 78 [21760/60000 (36%)]\tLoss: 96.021591\n",
      "Train Epoch: 78 [23040/60000 (38%)]\tLoss: 98.291809\n",
      "Train Epoch: 78 [24320/60000 (41%)]\tLoss: 97.103348\n",
      "Train Epoch: 78 [25600/60000 (43%)]\tLoss: 95.319626\n",
      "Train Epoch: 78 [26880/60000 (45%)]\tLoss: 101.339180\n",
      "Train Epoch: 78 [28160/60000 (47%)]\tLoss: 98.256935\n",
      "Train Epoch: 78 [29440/60000 (49%)]\tLoss: 94.833626\n",
      "Train Epoch: 78 [30720/60000 (51%)]\tLoss: 96.348930\n",
      "Train Epoch: 78 [32000/60000 (53%)]\tLoss: 99.470627\n",
      "Train Epoch: 78 [33280/60000 (55%)]\tLoss: 97.539497\n",
      "Train Epoch: 78 [34560/60000 (58%)]\tLoss: 99.646187\n",
      "Train Epoch: 78 [35840/60000 (60%)]\tLoss: 98.142059\n",
      "Train Epoch: 78 [37120/60000 (62%)]\tLoss: 98.793610\n",
      "Train Epoch: 78 [38400/60000 (64%)]\tLoss: 95.322227\n",
      "Train Epoch: 78 [39680/60000 (66%)]\tLoss: 100.661331\n",
      "Train Epoch: 78 [40960/60000 (68%)]\tLoss: 93.699066\n",
      "Train Epoch: 78 [42240/60000 (70%)]\tLoss: 98.703537\n",
      "Train Epoch: 78 [43520/60000 (72%)]\tLoss: 96.287582\n",
      "Train Epoch: 78 [44800/60000 (75%)]\tLoss: 97.583939\n",
      "Train Epoch: 78 [46080/60000 (77%)]\tLoss: 94.396805\n",
      "Train Epoch: 78 [47360/60000 (79%)]\tLoss: 96.076767\n",
      "Train Epoch: 78 [48640/60000 (81%)]\tLoss: 96.923691\n",
      "Train Epoch: 78 [49920/60000 (83%)]\tLoss: 96.401398\n",
      "Train Epoch: 78 [51200/60000 (85%)]\tLoss: 101.781479\n",
      "Train Epoch: 78 [52480/60000 (87%)]\tLoss: 93.177567\n",
      "Train Epoch: 78 [53760/60000 (90%)]\tLoss: 99.240479\n",
      "Train Epoch: 78 [55040/60000 (92%)]\tLoss: 96.814560\n",
      "Train Epoch: 78 [56320/60000 (94%)]\tLoss: 101.335297\n",
      "Train Epoch: 78 [57600/60000 (96%)]\tLoss: 98.240875\n",
      "Train Epoch: 78 [58880/60000 (98%)]\tLoss: 97.408989\n",
      "====> Epoch: 78 Average loss: 97.2963\n",
      "====> Test set loss: 99.1872\n",
      "Train Epoch: 79 [0/60000 (0%)]\tLoss: 94.052414\n",
      "Train Epoch: 79 [1280/60000 (2%)]\tLoss: 94.858322\n",
      "Train Epoch: 79 [2560/60000 (4%)]\tLoss: 95.782944\n",
      "Train Epoch: 79 [3840/60000 (6%)]\tLoss: 96.226227\n",
      "Train Epoch: 79 [5120/60000 (9%)]\tLoss: 96.058182\n",
      "Train Epoch: 79 [6400/60000 (11%)]\tLoss: 101.184662\n",
      "Train Epoch: 79 [7680/60000 (13%)]\tLoss: 100.564316\n",
      "Train Epoch: 79 [8960/60000 (15%)]\tLoss: 100.643585\n",
      "Train Epoch: 79 [10240/60000 (17%)]\tLoss: 102.569321\n",
      "Train Epoch: 79 [11520/60000 (19%)]\tLoss: 98.488907\n",
      "Train Epoch: 79 [12800/60000 (21%)]\tLoss: 93.876297\n",
      "Train Epoch: 79 [14080/60000 (23%)]\tLoss: 95.261139\n",
      "Train Epoch: 79 [15360/60000 (26%)]\tLoss: 94.946075\n",
      "Train Epoch: 79 [16640/60000 (28%)]\tLoss: 94.213608\n",
      "Train Epoch: 79 [17920/60000 (30%)]\tLoss: 94.527588\n",
      "Train Epoch: 79 [19200/60000 (32%)]\tLoss: 99.563339\n",
      "Train Epoch: 79 [20480/60000 (34%)]\tLoss: 92.255890\n",
      "Train Epoch: 79 [21760/60000 (36%)]\tLoss: 96.178764\n",
      "Train Epoch: 79 [23040/60000 (38%)]\tLoss: 97.893852\n",
      "Train Epoch: 79 [24320/60000 (41%)]\tLoss: 95.037628\n",
      "Train Epoch: 79 [25600/60000 (43%)]\tLoss: 95.314888\n",
      "Train Epoch: 79 [26880/60000 (45%)]\tLoss: 96.103668\n",
      "Train Epoch: 79 [28160/60000 (47%)]\tLoss: 95.928589\n",
      "Train Epoch: 79 [29440/60000 (49%)]\tLoss: 101.887772\n",
      "Train Epoch: 79 [30720/60000 (51%)]\tLoss: 99.832275\n",
      "Train Epoch: 79 [32000/60000 (53%)]\tLoss: 98.350098\n",
      "Train Epoch: 79 [33280/60000 (55%)]\tLoss: 93.413063\n",
      "Train Epoch: 79 [34560/60000 (58%)]\tLoss: 99.284225\n",
      "Train Epoch: 79 [35840/60000 (60%)]\tLoss: 98.388191\n",
      "Train Epoch: 79 [37120/60000 (62%)]\tLoss: 96.969101\n",
      "Train Epoch: 79 [38400/60000 (64%)]\tLoss: 95.164261\n",
      "Train Epoch: 79 [39680/60000 (66%)]\tLoss: 95.330101\n",
      "Train Epoch: 79 [40960/60000 (68%)]\tLoss: 97.764832\n",
      "Train Epoch: 79 [42240/60000 (70%)]\tLoss: 96.369011\n",
      "Train Epoch: 79 [43520/60000 (72%)]\tLoss: 96.959000\n",
      "Train Epoch: 79 [44800/60000 (75%)]\tLoss: 97.339951\n",
      "Train Epoch: 79 [46080/60000 (77%)]\tLoss: 95.811714\n",
      "Train Epoch: 79 [47360/60000 (79%)]\tLoss: 95.640472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 79 [48640/60000 (81%)]\tLoss: 97.848923\n",
      "Train Epoch: 79 [49920/60000 (83%)]\tLoss: 95.590210\n",
      "Train Epoch: 79 [51200/60000 (85%)]\tLoss: 94.798637\n",
      "Train Epoch: 79 [52480/60000 (87%)]\tLoss: 100.498795\n",
      "Train Epoch: 79 [53760/60000 (90%)]\tLoss: 98.866447\n",
      "Train Epoch: 79 [55040/60000 (92%)]\tLoss: 96.469604\n",
      "Train Epoch: 79 [56320/60000 (94%)]\tLoss: 101.111191\n",
      "Train Epoch: 79 [57600/60000 (96%)]\tLoss: 97.786377\n",
      "Train Epoch: 79 [58880/60000 (98%)]\tLoss: 100.784225\n",
      "====> Epoch: 79 Average loss: 97.2300\n",
      "====> Test set loss: 99.8560\n",
      "Train Epoch: 80 [0/60000 (0%)]\tLoss: 98.826157\n",
      "Train Epoch: 80 [1280/60000 (2%)]\tLoss: 95.669006\n",
      "Train Epoch: 80 [2560/60000 (4%)]\tLoss: 94.651047\n",
      "Train Epoch: 80 [3840/60000 (6%)]\tLoss: 94.581306\n",
      "Train Epoch: 80 [5120/60000 (9%)]\tLoss: 95.363388\n",
      "Train Epoch: 80 [6400/60000 (11%)]\tLoss: 97.851578\n",
      "Train Epoch: 80 [7680/60000 (13%)]\tLoss: 101.196930\n",
      "Train Epoch: 80 [8960/60000 (15%)]\tLoss: 97.901978\n",
      "Train Epoch: 80 [10240/60000 (17%)]\tLoss: 95.890373\n",
      "Train Epoch: 80 [11520/60000 (19%)]\tLoss: 94.797432\n",
      "Train Epoch: 80 [12800/60000 (21%)]\tLoss: 97.293198\n",
      "Train Epoch: 80 [14080/60000 (23%)]\tLoss: 96.562866\n",
      "Train Epoch: 80 [15360/60000 (26%)]\tLoss: 98.255966\n",
      "Train Epoch: 80 [16640/60000 (28%)]\tLoss: 95.375275\n",
      "Train Epoch: 80 [17920/60000 (30%)]\tLoss: 96.822472\n",
      "Train Epoch: 80 [19200/60000 (32%)]\tLoss: 98.524734\n",
      "Train Epoch: 80 [20480/60000 (34%)]\tLoss: 94.880524\n",
      "Train Epoch: 80 [21760/60000 (36%)]\tLoss: 97.158104\n",
      "Train Epoch: 80 [23040/60000 (38%)]\tLoss: 97.088280\n",
      "Train Epoch: 80 [24320/60000 (41%)]\tLoss: 105.996262\n",
      "Train Epoch: 80 [25600/60000 (43%)]\tLoss: 97.766327\n",
      "Train Epoch: 80 [26880/60000 (45%)]\tLoss: 94.087013\n",
      "Train Epoch: 80 [28160/60000 (47%)]\tLoss: 94.653458\n",
      "Train Epoch: 80 [29440/60000 (49%)]\tLoss: 101.896576\n",
      "Train Epoch: 80 [30720/60000 (51%)]\tLoss: 97.743843\n",
      "Train Epoch: 80 [32000/60000 (53%)]\tLoss: 95.193207\n",
      "Train Epoch: 80 [33280/60000 (55%)]\tLoss: 99.297409\n",
      "Train Epoch: 80 [34560/60000 (58%)]\tLoss: 100.020340\n",
      "Train Epoch: 80 [35840/60000 (60%)]\tLoss: 97.297478\n",
      "Train Epoch: 80 [37120/60000 (62%)]\tLoss: 98.828949\n",
      "Train Epoch: 80 [38400/60000 (64%)]\tLoss: 98.627701\n",
      "Train Epoch: 80 [39680/60000 (66%)]\tLoss: 98.411011\n",
      "Train Epoch: 80 [40960/60000 (68%)]\tLoss: 99.276215\n",
      "Train Epoch: 80 [42240/60000 (70%)]\tLoss: 95.259789\n",
      "Train Epoch: 80 [43520/60000 (72%)]\tLoss: 99.262123\n",
      "Train Epoch: 80 [44800/60000 (75%)]\tLoss: 96.676064\n",
      "Train Epoch: 80 [46080/60000 (77%)]\tLoss: 96.540573\n",
      "Train Epoch: 80 [47360/60000 (79%)]\tLoss: 98.076080\n",
      "Train Epoch: 80 [48640/60000 (81%)]\tLoss: 103.007355\n",
      "Train Epoch: 80 [49920/60000 (83%)]\tLoss: 98.850006\n",
      "Train Epoch: 80 [51200/60000 (85%)]\tLoss: 97.719986\n",
      "Train Epoch: 80 [52480/60000 (87%)]\tLoss: 97.658585\n",
      "Train Epoch: 80 [53760/60000 (90%)]\tLoss: 96.795181\n",
      "Train Epoch: 80 [55040/60000 (92%)]\tLoss: 95.234589\n",
      "Train Epoch: 80 [56320/60000 (94%)]\tLoss: 97.328842\n",
      "Train Epoch: 80 [57600/60000 (96%)]\tLoss: 100.977684\n",
      "Train Epoch: 80 [58880/60000 (98%)]\tLoss: 99.960312\n",
      "====> Epoch: 80 Average loss: 97.2512\n",
      "====> Test set loss: 99.2827\n",
      "Train Epoch: 81 [0/60000 (0%)]\tLoss: 93.507355\n",
      "Train Epoch: 81 [1280/60000 (2%)]\tLoss: 95.770615\n",
      "Train Epoch: 81 [2560/60000 (4%)]\tLoss: 95.151619\n",
      "Train Epoch: 81 [3840/60000 (6%)]\tLoss: 94.842575\n",
      "Train Epoch: 81 [5120/60000 (9%)]\tLoss: 95.979401\n",
      "Train Epoch: 81 [6400/60000 (11%)]\tLoss: 98.018356\n",
      "Train Epoch: 81 [7680/60000 (13%)]\tLoss: 96.566170\n",
      "Train Epoch: 81 [8960/60000 (15%)]\tLoss: 98.485718\n",
      "Train Epoch: 81 [10240/60000 (17%)]\tLoss: 100.164742\n",
      "Train Epoch: 81 [11520/60000 (19%)]\tLoss: 92.652428\n",
      "Train Epoch: 81 [12800/60000 (21%)]\tLoss: 97.828339\n",
      "Train Epoch: 81 [14080/60000 (23%)]\tLoss: 97.214859\n",
      "Train Epoch: 81 [15360/60000 (26%)]\tLoss: 94.373528\n",
      "Train Epoch: 81 [16640/60000 (28%)]\tLoss: 101.543762\n",
      "Train Epoch: 81 [17920/60000 (30%)]\tLoss: 95.627480\n",
      "Train Epoch: 81 [19200/60000 (32%)]\tLoss: 99.140366\n",
      "Train Epoch: 81 [20480/60000 (34%)]\tLoss: 94.883560\n",
      "Train Epoch: 81 [21760/60000 (36%)]\tLoss: 97.770882\n",
      "Train Epoch: 81 [23040/60000 (38%)]\tLoss: 93.015221\n",
      "Train Epoch: 81 [24320/60000 (41%)]\tLoss: 99.252510\n",
      "Train Epoch: 81 [25600/60000 (43%)]\tLoss: 96.239433\n",
      "Train Epoch: 81 [26880/60000 (45%)]\tLoss: 94.576164\n",
      "Train Epoch: 81 [28160/60000 (47%)]\tLoss: 98.185310\n",
      "Train Epoch: 81 [29440/60000 (49%)]\tLoss: 95.393059\n",
      "Train Epoch: 81 [30720/60000 (51%)]\tLoss: 97.689125\n",
      "Train Epoch: 81 [32000/60000 (53%)]\tLoss: 93.880066\n",
      "Train Epoch: 81 [33280/60000 (55%)]\tLoss: 94.071335\n",
      "Train Epoch: 81 [34560/60000 (58%)]\tLoss: 98.594040\n",
      "Train Epoch: 81 [35840/60000 (60%)]\tLoss: 96.499146\n",
      "Train Epoch: 81 [37120/60000 (62%)]\tLoss: 98.380646\n",
      "Train Epoch: 81 [38400/60000 (64%)]\tLoss: 95.491539\n",
      "Train Epoch: 81 [39680/60000 (66%)]\tLoss: 100.634789\n",
      "Train Epoch: 81 [40960/60000 (68%)]\tLoss: 97.071220\n",
      "Train Epoch: 81 [42240/60000 (70%)]\tLoss: 98.591682\n",
      "Train Epoch: 81 [43520/60000 (72%)]\tLoss: 99.307083\n",
      "Train Epoch: 81 [44800/60000 (75%)]\tLoss: 99.867378\n",
      "Train Epoch: 81 [46080/60000 (77%)]\tLoss: 98.333107\n",
      "Train Epoch: 81 [47360/60000 (79%)]\tLoss: 98.087723\n",
      "Train Epoch: 81 [48640/60000 (81%)]\tLoss: 94.408249\n",
      "Train Epoch: 81 [49920/60000 (83%)]\tLoss: 96.527802\n",
      "Train Epoch: 81 [51200/60000 (85%)]\tLoss: 97.840881\n",
      "Train Epoch: 81 [52480/60000 (87%)]\tLoss: 96.357971\n",
      "Train Epoch: 81 [53760/60000 (90%)]\tLoss: 98.307465\n",
      "Train Epoch: 81 [55040/60000 (92%)]\tLoss: 97.137825\n",
      "Train Epoch: 81 [56320/60000 (94%)]\tLoss: 97.577591\n",
      "Train Epoch: 81 [57600/60000 (96%)]\tLoss: 93.063972\n",
      "Train Epoch: 81 [58880/60000 (98%)]\tLoss: 101.154221\n",
      "====> Epoch: 81 Average loss: 97.2017\n",
      "====> Test set loss: 99.4667\n",
      "Train Epoch: 82 [0/60000 (0%)]\tLoss: 99.172997\n",
      "Train Epoch: 82 [1280/60000 (2%)]\tLoss: 99.379822\n",
      "Train Epoch: 82 [2560/60000 (4%)]\tLoss: 98.559113\n",
      "Train Epoch: 82 [3840/60000 (6%)]\tLoss: 97.860443\n",
      "Train Epoch: 82 [5120/60000 (9%)]\tLoss: 96.944061\n",
      "Train Epoch: 82 [6400/60000 (11%)]\tLoss: 98.392662\n",
      "Train Epoch: 82 [7680/60000 (13%)]\tLoss: 98.794647\n",
      "Train Epoch: 82 [8960/60000 (15%)]\tLoss: 97.131287\n",
      "Train Epoch: 82 [10240/60000 (17%)]\tLoss: 95.189362\n",
      "Train Epoch: 82 [11520/60000 (19%)]\tLoss: 99.075066\n",
      "Train Epoch: 82 [12800/60000 (21%)]\tLoss: 100.304520\n",
      "Train Epoch: 82 [14080/60000 (23%)]\tLoss: 95.702728\n",
      "Train Epoch: 82 [15360/60000 (26%)]\tLoss: 97.208794\n",
      "Train Epoch: 82 [16640/60000 (28%)]\tLoss: 100.835022\n",
      "Train Epoch: 82 [17920/60000 (30%)]\tLoss: 94.855774\n",
      "Train Epoch: 82 [19200/60000 (32%)]\tLoss: 96.974876\n",
      "Train Epoch: 82 [20480/60000 (34%)]\tLoss: 97.824333\n",
      "Train Epoch: 82 [21760/60000 (36%)]\tLoss: 97.667252\n",
      "Train Epoch: 82 [23040/60000 (38%)]\tLoss: 96.046814\n",
      "Train Epoch: 82 [24320/60000 (41%)]\tLoss: 92.802101\n",
      "Train Epoch: 82 [25600/60000 (43%)]\tLoss: 98.784431\n",
      "Train Epoch: 82 [26880/60000 (45%)]\tLoss: 101.105270\n",
      "Train Epoch: 82 [28160/60000 (47%)]\tLoss: 96.718269\n",
      "Train Epoch: 82 [29440/60000 (49%)]\tLoss: 94.197258\n",
      "Train Epoch: 82 [30720/60000 (51%)]\tLoss: 100.985641\n",
      "Train Epoch: 82 [32000/60000 (53%)]\tLoss: 99.670799\n",
      "Train Epoch: 82 [33280/60000 (55%)]\tLoss: 98.061180\n",
      "Train Epoch: 82 [34560/60000 (58%)]\tLoss: 99.806168\n",
      "Train Epoch: 82 [35840/60000 (60%)]\tLoss: 97.548676\n",
      "Train Epoch: 82 [37120/60000 (62%)]\tLoss: 96.379807\n",
      "Train Epoch: 82 [38400/60000 (64%)]\tLoss: 97.308807\n",
      "Train Epoch: 82 [39680/60000 (66%)]\tLoss: 94.989731\n",
      "Train Epoch: 82 [40960/60000 (68%)]\tLoss: 99.982864\n",
      "Train Epoch: 82 [42240/60000 (70%)]\tLoss: 99.651566\n",
      "Train Epoch: 82 [43520/60000 (72%)]\tLoss: 95.503555\n",
      "Train Epoch: 82 [44800/60000 (75%)]\tLoss: 97.629547\n",
      "Train Epoch: 82 [46080/60000 (77%)]\tLoss: 96.836212\n",
      "Train Epoch: 82 [47360/60000 (79%)]\tLoss: 94.579559\n",
      "Train Epoch: 82 [48640/60000 (81%)]\tLoss: 97.821999\n",
      "Train Epoch: 82 [49920/60000 (83%)]\tLoss: 95.061005\n",
      "Train Epoch: 82 [51200/60000 (85%)]\tLoss: 95.830795\n",
      "Train Epoch: 82 [52480/60000 (87%)]\tLoss: 95.216217\n",
      "Train Epoch: 82 [53760/60000 (90%)]\tLoss: 98.761497\n",
      "Train Epoch: 82 [55040/60000 (92%)]\tLoss: 95.551826\n",
      "Train Epoch: 82 [56320/60000 (94%)]\tLoss: 97.605423\n",
      "Train Epoch: 82 [57600/60000 (96%)]\tLoss: 95.477448\n",
      "Train Epoch: 82 [58880/60000 (98%)]\tLoss: 100.418251\n",
      "====> Epoch: 82 Average loss: 97.1699\n",
      "====> Test set loss: 99.3549\n",
      "Train Epoch: 83 [0/60000 (0%)]\tLoss: 96.053001\n",
      "Train Epoch: 83 [1280/60000 (2%)]\tLoss: 96.463852\n",
      "Train Epoch: 83 [2560/60000 (4%)]\tLoss: 96.501846\n",
      "Train Epoch: 83 [3840/60000 (6%)]\tLoss: 99.846069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 83 [5120/60000 (9%)]\tLoss: 96.675568\n",
      "Train Epoch: 83 [6400/60000 (11%)]\tLoss: 99.327621\n",
      "Train Epoch: 83 [7680/60000 (13%)]\tLoss: 97.888008\n",
      "Train Epoch: 83 [8960/60000 (15%)]\tLoss: 99.818642\n",
      "Train Epoch: 83 [10240/60000 (17%)]\tLoss: 96.206970\n",
      "Train Epoch: 83 [11520/60000 (19%)]\tLoss: 94.052444\n",
      "Train Epoch: 83 [12800/60000 (21%)]\tLoss: 93.123581\n",
      "Train Epoch: 83 [14080/60000 (23%)]\tLoss: 94.899399\n",
      "Train Epoch: 83 [15360/60000 (26%)]\tLoss: 97.908348\n",
      "Train Epoch: 83 [16640/60000 (28%)]\tLoss: 93.690865\n",
      "Train Epoch: 83 [17920/60000 (30%)]\tLoss: 99.444855\n",
      "Train Epoch: 83 [19200/60000 (32%)]\tLoss: 96.976974\n",
      "Train Epoch: 83 [20480/60000 (34%)]\tLoss: 96.307129\n",
      "Train Epoch: 83 [21760/60000 (36%)]\tLoss: 98.441635\n",
      "Train Epoch: 83 [23040/60000 (38%)]\tLoss: 96.788361\n",
      "Train Epoch: 83 [24320/60000 (41%)]\tLoss: 99.175995\n",
      "Train Epoch: 83 [25600/60000 (43%)]\tLoss: 99.424988\n",
      "Train Epoch: 83 [26880/60000 (45%)]\tLoss: 100.874146\n",
      "Train Epoch: 83 [28160/60000 (47%)]\tLoss: 99.444672\n",
      "Train Epoch: 83 [29440/60000 (49%)]\tLoss: 95.614609\n",
      "Train Epoch: 83 [30720/60000 (51%)]\tLoss: 96.874969\n",
      "Train Epoch: 83 [32000/60000 (53%)]\tLoss: 100.044754\n",
      "Train Epoch: 83 [33280/60000 (55%)]\tLoss: 97.636917\n",
      "Train Epoch: 83 [34560/60000 (58%)]\tLoss: 100.933983\n",
      "Train Epoch: 83 [35840/60000 (60%)]\tLoss: 98.274162\n",
      "Train Epoch: 83 [37120/60000 (62%)]\tLoss: 97.913109\n",
      "Train Epoch: 83 [38400/60000 (64%)]\tLoss: 99.479736\n",
      "Train Epoch: 83 [39680/60000 (66%)]\tLoss: 95.434135\n",
      "Train Epoch: 83 [40960/60000 (68%)]\tLoss: 101.331169\n",
      "Train Epoch: 83 [42240/60000 (70%)]\tLoss: 102.329079\n",
      "Train Epoch: 83 [43520/60000 (72%)]\tLoss: 101.063034\n",
      "Train Epoch: 83 [44800/60000 (75%)]\tLoss: 101.191925\n",
      "Train Epoch: 83 [46080/60000 (77%)]\tLoss: 97.254044\n",
      "Train Epoch: 83 [47360/60000 (79%)]\tLoss: 98.648582\n",
      "Train Epoch: 83 [48640/60000 (81%)]\tLoss: 97.847366\n",
      "Train Epoch: 83 [49920/60000 (83%)]\tLoss: 94.015488\n",
      "Train Epoch: 83 [51200/60000 (85%)]\tLoss: 99.173294\n",
      "Train Epoch: 83 [52480/60000 (87%)]\tLoss: 96.891663\n",
      "Train Epoch: 83 [53760/60000 (90%)]\tLoss: 97.132111\n",
      "Train Epoch: 83 [55040/60000 (92%)]\tLoss: 93.338470\n",
      "Train Epoch: 83 [56320/60000 (94%)]\tLoss: 97.346504\n",
      "Train Epoch: 83 [57600/60000 (96%)]\tLoss: 100.786102\n",
      "Train Epoch: 83 [58880/60000 (98%)]\tLoss: 96.407158\n",
      "====> Epoch: 83 Average loss: 97.1669\n",
      "====> Test set loss: 99.6957\n",
      "Train Epoch: 84 [0/60000 (0%)]\tLoss: 95.548416\n",
      "Train Epoch: 84 [1280/60000 (2%)]\tLoss: 96.674866\n",
      "Train Epoch: 84 [2560/60000 (4%)]\tLoss: 95.699722\n",
      "Train Epoch: 84 [3840/60000 (6%)]\tLoss: 96.844269\n",
      "Train Epoch: 84 [5120/60000 (9%)]\tLoss: 95.261734\n",
      "Train Epoch: 84 [6400/60000 (11%)]\tLoss: 97.393555\n",
      "Train Epoch: 84 [7680/60000 (13%)]\tLoss: 92.457718\n",
      "Train Epoch: 84 [8960/60000 (15%)]\tLoss: 95.905685\n",
      "Train Epoch: 84 [10240/60000 (17%)]\tLoss: 96.496399\n",
      "Train Epoch: 84 [11520/60000 (19%)]\tLoss: 95.224869\n",
      "Train Epoch: 84 [12800/60000 (21%)]\tLoss: 98.432129\n",
      "Train Epoch: 84 [14080/60000 (23%)]\tLoss: 96.342178\n",
      "Train Epoch: 84 [15360/60000 (26%)]\tLoss: 94.540016\n",
      "Train Epoch: 84 [16640/60000 (28%)]\tLoss: 99.250107\n",
      "Train Epoch: 84 [17920/60000 (30%)]\tLoss: 90.650131\n",
      "Train Epoch: 84 [19200/60000 (32%)]\tLoss: 98.908180\n",
      "Train Epoch: 84 [20480/60000 (34%)]\tLoss: 97.554199\n",
      "Train Epoch: 84 [21760/60000 (36%)]\tLoss: 97.886719\n",
      "Train Epoch: 84 [23040/60000 (38%)]\tLoss: 100.326782\n",
      "Train Epoch: 84 [24320/60000 (41%)]\tLoss: 95.153641\n",
      "Train Epoch: 84 [25600/60000 (43%)]\tLoss: 94.177200\n",
      "Train Epoch: 84 [26880/60000 (45%)]\tLoss: 99.385567\n",
      "Train Epoch: 84 [28160/60000 (47%)]\tLoss: 97.291222\n",
      "Train Epoch: 84 [29440/60000 (49%)]\tLoss: 94.230835\n",
      "Train Epoch: 84 [30720/60000 (51%)]\tLoss: 97.980148\n",
      "Train Epoch: 84 [32000/60000 (53%)]\tLoss: 96.878296\n",
      "Train Epoch: 84 [33280/60000 (55%)]\tLoss: 97.179184\n",
      "Train Epoch: 84 [34560/60000 (58%)]\tLoss: 98.006424\n",
      "Train Epoch: 84 [35840/60000 (60%)]\tLoss: 99.658264\n",
      "Train Epoch: 84 [37120/60000 (62%)]\tLoss: 96.037926\n",
      "Train Epoch: 84 [38400/60000 (64%)]\tLoss: 96.414688\n",
      "Train Epoch: 84 [39680/60000 (66%)]\tLoss: 94.551147\n",
      "Train Epoch: 84 [40960/60000 (68%)]\tLoss: 96.295532\n",
      "Train Epoch: 84 [42240/60000 (70%)]\tLoss: 96.562225\n",
      "Train Epoch: 84 [43520/60000 (72%)]\tLoss: 96.513275\n",
      "Train Epoch: 84 [44800/60000 (75%)]\tLoss: 98.480965\n",
      "Train Epoch: 84 [46080/60000 (77%)]\tLoss: 93.923668\n",
      "Train Epoch: 84 [47360/60000 (79%)]\tLoss: 99.493279\n",
      "Train Epoch: 84 [48640/60000 (81%)]\tLoss: 96.198715\n",
      "Train Epoch: 84 [49920/60000 (83%)]\tLoss: 99.030075\n",
      "Train Epoch: 84 [51200/60000 (85%)]\tLoss: 95.179024\n",
      "Train Epoch: 84 [52480/60000 (87%)]\tLoss: 97.159706\n",
      "Train Epoch: 84 [53760/60000 (90%)]\tLoss: 97.475220\n",
      "Train Epoch: 84 [55040/60000 (92%)]\tLoss: 99.724472\n",
      "Train Epoch: 84 [56320/60000 (94%)]\tLoss: 97.735901\n",
      "Train Epoch: 84 [57600/60000 (96%)]\tLoss: 98.953804\n",
      "Train Epoch: 84 [58880/60000 (98%)]\tLoss: 99.055801\n",
      "====> Epoch: 84 Average loss: 97.1362\n",
      "====> Test set loss: 99.0256\n",
      "Train Epoch: 85 [0/60000 (0%)]\tLoss: 100.548538\n",
      "Train Epoch: 85 [1280/60000 (2%)]\tLoss: 93.828613\n",
      "Train Epoch: 85 [2560/60000 (4%)]\tLoss: 99.999786\n",
      "Train Epoch: 85 [3840/60000 (6%)]\tLoss: 99.660034\n",
      "Train Epoch: 85 [5120/60000 (9%)]\tLoss: 96.139221\n",
      "Train Epoch: 85 [6400/60000 (11%)]\tLoss: 96.226166\n",
      "Train Epoch: 85 [7680/60000 (13%)]\tLoss: 97.534576\n",
      "Train Epoch: 85 [8960/60000 (15%)]\tLoss: 98.833847\n",
      "Train Epoch: 85 [10240/60000 (17%)]\tLoss: 99.916008\n",
      "Train Epoch: 85 [11520/60000 (19%)]\tLoss: 98.007393\n",
      "Train Epoch: 85 [12800/60000 (21%)]\tLoss: 97.640594\n",
      "Train Epoch: 85 [14080/60000 (23%)]\tLoss: 98.181450\n",
      "Train Epoch: 85 [15360/60000 (26%)]\tLoss: 96.995605\n",
      "Train Epoch: 85 [16640/60000 (28%)]\tLoss: 98.364441\n",
      "Train Epoch: 85 [17920/60000 (30%)]\tLoss: 97.919479\n",
      "Train Epoch: 85 [19200/60000 (32%)]\tLoss: 97.281723\n",
      "Train Epoch: 85 [20480/60000 (34%)]\tLoss: 100.778717\n",
      "Train Epoch: 85 [21760/60000 (36%)]\tLoss: 98.182510\n",
      "Train Epoch: 85 [23040/60000 (38%)]\tLoss: 100.140701\n",
      "Train Epoch: 85 [24320/60000 (41%)]\tLoss: 97.180412\n",
      "Train Epoch: 85 [25600/60000 (43%)]\tLoss: 101.172958\n",
      "Train Epoch: 85 [26880/60000 (45%)]\tLoss: 95.968964\n",
      "Train Epoch: 85 [28160/60000 (47%)]\tLoss: 100.811073\n",
      "Train Epoch: 85 [29440/60000 (49%)]\tLoss: 96.020348\n",
      "Train Epoch: 85 [30720/60000 (51%)]\tLoss: 98.401901\n",
      "Train Epoch: 85 [32000/60000 (53%)]\tLoss: 94.734482\n",
      "Train Epoch: 85 [33280/60000 (55%)]\tLoss: 98.933899\n",
      "Train Epoch: 85 [34560/60000 (58%)]\tLoss: 95.039612\n",
      "Train Epoch: 85 [35840/60000 (60%)]\tLoss: 98.461182\n",
      "Train Epoch: 85 [37120/60000 (62%)]\tLoss: 98.337494\n",
      "Train Epoch: 85 [38400/60000 (64%)]\tLoss: 98.743279\n",
      "Train Epoch: 85 [39680/60000 (66%)]\tLoss: 95.357025\n",
      "Train Epoch: 85 [40960/60000 (68%)]\tLoss: 96.248405\n",
      "Train Epoch: 85 [42240/60000 (70%)]\tLoss: 99.139549\n",
      "Train Epoch: 85 [43520/60000 (72%)]\tLoss: 98.210876\n",
      "Train Epoch: 85 [44800/60000 (75%)]\tLoss: 101.397514\n",
      "Train Epoch: 85 [46080/60000 (77%)]\tLoss: 98.672867\n",
      "Train Epoch: 85 [47360/60000 (79%)]\tLoss: 98.377914\n",
      "Train Epoch: 85 [48640/60000 (81%)]\tLoss: 98.214615\n",
      "Train Epoch: 85 [49920/60000 (83%)]\tLoss: 97.399025\n",
      "Train Epoch: 85 [51200/60000 (85%)]\tLoss: 98.426773\n",
      "Train Epoch: 85 [52480/60000 (87%)]\tLoss: 96.198074\n",
      "Train Epoch: 85 [53760/60000 (90%)]\tLoss: 97.507324\n",
      "Train Epoch: 85 [55040/60000 (92%)]\tLoss: 95.027725\n",
      "Train Epoch: 85 [56320/60000 (94%)]\tLoss: 94.932907\n",
      "Train Epoch: 85 [57600/60000 (96%)]\tLoss: 99.928261\n",
      "Train Epoch: 85 [58880/60000 (98%)]\tLoss: 94.960159\n",
      "====> Epoch: 85 Average loss: 97.1128\n",
      "====> Test set loss: 99.2963\n",
      "Train Epoch: 86 [0/60000 (0%)]\tLoss: 98.388519\n",
      "Train Epoch: 86 [1280/60000 (2%)]\tLoss: 94.981567\n",
      "Train Epoch: 86 [2560/60000 (4%)]\tLoss: 95.413612\n",
      "Train Epoch: 86 [3840/60000 (6%)]\tLoss: 96.609924\n",
      "Train Epoch: 86 [5120/60000 (9%)]\tLoss: 97.463577\n",
      "Train Epoch: 86 [6400/60000 (11%)]\tLoss: 95.507660\n",
      "Train Epoch: 86 [7680/60000 (13%)]\tLoss: 93.803467\n",
      "Train Epoch: 86 [8960/60000 (15%)]\tLoss: 94.544205\n",
      "Train Epoch: 86 [10240/60000 (17%)]\tLoss: 96.936165\n",
      "Train Epoch: 86 [11520/60000 (19%)]\tLoss: 95.509956\n",
      "Train Epoch: 86 [12800/60000 (21%)]\tLoss: 92.989166\n",
      "Train Epoch: 86 [14080/60000 (23%)]\tLoss: 97.328529\n",
      "Train Epoch: 86 [15360/60000 (26%)]\tLoss: 98.093071\n",
      "Train Epoch: 86 [16640/60000 (28%)]\tLoss: 98.606354\n",
      "Train Epoch: 86 [17920/60000 (30%)]\tLoss: 98.074249\n",
      "Train Epoch: 86 [19200/60000 (32%)]\tLoss: 94.535217\n",
      "Train Epoch: 86 [20480/60000 (34%)]\tLoss: 96.191742\n",
      "Train Epoch: 86 [21760/60000 (36%)]\tLoss: 99.296738\n",
      "Train Epoch: 86 [23040/60000 (38%)]\tLoss: 100.293930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 86 [24320/60000 (41%)]\tLoss: 95.244682\n",
      "Train Epoch: 86 [25600/60000 (43%)]\tLoss: 97.827820\n",
      "Train Epoch: 86 [26880/60000 (45%)]\tLoss: 95.262772\n",
      "Train Epoch: 86 [28160/60000 (47%)]\tLoss: 98.542572\n",
      "Train Epoch: 86 [29440/60000 (49%)]\tLoss: 95.672592\n",
      "Train Epoch: 86 [30720/60000 (51%)]\tLoss: 98.097244\n",
      "Train Epoch: 86 [32000/60000 (53%)]\tLoss: 98.227264\n",
      "Train Epoch: 86 [33280/60000 (55%)]\tLoss: 91.387329\n",
      "Train Epoch: 86 [34560/60000 (58%)]\tLoss: 96.120041\n",
      "Train Epoch: 86 [35840/60000 (60%)]\tLoss: 94.524918\n",
      "Train Epoch: 86 [37120/60000 (62%)]\tLoss: 96.377396\n",
      "Train Epoch: 86 [38400/60000 (64%)]\tLoss: 100.853271\n",
      "Train Epoch: 86 [39680/60000 (66%)]\tLoss: 94.683151\n",
      "Train Epoch: 86 [40960/60000 (68%)]\tLoss: 100.107895\n",
      "Train Epoch: 86 [42240/60000 (70%)]\tLoss: 95.999733\n",
      "Train Epoch: 86 [43520/60000 (72%)]\tLoss: 97.651222\n",
      "Train Epoch: 86 [44800/60000 (75%)]\tLoss: 97.035629\n",
      "Train Epoch: 86 [46080/60000 (77%)]\tLoss: 96.904144\n",
      "Train Epoch: 86 [47360/60000 (79%)]\tLoss: 98.704498\n",
      "Train Epoch: 86 [48640/60000 (81%)]\tLoss: 99.438828\n",
      "Train Epoch: 86 [49920/60000 (83%)]\tLoss: 93.197525\n",
      "Train Epoch: 86 [51200/60000 (85%)]\tLoss: 98.121880\n",
      "Train Epoch: 86 [52480/60000 (87%)]\tLoss: 96.739853\n",
      "Train Epoch: 86 [53760/60000 (90%)]\tLoss: 95.529190\n",
      "Train Epoch: 86 [55040/60000 (92%)]\tLoss: 100.219879\n",
      "Train Epoch: 86 [56320/60000 (94%)]\tLoss: 99.462799\n",
      "Train Epoch: 86 [57600/60000 (96%)]\tLoss: 97.525871\n",
      "Train Epoch: 86 [58880/60000 (98%)]\tLoss: 102.013367\n",
      "====> Epoch: 86 Average loss: 97.1072\n",
      "====> Test set loss: 99.0395\n",
      "Train Epoch: 87 [0/60000 (0%)]\tLoss: 97.277176\n",
      "Train Epoch: 87 [1280/60000 (2%)]\tLoss: 93.386139\n",
      "Train Epoch: 87 [2560/60000 (4%)]\tLoss: 97.321091\n",
      "Train Epoch: 87 [3840/60000 (6%)]\tLoss: 96.811775\n",
      "Train Epoch: 87 [5120/60000 (9%)]\tLoss: 96.896469\n",
      "Train Epoch: 87 [6400/60000 (11%)]\tLoss: 99.289291\n",
      "Train Epoch: 87 [7680/60000 (13%)]\tLoss: 93.022491\n",
      "Train Epoch: 87 [8960/60000 (15%)]\tLoss: 96.284134\n",
      "Train Epoch: 87 [10240/60000 (17%)]\tLoss: 96.632256\n",
      "Train Epoch: 87 [11520/60000 (19%)]\tLoss: 97.481178\n",
      "Train Epoch: 87 [12800/60000 (21%)]\tLoss: 96.338020\n",
      "Train Epoch: 87 [14080/60000 (23%)]\tLoss: 101.117722\n",
      "Train Epoch: 87 [15360/60000 (26%)]\tLoss: 99.060623\n",
      "Train Epoch: 87 [16640/60000 (28%)]\tLoss: 97.225510\n",
      "Train Epoch: 87 [17920/60000 (30%)]\tLoss: 95.625801\n",
      "Train Epoch: 87 [19200/60000 (32%)]\tLoss: 91.314941\n",
      "Train Epoch: 87 [20480/60000 (34%)]\tLoss: 100.615417\n",
      "Train Epoch: 87 [21760/60000 (36%)]\tLoss: 100.115288\n",
      "Train Epoch: 87 [23040/60000 (38%)]\tLoss: 98.650330\n",
      "Train Epoch: 87 [24320/60000 (41%)]\tLoss: 100.832237\n",
      "Train Epoch: 87 [25600/60000 (43%)]\tLoss: 99.445587\n",
      "Train Epoch: 87 [26880/60000 (45%)]\tLoss: 100.024971\n",
      "Train Epoch: 87 [28160/60000 (47%)]\tLoss: 93.851616\n",
      "Train Epoch: 87 [29440/60000 (49%)]\tLoss: 95.863266\n",
      "Train Epoch: 87 [30720/60000 (51%)]\tLoss: 98.049561\n",
      "Train Epoch: 87 [32000/60000 (53%)]\tLoss: 98.331032\n",
      "Train Epoch: 87 [33280/60000 (55%)]\tLoss: 98.183716\n",
      "Train Epoch: 87 [34560/60000 (58%)]\tLoss: 95.524162\n",
      "Train Epoch: 87 [35840/60000 (60%)]\tLoss: 99.132935\n",
      "Train Epoch: 87 [37120/60000 (62%)]\tLoss: 100.275711\n",
      "Train Epoch: 87 [38400/60000 (64%)]\tLoss: 94.401657\n",
      "Train Epoch: 87 [39680/60000 (66%)]\tLoss: 98.293747\n",
      "Train Epoch: 87 [40960/60000 (68%)]\tLoss: 95.860252\n",
      "Train Epoch: 87 [42240/60000 (70%)]\tLoss: 98.644577\n",
      "Train Epoch: 87 [43520/60000 (72%)]\tLoss: 95.319283\n",
      "Train Epoch: 87 [44800/60000 (75%)]\tLoss: 97.630600\n",
      "Train Epoch: 87 [46080/60000 (77%)]\tLoss: 101.274994\n",
      "Train Epoch: 87 [47360/60000 (79%)]\tLoss: 96.786613\n",
      "Train Epoch: 87 [48640/60000 (81%)]\tLoss: 95.240372\n",
      "Train Epoch: 87 [49920/60000 (83%)]\tLoss: 97.654396\n",
      "Train Epoch: 87 [51200/60000 (85%)]\tLoss: 97.281670\n",
      "Train Epoch: 87 [52480/60000 (87%)]\tLoss: 97.851181\n",
      "Train Epoch: 87 [53760/60000 (90%)]\tLoss: 94.966843\n",
      "Train Epoch: 87 [55040/60000 (92%)]\tLoss: 96.580200\n",
      "Train Epoch: 87 [56320/60000 (94%)]\tLoss: 93.100464\n",
      "Train Epoch: 87 [57600/60000 (96%)]\tLoss: 94.853081\n",
      "Train Epoch: 87 [58880/60000 (98%)]\tLoss: 96.032387\n",
      "====> Epoch: 87 Average loss: 97.0178\n",
      "====> Test set loss: 99.3423\n",
      "Train Epoch: 88 [0/60000 (0%)]\tLoss: 98.523361\n",
      "Train Epoch: 88 [1280/60000 (2%)]\tLoss: 95.514740\n",
      "Train Epoch: 88 [2560/60000 (4%)]\tLoss: 95.228226\n",
      "Train Epoch: 88 [3840/60000 (6%)]\tLoss: 96.177902\n",
      "Train Epoch: 88 [5120/60000 (9%)]\tLoss: 99.240082\n",
      "Train Epoch: 88 [6400/60000 (11%)]\tLoss: 97.133087\n",
      "Train Epoch: 88 [7680/60000 (13%)]\tLoss: 95.145050\n",
      "Train Epoch: 88 [8960/60000 (15%)]\tLoss: 99.473801\n",
      "Train Epoch: 88 [10240/60000 (17%)]\tLoss: 94.800423\n",
      "Train Epoch: 88 [11520/60000 (19%)]\tLoss: 99.010773\n",
      "Train Epoch: 88 [12800/60000 (21%)]\tLoss: 96.717773\n",
      "Train Epoch: 88 [14080/60000 (23%)]\tLoss: 96.784485\n",
      "Train Epoch: 88 [15360/60000 (26%)]\tLoss: 94.124626\n",
      "Train Epoch: 88 [16640/60000 (28%)]\tLoss: 95.581604\n",
      "Train Epoch: 88 [17920/60000 (30%)]\tLoss: 93.491669\n",
      "Train Epoch: 88 [19200/60000 (32%)]\tLoss: 94.400421\n",
      "Train Epoch: 88 [20480/60000 (34%)]\tLoss: 92.011024\n",
      "Train Epoch: 88 [21760/60000 (36%)]\tLoss: 97.422333\n",
      "Train Epoch: 88 [23040/60000 (38%)]\tLoss: 93.229240\n",
      "Train Epoch: 88 [24320/60000 (41%)]\tLoss: 100.549011\n",
      "Train Epoch: 88 [25600/60000 (43%)]\tLoss: 97.110641\n",
      "Train Epoch: 88 [26880/60000 (45%)]\tLoss: 100.672775\n",
      "Train Epoch: 88 [28160/60000 (47%)]\tLoss: 97.561432\n",
      "Train Epoch: 88 [29440/60000 (49%)]\tLoss: 93.404633\n",
      "Train Epoch: 88 [30720/60000 (51%)]\tLoss: 95.057037\n",
      "Train Epoch: 88 [32000/60000 (53%)]\tLoss: 99.095757\n",
      "Train Epoch: 88 [33280/60000 (55%)]\tLoss: 96.616928\n",
      "Train Epoch: 88 [34560/60000 (58%)]\tLoss: 96.241554\n",
      "Train Epoch: 88 [35840/60000 (60%)]\tLoss: 98.420776\n",
      "Train Epoch: 88 [37120/60000 (62%)]\tLoss: 91.701271\n",
      "Train Epoch: 88 [38400/60000 (64%)]\tLoss: 102.314041\n",
      "Train Epoch: 88 [39680/60000 (66%)]\tLoss: 98.082443\n",
      "Train Epoch: 88 [40960/60000 (68%)]\tLoss: 101.989777\n",
      "Train Epoch: 88 [42240/60000 (70%)]\tLoss: 96.244461\n",
      "Train Epoch: 88 [43520/60000 (72%)]\tLoss: 99.249573\n",
      "Train Epoch: 88 [44800/60000 (75%)]\tLoss: 98.522438\n",
      "Train Epoch: 88 [46080/60000 (77%)]\tLoss: 100.268333\n",
      "Train Epoch: 88 [47360/60000 (79%)]\tLoss: 97.165466\n",
      "Train Epoch: 88 [48640/60000 (81%)]\tLoss: 92.255219\n",
      "Train Epoch: 88 [49920/60000 (83%)]\tLoss: 97.134659\n",
      "Train Epoch: 88 [51200/60000 (85%)]\tLoss: 97.005211\n",
      "Train Epoch: 88 [52480/60000 (87%)]\tLoss: 95.930801\n",
      "Train Epoch: 88 [53760/60000 (90%)]\tLoss: 97.263931\n",
      "Train Epoch: 88 [55040/60000 (92%)]\tLoss: 100.101212\n",
      "Train Epoch: 88 [56320/60000 (94%)]\tLoss: 101.897888\n",
      "Train Epoch: 88 [57600/60000 (96%)]\tLoss: 98.341843\n",
      "Train Epoch: 88 [58880/60000 (98%)]\tLoss: 96.026436\n",
      "====> Epoch: 88 Average loss: 97.0713\n",
      "====> Test set loss: 99.1499\n",
      "Train Epoch: 89 [0/60000 (0%)]\tLoss: 98.441956\n",
      "Train Epoch: 89 [1280/60000 (2%)]\tLoss: 98.446823\n",
      "Train Epoch: 89 [2560/60000 (4%)]\tLoss: 97.556847\n",
      "Train Epoch: 89 [3840/60000 (6%)]\tLoss: 97.052734\n",
      "Train Epoch: 89 [5120/60000 (9%)]\tLoss: 97.381989\n",
      "Train Epoch: 89 [6400/60000 (11%)]\tLoss: 98.613541\n",
      "Train Epoch: 89 [7680/60000 (13%)]\tLoss: 99.378555\n",
      "Train Epoch: 89 [8960/60000 (15%)]\tLoss: 98.482765\n",
      "Train Epoch: 89 [10240/60000 (17%)]\tLoss: 97.982422\n",
      "Train Epoch: 89 [11520/60000 (19%)]\tLoss: 95.504898\n",
      "Train Epoch: 89 [12800/60000 (21%)]\tLoss: 100.183601\n",
      "Train Epoch: 89 [14080/60000 (23%)]\tLoss: 98.981842\n",
      "Train Epoch: 89 [15360/60000 (26%)]\tLoss: 93.592133\n",
      "Train Epoch: 89 [16640/60000 (28%)]\tLoss: 96.693481\n",
      "Train Epoch: 89 [17920/60000 (30%)]\tLoss: 98.311241\n",
      "Train Epoch: 89 [19200/60000 (32%)]\tLoss: 96.512512\n",
      "Train Epoch: 89 [20480/60000 (34%)]\tLoss: 100.569801\n",
      "Train Epoch: 89 [21760/60000 (36%)]\tLoss: 97.443115\n",
      "Train Epoch: 89 [23040/60000 (38%)]\tLoss: 97.130554\n",
      "Train Epoch: 89 [24320/60000 (41%)]\tLoss: 97.393051\n",
      "Train Epoch: 89 [25600/60000 (43%)]\tLoss: 96.135071\n",
      "Train Epoch: 89 [26880/60000 (45%)]\tLoss: 96.755203\n",
      "Train Epoch: 89 [28160/60000 (47%)]\tLoss: 94.026009\n",
      "Train Epoch: 89 [29440/60000 (49%)]\tLoss: 97.843468\n",
      "Train Epoch: 89 [30720/60000 (51%)]\tLoss: 94.841347\n",
      "Train Epoch: 89 [32000/60000 (53%)]\tLoss: 99.361816\n",
      "Train Epoch: 89 [33280/60000 (55%)]\tLoss: 95.008514\n",
      "Train Epoch: 89 [34560/60000 (58%)]\tLoss: 97.641876\n",
      "Train Epoch: 89 [35840/60000 (60%)]\tLoss: 98.045105\n",
      "Train Epoch: 89 [37120/60000 (62%)]\tLoss: 95.391891\n",
      "Train Epoch: 89 [38400/60000 (64%)]\tLoss: 91.782982\n",
      "Train Epoch: 89 [39680/60000 (66%)]\tLoss: 98.071678\n",
      "Train Epoch: 89 [40960/60000 (68%)]\tLoss: 93.304031\n",
      "Train Epoch: 89 [42240/60000 (70%)]\tLoss: 97.852646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 89 [43520/60000 (72%)]\tLoss: 97.312241\n",
      "Train Epoch: 89 [44800/60000 (75%)]\tLoss: 96.070274\n",
      "Train Epoch: 89 [46080/60000 (77%)]\tLoss: 98.044258\n",
      "Train Epoch: 89 [47360/60000 (79%)]\tLoss: 96.123123\n",
      "Train Epoch: 89 [48640/60000 (81%)]\tLoss: 97.924683\n",
      "Train Epoch: 89 [49920/60000 (83%)]\tLoss: 97.036682\n",
      "Train Epoch: 89 [51200/60000 (85%)]\tLoss: 95.806091\n",
      "Train Epoch: 89 [52480/60000 (87%)]\tLoss: 99.816902\n",
      "Train Epoch: 89 [53760/60000 (90%)]\tLoss: 96.708008\n",
      "Train Epoch: 89 [55040/60000 (92%)]\tLoss: 92.007309\n",
      "Train Epoch: 89 [56320/60000 (94%)]\tLoss: 98.488617\n",
      "Train Epoch: 89 [57600/60000 (96%)]\tLoss: 96.188202\n",
      "Train Epoch: 89 [58880/60000 (98%)]\tLoss: 98.599052\n",
      "====> Epoch: 89 Average loss: 97.0227\n",
      "====> Test set loss: 99.1738\n",
      "Train Epoch: 90 [0/60000 (0%)]\tLoss: 96.899994\n",
      "Train Epoch: 90 [1280/60000 (2%)]\tLoss: 95.779617\n",
      "Train Epoch: 90 [2560/60000 (4%)]\tLoss: 92.295654\n",
      "Train Epoch: 90 [3840/60000 (6%)]\tLoss: 98.830322\n",
      "Train Epoch: 90 [5120/60000 (9%)]\tLoss: 94.706245\n",
      "Train Epoch: 90 [6400/60000 (11%)]\tLoss: 96.867477\n",
      "Train Epoch: 90 [7680/60000 (13%)]\tLoss: 95.547249\n",
      "Train Epoch: 90 [8960/60000 (15%)]\tLoss: 98.373764\n",
      "Train Epoch: 90 [10240/60000 (17%)]\tLoss: 94.069328\n",
      "Train Epoch: 90 [11520/60000 (19%)]\tLoss: 99.210373\n",
      "Train Epoch: 90 [12800/60000 (21%)]\tLoss: 95.750580\n",
      "Train Epoch: 90 [14080/60000 (23%)]\tLoss: 96.617783\n",
      "Train Epoch: 90 [15360/60000 (26%)]\tLoss: 96.851677\n",
      "Train Epoch: 90 [16640/60000 (28%)]\tLoss: 95.151230\n",
      "Train Epoch: 90 [17920/60000 (30%)]\tLoss: 97.702591\n",
      "Train Epoch: 90 [19200/60000 (32%)]\tLoss: 97.024536\n",
      "Train Epoch: 90 [20480/60000 (34%)]\tLoss: 95.496185\n",
      "Train Epoch: 90 [21760/60000 (36%)]\tLoss: 97.925064\n",
      "Train Epoch: 90 [23040/60000 (38%)]\tLoss: 100.540695\n",
      "Train Epoch: 90 [24320/60000 (41%)]\tLoss: 94.625603\n",
      "Train Epoch: 90 [25600/60000 (43%)]\tLoss: 97.345169\n",
      "Train Epoch: 90 [26880/60000 (45%)]\tLoss: 96.224365\n",
      "Train Epoch: 90 [28160/60000 (47%)]\tLoss: 95.558846\n",
      "Train Epoch: 90 [29440/60000 (49%)]\tLoss: 96.044067\n",
      "Train Epoch: 90 [30720/60000 (51%)]\tLoss: 96.320305\n",
      "Train Epoch: 90 [32000/60000 (53%)]\tLoss: 98.056587\n",
      "Train Epoch: 90 [33280/60000 (55%)]\tLoss: 96.828613\n",
      "Train Epoch: 90 [34560/60000 (58%)]\tLoss: 95.480026\n",
      "Train Epoch: 90 [35840/60000 (60%)]\tLoss: 97.404053\n",
      "Train Epoch: 90 [37120/60000 (62%)]\tLoss: 97.538307\n",
      "Train Epoch: 90 [38400/60000 (64%)]\tLoss: 96.708687\n",
      "Train Epoch: 90 [39680/60000 (66%)]\tLoss: 93.301300\n",
      "Train Epoch: 90 [40960/60000 (68%)]\tLoss: 96.958633\n",
      "Train Epoch: 90 [42240/60000 (70%)]\tLoss: 91.196686\n",
      "Train Epoch: 90 [43520/60000 (72%)]\tLoss: 94.981186\n",
      "Train Epoch: 90 [44800/60000 (75%)]\tLoss: 97.294174\n",
      "Train Epoch: 90 [46080/60000 (77%)]\tLoss: 95.624046\n",
      "Train Epoch: 90 [47360/60000 (79%)]\tLoss: 97.496262\n",
      "Train Epoch: 90 [48640/60000 (81%)]\tLoss: 97.402451\n",
      "Train Epoch: 90 [49920/60000 (83%)]\tLoss: 98.738892\n",
      "Train Epoch: 90 [51200/60000 (85%)]\tLoss: 96.598740\n",
      "Train Epoch: 90 [52480/60000 (87%)]\tLoss: 98.336960\n",
      "Train Epoch: 90 [53760/60000 (90%)]\tLoss: 94.491867\n",
      "Train Epoch: 90 [55040/60000 (92%)]\tLoss: 98.510345\n",
      "Train Epoch: 90 [56320/60000 (94%)]\tLoss: 92.863037\n",
      "Train Epoch: 90 [57600/60000 (96%)]\tLoss: 96.538727\n",
      "Train Epoch: 90 [58880/60000 (98%)]\tLoss: 94.311462\n",
      "====> Epoch: 90 Average loss: 96.9786\n",
      "====> Test set loss: 99.4335\n",
      "Train Epoch: 91 [0/60000 (0%)]\tLoss: 94.885895\n",
      "Train Epoch: 91 [1280/60000 (2%)]\tLoss: 95.595207\n",
      "Train Epoch: 91 [2560/60000 (4%)]\tLoss: 96.130646\n",
      "Train Epoch: 91 [3840/60000 (6%)]\tLoss: 94.556244\n",
      "Train Epoch: 91 [5120/60000 (9%)]\tLoss: 96.497520\n",
      "Train Epoch: 91 [6400/60000 (11%)]\tLoss: 98.579666\n",
      "Train Epoch: 91 [7680/60000 (13%)]\tLoss: 99.055481\n",
      "Train Epoch: 91 [8960/60000 (15%)]\tLoss: 97.407990\n",
      "Train Epoch: 91 [10240/60000 (17%)]\tLoss: 98.847313\n",
      "Train Epoch: 91 [11520/60000 (19%)]\tLoss: 98.007263\n",
      "Train Epoch: 91 [12800/60000 (21%)]\tLoss: 97.239243\n",
      "Train Epoch: 91 [14080/60000 (23%)]\tLoss: 93.852753\n",
      "Train Epoch: 91 [15360/60000 (26%)]\tLoss: 98.279411\n",
      "Train Epoch: 91 [16640/60000 (28%)]\tLoss: 95.083679\n",
      "Train Epoch: 91 [17920/60000 (30%)]\tLoss: 99.263718\n",
      "Train Epoch: 91 [19200/60000 (32%)]\tLoss: 99.759216\n",
      "Train Epoch: 91 [20480/60000 (34%)]\tLoss: 98.000519\n",
      "Train Epoch: 91 [21760/60000 (36%)]\tLoss: 94.313202\n",
      "Train Epoch: 91 [23040/60000 (38%)]\tLoss: 97.756416\n",
      "Train Epoch: 91 [24320/60000 (41%)]\tLoss: 97.620468\n",
      "Train Epoch: 91 [25600/60000 (43%)]\tLoss: 96.063736\n",
      "Train Epoch: 91 [26880/60000 (45%)]\tLoss: 97.282715\n",
      "Train Epoch: 91 [28160/60000 (47%)]\tLoss: 95.006790\n",
      "Train Epoch: 91 [29440/60000 (49%)]\tLoss: 96.045250\n",
      "Train Epoch: 91 [30720/60000 (51%)]\tLoss: 101.571091\n",
      "Train Epoch: 91 [32000/60000 (53%)]\tLoss: 97.171585\n",
      "Train Epoch: 91 [33280/60000 (55%)]\tLoss: 95.357903\n",
      "Train Epoch: 91 [34560/60000 (58%)]\tLoss: 96.964157\n",
      "Train Epoch: 91 [35840/60000 (60%)]\tLoss: 98.152519\n",
      "Train Epoch: 91 [37120/60000 (62%)]\tLoss: 97.363464\n",
      "Train Epoch: 91 [38400/60000 (64%)]\tLoss: 92.483963\n",
      "Train Epoch: 91 [39680/60000 (66%)]\tLoss: 97.826958\n",
      "Train Epoch: 91 [40960/60000 (68%)]\tLoss: 98.920822\n",
      "Train Epoch: 91 [42240/60000 (70%)]\tLoss: 97.756111\n",
      "Train Epoch: 91 [43520/60000 (72%)]\tLoss: 99.900269\n",
      "Train Epoch: 91 [44800/60000 (75%)]\tLoss: 95.380180\n",
      "Train Epoch: 91 [46080/60000 (77%)]\tLoss: 100.554138\n",
      "Train Epoch: 91 [47360/60000 (79%)]\tLoss: 98.696793\n",
      "Train Epoch: 91 [48640/60000 (81%)]\tLoss: 97.561440\n",
      "Train Epoch: 91 [49920/60000 (83%)]\tLoss: 96.407631\n",
      "Train Epoch: 91 [51200/60000 (85%)]\tLoss: 96.984749\n",
      "Train Epoch: 91 [52480/60000 (87%)]\tLoss: 93.855652\n",
      "Train Epoch: 91 [53760/60000 (90%)]\tLoss: 98.426514\n",
      "Train Epoch: 91 [55040/60000 (92%)]\tLoss: 99.376259\n",
      "Train Epoch: 91 [56320/60000 (94%)]\tLoss: 94.049438\n",
      "Train Epoch: 91 [57600/60000 (96%)]\tLoss: 95.370758\n",
      "Train Epoch: 91 [58880/60000 (98%)]\tLoss: 95.152840\n",
      "====> Epoch: 91 Average loss: 96.9955\n",
      "====> Test set loss: 99.2537\n",
      "Train Epoch: 92 [0/60000 (0%)]\tLoss: 97.601608\n",
      "Train Epoch: 92 [1280/60000 (2%)]\tLoss: 100.493202\n",
      "Train Epoch: 92 [2560/60000 (4%)]\tLoss: 99.311966\n",
      "Train Epoch: 92 [3840/60000 (6%)]\tLoss: 93.973831\n",
      "Train Epoch: 92 [5120/60000 (9%)]\tLoss: 95.228050\n",
      "Train Epoch: 92 [6400/60000 (11%)]\tLoss: 95.110901\n",
      "Train Epoch: 92 [7680/60000 (13%)]\tLoss: 95.478134\n",
      "Train Epoch: 92 [8960/60000 (15%)]\tLoss: 97.153053\n",
      "Train Epoch: 92 [10240/60000 (17%)]\tLoss: 93.514648\n",
      "Train Epoch: 92 [11520/60000 (19%)]\tLoss: 96.097824\n",
      "Train Epoch: 92 [12800/60000 (21%)]\tLoss: 98.911568\n",
      "Train Epoch: 92 [14080/60000 (23%)]\tLoss: 96.466156\n",
      "Train Epoch: 92 [15360/60000 (26%)]\tLoss: 95.571663\n",
      "Train Epoch: 92 [16640/60000 (28%)]\tLoss: 98.608719\n",
      "Train Epoch: 92 [17920/60000 (30%)]\tLoss: 95.445068\n",
      "Train Epoch: 92 [19200/60000 (32%)]\tLoss: 93.104118\n",
      "Train Epoch: 92 [20480/60000 (34%)]\tLoss: 99.879852\n",
      "Train Epoch: 92 [21760/60000 (36%)]\tLoss: 97.866417\n",
      "Train Epoch: 92 [23040/60000 (38%)]\tLoss: 98.598740\n",
      "Train Epoch: 92 [24320/60000 (41%)]\tLoss: 98.042152\n",
      "Train Epoch: 92 [25600/60000 (43%)]\tLoss: 96.694031\n",
      "Train Epoch: 92 [26880/60000 (45%)]\tLoss: 98.870178\n",
      "Train Epoch: 92 [28160/60000 (47%)]\tLoss: 96.622902\n",
      "Train Epoch: 92 [29440/60000 (49%)]\tLoss: 94.587547\n",
      "Train Epoch: 92 [30720/60000 (51%)]\tLoss: 99.240891\n",
      "Train Epoch: 92 [32000/60000 (53%)]\tLoss: 96.294495\n",
      "Train Epoch: 92 [33280/60000 (55%)]\tLoss: 99.638901\n",
      "Train Epoch: 92 [34560/60000 (58%)]\tLoss: 94.339943\n",
      "Train Epoch: 92 [35840/60000 (60%)]\tLoss: 97.579094\n",
      "Train Epoch: 92 [37120/60000 (62%)]\tLoss: 96.656738\n",
      "Train Epoch: 92 [38400/60000 (64%)]\tLoss: 100.698471\n",
      "Train Epoch: 92 [39680/60000 (66%)]\tLoss: 94.622864\n",
      "Train Epoch: 92 [40960/60000 (68%)]\tLoss: 93.128632\n",
      "Train Epoch: 92 [42240/60000 (70%)]\tLoss: 96.433044\n",
      "Train Epoch: 92 [43520/60000 (72%)]\tLoss: 98.299606\n",
      "Train Epoch: 92 [44800/60000 (75%)]\tLoss: 94.383987\n",
      "Train Epoch: 92 [46080/60000 (77%)]\tLoss: 97.267975\n",
      "Train Epoch: 92 [47360/60000 (79%)]\tLoss: 97.889755\n",
      "Train Epoch: 92 [48640/60000 (81%)]\tLoss: 99.561516\n",
      "Train Epoch: 92 [49920/60000 (83%)]\tLoss: 99.640182\n",
      "Train Epoch: 92 [51200/60000 (85%)]\tLoss: 96.382805\n",
      "Train Epoch: 92 [52480/60000 (87%)]\tLoss: 96.778519\n",
      "Train Epoch: 92 [53760/60000 (90%)]\tLoss: 99.555779\n",
      "Train Epoch: 92 [55040/60000 (92%)]\tLoss: 98.614960\n",
      "Train Epoch: 92 [56320/60000 (94%)]\tLoss: 96.687225\n",
      "Train Epoch: 92 [57600/60000 (96%)]\tLoss: 96.125809\n",
      "Train Epoch: 92 [58880/60000 (98%)]\tLoss: 98.853928\n",
      "====> Epoch: 92 Average loss: 96.9646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 99.1596\n",
      "Train Epoch: 93 [0/60000 (0%)]\tLoss: 94.865540\n",
      "Train Epoch: 93 [1280/60000 (2%)]\tLoss: 94.800018\n",
      "Train Epoch: 93 [2560/60000 (4%)]\tLoss: 96.009605\n",
      "Train Epoch: 93 [3840/60000 (6%)]\tLoss: 99.285179\n",
      "Train Epoch: 93 [5120/60000 (9%)]\tLoss: 100.211433\n",
      "Train Epoch: 93 [6400/60000 (11%)]\tLoss: 95.819748\n",
      "Train Epoch: 93 [7680/60000 (13%)]\tLoss: 95.924667\n",
      "Train Epoch: 93 [8960/60000 (15%)]\tLoss: 94.480637\n",
      "Train Epoch: 93 [10240/60000 (17%)]\tLoss: 94.362335\n",
      "Train Epoch: 93 [11520/60000 (19%)]\tLoss: 94.969948\n",
      "Train Epoch: 93 [12800/60000 (21%)]\tLoss: 96.116219\n",
      "Train Epoch: 93 [14080/60000 (23%)]\tLoss: 97.303818\n",
      "Train Epoch: 93 [15360/60000 (26%)]\tLoss: 93.508224\n",
      "Train Epoch: 93 [16640/60000 (28%)]\tLoss: 96.978889\n",
      "Train Epoch: 93 [17920/60000 (30%)]\tLoss: 99.987564\n",
      "Train Epoch: 93 [19200/60000 (32%)]\tLoss: 94.318611\n",
      "Train Epoch: 93 [20480/60000 (34%)]\tLoss: 93.916870\n",
      "Train Epoch: 93 [21760/60000 (36%)]\tLoss: 99.071136\n",
      "Train Epoch: 93 [23040/60000 (38%)]\tLoss: 99.678917\n",
      "Train Epoch: 93 [24320/60000 (41%)]\tLoss: 96.772820\n",
      "Train Epoch: 93 [25600/60000 (43%)]\tLoss: 95.913628\n",
      "Train Epoch: 93 [26880/60000 (45%)]\tLoss: 96.199966\n",
      "Train Epoch: 93 [28160/60000 (47%)]\tLoss: 99.470566\n",
      "Train Epoch: 93 [29440/60000 (49%)]\tLoss: 95.869354\n",
      "Train Epoch: 93 [30720/60000 (51%)]\tLoss: 96.975067\n",
      "Train Epoch: 93 [32000/60000 (53%)]\tLoss: 97.461700\n",
      "Train Epoch: 93 [33280/60000 (55%)]\tLoss: 92.165192\n",
      "Train Epoch: 93 [34560/60000 (58%)]\tLoss: 101.114601\n",
      "Train Epoch: 93 [35840/60000 (60%)]\tLoss: 96.234482\n",
      "Train Epoch: 93 [37120/60000 (62%)]\tLoss: 99.462166\n",
      "Train Epoch: 93 [38400/60000 (64%)]\tLoss: 97.762840\n",
      "Train Epoch: 93 [39680/60000 (66%)]\tLoss: 96.213242\n",
      "Train Epoch: 93 [40960/60000 (68%)]\tLoss: 96.812408\n",
      "Train Epoch: 93 [42240/60000 (70%)]\tLoss: 93.465111\n",
      "Train Epoch: 93 [43520/60000 (72%)]\tLoss: 93.066254\n",
      "Train Epoch: 93 [44800/60000 (75%)]\tLoss: 100.593857\n",
      "Train Epoch: 93 [46080/60000 (77%)]\tLoss: 98.807983\n",
      "Train Epoch: 93 [47360/60000 (79%)]\tLoss: 99.389381\n",
      "Train Epoch: 93 [48640/60000 (81%)]\tLoss: 96.395882\n",
      "Train Epoch: 93 [49920/60000 (83%)]\tLoss: 96.454185\n",
      "Train Epoch: 93 [51200/60000 (85%)]\tLoss: 96.299377\n",
      "Train Epoch: 93 [52480/60000 (87%)]\tLoss: 95.880066\n",
      "Train Epoch: 93 [53760/60000 (90%)]\tLoss: 95.408463\n",
      "Train Epoch: 93 [55040/60000 (92%)]\tLoss: 99.693741\n",
      "Train Epoch: 93 [56320/60000 (94%)]\tLoss: 98.671951\n",
      "Train Epoch: 93 [57600/60000 (96%)]\tLoss: 92.339073\n",
      "Train Epoch: 93 [58880/60000 (98%)]\tLoss: 98.016937\n",
      "====> Epoch: 93 Average loss: 96.9264\n",
      "====> Test set loss: 99.1257\n",
      "Train Epoch: 94 [0/60000 (0%)]\tLoss: 91.672836\n",
      "Train Epoch: 94 [1280/60000 (2%)]\tLoss: 96.607742\n",
      "Train Epoch: 94 [2560/60000 (4%)]\tLoss: 96.521194\n",
      "Train Epoch: 94 [3840/60000 (6%)]\tLoss: 97.170380\n",
      "Train Epoch: 94 [5120/60000 (9%)]\tLoss: 96.098183\n",
      "Train Epoch: 94 [6400/60000 (11%)]\tLoss: 95.223587\n",
      "Train Epoch: 94 [7680/60000 (13%)]\tLoss: 97.175873\n",
      "Train Epoch: 94 [8960/60000 (15%)]\tLoss: 99.035439\n",
      "Train Epoch: 94 [10240/60000 (17%)]\tLoss: 98.497276\n",
      "Train Epoch: 94 [11520/60000 (19%)]\tLoss: 96.845779\n",
      "Train Epoch: 94 [12800/60000 (21%)]\tLoss: 94.258408\n",
      "Train Epoch: 94 [14080/60000 (23%)]\tLoss: 96.895660\n",
      "Train Epoch: 94 [15360/60000 (26%)]\tLoss: 96.092384\n",
      "Train Epoch: 94 [16640/60000 (28%)]\tLoss: 95.209824\n",
      "Train Epoch: 94 [17920/60000 (30%)]\tLoss: 99.909164\n",
      "Train Epoch: 94 [19200/60000 (32%)]\tLoss: 97.534332\n",
      "Train Epoch: 94 [20480/60000 (34%)]\tLoss: 96.303436\n",
      "Train Epoch: 94 [21760/60000 (36%)]\tLoss: 95.965485\n",
      "Train Epoch: 94 [23040/60000 (38%)]\tLoss: 95.976486\n",
      "Train Epoch: 94 [24320/60000 (41%)]\tLoss: 98.547173\n",
      "Train Epoch: 94 [25600/60000 (43%)]\tLoss: 97.021622\n",
      "Train Epoch: 94 [26880/60000 (45%)]\tLoss: 96.047585\n",
      "Train Epoch: 94 [28160/60000 (47%)]\tLoss: 96.691879\n",
      "Train Epoch: 94 [29440/60000 (49%)]\tLoss: 96.250984\n",
      "Train Epoch: 94 [30720/60000 (51%)]\tLoss: 100.469284\n",
      "Train Epoch: 94 [32000/60000 (53%)]\tLoss: 94.930298\n",
      "Train Epoch: 94 [33280/60000 (55%)]\tLoss: 96.404167\n",
      "Train Epoch: 94 [34560/60000 (58%)]\tLoss: 99.799026\n",
      "Train Epoch: 94 [35840/60000 (60%)]\tLoss: 98.573669\n",
      "Train Epoch: 94 [37120/60000 (62%)]\tLoss: 95.759933\n",
      "Train Epoch: 94 [38400/60000 (64%)]\tLoss: 100.140411\n",
      "Train Epoch: 94 [39680/60000 (66%)]\tLoss: 95.612526\n",
      "Train Epoch: 94 [40960/60000 (68%)]\tLoss: 95.109612\n",
      "Train Epoch: 94 [42240/60000 (70%)]\tLoss: 94.383583\n",
      "Train Epoch: 94 [43520/60000 (72%)]\tLoss: 97.629761\n",
      "Train Epoch: 94 [44800/60000 (75%)]\tLoss: 98.007027\n",
      "Train Epoch: 94 [46080/60000 (77%)]\tLoss: 95.309723\n",
      "Train Epoch: 94 [47360/60000 (79%)]\tLoss: 100.145592\n",
      "Train Epoch: 94 [48640/60000 (81%)]\tLoss: 97.202362\n",
      "Train Epoch: 94 [49920/60000 (83%)]\tLoss: 96.974625\n",
      "Train Epoch: 94 [51200/60000 (85%)]\tLoss: 94.426788\n",
      "Train Epoch: 94 [52480/60000 (87%)]\tLoss: 99.808647\n",
      "Train Epoch: 94 [53760/60000 (90%)]\tLoss: 94.867554\n",
      "Train Epoch: 94 [55040/60000 (92%)]\tLoss: 98.179085\n",
      "Train Epoch: 94 [56320/60000 (94%)]\tLoss: 97.636734\n",
      "Train Epoch: 94 [57600/60000 (96%)]\tLoss: 96.239708\n",
      "Train Epoch: 94 [58880/60000 (98%)]\tLoss: 93.945282\n",
      "====> Epoch: 94 Average loss: 96.8941\n",
      "====> Test set loss: 99.1499\n",
      "Train Epoch: 95 [0/60000 (0%)]\tLoss: 96.940598\n",
      "Train Epoch: 95 [1280/60000 (2%)]\tLoss: 100.195145\n",
      "Train Epoch: 95 [2560/60000 (4%)]\tLoss: 96.210678\n",
      "Train Epoch: 95 [3840/60000 (6%)]\tLoss: 98.752617\n",
      "Train Epoch: 95 [5120/60000 (9%)]\tLoss: 101.761978\n",
      "Train Epoch: 95 [6400/60000 (11%)]\tLoss: 94.377747\n",
      "Train Epoch: 95 [7680/60000 (13%)]\tLoss: 95.573334\n",
      "Train Epoch: 95 [8960/60000 (15%)]\tLoss: 94.306541\n",
      "Train Epoch: 95 [10240/60000 (17%)]\tLoss: 96.077248\n",
      "Train Epoch: 95 [11520/60000 (19%)]\tLoss: 94.170258\n",
      "Train Epoch: 95 [12800/60000 (21%)]\tLoss: 97.137512\n",
      "Train Epoch: 95 [14080/60000 (23%)]\tLoss: 95.505371\n",
      "Train Epoch: 95 [15360/60000 (26%)]\tLoss: 97.853943\n",
      "Train Epoch: 95 [16640/60000 (28%)]\tLoss: 94.476685\n",
      "Train Epoch: 95 [17920/60000 (30%)]\tLoss: 95.364937\n",
      "Train Epoch: 95 [19200/60000 (32%)]\tLoss: 99.338089\n",
      "Train Epoch: 95 [20480/60000 (34%)]\tLoss: 94.458977\n",
      "Train Epoch: 95 [21760/60000 (36%)]\tLoss: 96.607651\n",
      "Train Epoch: 95 [23040/60000 (38%)]\tLoss: 99.779228\n",
      "Train Epoch: 95 [24320/60000 (41%)]\tLoss: 96.949333\n",
      "Train Epoch: 95 [25600/60000 (43%)]\tLoss: 95.853073\n",
      "Train Epoch: 95 [26880/60000 (45%)]\tLoss: 96.481384\n",
      "Train Epoch: 95 [28160/60000 (47%)]\tLoss: 98.157822\n",
      "Train Epoch: 95 [29440/60000 (49%)]\tLoss: 93.024834\n",
      "Train Epoch: 95 [30720/60000 (51%)]\tLoss: 96.152405\n",
      "Train Epoch: 95 [32000/60000 (53%)]\tLoss: 96.642632\n",
      "Train Epoch: 95 [33280/60000 (55%)]\tLoss: 98.290451\n",
      "Train Epoch: 95 [34560/60000 (58%)]\tLoss: 97.375984\n",
      "Train Epoch: 95 [35840/60000 (60%)]\tLoss: 98.060577\n",
      "Train Epoch: 95 [37120/60000 (62%)]\tLoss: 97.614342\n",
      "Train Epoch: 95 [38400/60000 (64%)]\tLoss: 95.198296\n",
      "Train Epoch: 95 [39680/60000 (66%)]\tLoss: 94.936600\n",
      "Train Epoch: 95 [40960/60000 (68%)]\tLoss: 98.962250\n",
      "Train Epoch: 95 [42240/60000 (70%)]\tLoss: 96.825096\n",
      "Train Epoch: 95 [43520/60000 (72%)]\tLoss: 95.910477\n",
      "Train Epoch: 95 [44800/60000 (75%)]\tLoss: 92.111343\n",
      "Train Epoch: 95 [46080/60000 (77%)]\tLoss: 98.653740\n",
      "Train Epoch: 95 [47360/60000 (79%)]\tLoss: 95.086349\n",
      "Train Epoch: 95 [48640/60000 (81%)]\tLoss: 98.705559\n",
      "Train Epoch: 95 [49920/60000 (83%)]\tLoss: 96.854301\n",
      "Train Epoch: 95 [51200/60000 (85%)]\tLoss: 100.066269\n",
      "Train Epoch: 95 [52480/60000 (87%)]\tLoss: 98.575829\n",
      "Train Epoch: 95 [53760/60000 (90%)]\tLoss: 96.083626\n",
      "Train Epoch: 95 [55040/60000 (92%)]\tLoss: 94.706963\n",
      "Train Epoch: 95 [56320/60000 (94%)]\tLoss: 96.385094\n",
      "Train Epoch: 95 [57600/60000 (96%)]\tLoss: 97.686615\n",
      "Train Epoch: 95 [58880/60000 (98%)]\tLoss: 99.561295\n",
      "====> Epoch: 95 Average loss: 96.9153\n",
      "====> Test set loss: 99.4033\n",
      "Train Epoch: 96 [0/60000 (0%)]\tLoss: 93.264717\n",
      "Train Epoch: 96 [1280/60000 (2%)]\tLoss: 95.336922\n",
      "Train Epoch: 96 [2560/60000 (4%)]\tLoss: 97.913544\n",
      "Train Epoch: 96 [3840/60000 (6%)]\tLoss: 96.856079\n",
      "Train Epoch: 96 [5120/60000 (9%)]\tLoss: 98.583282\n",
      "Train Epoch: 96 [6400/60000 (11%)]\tLoss: 95.136993\n",
      "Train Epoch: 96 [7680/60000 (13%)]\tLoss: 94.635086\n",
      "Train Epoch: 96 [8960/60000 (15%)]\tLoss: 97.171249\n",
      "Train Epoch: 96 [10240/60000 (17%)]\tLoss: 99.899452\n",
      "Train Epoch: 96 [11520/60000 (19%)]\tLoss: 98.846321\n",
      "Train Epoch: 96 [12800/60000 (21%)]\tLoss: 97.233810\n",
      "Train Epoch: 96 [14080/60000 (23%)]\tLoss: 97.613274\n",
      "Train Epoch: 96 [15360/60000 (26%)]\tLoss: 95.887543\n",
      "Train Epoch: 96 [16640/60000 (28%)]\tLoss: 95.811066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 96 [17920/60000 (30%)]\tLoss: 96.940872\n",
      "Train Epoch: 96 [19200/60000 (32%)]\tLoss: 93.953384\n",
      "Train Epoch: 96 [20480/60000 (34%)]\tLoss: 96.575089\n",
      "Train Epoch: 96 [21760/60000 (36%)]\tLoss: 96.910049\n",
      "Train Epoch: 96 [23040/60000 (38%)]\tLoss: 97.133316\n",
      "Train Epoch: 96 [24320/60000 (41%)]\tLoss: 94.764526\n",
      "Train Epoch: 96 [25600/60000 (43%)]\tLoss: 99.079918\n",
      "Train Epoch: 96 [26880/60000 (45%)]\tLoss: 97.491585\n",
      "Train Epoch: 96 [28160/60000 (47%)]\tLoss: 100.486176\n",
      "Train Epoch: 96 [29440/60000 (49%)]\tLoss: 98.032997\n",
      "Train Epoch: 96 [30720/60000 (51%)]\tLoss: 100.690765\n",
      "Train Epoch: 96 [32000/60000 (53%)]\tLoss: 96.075851\n",
      "Train Epoch: 96 [33280/60000 (55%)]\tLoss: 98.657074\n",
      "Train Epoch: 96 [34560/60000 (58%)]\tLoss: 99.846924\n",
      "Train Epoch: 96 [35840/60000 (60%)]\tLoss: 98.552834\n",
      "Train Epoch: 96 [37120/60000 (62%)]\tLoss: 97.662849\n",
      "Train Epoch: 96 [38400/60000 (64%)]\tLoss: 96.401398\n",
      "Train Epoch: 96 [39680/60000 (66%)]\tLoss: 99.801910\n",
      "Train Epoch: 96 [40960/60000 (68%)]\tLoss: 96.867378\n",
      "Train Epoch: 96 [42240/60000 (70%)]\tLoss: 95.854149\n",
      "Train Epoch: 96 [43520/60000 (72%)]\tLoss: 96.140137\n",
      "Train Epoch: 96 [44800/60000 (75%)]\tLoss: 97.726669\n",
      "Train Epoch: 96 [46080/60000 (77%)]\tLoss: 97.331642\n",
      "Train Epoch: 96 [47360/60000 (79%)]\tLoss: 98.401398\n",
      "Train Epoch: 96 [48640/60000 (81%)]\tLoss: 96.678894\n",
      "Train Epoch: 96 [49920/60000 (83%)]\tLoss: 95.036743\n",
      "Train Epoch: 96 [51200/60000 (85%)]\tLoss: 96.472946\n",
      "Train Epoch: 96 [52480/60000 (87%)]\tLoss: 99.711784\n",
      "Train Epoch: 96 [53760/60000 (90%)]\tLoss: 98.393417\n",
      "Train Epoch: 96 [55040/60000 (92%)]\tLoss: 98.839996\n",
      "Train Epoch: 96 [56320/60000 (94%)]\tLoss: 97.994965\n",
      "Train Epoch: 96 [57600/60000 (96%)]\tLoss: 97.496025\n",
      "Train Epoch: 96 [58880/60000 (98%)]\tLoss: 95.681610\n",
      "====> Epoch: 96 Average loss: 96.8800\n",
      "====> Test set loss: 99.4504\n",
      "Train Epoch: 97 [0/60000 (0%)]\tLoss: 100.577957\n",
      "Train Epoch: 97 [1280/60000 (2%)]\tLoss: 96.460594\n",
      "Train Epoch: 97 [2560/60000 (4%)]\tLoss: 96.728241\n",
      "Train Epoch: 97 [3840/60000 (6%)]\tLoss: 95.248352\n",
      "Train Epoch: 97 [5120/60000 (9%)]\tLoss: 94.477043\n",
      "Train Epoch: 97 [6400/60000 (11%)]\tLoss: 96.894814\n",
      "Train Epoch: 97 [7680/60000 (13%)]\tLoss: 96.952194\n",
      "Train Epoch: 97 [8960/60000 (15%)]\tLoss: 93.061493\n",
      "Train Epoch: 97 [10240/60000 (17%)]\tLoss: 96.220573\n",
      "Train Epoch: 97 [11520/60000 (19%)]\tLoss: 97.025604\n",
      "Train Epoch: 97 [12800/60000 (21%)]\tLoss: 94.367767\n",
      "Train Epoch: 97 [14080/60000 (23%)]\tLoss: 95.204353\n",
      "Train Epoch: 97 [15360/60000 (26%)]\tLoss: 97.440163\n",
      "Train Epoch: 97 [16640/60000 (28%)]\tLoss: 93.895790\n",
      "Train Epoch: 97 [17920/60000 (30%)]\tLoss: 96.864929\n",
      "Train Epoch: 97 [19200/60000 (32%)]\tLoss: 99.614105\n",
      "Train Epoch: 97 [20480/60000 (34%)]\tLoss: 96.000626\n",
      "Train Epoch: 97 [21760/60000 (36%)]\tLoss: 97.015839\n",
      "Train Epoch: 97 [23040/60000 (38%)]\tLoss: 94.157196\n",
      "Train Epoch: 97 [24320/60000 (41%)]\tLoss: 96.803627\n",
      "Train Epoch: 97 [25600/60000 (43%)]\tLoss: 98.281204\n",
      "Train Epoch: 97 [26880/60000 (45%)]\tLoss: 100.134277\n",
      "Train Epoch: 97 [28160/60000 (47%)]\tLoss: 98.175690\n",
      "Train Epoch: 97 [29440/60000 (49%)]\tLoss: 97.242737\n",
      "Train Epoch: 97 [30720/60000 (51%)]\tLoss: 100.724739\n",
      "Train Epoch: 97 [32000/60000 (53%)]\tLoss: 99.705643\n",
      "Train Epoch: 97 [33280/60000 (55%)]\tLoss: 95.937759\n",
      "Train Epoch: 97 [34560/60000 (58%)]\tLoss: 98.170105\n",
      "Train Epoch: 97 [35840/60000 (60%)]\tLoss: 97.174072\n",
      "Train Epoch: 97 [37120/60000 (62%)]\tLoss: 98.472816\n",
      "Train Epoch: 97 [38400/60000 (64%)]\tLoss: 97.766205\n",
      "Train Epoch: 97 [39680/60000 (66%)]\tLoss: 93.386566\n",
      "Train Epoch: 97 [40960/60000 (68%)]\tLoss: 97.437851\n",
      "Train Epoch: 97 [42240/60000 (70%)]\tLoss: 98.391144\n",
      "Train Epoch: 97 [43520/60000 (72%)]\tLoss: 99.036179\n",
      "Train Epoch: 97 [44800/60000 (75%)]\tLoss: 93.061859\n",
      "Train Epoch: 97 [46080/60000 (77%)]\tLoss: 96.704826\n",
      "Train Epoch: 97 [47360/60000 (79%)]\tLoss: 98.637413\n",
      "Train Epoch: 97 [48640/60000 (81%)]\tLoss: 98.532639\n",
      "Train Epoch: 97 [49920/60000 (83%)]\tLoss: 96.579285\n",
      "Train Epoch: 97 [51200/60000 (85%)]\tLoss: 97.224968\n",
      "Train Epoch: 97 [52480/60000 (87%)]\tLoss: 98.953224\n",
      "Train Epoch: 97 [53760/60000 (90%)]\tLoss: 98.818970\n",
      "Train Epoch: 97 [55040/60000 (92%)]\tLoss: 98.292343\n",
      "Train Epoch: 97 [56320/60000 (94%)]\tLoss: 96.735374\n",
      "Train Epoch: 97 [57600/60000 (96%)]\tLoss: 93.670258\n",
      "Train Epoch: 97 [58880/60000 (98%)]\tLoss: 95.903503\n",
      "====> Epoch: 97 Average loss: 96.8779\n",
      "====> Test set loss: 99.1594\n",
      "Train Epoch: 98 [0/60000 (0%)]\tLoss: 97.492012\n",
      "Train Epoch: 98 [1280/60000 (2%)]\tLoss: 93.256088\n",
      "Train Epoch: 98 [2560/60000 (4%)]\tLoss: 93.202698\n",
      "Train Epoch: 98 [3840/60000 (6%)]\tLoss: 97.033546\n",
      "Train Epoch: 98 [5120/60000 (9%)]\tLoss: 96.133698\n",
      "Train Epoch: 98 [6400/60000 (11%)]\tLoss: 91.965912\n",
      "Train Epoch: 98 [7680/60000 (13%)]\tLoss: 98.635841\n",
      "Train Epoch: 98 [8960/60000 (15%)]\tLoss: 96.695877\n",
      "Train Epoch: 98 [10240/60000 (17%)]\tLoss: 99.178375\n",
      "Train Epoch: 98 [11520/60000 (19%)]\tLoss: 95.902527\n",
      "Train Epoch: 98 [12800/60000 (21%)]\tLoss: 95.354858\n",
      "Train Epoch: 98 [14080/60000 (23%)]\tLoss: 97.847488\n",
      "Train Epoch: 98 [15360/60000 (26%)]\tLoss: 98.074547\n",
      "Train Epoch: 98 [16640/60000 (28%)]\tLoss: 96.473900\n",
      "Train Epoch: 98 [17920/60000 (30%)]\tLoss: 93.401810\n",
      "Train Epoch: 98 [19200/60000 (32%)]\tLoss: 96.540474\n",
      "Train Epoch: 98 [20480/60000 (34%)]\tLoss: 101.063065\n",
      "Train Epoch: 98 [21760/60000 (36%)]\tLoss: 96.862015\n",
      "Train Epoch: 98 [23040/60000 (38%)]\tLoss: 94.286026\n",
      "Train Epoch: 98 [24320/60000 (41%)]\tLoss: 96.895363\n",
      "Train Epoch: 98 [25600/60000 (43%)]\tLoss: 101.528137\n",
      "Train Epoch: 98 [26880/60000 (45%)]\tLoss: 93.975845\n",
      "Train Epoch: 98 [28160/60000 (47%)]\tLoss: 95.160172\n",
      "Train Epoch: 98 [29440/60000 (49%)]\tLoss: 92.569427\n",
      "Train Epoch: 98 [30720/60000 (51%)]\tLoss: 97.753609\n",
      "Train Epoch: 98 [32000/60000 (53%)]\tLoss: 97.617737\n",
      "Train Epoch: 98 [33280/60000 (55%)]\tLoss: 94.853264\n",
      "Train Epoch: 98 [34560/60000 (58%)]\tLoss: 96.627655\n",
      "Train Epoch: 98 [35840/60000 (60%)]\tLoss: 93.640205\n",
      "Train Epoch: 98 [37120/60000 (62%)]\tLoss: 93.678566\n",
      "Train Epoch: 98 [38400/60000 (64%)]\tLoss: 100.155540\n",
      "Train Epoch: 98 [39680/60000 (66%)]\tLoss: 94.556488\n",
      "Train Epoch: 98 [40960/60000 (68%)]\tLoss: 95.081909\n",
      "Train Epoch: 98 [42240/60000 (70%)]\tLoss: 94.984215\n",
      "Train Epoch: 98 [43520/60000 (72%)]\tLoss: 99.227890\n",
      "Train Epoch: 98 [44800/60000 (75%)]\tLoss: 98.841629\n",
      "Train Epoch: 98 [46080/60000 (77%)]\tLoss: 95.921295\n",
      "Train Epoch: 98 [47360/60000 (79%)]\tLoss: 97.823647\n",
      "Train Epoch: 98 [48640/60000 (81%)]\tLoss: 95.998001\n",
      "Train Epoch: 98 [49920/60000 (83%)]\tLoss: 95.088531\n",
      "Train Epoch: 98 [51200/60000 (85%)]\tLoss: 101.440338\n",
      "Train Epoch: 98 [52480/60000 (87%)]\tLoss: 94.006187\n",
      "Train Epoch: 98 [53760/60000 (90%)]\tLoss: 96.185844\n",
      "Train Epoch: 98 [55040/60000 (92%)]\tLoss: 95.788757\n",
      "Train Epoch: 98 [56320/60000 (94%)]\tLoss: 97.660690\n",
      "Train Epoch: 98 [57600/60000 (96%)]\tLoss: 92.996216\n",
      "Train Epoch: 98 [58880/60000 (98%)]\tLoss: 95.550018\n",
      "====> Epoch: 98 Average loss: 96.8241\n",
      "====> Test set loss: 99.3363\n",
      "Train Epoch: 99 [0/60000 (0%)]\tLoss: 99.409126\n",
      "Train Epoch: 99 [1280/60000 (2%)]\tLoss: 97.676437\n",
      "Train Epoch: 99 [2560/60000 (4%)]\tLoss: 96.646957\n",
      "Train Epoch: 99 [3840/60000 (6%)]\tLoss: 95.796066\n",
      "Train Epoch: 99 [5120/60000 (9%)]\tLoss: 100.902588\n",
      "Train Epoch: 99 [6400/60000 (11%)]\tLoss: 95.770058\n",
      "Train Epoch: 99 [7680/60000 (13%)]\tLoss: 92.861290\n",
      "Train Epoch: 99 [8960/60000 (15%)]\tLoss: 96.768829\n",
      "Train Epoch: 99 [10240/60000 (17%)]\tLoss: 96.238876\n",
      "Train Epoch: 99 [11520/60000 (19%)]\tLoss: 94.736748\n",
      "Train Epoch: 99 [12800/60000 (21%)]\tLoss: 98.792099\n",
      "Train Epoch: 99 [14080/60000 (23%)]\tLoss: 97.476837\n",
      "Train Epoch: 99 [15360/60000 (26%)]\tLoss: 97.090729\n",
      "Train Epoch: 99 [16640/60000 (28%)]\tLoss: 99.838303\n",
      "Train Epoch: 99 [17920/60000 (30%)]\tLoss: 93.834984\n",
      "Train Epoch: 99 [19200/60000 (32%)]\tLoss: 99.867996\n",
      "Train Epoch: 99 [20480/60000 (34%)]\tLoss: 92.806671\n",
      "Train Epoch: 99 [21760/60000 (36%)]\tLoss: 96.166733\n",
      "Train Epoch: 99 [23040/60000 (38%)]\tLoss: 100.007584\n",
      "Train Epoch: 99 [24320/60000 (41%)]\tLoss: 93.234306\n",
      "Train Epoch: 99 [25600/60000 (43%)]\tLoss: 92.385864\n",
      "Train Epoch: 99 [26880/60000 (45%)]\tLoss: 96.592041\n",
      "Train Epoch: 99 [28160/60000 (47%)]\tLoss: 94.980225\n",
      "Train Epoch: 99 [29440/60000 (49%)]\tLoss: 99.947235\n",
      "Train Epoch: 99 [30720/60000 (51%)]\tLoss: 97.635757\n",
      "Train Epoch: 99 [32000/60000 (53%)]\tLoss: 95.457382\n",
      "Train Epoch: 99 [33280/60000 (55%)]\tLoss: 92.400719\n",
      "Train Epoch: 99 [34560/60000 (58%)]\tLoss: 98.368172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 99 [35840/60000 (60%)]\tLoss: 95.235680\n",
      "Train Epoch: 99 [37120/60000 (62%)]\tLoss: 95.667152\n",
      "Train Epoch: 99 [38400/60000 (64%)]\tLoss: 96.163277\n",
      "Train Epoch: 99 [39680/60000 (66%)]\tLoss: 98.450226\n",
      "Train Epoch: 99 [40960/60000 (68%)]\tLoss: 95.615601\n",
      "Train Epoch: 99 [42240/60000 (70%)]\tLoss: 98.031540\n",
      "Train Epoch: 99 [43520/60000 (72%)]\tLoss: 99.097855\n",
      "Train Epoch: 99 [44800/60000 (75%)]\tLoss: 97.475418\n",
      "Train Epoch: 99 [46080/60000 (77%)]\tLoss: 95.767303\n",
      "Train Epoch: 99 [47360/60000 (79%)]\tLoss: 102.281693\n",
      "Train Epoch: 99 [48640/60000 (81%)]\tLoss: 96.940536\n",
      "Train Epoch: 99 [49920/60000 (83%)]\tLoss: 98.682983\n",
      "Train Epoch: 99 [51200/60000 (85%)]\tLoss: 95.322418\n",
      "Train Epoch: 99 [52480/60000 (87%)]\tLoss: 98.222382\n",
      "Train Epoch: 99 [53760/60000 (90%)]\tLoss: 99.842201\n",
      "Train Epoch: 99 [55040/60000 (92%)]\tLoss: 95.470039\n",
      "Train Epoch: 99 [56320/60000 (94%)]\tLoss: 97.147179\n",
      "Train Epoch: 99 [57600/60000 (96%)]\tLoss: 101.618263\n",
      "Train Epoch: 99 [58880/60000 (98%)]\tLoss: 96.957962\n",
      "====> Epoch: 99 Average loss: 96.8429\n",
      "====> Test set loss: 99.3587\n",
      "Train Epoch: 100 [0/60000 (0%)]\tLoss: 96.862503\n",
      "Train Epoch: 100 [1280/60000 (2%)]\tLoss: 95.969330\n",
      "Train Epoch: 100 [2560/60000 (4%)]\tLoss: 96.055328\n",
      "Train Epoch: 100 [3840/60000 (6%)]\tLoss: 95.515640\n",
      "Train Epoch: 100 [5120/60000 (9%)]\tLoss: 99.250313\n",
      "Train Epoch: 100 [6400/60000 (11%)]\tLoss: 92.939041\n",
      "Train Epoch: 100 [7680/60000 (13%)]\tLoss: 99.896828\n",
      "Train Epoch: 100 [8960/60000 (15%)]\tLoss: 95.925446\n",
      "Train Epoch: 100 [10240/60000 (17%)]\tLoss: 94.803864\n",
      "Train Epoch: 100 [11520/60000 (19%)]\tLoss: 96.150787\n",
      "Train Epoch: 100 [12800/60000 (21%)]\tLoss: 96.216301\n",
      "Train Epoch: 100 [14080/60000 (23%)]\tLoss: 96.298096\n",
      "Train Epoch: 100 [15360/60000 (26%)]\tLoss: 93.733566\n",
      "Train Epoch: 100 [16640/60000 (28%)]\tLoss: 95.632355\n",
      "Train Epoch: 100 [17920/60000 (30%)]\tLoss: 98.017097\n",
      "Train Epoch: 100 [19200/60000 (32%)]\tLoss: 95.388443\n",
      "Train Epoch: 100 [20480/60000 (34%)]\tLoss: 94.478600\n",
      "Train Epoch: 100 [21760/60000 (36%)]\tLoss: 99.390930\n",
      "Train Epoch: 100 [23040/60000 (38%)]\tLoss: 92.560890\n",
      "Train Epoch: 100 [24320/60000 (41%)]\tLoss: 101.859116\n",
      "Train Epoch: 100 [25600/60000 (43%)]\tLoss: 91.482147\n",
      "Train Epoch: 100 [26880/60000 (45%)]\tLoss: 96.313530\n",
      "Train Epoch: 100 [28160/60000 (47%)]\tLoss: 96.948288\n",
      "Train Epoch: 100 [29440/60000 (49%)]\tLoss: 94.201523\n",
      "Train Epoch: 100 [30720/60000 (51%)]\tLoss: 99.206512\n",
      "Train Epoch: 100 [32000/60000 (53%)]\tLoss: 98.945908\n",
      "Train Epoch: 100 [33280/60000 (55%)]\tLoss: 94.509079\n",
      "Train Epoch: 100 [34560/60000 (58%)]\tLoss: 103.164482\n",
      "Train Epoch: 100 [35840/60000 (60%)]\tLoss: 97.800797\n",
      "Train Epoch: 100 [37120/60000 (62%)]\tLoss: 99.910255\n",
      "Train Epoch: 100 [38400/60000 (64%)]\tLoss: 98.703133\n",
      "Train Epoch: 100 [39680/60000 (66%)]\tLoss: 97.471680\n",
      "Train Epoch: 100 [40960/60000 (68%)]\tLoss: 98.833427\n",
      "Train Epoch: 100 [42240/60000 (70%)]\tLoss: 98.846130\n",
      "Train Epoch: 100 [43520/60000 (72%)]\tLoss: 99.529739\n",
      "Train Epoch: 100 [44800/60000 (75%)]\tLoss: 99.037178\n",
      "Train Epoch: 100 [46080/60000 (77%)]\tLoss: 95.608047\n",
      "Train Epoch: 100 [47360/60000 (79%)]\tLoss: 96.853264\n",
      "Train Epoch: 100 [48640/60000 (81%)]\tLoss: 97.056351\n",
      "Train Epoch: 100 [49920/60000 (83%)]\tLoss: 97.605728\n",
      "Train Epoch: 100 [51200/60000 (85%)]\tLoss: 98.373398\n",
      "Train Epoch: 100 [52480/60000 (87%)]\tLoss: 98.220238\n",
      "Train Epoch: 100 [53760/60000 (90%)]\tLoss: 96.705795\n",
      "Train Epoch: 100 [55040/60000 (92%)]\tLoss: 103.181793\n",
      "Train Epoch: 100 [56320/60000 (94%)]\tLoss: 101.915085\n",
      "Train Epoch: 100 [57600/60000 (96%)]\tLoss: 93.592987\n",
      "Train Epoch: 100 [58880/60000 (98%)]\tLoss: 97.058121\n",
      "====> Epoch: 100 Average loss: 96.7721\n",
      "====> Test set loss: 99.2598\n"
     ]
    }
   ],
   "source": [
    "## VAE TRAINING & TESTING\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_vae(epoch)\n",
    "    test_vae(epoch)\n",
    "    with torch.no_grad():\n",
    "        sample = torch.randn(64, nz).to(device)\n",
    "        sample = vae.decode(sample).cpu()\n",
    "        save_image(\n",
    "            sample.view(64, 1, 28, 28),\n",
    "            vae_results_directory + \"/sample_\" + str(epoch) + \".png\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff0cb5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), os.path.join(curr_dirname, \"vae.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b0ce2692",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_z = torch.randn(2, nz).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "13f1ecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_vae = vae.decode(sample_z).cpu()\n",
    "sample_gan = generator(sample_z).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a2236b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2af85afd0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.imshow(transforms.functional.to_pil_image(sample_gan.view(2, 1, 28, 28)[0]),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f32d464a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2af8d4d60>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.imshow(transforms.functional.to_pil_image(sample_vae.view(2, 1, 28, 28)[0]),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "be1f271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_z1 = torch.randn(2, nz).to(device)\n",
    "sample_z2 = torch.randn(2, nz).to(device)\n",
    "sample_z3 = sample_z1 + sample_z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2b044227",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_vae1 = vae.decode(sample_z1).cpu()\n",
    "sample_vae2 = vae.decode(sample_z2).cpu()\n",
    "sample_vae3 = vae.decode(sample_z3).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5f2ff097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2afc09280>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.imshow(transforms.functional.to_pil_image(sample_vae1.view(2, 1, 28, 28)[0]),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b3a8d020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2afc7b4c0>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.imshow(transforms.functional.to_pil_image(sample_vae2.view(2, 1, 28, 28)[0]),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "df966ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2afcef400>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.imshow(transforms.functional.to_pil_image(sample_vae3.view(2, 1, 28, 28)[0]),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "56a6a21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_z1 = torch.randn(2, nz).to(device)\n",
    "sample_z2 = torch.randn(2, nz).to(device)\n",
    "sample_z3 = sample_z1 + sample_z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7e651873",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_gan1 = generator(sample_z1).cpu()\n",
    "sample_gan2 = generator(sample_z2).cpu()\n",
    "sample_gan3 = generator(sample_z3).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "29f14ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b1200cd0>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.imshow(transforms.functional.to_pil_image(sample_gan1.view(2, 1, 28, 28)[0]),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f60fad3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b12777f0>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.imshow(transforms.functional.to_pil_image(sample_gan2.view(2, 1, 28, 28)[0]),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "11018f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b12e9d60>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.imshow(transforms.functional.to_pil_image(sample_gan3.view(2, 1, 28, 28)[0]),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "536a4999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "0bbda781",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(gan_losses, columns =['g_loss', 'real_loss', 'fake_loss', 'd_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "15ce4590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.plot(df['g_loss'], label=\"Generator Loss\")\n",
    "matplotlib.pyplot.plot(df['d_loss'], label=\"Discriminator Loss\")\n",
    "matplotlib.pyplot.xlabel('epoch')\n",
    "matplotlib.pyplot.ylabel('loss')\n",
    "matplotlib.pyplot.legend(loc=\"upper left\")\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800e052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.pyplot.plot(df['d_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "223ee8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.plot(vae_train_losses)\n",
    "matplotlib.pyplot.xlabel('epoch')\n",
    "matplotlib.pyplot.ylabel('loss')\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "8f216b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.plot(vae_test_losses)\n",
    "matplotlib.pyplot.xlabel('epoch')\n",
    "matplotlib.pyplot.ylabel('loss')\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594c4929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
