{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d971c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc0d9158",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2034411",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "077ffd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 5\n",
    "batch_size = 128\n",
    "nz = 100\n",
    "lr = 0.001\n",
    "b1 = 0.5\n",
    "b2 = 0.9999\n",
    "epochs = 100\n",
    "log_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17d43c6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10bee26b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf207795",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dirname = os.path.dirname('.')\n",
    "data_directory = os.path.join(curr_dirname, \"data\")\n",
    "gan_results_directory_name = \"results/gan\"\n",
    "os.makedirs(gan_results_directory_name, exist_ok=True)\n",
    "gan_results_directory = os.path.join(curr_dirname, gan_results_directory_name)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        data_directory, train=True, download=False, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5],[0.5])])\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "babee2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88e98723",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # create a reusable block with #in_feat input features and #out_feat output features\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(nz, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            nn.Linear(512, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # create a reusable block with #in_feat input features and #out_feat output features\n",
    "        def block(in_feat, out_feat):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(int(np.prod(img_shape)), 512),\n",
    "            *block(512, 256),\n",
    "            *block(256, 128),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        out = self.model(img_flat)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66126659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "adversarial_loss = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71452237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize generator and discriminator\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "580ba82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizers\n",
    "optimizer_generator = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cb9fed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ea17137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tDiscriminator Loss: 0.711198\tGenerator Loss: 0.666229\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tDiscriminator Loss: 0.521487\tGenerator Loss: 0.622121\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tDiscriminator Loss: 1.194551\tGenerator Loss: 0.103500\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tDiscriminator Loss: 0.869718\tGenerator Loss: 0.207165\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tDiscriminator Loss: 0.362045\tGenerator Loss: 1.056185\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tDiscriminator Loss: 0.630140\tGenerator Loss: 0.558165\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tDiscriminator Loss: 0.627403\tGenerator Loss: 0.582092\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tDiscriminator Loss: 0.666225\tGenerator Loss: 0.586890\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tDiscriminator Loss: 0.589806\tGenerator Loss: 0.464700\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tDiscriminator Loss: 0.600123\tGenerator Loss: 0.486179\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tDiscriminator Loss: 0.539223\tGenerator Loss: 0.531886\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tDiscriminator Loss: 0.395330\tGenerator Loss: 0.811076\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tDiscriminator Loss: 0.787820\tGenerator Loss: 0.261757\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tDiscriminator Loss: 0.847677\tGenerator Loss: 0.259323\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tDiscriminator Loss: 0.573026\tGenerator Loss: 0.702775\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tDiscriminator Loss: 0.513794\tGenerator Loss: 1.035990\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tDiscriminator Loss: 0.498452\tGenerator Loss: 1.233723\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tDiscriminator Loss: 0.602060\tGenerator Loss: 0.456337\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tDiscriminator Loss: 0.600953\tGenerator Loss: 0.906495\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tDiscriminator Loss: 0.449989\tGenerator Loss: 1.180347\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tDiscriminator Loss: 0.638705\tGenerator Loss: 0.593081\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tDiscriminator Loss: 0.612503\tGenerator Loss: 1.158237\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tDiscriminator Loss: 0.389091\tGenerator Loss: 1.505130\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tDiscriminator Loss: 0.620291\tGenerator Loss: 0.926345\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tDiscriminator Loss: 0.785996\tGenerator Loss: 0.297168\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tDiscriminator Loss: 0.444268\tGenerator Loss: 1.240451\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tDiscriminator Loss: 0.594630\tGenerator Loss: 0.972931\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tDiscriminator Loss: 0.494877\tGenerator Loss: 1.851069\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tDiscriminator Loss: 0.667184\tGenerator Loss: 0.757764\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tDiscriminator Loss: 0.559815\tGenerator Loss: 1.056704\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tDiscriminator Loss: 0.577833\tGenerator Loss: 0.857825\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tDiscriminator Loss: 0.670348\tGenerator Loss: 0.489353\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tDiscriminator Loss: 0.584642\tGenerator Loss: 1.055177\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tDiscriminator Loss: 0.539348\tGenerator Loss: 0.858329\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tDiscriminator Loss: 0.553094\tGenerator Loss: 0.834332\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tDiscriminator Loss: 0.501413\tGenerator Loss: 1.052518\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tDiscriminator Loss: 0.697276\tGenerator Loss: 0.894136\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tDiscriminator Loss: 0.754382\tGenerator Loss: 1.162658\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tDiscriminator Loss: 0.647768\tGenerator Loss: 0.517303\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tDiscriminator Loss: 0.671039\tGenerator Loss: 0.675726\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tDiscriminator Loss: 0.506397\tGenerator Loss: 1.037357\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tDiscriminator Loss: 0.554175\tGenerator Loss: 1.466243\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tDiscriminator Loss: 0.587529\tGenerator Loss: 0.885247\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tDiscriminator Loss: 1.293561\tGenerator Loss: 0.088431\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tDiscriminator Loss: 0.567964\tGenerator Loss: 1.119280\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tDiscriminator Loss: 0.479248\tGenerator Loss: 1.308500\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tDiscriminator Loss: 0.620887\tGenerator Loss: 0.549721\n",
      "Train Epoch: 2 [0/60000 (0%)]\tDiscriminator Loss: 0.631111\tGenerator Loss: 1.839227\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tDiscriminator Loss: 0.641292\tGenerator Loss: 0.492250\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tDiscriminator Loss: 0.508791\tGenerator Loss: 1.422544\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tDiscriminator Loss: 0.591691\tGenerator Loss: 0.903056\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tDiscriminator Loss: 0.663504\tGenerator Loss: 1.930112\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tDiscriminator Loss: 0.608246\tGenerator Loss: 0.572617\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tDiscriminator Loss: 0.609866\tGenerator Loss: 1.011287\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tDiscriminator Loss: 0.660015\tGenerator Loss: 0.547012\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tDiscriminator Loss: 0.645075\tGenerator Loss: 0.942128\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tDiscriminator Loss: 0.450521\tGenerator Loss: 1.521977\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tDiscriminator Loss: 0.579329\tGenerator Loss: 1.620386\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tDiscriminator Loss: 0.563115\tGenerator Loss: 1.038413\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tDiscriminator Loss: 0.486790\tGenerator Loss: 1.264117\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tDiscriminator Loss: 0.515465\tGenerator Loss: 1.164002\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tDiscriminator Loss: 0.934736\tGenerator Loss: 0.237249\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tDiscriminator Loss: 0.673978\tGenerator Loss: 2.547098\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tDiscriminator Loss: 0.568991\tGenerator Loss: 1.028549\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tDiscriminator Loss: 0.702004\tGenerator Loss: 0.406718\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tDiscriminator Loss: 0.551205\tGenerator Loss: 1.015264\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tDiscriminator Loss: 0.631293\tGenerator Loss: 0.740212\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tDiscriminator Loss: 0.513324\tGenerator Loss: 1.139419\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tDiscriminator Loss: 0.461370\tGenerator Loss: 1.220070\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tDiscriminator Loss: 0.586972\tGenerator Loss: 0.839467\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tDiscriminator Loss: 0.502927\tGenerator Loss: 1.438766\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tDiscriminator Loss: 0.586658\tGenerator Loss: 0.856340\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tDiscriminator Loss: 0.757161\tGenerator Loss: 0.341353\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tDiscriminator Loss: 0.581728\tGenerator Loss: 0.846233\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tDiscriminator Loss: 0.640338\tGenerator Loss: 1.903025\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tDiscriminator Loss: 0.486827\tGenerator Loss: 0.939213\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tDiscriminator Loss: 0.616817\tGenerator Loss: 0.986035\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tDiscriminator Loss: 0.627184\tGenerator Loss: 1.537720\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tDiscriminator Loss: 0.529427\tGenerator Loss: 0.971509\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tDiscriminator Loss: 0.556838\tGenerator Loss: 0.922968\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tDiscriminator Loss: 0.788523\tGenerator Loss: 0.308063\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tDiscriminator Loss: 0.551890\tGenerator Loss: 0.773226\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tDiscriminator Loss: 0.619016\tGenerator Loss: 0.976750\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tDiscriminator Loss: 0.565524\tGenerator Loss: 0.961491\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tDiscriminator Loss: 0.987137\tGenerator Loss: 0.190182\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tDiscriminator Loss: 0.548307\tGenerator Loss: 1.025014\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tDiscriminator Loss: 0.561840\tGenerator Loss: 0.792480\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tDiscriminator Loss: 0.543612\tGenerator Loss: 1.128935\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tDiscriminator Loss: 0.560314\tGenerator Loss: 1.562212\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tDiscriminator Loss: 0.521462\tGenerator Loss: 1.147586\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tDiscriminator Loss: 0.859823\tGenerator Loss: 0.286424\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tDiscriminator Loss: 0.589166\tGenerator Loss: 0.934879\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tDiscriminator Loss: 0.556440\tGenerator Loss: 1.145169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [58880/60000 (98%)]\tDiscriminator Loss: 0.566432\tGenerator Loss: 0.942029\n",
      "Train Epoch: 3 [0/60000 (0%)]\tDiscriminator Loss: 0.617196\tGenerator Loss: 0.833334\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tDiscriminator Loss: 1.324072\tGenerator Loss: 0.087511\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tDiscriminator Loss: 0.571600\tGenerator Loss: 1.046800\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tDiscriminator Loss: 0.643222\tGenerator Loss: 0.764060\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tDiscriminator Loss: 0.810405\tGenerator Loss: 0.280412\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tDiscriminator Loss: 0.547568\tGenerator Loss: 1.040753\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tDiscriminator Loss: 0.618633\tGenerator Loss: 0.651225\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tDiscriminator Loss: 0.620735\tGenerator Loss: 0.464904\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tDiscriminator Loss: 0.541358\tGenerator Loss: 1.121333\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tDiscriminator Loss: 0.798651\tGenerator Loss: 0.303179\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tDiscriminator Loss: 0.528594\tGenerator Loss: 1.216109\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tDiscriminator Loss: 0.557699\tGenerator Loss: 1.040437\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tDiscriminator Loss: 0.557901\tGenerator Loss: 1.714889\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tDiscriminator Loss: 0.734907\tGenerator Loss: 0.358400\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tDiscriminator Loss: 0.530388\tGenerator Loss: 0.937617\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tDiscriminator Loss: 0.537680\tGenerator Loss: 0.914309\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tDiscriminator Loss: 0.634071\tGenerator Loss: 0.672759\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tDiscriminator Loss: 0.566493\tGenerator Loss: 0.948114\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tDiscriminator Loss: 0.587969\tGenerator Loss: 1.613777\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tDiscriminator Loss: 0.522904\tGenerator Loss: 0.945474\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tDiscriminator Loss: 0.660737\tGenerator Loss: 0.729065\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tDiscriminator Loss: 0.585891\tGenerator Loss: 0.861597\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tDiscriminator Loss: 0.753483\tGenerator Loss: 0.339341\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tDiscriminator Loss: 0.554158\tGenerator Loss: 1.157529\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tDiscriminator Loss: 0.721544\tGenerator Loss: 0.379178\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tDiscriminator Loss: 0.475294\tGenerator Loss: 1.582179\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tDiscriminator Loss: 0.544982\tGenerator Loss: 0.854845\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tDiscriminator Loss: 0.557763\tGenerator Loss: 0.698513\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tDiscriminator Loss: 0.497197\tGenerator Loss: 1.594049\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tDiscriminator Loss: 0.499988\tGenerator Loss: 0.955008\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tDiscriminator Loss: 0.500166\tGenerator Loss: 1.045265\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tDiscriminator Loss: 0.475128\tGenerator Loss: 0.995054\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tDiscriminator Loss: 1.012231\tGenerator Loss: 0.177987\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tDiscriminator Loss: 0.508543\tGenerator Loss: 1.069019\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tDiscriminator Loss: 0.893927\tGenerator Loss: 0.236920\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tDiscriminator Loss: 0.493273\tGenerator Loss: 1.161031\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tDiscriminator Loss: 0.582213\tGenerator Loss: 0.652443\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tDiscriminator Loss: 0.752068\tGenerator Loss: 0.377919\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tDiscriminator Loss: 0.454493\tGenerator Loss: 1.252440\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tDiscriminator Loss: 0.435319\tGenerator Loss: 1.489072\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tDiscriminator Loss: 0.619720\tGenerator Loss: 0.502504\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tDiscriminator Loss: 0.543854\tGenerator Loss: 1.294062\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tDiscriminator Loss: 0.905035\tGenerator Loss: 3.265583\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tDiscriminator Loss: 0.524300\tGenerator Loss: 1.114806\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tDiscriminator Loss: 0.596065\tGenerator Loss: 0.777734\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tDiscriminator Loss: 0.472702\tGenerator Loss: 1.540220\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tDiscriminator Loss: 0.725993\tGenerator Loss: 0.412347\n",
      "Train Epoch: 4 [0/60000 (0%)]\tDiscriminator Loss: 0.415358\tGenerator Loss: 2.011211\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tDiscriminator Loss: 0.588699\tGenerator Loss: 0.910511\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tDiscriminator Loss: 0.524854\tGenerator Loss: 1.509428\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tDiscriminator Loss: 0.460024\tGenerator Loss: 1.212863\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tDiscriminator Loss: 0.468838\tGenerator Loss: 1.441210\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tDiscriminator Loss: 0.579874\tGenerator Loss: 1.999004\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tDiscriminator Loss: 0.509376\tGenerator Loss: 1.325147\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tDiscriminator Loss: 0.486577\tGenerator Loss: 0.982974\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tDiscriminator Loss: 0.546391\tGenerator Loss: 0.865828\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tDiscriminator Loss: 0.525622\tGenerator Loss: 1.396519\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tDiscriminator Loss: 0.572458\tGenerator Loss: 2.137208\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tDiscriminator Loss: 0.456927\tGenerator Loss: 1.182273\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tDiscriminator Loss: 0.518894\tGenerator Loss: 0.748722\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tDiscriminator Loss: 0.490603\tGenerator Loss: 1.548890\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tDiscriminator Loss: 0.656788\tGenerator Loss: 0.463284\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tDiscriminator Loss: 0.914197\tGenerator Loss: 0.230703\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tDiscriminator Loss: 0.553244\tGenerator Loss: 1.069099\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tDiscriminator Loss: 0.438490\tGenerator Loss: 1.558431\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tDiscriminator Loss: 0.419106\tGenerator Loss: 1.138667\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tDiscriminator Loss: 0.456794\tGenerator Loss: 1.176732\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tDiscriminator Loss: 0.407656\tGenerator Loss: 1.339577\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tDiscriminator Loss: 0.585689\tGenerator Loss: 0.542458\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tDiscriminator Loss: 0.421065\tGenerator Loss: 1.295269\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tDiscriminator Loss: 0.460870\tGenerator Loss: 1.048983\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tDiscriminator Loss: 0.493579\tGenerator Loss: 1.098058\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tDiscriminator Loss: 0.652945\tGenerator Loss: 0.543197\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tDiscriminator Loss: 0.485572\tGenerator Loss: 1.275920\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tDiscriminator Loss: 0.550066\tGenerator Loss: 0.692953\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tDiscriminator Loss: 0.490700\tGenerator Loss: 1.494560\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tDiscriminator Loss: 0.482317\tGenerator Loss: 1.890887\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tDiscriminator Loss: 0.441411\tGenerator Loss: 0.954376\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tDiscriminator Loss: 0.426727\tGenerator Loss: 1.069017\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tDiscriminator Loss: 0.707148\tGenerator Loss: 0.393096\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tDiscriminator Loss: 0.450045\tGenerator Loss: 1.480315\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tDiscriminator Loss: 0.492351\tGenerator Loss: 0.974184\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tDiscriminator Loss: 0.470695\tGenerator Loss: 1.518493\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tDiscriminator Loss: 0.731462\tGenerator Loss: 0.519669\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tDiscriminator Loss: 0.576069\tGenerator Loss: 0.924482\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tDiscriminator Loss: 0.488332\tGenerator Loss: 1.301770\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tDiscriminator Loss: 0.537897\tGenerator Loss: 0.841057\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tDiscriminator Loss: 0.542909\tGenerator Loss: 0.754405\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tDiscriminator Loss: 0.769458\tGenerator Loss: 0.356510\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tDiscriminator Loss: 0.491371\tGenerator Loss: 1.314959\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tDiscriminator Loss: 0.501056\tGenerator Loss: 1.397897\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tDiscriminator Loss: 0.444440\tGenerator Loss: 1.298766\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tDiscriminator Loss: 0.455391\tGenerator Loss: 1.186960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [58880/60000 (98%)]\tDiscriminator Loss: 0.475191\tGenerator Loss: 1.066472\n",
      "Train Epoch: 5 [0/60000 (0%)]\tDiscriminator Loss: 0.513441\tGenerator Loss: 0.911556\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tDiscriminator Loss: 0.449960\tGenerator Loss: 1.207459\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tDiscriminator Loss: 0.628365\tGenerator Loss: 2.359217\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tDiscriminator Loss: 0.488989\tGenerator Loss: 1.406993\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tDiscriminator Loss: 0.565652\tGenerator Loss: 0.846143\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tDiscriminator Loss: 0.508339\tGenerator Loss: 1.257663\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tDiscriminator Loss: 0.661216\tGenerator Loss: 1.743681\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tDiscriminator Loss: 0.486461\tGenerator Loss: 1.212131\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tDiscriminator Loss: 0.524580\tGenerator Loss: 1.048466\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tDiscriminator Loss: 0.676292\tGenerator Loss: 0.470069\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tDiscriminator Loss: 0.923458\tGenerator Loss: 0.209289\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tDiscriminator Loss: 0.607392\tGenerator Loss: 0.602613\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tDiscriminator Loss: 0.630040\tGenerator Loss: 0.671546\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tDiscriminator Loss: 1.109148\tGenerator Loss: 0.161474\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tDiscriminator Loss: 0.446246\tGenerator Loss: 1.440755\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tDiscriminator Loss: 0.521698\tGenerator Loss: 1.090344\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tDiscriminator Loss: 0.616056\tGenerator Loss: 0.612404\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tDiscriminator Loss: 0.534304\tGenerator Loss: 1.007809\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tDiscriminator Loss: 0.539276\tGenerator Loss: 1.031803\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tDiscriminator Loss: 0.619265\tGenerator Loss: 0.571824\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tDiscriminator Loss: 0.495118\tGenerator Loss: 1.331429\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tDiscriminator Loss: 0.783077\tGenerator Loss: 2.798292\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tDiscriminator Loss: 0.517611\tGenerator Loss: 1.110356\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tDiscriminator Loss: 0.556776\tGenerator Loss: 1.058756\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tDiscriminator Loss: 0.888766\tGenerator Loss: 0.267502\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tDiscriminator Loss: 0.541776\tGenerator Loss: 1.016084\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tDiscriminator Loss: 0.497063\tGenerator Loss: 0.980429\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tDiscriminator Loss: 0.574016\tGenerator Loss: 0.935363\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tDiscriminator Loss: 0.534371\tGenerator Loss: 1.423619\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tDiscriminator Loss: 0.502795\tGenerator Loss: 1.243147\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tDiscriminator Loss: 0.464915\tGenerator Loss: 1.770209\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tDiscriminator Loss: 0.555740\tGenerator Loss: 1.124213\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tDiscriminator Loss: 0.573432\tGenerator Loss: 0.957369\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tDiscriminator Loss: 1.745541\tGenerator Loss: 0.047392\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tDiscriminator Loss: 0.596351\tGenerator Loss: 0.869906\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tDiscriminator Loss: 0.510371\tGenerator Loss: 1.027990\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tDiscriminator Loss: 0.538886\tGenerator Loss: 0.904354\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tDiscriminator Loss: 0.584481\tGenerator Loss: 0.864412\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tDiscriminator Loss: 0.588957\tGenerator Loss: 2.022987\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tDiscriminator Loss: 0.535975\tGenerator Loss: 0.961816\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tDiscriminator Loss: 0.699544\tGenerator Loss: 0.409866\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tDiscriminator Loss: 0.493740\tGenerator Loss: 1.399421\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tDiscriminator Loss: 0.489106\tGenerator Loss: 1.480549\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tDiscriminator Loss: 0.823819\tGenerator Loss: 0.302301\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tDiscriminator Loss: 0.470955\tGenerator Loss: 1.131249\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tDiscriminator Loss: 0.554685\tGenerator Loss: 0.847709\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tDiscriminator Loss: 0.927031\tGenerator Loss: 0.237202\n",
      "Train Epoch: 6 [0/60000 (0%)]\tDiscriminator Loss: 0.496358\tGenerator Loss: 1.068465\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tDiscriminator Loss: 0.456736\tGenerator Loss: 1.537432\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tDiscriminator Loss: 0.569093\tGenerator Loss: 1.153864\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tDiscriminator Loss: 0.564243\tGenerator Loss: 0.823204\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tDiscriminator Loss: 1.098678\tGenerator Loss: 0.153649\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tDiscriminator Loss: 0.516956\tGenerator Loss: 1.025517\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tDiscriminator Loss: 0.571035\tGenerator Loss: 0.672408\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tDiscriminator Loss: 0.496981\tGenerator Loss: 1.073710\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tDiscriminator Loss: 0.572426\tGenerator Loss: 0.754628\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tDiscriminator Loss: 0.572428\tGenerator Loss: 0.955175\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tDiscriminator Loss: 0.517917\tGenerator Loss: 1.152796\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tDiscriminator Loss: 0.499563\tGenerator Loss: 1.168865\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tDiscriminator Loss: 0.503085\tGenerator Loss: 1.058729\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tDiscriminator Loss: 0.544132\tGenerator Loss: 0.942682\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tDiscriminator Loss: 0.577506\tGenerator Loss: 0.735767\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tDiscriminator Loss: 0.485199\tGenerator Loss: 1.389529\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tDiscriminator Loss: 0.514542\tGenerator Loss: 1.063848\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tDiscriminator Loss: 0.550729\tGenerator Loss: 1.450287\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tDiscriminator Loss: 0.519298\tGenerator Loss: 0.973456\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tDiscriminator Loss: 0.548435\tGenerator Loss: 0.959061\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tDiscriminator Loss: 0.517618\tGenerator Loss: 1.042972\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tDiscriminator Loss: 0.538006\tGenerator Loss: 1.125887\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tDiscriminator Loss: 0.704924\tGenerator Loss: 0.409353\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tDiscriminator Loss: 0.554921\tGenerator Loss: 1.308335\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tDiscriminator Loss: 0.581263\tGenerator Loss: 0.804063\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tDiscriminator Loss: 0.591963\tGenerator Loss: 1.625206\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tDiscriminator Loss: 0.520485\tGenerator Loss: 0.740616\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tDiscriminator Loss: 0.508627\tGenerator Loss: 1.364463\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tDiscriminator Loss: 0.515411\tGenerator Loss: 1.179560\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tDiscriminator Loss: 0.569672\tGenerator Loss: 1.471190\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tDiscriminator Loss: 0.488376\tGenerator Loss: 1.254390\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tDiscriminator Loss: 0.604667\tGenerator Loss: 0.673163\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tDiscriminator Loss: 0.616815\tGenerator Loss: 0.747648\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tDiscriminator Loss: 0.562723\tGenerator Loss: 1.506332\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tDiscriminator Loss: 0.529217\tGenerator Loss: 1.319943\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tDiscriminator Loss: 0.568980\tGenerator Loss: 1.028769\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tDiscriminator Loss: 0.510982\tGenerator Loss: 0.921322\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tDiscriminator Loss: 0.512441\tGenerator Loss: 1.249565\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tDiscriminator Loss: 0.583544\tGenerator Loss: 1.971155\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tDiscriminator Loss: 0.544124\tGenerator Loss: 1.060235\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tDiscriminator Loss: 0.542139\tGenerator Loss: 0.847780\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tDiscriminator Loss: 0.609064\tGenerator Loss: 0.681222\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tDiscriminator Loss: 0.603124\tGenerator Loss: 1.363429\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tDiscriminator Loss: 0.564097\tGenerator Loss: 1.564229\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tDiscriminator Loss: 0.556596\tGenerator Loss: 0.743620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [57600/60000 (96%)]\tDiscriminator Loss: 0.636780\tGenerator Loss: 1.601812\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tDiscriminator Loss: 0.566474\tGenerator Loss: 0.706199\n",
      "Train Epoch: 7 [0/60000 (0%)]\tDiscriminator Loss: 0.500560\tGenerator Loss: 1.101745\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tDiscriminator Loss: 0.506290\tGenerator Loss: 1.596892\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tDiscriminator Loss: 0.542567\tGenerator Loss: 1.180638\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tDiscriminator Loss: 0.551228\tGenerator Loss: 1.184814\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tDiscriminator Loss: 0.543596\tGenerator Loss: 1.516214\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tDiscriminator Loss: 0.519299\tGenerator Loss: 0.984030\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tDiscriminator Loss: 0.592543\tGenerator Loss: 0.680236\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tDiscriminator Loss: 0.734344\tGenerator Loss: 2.140647\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tDiscriminator Loss: 0.492877\tGenerator Loss: 1.027861\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tDiscriminator Loss: 0.614303\tGenerator Loss: 1.014997\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tDiscriminator Loss: 0.489445\tGenerator Loss: 1.056245\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tDiscriminator Loss: 0.667854\tGenerator Loss: 0.538049\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tDiscriminator Loss: 0.585961\tGenerator Loss: 0.981070\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tDiscriminator Loss: 0.521888\tGenerator Loss: 1.174823\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tDiscriminator Loss: 0.478860\tGenerator Loss: 1.246197\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tDiscriminator Loss: 0.542414\tGenerator Loss: 0.820579\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tDiscriminator Loss: 0.515622\tGenerator Loss: 1.474876\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tDiscriminator Loss: 0.478014\tGenerator Loss: 1.077734\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tDiscriminator Loss: 0.543446\tGenerator Loss: 1.058769\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tDiscriminator Loss: 0.552742\tGenerator Loss: 1.147662\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tDiscriminator Loss: 0.500662\tGenerator Loss: 1.036877\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tDiscriminator Loss: 0.515941\tGenerator Loss: 1.006061\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tDiscriminator Loss: 0.544950\tGenerator Loss: 0.780498\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tDiscriminator Loss: 0.523303\tGenerator Loss: 0.899330\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tDiscriminator Loss: 0.555876\tGenerator Loss: 1.709178\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tDiscriminator Loss: 0.508672\tGenerator Loss: 1.039645\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tDiscriminator Loss: 0.613050\tGenerator Loss: 1.439186\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tDiscriminator Loss: 0.611219\tGenerator Loss: 0.853094\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tDiscriminator Loss: 0.513640\tGenerator Loss: 1.095778\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tDiscriminator Loss: 0.634868\tGenerator Loss: 0.641470\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tDiscriminator Loss: 0.529770\tGenerator Loss: 1.082266\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tDiscriminator Loss: 0.553110\tGenerator Loss: 0.755207\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tDiscriminator Loss: 0.606109\tGenerator Loss: 1.267141\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tDiscriminator Loss: 0.699577\tGenerator Loss: 0.535841\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tDiscriminator Loss: 0.550319\tGenerator Loss: 1.201039\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tDiscriminator Loss: 0.515767\tGenerator Loss: 1.126980\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tDiscriminator Loss: 0.544767\tGenerator Loss: 1.694475\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tDiscriminator Loss: 0.436883\tGenerator Loss: 1.113626\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tDiscriminator Loss: 0.491978\tGenerator Loss: 1.142125\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tDiscriminator Loss: 0.482670\tGenerator Loss: 1.239192\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tDiscriminator Loss: 0.517578\tGenerator Loss: 0.977318\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tDiscriminator Loss: 0.531372\tGenerator Loss: 1.396031\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tDiscriminator Loss: 0.647568\tGenerator Loss: 0.550310\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tDiscriminator Loss: 0.706314\tGenerator Loss: 0.743381\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tDiscriminator Loss: 0.584052\tGenerator Loss: 0.852719\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tDiscriminator Loss: 0.524541\tGenerator Loss: 0.967248\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tDiscriminator Loss: 0.545163\tGenerator Loss: 1.597077\n",
      "Train Epoch: 8 [0/60000 (0%)]\tDiscriminator Loss: 0.556382\tGenerator Loss: 2.036088\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tDiscriminator Loss: 0.483290\tGenerator Loss: 1.186939\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tDiscriminator Loss: 0.518829\tGenerator Loss: 1.079513\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tDiscriminator Loss: 0.465965\tGenerator Loss: 1.182904\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tDiscriminator Loss: 0.517576\tGenerator Loss: 1.460260\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tDiscriminator Loss: 0.516234\tGenerator Loss: 1.220404\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tDiscriminator Loss: 0.537722\tGenerator Loss: 1.317239\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tDiscriminator Loss: 0.494681\tGenerator Loss: 1.267893\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tDiscriminator Loss: 0.483884\tGenerator Loss: 0.935996\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tDiscriminator Loss: 0.496929\tGenerator Loss: 1.060003\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tDiscriminator Loss: 0.541016\tGenerator Loss: 1.116132\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tDiscriminator Loss: 0.565484\tGenerator Loss: 1.015436\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tDiscriminator Loss: 0.576284\tGenerator Loss: 1.507763\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tDiscriminator Loss: 0.463348\tGenerator Loss: 1.316102\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tDiscriminator Loss: 0.502623\tGenerator Loss: 1.201777\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tDiscriminator Loss: 0.433697\tGenerator Loss: 1.471441\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tDiscriminator Loss: 0.536477\tGenerator Loss: 1.219509\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tDiscriminator Loss: 0.475693\tGenerator Loss: 1.150409\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tDiscriminator Loss: 0.485949\tGenerator Loss: 1.269344\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tDiscriminator Loss: 0.578331\tGenerator Loss: 0.823680\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tDiscriminator Loss: 0.608147\tGenerator Loss: 0.784305\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tDiscriminator Loss: 0.708535\tGenerator Loss: 0.848957\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tDiscriminator Loss: 0.755491\tGenerator Loss: 0.366331\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tDiscriminator Loss: 0.581118\tGenerator Loss: 0.905167\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tDiscriminator Loss: 0.551303\tGenerator Loss: 1.044232\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tDiscriminator Loss: 0.560944\tGenerator Loss: 1.082000\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tDiscriminator Loss: 0.530044\tGenerator Loss: 1.008960\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tDiscriminator Loss: 0.552836\tGenerator Loss: 1.127991\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tDiscriminator Loss: 0.528100\tGenerator Loss: 1.311428\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tDiscriminator Loss: 0.519807\tGenerator Loss: 1.304030\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tDiscriminator Loss: 0.517571\tGenerator Loss: 1.564470\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tDiscriminator Loss: 0.506522\tGenerator Loss: 0.816154\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tDiscriminator Loss: 0.511103\tGenerator Loss: 1.265735\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tDiscriminator Loss: 0.553261\tGenerator Loss: 1.733292\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tDiscriminator Loss: 0.486163\tGenerator Loss: 1.168708\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tDiscriminator Loss: 0.511996\tGenerator Loss: 1.267918\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tDiscriminator Loss: 0.555151\tGenerator Loss: 1.018815\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tDiscriminator Loss: 0.727204\tGenerator Loss: 2.194474\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tDiscriminator Loss: 0.554224\tGenerator Loss: 1.481024\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tDiscriminator Loss: 0.583246\tGenerator Loss: 1.396120\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tDiscriminator Loss: 0.578162\tGenerator Loss: 1.076085\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tDiscriminator Loss: 0.544395\tGenerator Loss: 1.086411\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tDiscriminator Loss: 0.559548\tGenerator Loss: 0.923003\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tDiscriminator Loss: 0.609880\tGenerator Loss: 1.394094\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tDiscriminator Loss: 0.660836\tGenerator Loss: 0.560900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [57600/60000 (96%)]\tDiscriminator Loss: 0.546461\tGenerator Loss: 1.276762\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tDiscriminator Loss: 0.574388\tGenerator Loss: 0.601150\n",
      "Train Epoch: 9 [0/60000 (0%)]\tDiscriminator Loss: 0.517441\tGenerator Loss: 1.459520\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tDiscriminator Loss: 0.489800\tGenerator Loss: 1.041033\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tDiscriminator Loss: 0.543237\tGenerator Loss: 0.938920\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tDiscriminator Loss: 0.558711\tGenerator Loss: 0.944400\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tDiscriminator Loss: 0.608088\tGenerator Loss: 0.855347\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tDiscriminator Loss: 0.532504\tGenerator Loss: 1.146721\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tDiscriminator Loss: 0.521209\tGenerator Loss: 1.382442\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tDiscriminator Loss: 0.508463\tGenerator Loss: 1.103736\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tDiscriminator Loss: 0.533914\tGenerator Loss: 1.156735\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tDiscriminator Loss: 0.588709\tGenerator Loss: 0.857721\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tDiscriminator Loss: 0.492614\tGenerator Loss: 1.538353\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tDiscriminator Loss: 0.494597\tGenerator Loss: 1.230902\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tDiscriminator Loss: 0.449919\tGenerator Loss: 1.475045\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tDiscriminator Loss: 0.631575\tGenerator Loss: 0.613069\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tDiscriminator Loss: 0.601704\tGenerator Loss: 1.579565\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tDiscriminator Loss: 0.577716\tGenerator Loss: 1.281830\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tDiscriminator Loss: 0.564206\tGenerator Loss: 0.918554\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tDiscriminator Loss: 0.483750\tGenerator Loss: 1.474800\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tDiscriminator Loss: 0.566661\tGenerator Loss: 0.812404\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tDiscriminator Loss: 0.545439\tGenerator Loss: 0.974009\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tDiscriminator Loss: 0.503701\tGenerator Loss: 1.051346\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tDiscriminator Loss: 0.520359\tGenerator Loss: 1.169364\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tDiscriminator Loss: 0.541607\tGenerator Loss: 1.425407\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tDiscriminator Loss: 0.580525\tGenerator Loss: 0.648207\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tDiscriminator Loss: 0.576189\tGenerator Loss: 0.714011\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tDiscriminator Loss: 0.568520\tGenerator Loss: 0.944237\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tDiscriminator Loss: 0.525889\tGenerator Loss: 1.391007\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tDiscriminator Loss: 0.584718\tGenerator Loss: 0.848423\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tDiscriminator Loss: 0.531535\tGenerator Loss: 0.888219\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tDiscriminator Loss: 0.534633\tGenerator Loss: 1.378780\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tDiscriminator Loss: 0.511536\tGenerator Loss: 1.323306\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tDiscriminator Loss: 0.511189\tGenerator Loss: 1.008159\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tDiscriminator Loss: 0.455247\tGenerator Loss: 1.084703\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tDiscriminator Loss: 0.439479\tGenerator Loss: 1.771630\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tDiscriminator Loss: 0.476850\tGenerator Loss: 1.281353\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tDiscriminator Loss: 0.457585\tGenerator Loss: 1.434145\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tDiscriminator Loss: 0.565527\tGenerator Loss: 0.868483\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tDiscriminator Loss: 0.674508\tGenerator Loss: 0.578615\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tDiscriminator Loss: 0.571042\tGenerator Loss: 1.397431\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tDiscriminator Loss: 0.598713\tGenerator Loss: 1.010559\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tDiscriminator Loss: 0.613719\tGenerator Loss: 1.820048\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tDiscriminator Loss: 0.442095\tGenerator Loss: 1.272976\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tDiscriminator Loss: 0.467950\tGenerator Loss: 1.489776\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tDiscriminator Loss: 0.445388\tGenerator Loss: 1.398837\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tDiscriminator Loss: 0.460155\tGenerator Loss: 1.360211\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tDiscriminator Loss: 0.580543\tGenerator Loss: 1.485001\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tDiscriminator Loss: 0.542283\tGenerator Loss: 1.100493\n",
      "Train Epoch: 10 [0/60000 (0%)]\tDiscriminator Loss: 0.591578\tGenerator Loss: 1.436318\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tDiscriminator Loss: 0.601890\tGenerator Loss: 0.818604\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tDiscriminator Loss: 0.540834\tGenerator Loss: 1.390334\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tDiscriminator Loss: 0.581009\tGenerator Loss: 2.058388\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tDiscriminator Loss: 0.459001\tGenerator Loss: 1.394578\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tDiscriminator Loss: 0.524010\tGenerator Loss: 1.081181\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tDiscriminator Loss: 0.495260\tGenerator Loss: 1.155336\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tDiscriminator Loss: 0.589917\tGenerator Loss: 0.964849\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tDiscriminator Loss: 0.505589\tGenerator Loss: 1.215491\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tDiscriminator Loss: 0.604724\tGenerator Loss: 1.142695\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tDiscriminator Loss: 0.545428\tGenerator Loss: 1.319664\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tDiscriminator Loss: 0.547728\tGenerator Loss: 0.823526\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tDiscriminator Loss: 0.562720\tGenerator Loss: 1.155961\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tDiscriminator Loss: 0.478463\tGenerator Loss: 1.685119\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tDiscriminator Loss: 0.519531\tGenerator Loss: 1.094734\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tDiscriminator Loss: 0.501952\tGenerator Loss: 1.081825\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tDiscriminator Loss: 0.513740\tGenerator Loss: 1.435740\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tDiscriminator Loss: 0.572954\tGenerator Loss: 1.112583\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tDiscriminator Loss: 0.559548\tGenerator Loss: 1.405718\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tDiscriminator Loss: 0.633109\tGenerator Loss: 0.649545\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tDiscriminator Loss: 0.532995\tGenerator Loss: 1.304669\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tDiscriminator Loss: 0.477763\tGenerator Loss: 1.151435\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tDiscriminator Loss: 0.455161\tGenerator Loss: 1.157102\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tDiscriminator Loss: 0.512693\tGenerator Loss: 1.121522\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tDiscriminator Loss: 0.516493\tGenerator Loss: 1.242288\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tDiscriminator Loss: 0.526498\tGenerator Loss: 1.375326\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tDiscriminator Loss: 0.555865\tGenerator Loss: 1.396170\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tDiscriminator Loss: 0.604674\tGenerator Loss: 1.388089\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tDiscriminator Loss: 0.617980\tGenerator Loss: 1.827686\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tDiscriminator Loss: 0.575051\tGenerator Loss: 1.176539\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tDiscriminator Loss: 0.556990\tGenerator Loss: 0.917779\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tDiscriminator Loss: 0.528017\tGenerator Loss: 1.342130\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tDiscriminator Loss: 0.472758\tGenerator Loss: 1.382335\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tDiscriminator Loss: 0.504486\tGenerator Loss: 1.581653\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tDiscriminator Loss: 0.480093\tGenerator Loss: 1.165006\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tDiscriminator Loss: 0.489197\tGenerator Loss: 1.334463\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tDiscriminator Loss: 0.489304\tGenerator Loss: 1.111513\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tDiscriminator Loss: 0.480138\tGenerator Loss: 1.150086\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tDiscriminator Loss: 0.488790\tGenerator Loss: 1.271980\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tDiscriminator Loss: 0.488936\tGenerator Loss: 1.331699\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tDiscriminator Loss: 0.490844\tGenerator Loss: 1.245659\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tDiscriminator Loss: 0.546825\tGenerator Loss: 1.112275\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tDiscriminator Loss: 0.652842\tGenerator Loss: 1.692686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [55040/60000 (92%)]\tDiscriminator Loss: 0.552895\tGenerator Loss: 0.751962\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tDiscriminator Loss: 0.553683\tGenerator Loss: 1.451341\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tDiscriminator Loss: 0.528087\tGenerator Loss: 1.144094\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tDiscriminator Loss: 0.476177\tGenerator Loss: 1.300273\n",
      "Train Epoch: 11 [0/60000 (0%)]\tDiscriminator Loss: 0.495826\tGenerator Loss: 1.626238\n",
      "Train Epoch: 11 [1280/60000 (2%)]\tDiscriminator Loss: 0.745642\tGenerator Loss: 0.425721\n",
      "Train Epoch: 11 [2560/60000 (4%)]\tDiscriminator Loss: 0.584941\tGenerator Loss: 0.944631\n",
      "Train Epoch: 11 [3840/60000 (6%)]\tDiscriminator Loss: 0.522118\tGenerator Loss: 1.170533\n",
      "Train Epoch: 11 [5120/60000 (9%)]\tDiscriminator Loss: 0.518613\tGenerator Loss: 1.091035\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tDiscriminator Loss: 0.470398\tGenerator Loss: 1.158306\n",
      "Train Epoch: 11 [7680/60000 (13%)]\tDiscriminator Loss: 0.450975\tGenerator Loss: 1.554302\n",
      "Train Epoch: 11 [8960/60000 (15%)]\tDiscriminator Loss: 0.490337\tGenerator Loss: 1.351717\n",
      "Train Epoch: 11 [10240/60000 (17%)]\tDiscriminator Loss: 0.638377\tGenerator Loss: 1.632833\n",
      "Train Epoch: 11 [11520/60000 (19%)]\tDiscriminator Loss: 0.553185\tGenerator Loss: 0.989596\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tDiscriminator Loss: 0.544679\tGenerator Loss: 0.941079\n",
      "Train Epoch: 11 [14080/60000 (23%)]\tDiscriminator Loss: 0.491325\tGenerator Loss: 1.281371\n",
      "Train Epoch: 11 [15360/60000 (26%)]\tDiscriminator Loss: 0.547251\tGenerator Loss: 0.788497\n",
      "Train Epoch: 11 [16640/60000 (28%)]\tDiscriminator Loss: 0.590267\tGenerator Loss: 0.793770\n",
      "Train Epoch: 11 [17920/60000 (30%)]\tDiscriminator Loss: 0.586209\tGenerator Loss: 1.523356\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tDiscriminator Loss: 0.506284\tGenerator Loss: 1.203805\n",
      "Train Epoch: 11 [20480/60000 (34%)]\tDiscriminator Loss: 0.473446\tGenerator Loss: 1.292134\n",
      "Train Epoch: 11 [21760/60000 (36%)]\tDiscriminator Loss: 0.480092\tGenerator Loss: 1.071120\n",
      "Train Epoch: 11 [23040/60000 (38%)]\tDiscriminator Loss: 0.542189\tGenerator Loss: 1.006703\n",
      "Train Epoch: 11 [24320/60000 (41%)]\tDiscriminator Loss: 0.480953\tGenerator Loss: 1.396517\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tDiscriminator Loss: 0.521520\tGenerator Loss: 1.151043\n",
      "Train Epoch: 11 [26880/60000 (45%)]\tDiscriminator Loss: 0.527786\tGenerator Loss: 0.936842\n",
      "Train Epoch: 11 [28160/60000 (47%)]\tDiscriminator Loss: 0.526880\tGenerator Loss: 1.110850\n",
      "Train Epoch: 11 [29440/60000 (49%)]\tDiscriminator Loss: 0.551276\tGenerator Loss: 1.174737\n",
      "Train Epoch: 11 [30720/60000 (51%)]\tDiscriminator Loss: 0.642281\tGenerator Loss: 1.889803\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tDiscriminator Loss: 0.604610\tGenerator Loss: 1.170734\n",
      "Train Epoch: 11 [33280/60000 (55%)]\tDiscriminator Loss: 0.549909\tGenerator Loss: 1.261735\n",
      "Train Epoch: 11 [34560/60000 (58%)]\tDiscriminator Loss: 0.503398\tGenerator Loss: 1.078466\n",
      "Train Epoch: 11 [35840/60000 (60%)]\tDiscriminator Loss: 0.558346\tGenerator Loss: 0.710365\n",
      "Train Epoch: 11 [37120/60000 (62%)]\tDiscriminator Loss: 0.482198\tGenerator Loss: 1.513689\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tDiscriminator Loss: 0.557229\tGenerator Loss: 1.132634\n",
      "Train Epoch: 11 [39680/60000 (66%)]\tDiscriminator Loss: 0.630682\tGenerator Loss: 1.356618\n",
      "Train Epoch: 11 [40960/60000 (68%)]\tDiscriminator Loss: 0.565006\tGenerator Loss: 0.916293\n",
      "Train Epoch: 11 [42240/60000 (70%)]\tDiscriminator Loss: 0.563370\tGenerator Loss: 1.329204\n",
      "Train Epoch: 11 [43520/60000 (72%)]\tDiscriminator Loss: 0.505760\tGenerator Loss: 1.411217\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tDiscriminator Loss: 0.442761\tGenerator Loss: 1.307771\n",
      "Train Epoch: 11 [46080/60000 (77%)]\tDiscriminator Loss: 0.527391\tGenerator Loss: 1.338915\n",
      "Train Epoch: 11 [47360/60000 (79%)]\tDiscriminator Loss: 0.480339\tGenerator Loss: 1.395499\n",
      "Train Epoch: 11 [48640/60000 (81%)]\tDiscriminator Loss: 0.712762\tGenerator Loss: 0.515277\n",
      "Train Epoch: 11 [49920/60000 (83%)]\tDiscriminator Loss: 0.502745\tGenerator Loss: 1.078297\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tDiscriminator Loss: 0.500028\tGenerator Loss: 1.201280\n",
      "Train Epoch: 11 [52480/60000 (87%)]\tDiscriminator Loss: 0.513327\tGenerator Loss: 1.273153\n",
      "Train Epoch: 11 [53760/60000 (90%)]\tDiscriminator Loss: 0.483508\tGenerator Loss: 1.069005\n",
      "Train Epoch: 11 [55040/60000 (92%)]\tDiscriminator Loss: 0.510087\tGenerator Loss: 1.226094\n",
      "Train Epoch: 11 [56320/60000 (94%)]\tDiscriminator Loss: 0.611213\tGenerator Loss: 0.912219\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tDiscriminator Loss: 0.518009\tGenerator Loss: 1.044333\n",
      "Train Epoch: 11 [58880/60000 (98%)]\tDiscriminator Loss: 0.467839\tGenerator Loss: 1.905914\n",
      "Train Epoch: 12 [0/60000 (0%)]\tDiscriminator Loss: 0.531132\tGenerator Loss: 1.000566\n",
      "Train Epoch: 12 [1280/60000 (2%)]\tDiscriminator Loss: 0.542491\tGenerator Loss: 1.341645\n",
      "Train Epoch: 12 [2560/60000 (4%)]\tDiscriminator Loss: 0.478483\tGenerator Loss: 1.120645\n",
      "Train Epoch: 12 [3840/60000 (6%)]\tDiscriminator Loss: 0.506242\tGenerator Loss: 1.211802\n",
      "Train Epoch: 12 [5120/60000 (9%)]\tDiscriminator Loss: 0.546464\tGenerator Loss: 1.057842\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tDiscriminator Loss: 0.483469\tGenerator Loss: 1.297959\n",
      "Train Epoch: 12 [7680/60000 (13%)]\tDiscriminator Loss: 0.528211\tGenerator Loss: 0.931292\n",
      "Train Epoch: 12 [8960/60000 (15%)]\tDiscriminator Loss: 0.582314\tGenerator Loss: 1.118898\n",
      "Train Epoch: 12 [10240/60000 (17%)]\tDiscriminator Loss: 0.530672\tGenerator Loss: 1.130853\n",
      "Train Epoch: 12 [11520/60000 (19%)]\tDiscriminator Loss: 0.614364\tGenerator Loss: 1.468105\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tDiscriminator Loss: 0.503334\tGenerator Loss: 1.040967\n",
      "Train Epoch: 12 [14080/60000 (23%)]\tDiscriminator Loss: 0.486751\tGenerator Loss: 1.219219\n",
      "Train Epoch: 12 [15360/60000 (26%)]\tDiscriminator Loss: 0.512254\tGenerator Loss: 1.163003\n",
      "Train Epoch: 12 [16640/60000 (28%)]\tDiscriminator Loss: 0.548092\tGenerator Loss: 1.477701\n",
      "Train Epoch: 12 [17920/60000 (30%)]\tDiscriminator Loss: 0.564215\tGenerator Loss: 1.199216\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tDiscriminator Loss: 0.609688\tGenerator Loss: 1.500856\n",
      "Train Epoch: 12 [20480/60000 (34%)]\tDiscriminator Loss: 0.508935\tGenerator Loss: 1.593595\n",
      "Train Epoch: 12 [21760/60000 (36%)]\tDiscriminator Loss: 0.520287\tGenerator Loss: 1.133659\n",
      "Train Epoch: 12 [23040/60000 (38%)]\tDiscriminator Loss: 0.616974\tGenerator Loss: 1.221053\n",
      "Train Epoch: 12 [24320/60000 (41%)]\tDiscriminator Loss: 0.528497\tGenerator Loss: 0.942683\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tDiscriminator Loss: 0.521614\tGenerator Loss: 1.306920\n",
      "Train Epoch: 12 [26880/60000 (45%)]\tDiscriminator Loss: 0.486605\tGenerator Loss: 1.171155\n",
      "Train Epoch: 12 [28160/60000 (47%)]\tDiscriminator Loss: 0.499368\tGenerator Loss: 0.851308\n",
      "Train Epoch: 12 [29440/60000 (49%)]\tDiscriminator Loss: 0.497303\tGenerator Loss: 1.268411\n",
      "Train Epoch: 12 [30720/60000 (51%)]\tDiscriminator Loss: 0.514899\tGenerator Loss: 1.297181\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tDiscriminator Loss: 0.742800\tGenerator Loss: 1.896143\n",
      "Train Epoch: 12 [33280/60000 (55%)]\tDiscriminator Loss: 0.586030\tGenerator Loss: 0.868392\n",
      "Train Epoch: 12 [34560/60000 (58%)]\tDiscriminator Loss: 0.538031\tGenerator Loss: 0.937497\n",
      "Train Epoch: 12 [35840/60000 (60%)]\tDiscriminator Loss: 0.516782\tGenerator Loss: 1.319910\n",
      "Train Epoch: 12 [37120/60000 (62%)]\tDiscriminator Loss: 0.515470\tGenerator Loss: 1.305051\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tDiscriminator Loss: 0.569546\tGenerator Loss: 0.766604\n",
      "Train Epoch: 12 [39680/60000 (66%)]\tDiscriminator Loss: 0.565890\tGenerator Loss: 1.035343\n",
      "Train Epoch: 12 [40960/60000 (68%)]\tDiscriminator Loss: 0.670148\tGenerator Loss: 0.622028\n",
      "Train Epoch: 12 [42240/60000 (70%)]\tDiscriminator Loss: 0.552518\tGenerator Loss: 1.528433\n",
      "Train Epoch: 12 [43520/60000 (72%)]\tDiscriminator Loss: 0.527878\tGenerator Loss: 1.390915\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tDiscriminator Loss: 0.474621\tGenerator Loss: 1.670075\n",
      "Train Epoch: 12 [46080/60000 (77%)]\tDiscriminator Loss: 0.565726\tGenerator Loss: 1.048018\n",
      "Train Epoch: 12 [47360/60000 (79%)]\tDiscriminator Loss: 0.655804\tGenerator Loss: 0.706178\n",
      "Train Epoch: 12 [48640/60000 (81%)]\tDiscriminator Loss: 0.599571\tGenerator Loss: 1.117484\n",
      "Train Epoch: 12 [49920/60000 (83%)]\tDiscriminator Loss: 0.544252\tGenerator Loss: 1.122678\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tDiscriminator Loss: 0.554675\tGenerator Loss: 1.243413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12 [52480/60000 (87%)]\tDiscriminator Loss: 0.487743\tGenerator Loss: 1.405985\n",
      "Train Epoch: 12 [53760/60000 (90%)]\tDiscriminator Loss: 0.503676\tGenerator Loss: 1.609993\n",
      "Train Epoch: 12 [55040/60000 (92%)]\tDiscriminator Loss: 0.501505\tGenerator Loss: 1.072088\n",
      "Train Epoch: 12 [56320/60000 (94%)]\tDiscriminator Loss: 0.630844\tGenerator Loss: 0.791029\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tDiscriminator Loss: 0.524663\tGenerator Loss: 1.010241\n",
      "Train Epoch: 12 [58880/60000 (98%)]\tDiscriminator Loss: 0.529109\tGenerator Loss: 1.017939\n",
      "Train Epoch: 13 [0/60000 (0%)]\tDiscriminator Loss: 0.553863\tGenerator Loss: 0.893672\n",
      "Train Epoch: 13 [1280/60000 (2%)]\tDiscriminator Loss: 0.501655\tGenerator Loss: 1.422857\n",
      "Train Epoch: 13 [2560/60000 (4%)]\tDiscriminator Loss: 0.589750\tGenerator Loss: 1.656667\n",
      "Train Epoch: 13 [3840/60000 (6%)]\tDiscriminator Loss: 0.492599\tGenerator Loss: 1.039356\n",
      "Train Epoch: 13 [5120/60000 (9%)]\tDiscriminator Loss: 0.571255\tGenerator Loss: 1.423912\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tDiscriminator Loss: 0.544203\tGenerator Loss: 1.306076\n",
      "Train Epoch: 13 [7680/60000 (13%)]\tDiscriminator Loss: 0.532351\tGenerator Loss: 0.999162\n",
      "Train Epoch: 13 [8960/60000 (15%)]\tDiscriminator Loss: 0.711014\tGenerator Loss: 1.733079\n",
      "Train Epoch: 13 [10240/60000 (17%)]\tDiscriminator Loss: 0.484215\tGenerator Loss: 1.150272\n",
      "Train Epoch: 13 [11520/60000 (19%)]\tDiscriminator Loss: 0.499058\tGenerator Loss: 1.192260\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tDiscriminator Loss: 0.548286\tGenerator Loss: 1.256008\n",
      "Train Epoch: 13 [14080/60000 (23%)]\tDiscriminator Loss: 0.548389\tGenerator Loss: 0.779570\n",
      "Train Epoch: 13 [15360/60000 (26%)]\tDiscriminator Loss: 0.466991\tGenerator Loss: 1.315194\n",
      "Train Epoch: 13 [16640/60000 (28%)]\tDiscriminator Loss: 0.525932\tGenerator Loss: 0.943636\n",
      "Train Epoch: 13 [17920/60000 (30%)]\tDiscriminator Loss: 0.458403\tGenerator Loss: 1.435488\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tDiscriminator Loss: 0.504860\tGenerator Loss: 1.236778\n",
      "Train Epoch: 13 [20480/60000 (34%)]\tDiscriminator Loss: 0.566803\tGenerator Loss: 1.741075\n",
      "Train Epoch: 13 [21760/60000 (36%)]\tDiscriminator Loss: 0.595024\tGenerator Loss: 1.029536\n",
      "Train Epoch: 13 [23040/60000 (38%)]\tDiscriminator Loss: 0.553568\tGenerator Loss: 0.970539\n",
      "Train Epoch: 13 [24320/60000 (41%)]\tDiscriminator Loss: 0.555930\tGenerator Loss: 1.311778\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tDiscriminator Loss: 0.521347\tGenerator Loss: 1.326474\n",
      "Train Epoch: 13 [26880/60000 (45%)]\tDiscriminator Loss: 0.505635\tGenerator Loss: 1.155554\n",
      "Train Epoch: 13 [28160/60000 (47%)]\tDiscriminator Loss: 0.682396\tGenerator Loss: 0.653876\n",
      "Train Epoch: 13 [29440/60000 (49%)]\tDiscriminator Loss: 0.565044\tGenerator Loss: 1.278731\n",
      "Train Epoch: 13 [30720/60000 (51%)]\tDiscriminator Loss: 0.488732\tGenerator Loss: 1.240952\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tDiscriminator Loss: 0.440246\tGenerator Loss: 1.343018\n",
      "Train Epoch: 13 [33280/60000 (55%)]\tDiscriminator Loss: 0.528263\tGenerator Loss: 1.144844\n",
      "Train Epoch: 13 [34560/60000 (58%)]\tDiscriminator Loss: 0.523837\tGenerator Loss: 1.358308\n",
      "Train Epoch: 13 [35840/60000 (60%)]\tDiscriminator Loss: 0.700655\tGenerator Loss: 1.979311\n",
      "Train Epoch: 13 [37120/60000 (62%)]\tDiscriminator Loss: 0.589419\tGenerator Loss: 0.962499\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tDiscriminator Loss: 0.531204\tGenerator Loss: 1.174943\n",
      "Train Epoch: 13 [39680/60000 (66%)]\tDiscriminator Loss: 0.521991\tGenerator Loss: 1.206553\n",
      "Train Epoch: 13 [40960/60000 (68%)]\tDiscriminator Loss: 0.606265\tGenerator Loss: 0.741445\n",
      "Train Epoch: 13 [42240/60000 (70%)]\tDiscriminator Loss: 0.559289\tGenerator Loss: 1.145965\n",
      "Train Epoch: 13 [43520/60000 (72%)]\tDiscriminator Loss: 0.508656\tGenerator Loss: 1.479447\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tDiscriminator Loss: 0.536529\tGenerator Loss: 1.243645\n",
      "Train Epoch: 13 [46080/60000 (77%)]\tDiscriminator Loss: 0.545491\tGenerator Loss: 1.815949\n",
      "Train Epoch: 13 [47360/60000 (79%)]\tDiscriminator Loss: 0.565297\tGenerator Loss: 1.283113\n",
      "Train Epoch: 13 [48640/60000 (81%)]\tDiscriminator Loss: 0.555406\tGenerator Loss: 0.889583\n",
      "Train Epoch: 13 [49920/60000 (83%)]\tDiscriminator Loss: 0.524471\tGenerator Loss: 1.166279\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tDiscriminator Loss: 0.547879\tGenerator Loss: 1.145452\n",
      "Train Epoch: 13 [52480/60000 (87%)]\tDiscriminator Loss: 0.476904\tGenerator Loss: 1.616939\n",
      "Train Epoch: 13 [53760/60000 (90%)]\tDiscriminator Loss: 0.502440\tGenerator Loss: 1.140229\n",
      "Train Epoch: 13 [55040/60000 (92%)]\tDiscriminator Loss: 0.622048\tGenerator Loss: 0.906679\n",
      "Train Epoch: 13 [56320/60000 (94%)]\tDiscriminator Loss: 0.588336\tGenerator Loss: 0.900162\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tDiscriminator Loss: 0.579777\tGenerator Loss: 1.076055\n",
      "Train Epoch: 13 [58880/60000 (98%)]\tDiscriminator Loss: 0.627993\tGenerator Loss: 1.285169\n",
      "Train Epoch: 14 [0/60000 (0%)]\tDiscriminator Loss: 0.551075\tGenerator Loss: 0.921216\n",
      "Train Epoch: 14 [1280/60000 (2%)]\tDiscriminator Loss: 0.497209\tGenerator Loss: 1.266083\n",
      "Train Epoch: 14 [2560/60000 (4%)]\tDiscriminator Loss: 0.555222\tGenerator Loss: 1.078124\n",
      "Train Epoch: 14 [3840/60000 (6%)]\tDiscriminator Loss: 0.508690\tGenerator Loss: 1.348634\n",
      "Train Epoch: 14 [5120/60000 (9%)]\tDiscriminator Loss: 0.522879\tGenerator Loss: 1.547139\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tDiscriminator Loss: 0.451496\tGenerator Loss: 1.519064\n",
      "Train Epoch: 14 [7680/60000 (13%)]\tDiscriminator Loss: 0.474163\tGenerator Loss: 1.598789\n",
      "Train Epoch: 14 [8960/60000 (15%)]\tDiscriminator Loss: 0.581183\tGenerator Loss: 0.965391\n",
      "Train Epoch: 14 [10240/60000 (17%)]\tDiscriminator Loss: 0.538980\tGenerator Loss: 1.273020\n",
      "Train Epoch: 14 [11520/60000 (19%)]\tDiscriminator Loss: 0.604704\tGenerator Loss: 0.675071\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tDiscriminator Loss: 0.594890\tGenerator Loss: 0.838551\n",
      "Train Epoch: 14 [14080/60000 (23%)]\tDiscriminator Loss: 0.641395\tGenerator Loss: 0.781460\n",
      "Train Epoch: 14 [15360/60000 (26%)]\tDiscriminator Loss: 0.544785\tGenerator Loss: 1.019218\n",
      "Train Epoch: 14 [16640/60000 (28%)]\tDiscriminator Loss: 0.509213\tGenerator Loss: 1.298594\n",
      "Train Epoch: 14 [17920/60000 (30%)]\tDiscriminator Loss: 0.537547\tGenerator Loss: 1.261714\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tDiscriminator Loss: 0.496101\tGenerator Loss: 1.515564\n",
      "Train Epoch: 14 [20480/60000 (34%)]\tDiscriminator Loss: 0.502817\tGenerator Loss: 1.429637\n",
      "Train Epoch: 14 [21760/60000 (36%)]\tDiscriminator Loss: 0.525601\tGenerator Loss: 1.215304\n",
      "Train Epoch: 14 [23040/60000 (38%)]\tDiscriminator Loss: 0.579118\tGenerator Loss: 1.363176\n",
      "Train Epoch: 14 [24320/60000 (41%)]\tDiscriminator Loss: 0.542877\tGenerator Loss: 0.963829\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tDiscriminator Loss: 0.654454\tGenerator Loss: 1.030311\n",
      "Train Epoch: 14 [26880/60000 (45%)]\tDiscriminator Loss: 0.566569\tGenerator Loss: 1.137754\n",
      "Train Epoch: 14 [28160/60000 (47%)]\tDiscriminator Loss: 0.512544\tGenerator Loss: 1.023445\n",
      "Train Epoch: 14 [29440/60000 (49%)]\tDiscriminator Loss: 0.557776\tGenerator Loss: 1.108569\n",
      "Train Epoch: 14 [30720/60000 (51%)]\tDiscriminator Loss: 0.550271\tGenerator Loss: 1.136488\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tDiscriminator Loss: 0.590704\tGenerator Loss: 1.399097\n",
      "Train Epoch: 14 [33280/60000 (55%)]\tDiscriminator Loss: 0.535156\tGenerator Loss: 1.428173\n",
      "Train Epoch: 14 [34560/60000 (58%)]\tDiscriminator Loss: 0.557513\tGenerator Loss: 1.065089\n",
      "Train Epoch: 14 [35840/60000 (60%)]\tDiscriminator Loss: 0.512508\tGenerator Loss: 1.316919\n",
      "Train Epoch: 14 [37120/60000 (62%)]\tDiscriminator Loss: 0.526585\tGenerator Loss: 1.067805\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tDiscriminator Loss: 0.592754\tGenerator Loss: 1.314945\n",
      "Train Epoch: 14 [39680/60000 (66%)]\tDiscriminator Loss: 0.550000\tGenerator Loss: 1.257323\n",
      "Train Epoch: 14 [40960/60000 (68%)]\tDiscriminator Loss: 0.654717\tGenerator Loss: 1.590671\n",
      "Train Epoch: 14 [42240/60000 (70%)]\tDiscriminator Loss: 0.578636\tGenerator Loss: 1.072273\n",
      "Train Epoch: 14 [43520/60000 (72%)]\tDiscriminator Loss: 0.521793\tGenerator Loss: 1.405730\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tDiscriminator Loss: 0.565346\tGenerator Loss: 1.166164\n",
      "Train Epoch: 14 [46080/60000 (77%)]\tDiscriminator Loss: 0.505644\tGenerator Loss: 1.264135\n",
      "Train Epoch: 14 [47360/60000 (79%)]\tDiscriminator Loss: 0.519533\tGenerator Loss: 1.125475\n",
      "Train Epoch: 14 [48640/60000 (81%)]\tDiscriminator Loss: 0.556271\tGenerator Loss: 0.901632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 14 [49920/60000 (83%)]\tDiscriminator Loss: 0.588023\tGenerator Loss: 1.025753\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tDiscriminator Loss: 0.579788\tGenerator Loss: 1.694891\n",
      "Train Epoch: 14 [52480/60000 (87%)]\tDiscriminator Loss: 0.498048\tGenerator Loss: 1.583738\n",
      "Train Epoch: 14 [53760/60000 (90%)]\tDiscriminator Loss: 0.500673\tGenerator Loss: 1.365436\n",
      "Train Epoch: 14 [55040/60000 (92%)]\tDiscriminator Loss: 0.524934\tGenerator Loss: 1.477893\n",
      "Train Epoch: 14 [56320/60000 (94%)]\tDiscriminator Loss: 0.517573\tGenerator Loss: 1.446993\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tDiscriminator Loss: 0.508793\tGenerator Loss: 1.100777\n",
      "Train Epoch: 14 [58880/60000 (98%)]\tDiscriminator Loss: 0.493130\tGenerator Loss: 1.105219\n",
      "Train Epoch: 15 [0/60000 (0%)]\tDiscriminator Loss: 0.531193\tGenerator Loss: 1.246585\n",
      "Train Epoch: 15 [1280/60000 (2%)]\tDiscriminator Loss: 0.553089\tGenerator Loss: 1.343849\n",
      "Train Epoch: 15 [2560/60000 (4%)]\tDiscriminator Loss: 0.503642\tGenerator Loss: 1.445441\n",
      "Train Epoch: 15 [3840/60000 (6%)]\tDiscriminator Loss: 0.540979\tGenerator Loss: 1.415844\n",
      "Train Epoch: 15 [5120/60000 (9%)]\tDiscriminator Loss: 0.522794\tGenerator Loss: 1.359509\n",
      "Train Epoch: 15 [6400/60000 (11%)]\tDiscriminator Loss: 0.570968\tGenerator Loss: 0.921192\n",
      "Train Epoch: 15 [7680/60000 (13%)]\tDiscriminator Loss: 0.620880\tGenerator Loss: 0.678990\n",
      "Train Epoch: 15 [8960/60000 (15%)]\tDiscriminator Loss: 0.510155\tGenerator Loss: 1.321139\n",
      "Train Epoch: 15 [10240/60000 (17%)]\tDiscriminator Loss: 0.518261\tGenerator Loss: 1.299041\n",
      "Train Epoch: 15 [11520/60000 (19%)]\tDiscriminator Loss: 0.497772\tGenerator Loss: 1.285574\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tDiscriminator Loss: 0.519054\tGenerator Loss: 1.344762\n",
      "Train Epoch: 15 [14080/60000 (23%)]\tDiscriminator Loss: 0.555600\tGenerator Loss: 1.306246\n",
      "Train Epoch: 15 [15360/60000 (26%)]\tDiscriminator Loss: 0.630833\tGenerator Loss: 1.251400\n",
      "Train Epoch: 15 [16640/60000 (28%)]\tDiscriminator Loss: 0.617635\tGenerator Loss: 0.824888\n",
      "Train Epoch: 15 [17920/60000 (30%)]\tDiscriminator Loss: 0.562198\tGenerator Loss: 1.046762\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tDiscriminator Loss: 0.473301\tGenerator Loss: 1.048367\n",
      "Train Epoch: 15 [20480/60000 (34%)]\tDiscriminator Loss: 0.540418\tGenerator Loss: 1.227014\n",
      "Train Epoch: 15 [21760/60000 (36%)]\tDiscriminator Loss: 0.482099\tGenerator Loss: 1.331128\n",
      "Train Epoch: 15 [23040/60000 (38%)]\tDiscriminator Loss: 0.513437\tGenerator Loss: 1.259283\n",
      "Train Epoch: 15 [24320/60000 (41%)]\tDiscriminator Loss: 0.566014\tGenerator Loss: 1.287884\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tDiscriminator Loss: 0.557084\tGenerator Loss: 0.987701\n",
      "Train Epoch: 15 [26880/60000 (45%)]\tDiscriminator Loss: 0.608638\tGenerator Loss: 1.345748\n",
      "Train Epoch: 15 [28160/60000 (47%)]\tDiscriminator Loss: 0.557375\tGenerator Loss: 1.275929\n",
      "Train Epoch: 15 [29440/60000 (49%)]\tDiscriminator Loss: 0.532988\tGenerator Loss: 1.130181\n",
      "Train Epoch: 15 [30720/60000 (51%)]\tDiscriminator Loss: 0.572629\tGenerator Loss: 1.495897\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tDiscriminator Loss: 0.495413\tGenerator Loss: 1.032912\n",
      "Train Epoch: 15 [33280/60000 (55%)]\tDiscriminator Loss: 0.519045\tGenerator Loss: 1.569016\n",
      "Train Epoch: 15 [34560/60000 (58%)]\tDiscriminator Loss: 0.499549\tGenerator Loss: 1.412639\n",
      "Train Epoch: 15 [35840/60000 (60%)]\tDiscriminator Loss: 0.552812\tGenerator Loss: 1.055597\n",
      "Train Epoch: 15 [37120/60000 (62%)]\tDiscriminator Loss: 0.519632\tGenerator Loss: 1.067577\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tDiscriminator Loss: 0.566196\tGenerator Loss: 0.773785\n",
      "Train Epoch: 15 [39680/60000 (66%)]\tDiscriminator Loss: 0.602672\tGenerator Loss: 1.164246\n",
      "Train Epoch: 15 [40960/60000 (68%)]\tDiscriminator Loss: 0.569751\tGenerator Loss: 0.974191\n",
      "Train Epoch: 15 [42240/60000 (70%)]\tDiscriminator Loss: 0.497174\tGenerator Loss: 1.012225\n",
      "Train Epoch: 15 [43520/60000 (72%)]\tDiscriminator Loss: 0.465790\tGenerator Loss: 1.382116\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tDiscriminator Loss: 0.576473\tGenerator Loss: 1.276608\n",
      "Train Epoch: 15 [46080/60000 (77%)]\tDiscriminator Loss: 0.700109\tGenerator Loss: 1.860872\n",
      "Train Epoch: 15 [47360/60000 (79%)]\tDiscriminator Loss: 0.545103\tGenerator Loss: 0.988694\n",
      "Train Epoch: 15 [48640/60000 (81%)]\tDiscriminator Loss: 0.529140\tGenerator Loss: 1.117524\n",
      "Train Epoch: 15 [49920/60000 (83%)]\tDiscriminator Loss: 0.536464\tGenerator Loss: 1.282985\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tDiscriminator Loss: 0.566671\tGenerator Loss: 1.029245\n",
      "Train Epoch: 15 [52480/60000 (87%)]\tDiscriminator Loss: 0.552426\tGenerator Loss: 1.394475\n",
      "Train Epoch: 15 [53760/60000 (90%)]\tDiscriminator Loss: 0.498751\tGenerator Loss: 1.479606\n",
      "Train Epoch: 15 [55040/60000 (92%)]\tDiscriminator Loss: 0.520923\tGenerator Loss: 1.302056\n",
      "Train Epoch: 15 [56320/60000 (94%)]\tDiscriminator Loss: 0.642354\tGenerator Loss: 0.715874\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tDiscriminator Loss: 0.517559\tGenerator Loss: 1.403374\n",
      "Train Epoch: 15 [58880/60000 (98%)]\tDiscriminator Loss: 0.626786\tGenerator Loss: 1.365182\n",
      "Train Epoch: 16 [0/60000 (0%)]\tDiscriminator Loss: 0.593835\tGenerator Loss: 0.807437\n",
      "Train Epoch: 16 [1280/60000 (2%)]\tDiscriminator Loss: 0.604854\tGenerator Loss: 1.050750\n",
      "Train Epoch: 16 [2560/60000 (4%)]\tDiscriminator Loss: 0.515181\tGenerator Loss: 1.493724\n",
      "Train Epoch: 16 [3840/60000 (6%)]\tDiscriminator Loss: 0.479863\tGenerator Loss: 1.216911\n",
      "Train Epoch: 16 [5120/60000 (9%)]\tDiscriminator Loss: 0.514387\tGenerator Loss: 1.318555\n",
      "Train Epoch: 16 [6400/60000 (11%)]\tDiscriminator Loss: 0.570181\tGenerator Loss: 0.982655\n",
      "Train Epoch: 16 [7680/60000 (13%)]\tDiscriminator Loss: 0.530223\tGenerator Loss: 1.455089\n",
      "Train Epoch: 16 [8960/60000 (15%)]\tDiscriminator Loss: 0.524810\tGenerator Loss: 1.247903\n",
      "Train Epoch: 16 [10240/60000 (17%)]\tDiscriminator Loss: 0.541999\tGenerator Loss: 1.148508\n",
      "Train Epoch: 16 [11520/60000 (19%)]\tDiscriminator Loss: 0.511957\tGenerator Loss: 1.296031\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tDiscriminator Loss: 0.534358\tGenerator Loss: 1.393509\n",
      "Train Epoch: 16 [14080/60000 (23%)]\tDiscriminator Loss: 0.608650\tGenerator Loss: 0.814367\n",
      "Train Epoch: 16 [15360/60000 (26%)]\tDiscriminator Loss: 0.559159\tGenerator Loss: 0.969688\n",
      "Train Epoch: 16 [16640/60000 (28%)]\tDiscriminator Loss: 0.535805\tGenerator Loss: 1.417335\n",
      "Train Epoch: 16 [17920/60000 (30%)]\tDiscriminator Loss: 0.508701\tGenerator Loss: 1.387516\n",
      "Train Epoch: 16 [19200/60000 (32%)]\tDiscriminator Loss: 0.601792\tGenerator Loss: 0.737132\n",
      "Train Epoch: 16 [20480/60000 (34%)]\tDiscriminator Loss: 0.535333\tGenerator Loss: 1.213142\n",
      "Train Epoch: 16 [21760/60000 (36%)]\tDiscriminator Loss: 0.542928\tGenerator Loss: 0.926418\n",
      "Train Epoch: 16 [23040/60000 (38%)]\tDiscriminator Loss: 0.528975\tGenerator Loss: 1.684231\n",
      "Train Epoch: 16 [24320/60000 (41%)]\tDiscriminator Loss: 0.500868\tGenerator Loss: 1.333433\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tDiscriminator Loss: 0.505601\tGenerator Loss: 1.274659\n",
      "Train Epoch: 16 [26880/60000 (45%)]\tDiscriminator Loss: 0.559525\tGenerator Loss: 1.053453\n",
      "Train Epoch: 16 [28160/60000 (47%)]\tDiscriminator Loss: 0.513702\tGenerator Loss: 1.064852\n",
      "Train Epoch: 16 [29440/60000 (49%)]\tDiscriminator Loss: 0.546519\tGenerator Loss: 1.285714\n",
      "Train Epoch: 16 [30720/60000 (51%)]\tDiscriminator Loss: 0.527106\tGenerator Loss: 1.187251\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tDiscriminator Loss: 0.496300\tGenerator Loss: 1.373096\n",
      "Train Epoch: 16 [33280/60000 (55%)]\tDiscriminator Loss: 0.513042\tGenerator Loss: 1.157848\n",
      "Train Epoch: 16 [34560/60000 (58%)]\tDiscriminator Loss: 0.553661\tGenerator Loss: 1.262344\n",
      "Train Epoch: 16 [35840/60000 (60%)]\tDiscriminator Loss: 0.639759\tGenerator Loss: 1.059880\n",
      "Train Epoch: 16 [37120/60000 (62%)]\tDiscriminator Loss: 0.626075\tGenerator Loss: 1.182571\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tDiscriminator Loss: 0.556150\tGenerator Loss: 0.986387\n",
      "Train Epoch: 16 [39680/60000 (66%)]\tDiscriminator Loss: 0.477328\tGenerator Loss: 1.246000\n",
      "Train Epoch: 16 [40960/60000 (68%)]\tDiscriminator Loss: 0.505875\tGenerator Loss: 1.376087\n",
      "Train Epoch: 16 [42240/60000 (70%)]\tDiscriminator Loss: 0.507415\tGenerator Loss: 1.201618\n",
      "Train Epoch: 16 [43520/60000 (72%)]\tDiscriminator Loss: 0.611794\tGenerator Loss: 1.421299\n",
      "Train Epoch: 16 [44800/60000 (75%)]\tDiscriminator Loss: 0.581548\tGenerator Loss: 1.067448\n",
      "Train Epoch: 16 [46080/60000 (77%)]\tDiscriminator Loss: 0.465753\tGenerator Loss: 1.636490\n",
      "Train Epoch: 16 [47360/60000 (79%)]\tDiscriminator Loss: 0.487265\tGenerator Loss: 1.307458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 16 [48640/60000 (81%)]\tDiscriminator Loss: 0.528209\tGenerator Loss: 1.376871\n",
      "Train Epoch: 16 [49920/60000 (83%)]\tDiscriminator Loss: 0.517108\tGenerator Loss: 1.194168\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tDiscriminator Loss: 0.547979\tGenerator Loss: 1.438715\n",
      "Train Epoch: 16 [52480/60000 (87%)]\tDiscriminator Loss: 0.561482\tGenerator Loss: 1.028551\n",
      "Train Epoch: 16 [53760/60000 (90%)]\tDiscriminator Loss: 0.662864\tGenerator Loss: 1.154838\n",
      "Train Epoch: 16 [55040/60000 (92%)]\tDiscriminator Loss: 0.495947\tGenerator Loss: 1.018820\n",
      "Train Epoch: 16 [56320/60000 (94%)]\tDiscriminator Loss: 0.501873\tGenerator Loss: 1.167097\n",
      "Train Epoch: 16 [57600/60000 (96%)]\tDiscriminator Loss: 0.470263\tGenerator Loss: 1.382451\n",
      "Train Epoch: 16 [58880/60000 (98%)]\tDiscriminator Loss: 0.539765\tGenerator Loss: 0.884904\n",
      "Train Epoch: 17 [0/60000 (0%)]\tDiscriminator Loss: 0.522411\tGenerator Loss: 1.038978\n",
      "Train Epoch: 17 [1280/60000 (2%)]\tDiscriminator Loss: 0.538968\tGenerator Loss: 1.080601\n",
      "Train Epoch: 17 [2560/60000 (4%)]\tDiscriminator Loss: 0.586074\tGenerator Loss: 0.782422\n",
      "Train Epoch: 17 [3840/60000 (6%)]\tDiscriminator Loss: 0.526165\tGenerator Loss: 1.132067\n",
      "Train Epoch: 17 [5120/60000 (9%)]\tDiscriminator Loss: 0.570100\tGenerator Loss: 1.227211\n",
      "Train Epoch: 17 [6400/60000 (11%)]\tDiscriminator Loss: 0.539018\tGenerator Loss: 1.188285\n",
      "Train Epoch: 17 [7680/60000 (13%)]\tDiscriminator Loss: 0.551521\tGenerator Loss: 1.704041\n",
      "Train Epoch: 17 [8960/60000 (15%)]\tDiscriminator Loss: 0.488901\tGenerator Loss: 1.076556\n",
      "Train Epoch: 17 [10240/60000 (17%)]\tDiscriminator Loss: 0.496502\tGenerator Loss: 1.350864\n",
      "Train Epoch: 17 [11520/60000 (19%)]\tDiscriminator Loss: 0.486644\tGenerator Loss: 1.232050\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tDiscriminator Loss: 0.594547\tGenerator Loss: 1.417952\n",
      "Train Epoch: 17 [14080/60000 (23%)]\tDiscriminator Loss: 0.596518\tGenerator Loss: 0.777922\n",
      "Train Epoch: 17 [15360/60000 (26%)]\tDiscriminator Loss: 0.570296\tGenerator Loss: 0.989409\n",
      "Train Epoch: 17 [16640/60000 (28%)]\tDiscriminator Loss: 0.542390\tGenerator Loss: 1.334879\n",
      "Train Epoch: 17 [17920/60000 (30%)]\tDiscriminator Loss: 0.506047\tGenerator Loss: 1.172510\n",
      "Train Epoch: 17 [19200/60000 (32%)]\tDiscriminator Loss: 0.529248\tGenerator Loss: 1.051833\n",
      "Train Epoch: 17 [20480/60000 (34%)]\tDiscriminator Loss: 0.523336\tGenerator Loss: 1.259073\n",
      "Train Epoch: 17 [21760/60000 (36%)]\tDiscriminator Loss: 0.569047\tGenerator Loss: 0.799744\n",
      "Train Epoch: 17 [23040/60000 (38%)]\tDiscriminator Loss: 0.550344\tGenerator Loss: 1.633175\n",
      "Train Epoch: 17 [24320/60000 (41%)]\tDiscriminator Loss: 0.524360\tGenerator Loss: 1.356795\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tDiscriminator Loss: 0.539780\tGenerator Loss: 1.195487\n",
      "Train Epoch: 17 [26880/60000 (45%)]\tDiscriminator Loss: 0.563890\tGenerator Loss: 1.168247\n",
      "Train Epoch: 17 [28160/60000 (47%)]\tDiscriminator Loss: 0.493460\tGenerator Loss: 1.071992\n",
      "Train Epoch: 17 [29440/60000 (49%)]\tDiscriminator Loss: 0.572307\tGenerator Loss: 1.310174\n",
      "Train Epoch: 17 [30720/60000 (51%)]\tDiscriminator Loss: 0.491116\tGenerator Loss: 1.256657\n",
      "Train Epoch: 17 [32000/60000 (53%)]\tDiscriminator Loss: 0.558362\tGenerator Loss: 1.102988\n",
      "Train Epoch: 17 [33280/60000 (55%)]\tDiscriminator Loss: 0.541070\tGenerator Loss: 1.148231\n",
      "Train Epoch: 17 [34560/60000 (58%)]\tDiscriminator Loss: 0.513255\tGenerator Loss: 1.197440\n",
      "Train Epoch: 17 [35840/60000 (60%)]\tDiscriminator Loss: 0.568240\tGenerator Loss: 1.422685\n",
      "Train Epoch: 17 [37120/60000 (62%)]\tDiscriminator Loss: 0.484776\tGenerator Loss: 1.240312\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tDiscriminator Loss: 0.572378\tGenerator Loss: 1.235758\n",
      "Train Epoch: 17 [39680/60000 (66%)]\tDiscriminator Loss: 0.539729\tGenerator Loss: 1.349566\n",
      "Train Epoch: 17 [40960/60000 (68%)]\tDiscriminator Loss: 0.510468\tGenerator Loss: 1.384876\n",
      "Train Epoch: 17 [42240/60000 (70%)]\tDiscriminator Loss: 0.512624\tGenerator Loss: 1.653448\n",
      "Train Epoch: 17 [43520/60000 (72%)]\tDiscriminator Loss: 0.444738\tGenerator Loss: 1.424434\n",
      "Train Epoch: 17 [44800/60000 (75%)]\tDiscriminator Loss: 0.507594\tGenerator Loss: 1.883757\n",
      "Train Epoch: 17 [46080/60000 (77%)]\tDiscriminator Loss: 0.566208\tGenerator Loss: 0.777302\n",
      "Train Epoch: 17 [47360/60000 (79%)]\tDiscriminator Loss: 0.545705\tGenerator Loss: 1.110529\n",
      "Train Epoch: 17 [48640/60000 (81%)]\tDiscriminator Loss: 0.607753\tGenerator Loss: 0.684510\n",
      "Train Epoch: 17 [49920/60000 (83%)]\tDiscriminator Loss: 0.559739\tGenerator Loss: 1.155800\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tDiscriminator Loss: 0.493267\tGenerator Loss: 1.298154\n",
      "Train Epoch: 17 [52480/60000 (87%)]\tDiscriminator Loss: 0.490242\tGenerator Loss: 1.276986\n",
      "Train Epoch: 17 [53760/60000 (90%)]\tDiscriminator Loss: 0.552455\tGenerator Loss: 1.670648\n",
      "Train Epoch: 17 [55040/60000 (92%)]\tDiscriminator Loss: 0.529829\tGenerator Loss: 1.132334\n",
      "Train Epoch: 17 [56320/60000 (94%)]\tDiscriminator Loss: 0.604184\tGenerator Loss: 0.813291\n",
      "Train Epoch: 17 [57600/60000 (96%)]\tDiscriminator Loss: 0.649960\tGenerator Loss: 0.556361\n",
      "Train Epoch: 17 [58880/60000 (98%)]\tDiscriminator Loss: 0.537100\tGenerator Loss: 1.119899\n",
      "Train Epoch: 18 [0/60000 (0%)]\tDiscriminator Loss: 0.556588\tGenerator Loss: 1.345251\n",
      "Train Epoch: 18 [1280/60000 (2%)]\tDiscriminator Loss: 0.505823\tGenerator Loss: 1.143229\n",
      "Train Epoch: 18 [2560/60000 (4%)]\tDiscriminator Loss: 0.469142\tGenerator Loss: 1.764327\n",
      "Train Epoch: 18 [3840/60000 (6%)]\tDiscriminator Loss: 0.499276\tGenerator Loss: 1.032355\n",
      "Train Epoch: 18 [5120/60000 (9%)]\tDiscriminator Loss: 0.608278\tGenerator Loss: 1.409391\n",
      "Train Epoch: 18 [6400/60000 (11%)]\tDiscriminator Loss: 0.564762\tGenerator Loss: 1.255135\n",
      "Train Epoch: 18 [7680/60000 (13%)]\tDiscriminator Loss: 0.495595\tGenerator Loss: 1.346473\n",
      "Train Epoch: 18 [8960/60000 (15%)]\tDiscriminator Loss: 0.483382\tGenerator Loss: 1.679761\n",
      "Train Epoch: 18 [10240/60000 (17%)]\tDiscriminator Loss: 0.526825\tGenerator Loss: 1.112016\n",
      "Train Epoch: 18 [11520/60000 (19%)]\tDiscriminator Loss: 0.561106\tGenerator Loss: 1.162548\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tDiscriminator Loss: 0.561127\tGenerator Loss: 1.241702\n",
      "Train Epoch: 18 [14080/60000 (23%)]\tDiscriminator Loss: 0.569685\tGenerator Loss: 1.154287\n",
      "Train Epoch: 18 [15360/60000 (26%)]\tDiscriminator Loss: 0.567022\tGenerator Loss: 1.199237\n",
      "Train Epoch: 18 [16640/60000 (28%)]\tDiscriminator Loss: 0.499469\tGenerator Loss: 1.248311\n",
      "Train Epoch: 18 [17920/60000 (30%)]\tDiscriminator Loss: 0.537508\tGenerator Loss: 1.118262\n",
      "Train Epoch: 18 [19200/60000 (32%)]\tDiscriminator Loss: 0.543730\tGenerator Loss: 1.094460\n",
      "Train Epoch: 18 [20480/60000 (34%)]\tDiscriminator Loss: 0.645497\tGenerator Loss: 1.513078\n",
      "Train Epoch: 18 [21760/60000 (36%)]\tDiscriminator Loss: 0.454377\tGenerator Loss: 1.362446\n",
      "Train Epoch: 18 [23040/60000 (38%)]\tDiscriminator Loss: 0.539753\tGenerator Loss: 1.250453\n",
      "Train Epoch: 18 [24320/60000 (41%)]\tDiscriminator Loss: 0.470108\tGenerator Loss: 1.126963\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tDiscriminator Loss: 0.578092\tGenerator Loss: 1.525442\n",
      "Train Epoch: 18 [26880/60000 (45%)]\tDiscriminator Loss: 0.533958\tGenerator Loss: 1.235233\n",
      "Train Epoch: 18 [28160/60000 (47%)]\tDiscriminator Loss: 0.630158\tGenerator Loss: 0.681452\n",
      "Train Epoch: 18 [29440/60000 (49%)]\tDiscriminator Loss: 0.556544\tGenerator Loss: 1.101852\n",
      "Train Epoch: 18 [30720/60000 (51%)]\tDiscriminator Loss: 0.559708\tGenerator Loss: 1.030482\n",
      "Train Epoch: 18 [32000/60000 (53%)]\tDiscriminator Loss: 0.542836\tGenerator Loss: 0.990109\n",
      "Train Epoch: 18 [33280/60000 (55%)]\tDiscriminator Loss: 0.491158\tGenerator Loss: 1.168028\n",
      "Train Epoch: 18 [34560/60000 (58%)]\tDiscriminator Loss: 0.529636\tGenerator Loss: 0.878604\n",
      "Train Epoch: 18 [35840/60000 (60%)]\tDiscriminator Loss: 0.549426\tGenerator Loss: 1.203928\n",
      "Train Epoch: 18 [37120/60000 (62%)]\tDiscriminator Loss: 0.587223\tGenerator Loss: 1.184316\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tDiscriminator Loss: 0.518956\tGenerator Loss: 1.252017\n",
      "Train Epoch: 18 [39680/60000 (66%)]\tDiscriminator Loss: 0.539215\tGenerator Loss: 1.039455\n",
      "Train Epoch: 18 [40960/60000 (68%)]\tDiscriminator Loss: 0.505406\tGenerator Loss: 1.351043\n",
      "Train Epoch: 18 [42240/60000 (70%)]\tDiscriminator Loss: 0.512837\tGenerator Loss: 1.378441\n",
      "Train Epoch: 18 [43520/60000 (72%)]\tDiscriminator Loss: 0.518650\tGenerator Loss: 1.117780\n",
      "Train Epoch: 18 [44800/60000 (75%)]\tDiscriminator Loss: 0.564856\tGenerator Loss: 1.144356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 18 [46080/60000 (77%)]\tDiscriminator Loss: 0.552339\tGenerator Loss: 1.048023\n",
      "Train Epoch: 18 [47360/60000 (79%)]\tDiscriminator Loss: 0.635600\tGenerator Loss: 0.664048\n",
      "Train Epoch: 18 [48640/60000 (81%)]\tDiscriminator Loss: 0.525514\tGenerator Loss: 1.171909\n",
      "Train Epoch: 18 [49920/60000 (83%)]\tDiscriminator Loss: 0.518260\tGenerator Loss: 1.100082\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tDiscriminator Loss: 0.537567\tGenerator Loss: 0.993908\n",
      "Train Epoch: 18 [52480/60000 (87%)]\tDiscriminator Loss: 0.527542\tGenerator Loss: 1.065551\n",
      "Train Epoch: 18 [53760/60000 (90%)]\tDiscriminator Loss: 0.493470\tGenerator Loss: 1.232469\n",
      "Train Epoch: 18 [55040/60000 (92%)]\tDiscriminator Loss: 0.545575\tGenerator Loss: 0.888856\n",
      "Train Epoch: 18 [56320/60000 (94%)]\tDiscriminator Loss: 0.635518\tGenerator Loss: 0.791499\n",
      "Train Epoch: 18 [57600/60000 (96%)]\tDiscriminator Loss: 0.589488\tGenerator Loss: 0.957983\n",
      "Train Epoch: 18 [58880/60000 (98%)]\tDiscriminator Loss: 0.496091\tGenerator Loss: 2.235043\n",
      "Train Epoch: 19 [0/60000 (0%)]\tDiscriminator Loss: 0.468480\tGenerator Loss: 1.139494\n",
      "Train Epoch: 19 [1280/60000 (2%)]\tDiscriminator Loss: 0.482325\tGenerator Loss: 1.275851\n",
      "Train Epoch: 19 [2560/60000 (4%)]\tDiscriminator Loss: 0.537454\tGenerator Loss: 1.243881\n",
      "Train Epoch: 19 [3840/60000 (6%)]\tDiscriminator Loss: 0.562068\tGenerator Loss: 1.313386\n",
      "Train Epoch: 19 [5120/60000 (9%)]\tDiscriminator Loss: 0.538961\tGenerator Loss: 1.689887\n",
      "Train Epoch: 19 [6400/60000 (11%)]\tDiscriminator Loss: 0.613225\tGenerator Loss: 1.403390\n",
      "Train Epoch: 19 [7680/60000 (13%)]\tDiscriminator Loss: 0.640144\tGenerator Loss: 0.915638\n",
      "Train Epoch: 19 [8960/60000 (15%)]\tDiscriminator Loss: 0.489138\tGenerator Loss: 1.132521\n",
      "Train Epoch: 19 [10240/60000 (17%)]\tDiscriminator Loss: 0.549523\tGenerator Loss: 1.258482\n",
      "Train Epoch: 19 [11520/60000 (19%)]\tDiscriminator Loss: 0.476310\tGenerator Loss: 1.429003\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tDiscriminator Loss: 0.533346\tGenerator Loss: 1.375266\n",
      "Train Epoch: 19 [14080/60000 (23%)]\tDiscriminator Loss: 0.586180\tGenerator Loss: 1.102349\n",
      "Train Epoch: 19 [15360/60000 (26%)]\tDiscriminator Loss: 0.536202\tGenerator Loss: 1.277517\n",
      "Train Epoch: 19 [16640/60000 (28%)]\tDiscriminator Loss: 0.580327\tGenerator Loss: 0.842618\n",
      "Train Epoch: 19 [17920/60000 (30%)]\tDiscriminator Loss: 0.499987\tGenerator Loss: 1.641424\n",
      "Train Epoch: 19 [19200/60000 (32%)]\tDiscriminator Loss: 0.530764\tGenerator Loss: 1.031105\n",
      "Train Epoch: 19 [20480/60000 (34%)]\tDiscriminator Loss: 0.613826\tGenerator Loss: 0.820143\n",
      "Train Epoch: 19 [21760/60000 (36%)]\tDiscriminator Loss: 0.499941\tGenerator Loss: 1.293679\n",
      "Train Epoch: 19 [23040/60000 (38%)]\tDiscriminator Loss: 0.648585\tGenerator Loss: 1.397825\n",
      "Train Epoch: 19 [24320/60000 (41%)]\tDiscriminator Loss: 0.550729\tGenerator Loss: 0.964727\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tDiscriminator Loss: 0.558869\tGenerator Loss: 0.967178\n",
      "Train Epoch: 19 [26880/60000 (45%)]\tDiscriminator Loss: 0.533481\tGenerator Loss: 1.198240\n",
      "Train Epoch: 19 [28160/60000 (47%)]\tDiscriminator Loss: 0.553877\tGenerator Loss: 1.429946\n",
      "Train Epoch: 19 [29440/60000 (49%)]\tDiscriminator Loss: 0.541551\tGenerator Loss: 1.479391\n",
      "Train Epoch: 19 [30720/60000 (51%)]\tDiscriminator Loss: 0.518787\tGenerator Loss: 1.126352\n",
      "Train Epoch: 19 [32000/60000 (53%)]\tDiscriminator Loss: 0.454447\tGenerator Loss: 1.394401\n",
      "Train Epoch: 19 [33280/60000 (55%)]\tDiscriminator Loss: 0.504497\tGenerator Loss: 1.422002\n",
      "Train Epoch: 19 [34560/60000 (58%)]\tDiscriminator Loss: 0.618675\tGenerator Loss: 1.438898\n",
      "Train Epoch: 19 [35840/60000 (60%)]\tDiscriminator Loss: 0.560047\tGenerator Loss: 1.007478\n",
      "Train Epoch: 19 [37120/60000 (62%)]\tDiscriminator Loss: 0.613553\tGenerator Loss: 1.522878\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tDiscriminator Loss: 0.512654\tGenerator Loss: 1.095239\n",
      "Train Epoch: 19 [39680/60000 (66%)]\tDiscriminator Loss: 0.501203\tGenerator Loss: 1.353192\n",
      "Train Epoch: 19 [40960/60000 (68%)]\tDiscriminator Loss: 0.537931\tGenerator Loss: 1.690108\n",
      "Train Epoch: 19 [42240/60000 (70%)]\tDiscriminator Loss: 0.516637\tGenerator Loss: 1.130641\n",
      "Train Epoch: 19 [43520/60000 (72%)]\tDiscriminator Loss: 0.543264\tGenerator Loss: 1.518234\n",
      "Train Epoch: 19 [44800/60000 (75%)]\tDiscriminator Loss: 0.533039\tGenerator Loss: 0.992919\n",
      "Train Epoch: 19 [46080/60000 (77%)]\tDiscriminator Loss: 0.509561\tGenerator Loss: 1.365107\n",
      "Train Epoch: 19 [47360/60000 (79%)]\tDiscriminator Loss: 0.507054\tGenerator Loss: 1.183155\n",
      "Train Epoch: 19 [48640/60000 (81%)]\tDiscriminator Loss: 0.525668\tGenerator Loss: 1.256390\n",
      "Train Epoch: 19 [49920/60000 (83%)]\tDiscriminator Loss: 0.456805\tGenerator Loss: 1.478048\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tDiscriminator Loss: 0.570753\tGenerator Loss: 1.714350\n",
      "Train Epoch: 19 [52480/60000 (87%)]\tDiscriminator Loss: 0.538095\tGenerator Loss: 1.197129\n",
      "Train Epoch: 19 [53760/60000 (90%)]\tDiscriminator Loss: 0.531383\tGenerator Loss: 1.383758\n",
      "Train Epoch: 19 [55040/60000 (92%)]\tDiscriminator Loss: 0.620012\tGenerator Loss: 1.475222\n",
      "Train Epoch: 19 [56320/60000 (94%)]\tDiscriminator Loss: 0.550303\tGenerator Loss: 0.963159\n",
      "Train Epoch: 19 [57600/60000 (96%)]\tDiscriminator Loss: 0.470751\tGenerator Loss: 1.266498\n",
      "Train Epoch: 19 [58880/60000 (98%)]\tDiscriminator Loss: 0.505403\tGenerator Loss: 1.230853\n",
      "Train Epoch: 20 [0/60000 (0%)]\tDiscriminator Loss: 0.527630\tGenerator Loss: 1.289718\n",
      "Train Epoch: 20 [1280/60000 (2%)]\tDiscriminator Loss: 0.556702\tGenerator Loss: 1.199090\n",
      "Train Epoch: 20 [2560/60000 (4%)]\tDiscriminator Loss: 0.527786\tGenerator Loss: 1.055522\n",
      "Train Epoch: 20 [3840/60000 (6%)]\tDiscriminator Loss: 0.484675\tGenerator Loss: 1.410772\n",
      "Train Epoch: 20 [5120/60000 (9%)]\tDiscriminator Loss: 0.542329\tGenerator Loss: 0.959596\n",
      "Train Epoch: 20 [6400/60000 (11%)]\tDiscriminator Loss: 0.527605\tGenerator Loss: 1.139788\n",
      "Train Epoch: 20 [7680/60000 (13%)]\tDiscriminator Loss: 0.571980\tGenerator Loss: 0.975356\n",
      "Train Epoch: 20 [8960/60000 (15%)]\tDiscriminator Loss: 0.567945\tGenerator Loss: 0.928125\n",
      "Train Epoch: 20 [10240/60000 (17%)]\tDiscriminator Loss: 0.575429\tGenerator Loss: 0.815751\n",
      "Train Epoch: 20 [11520/60000 (19%)]\tDiscriminator Loss: 0.570190\tGenerator Loss: 1.474321\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tDiscriminator Loss: 0.546068\tGenerator Loss: 0.900399\n",
      "Train Epoch: 20 [14080/60000 (23%)]\tDiscriminator Loss: 0.557679\tGenerator Loss: 1.134693\n",
      "Train Epoch: 20 [15360/60000 (26%)]\tDiscriminator Loss: 0.570674\tGenerator Loss: 1.160937\n",
      "Train Epoch: 20 [16640/60000 (28%)]\tDiscriminator Loss: 0.547935\tGenerator Loss: 0.873424\n",
      "Train Epoch: 20 [17920/60000 (30%)]\tDiscriminator Loss: 0.497346\tGenerator Loss: 1.252542\n",
      "Train Epoch: 20 [19200/60000 (32%)]\tDiscriminator Loss: 0.506162\tGenerator Loss: 1.406670\n",
      "Train Epoch: 20 [20480/60000 (34%)]\tDiscriminator Loss: 0.509029\tGenerator Loss: 1.148473\n",
      "Train Epoch: 20 [21760/60000 (36%)]\tDiscriminator Loss: 0.543045\tGenerator Loss: 1.217478\n",
      "Train Epoch: 20 [23040/60000 (38%)]\tDiscriminator Loss: 0.543196\tGenerator Loss: 0.986194\n",
      "Train Epoch: 20 [24320/60000 (41%)]\tDiscriminator Loss: 0.568667\tGenerator Loss: 0.841986\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tDiscriminator Loss: 0.440767\tGenerator Loss: 1.269078\n",
      "Train Epoch: 20 [26880/60000 (45%)]\tDiscriminator Loss: 0.492350\tGenerator Loss: 1.130451\n",
      "Train Epoch: 20 [28160/60000 (47%)]\tDiscriminator Loss: 0.532238\tGenerator Loss: 1.505708\n",
      "Train Epoch: 20 [29440/60000 (49%)]\tDiscriminator Loss: 0.508865\tGenerator Loss: 1.328677\n",
      "Train Epoch: 20 [30720/60000 (51%)]\tDiscriminator Loss: 0.553492\tGenerator Loss: 1.374716\n",
      "Train Epoch: 20 [32000/60000 (53%)]\tDiscriminator Loss: 0.548194\tGenerator Loss: 0.997222\n",
      "Train Epoch: 20 [33280/60000 (55%)]\tDiscriminator Loss: 0.508622\tGenerator Loss: 1.175966\n",
      "Train Epoch: 20 [34560/60000 (58%)]\tDiscriminator Loss: 0.532596\tGenerator Loss: 1.213359\n",
      "Train Epoch: 20 [35840/60000 (60%)]\tDiscriminator Loss: 0.520968\tGenerator Loss: 1.106082\n",
      "Train Epoch: 20 [37120/60000 (62%)]\tDiscriminator Loss: 0.527044\tGenerator Loss: 1.031178\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tDiscriminator Loss: 0.626955\tGenerator Loss: 0.648061\n",
      "Train Epoch: 20 [39680/60000 (66%)]\tDiscriminator Loss: 0.583327\tGenerator Loss: 0.997596\n",
      "Train Epoch: 20 [40960/60000 (68%)]\tDiscriminator Loss: 0.545540\tGenerator Loss: 1.460911\n",
      "Train Epoch: 20 [42240/60000 (70%)]\tDiscriminator Loss: 0.575344\tGenerator Loss: 1.310668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 20 [43520/60000 (72%)]\tDiscriminator Loss: 0.527756\tGenerator Loss: 1.292833\n",
      "Train Epoch: 20 [44800/60000 (75%)]\tDiscriminator Loss: 0.488995\tGenerator Loss: 1.314187\n",
      "Train Epoch: 20 [46080/60000 (77%)]\tDiscriminator Loss: 0.530015\tGenerator Loss: 1.203072\n",
      "Train Epoch: 20 [47360/60000 (79%)]\tDiscriminator Loss: 0.545938\tGenerator Loss: 1.037363\n",
      "Train Epoch: 20 [48640/60000 (81%)]\tDiscriminator Loss: 0.628938\tGenerator Loss: 1.556301\n",
      "Train Epoch: 20 [49920/60000 (83%)]\tDiscriminator Loss: 0.685485\tGenerator Loss: 0.558397\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tDiscriminator Loss: 0.518784\tGenerator Loss: 1.095119\n",
      "Train Epoch: 20 [52480/60000 (87%)]\tDiscriminator Loss: 0.556988\tGenerator Loss: 1.030125\n",
      "Train Epoch: 20 [53760/60000 (90%)]\tDiscriminator Loss: 0.570304\tGenerator Loss: 1.168323\n",
      "Train Epoch: 20 [55040/60000 (92%)]\tDiscriminator Loss: 0.485155\tGenerator Loss: 1.387141\n",
      "Train Epoch: 20 [56320/60000 (94%)]\tDiscriminator Loss: 0.548808\tGenerator Loss: 1.419024\n",
      "Train Epoch: 20 [57600/60000 (96%)]\tDiscriminator Loss: 0.532708\tGenerator Loss: 1.100058\n",
      "Train Epoch: 20 [58880/60000 (98%)]\tDiscriminator Loss: 0.589326\tGenerator Loss: 1.384856\n",
      "Train Epoch: 21 [0/60000 (0%)]\tDiscriminator Loss: 0.563693\tGenerator Loss: 0.921710\n",
      "Train Epoch: 21 [1280/60000 (2%)]\tDiscriminator Loss: 0.572074\tGenerator Loss: 1.341000\n",
      "Train Epoch: 21 [2560/60000 (4%)]\tDiscriminator Loss: 0.589127\tGenerator Loss: 0.849388\n",
      "Train Epoch: 21 [3840/60000 (6%)]\tDiscriminator Loss: 0.568891\tGenerator Loss: 1.006257\n",
      "Train Epoch: 21 [5120/60000 (9%)]\tDiscriminator Loss: 0.573222\tGenerator Loss: 1.027829\n",
      "Train Epoch: 21 [6400/60000 (11%)]\tDiscriminator Loss: 0.467479\tGenerator Loss: 1.207313\n",
      "Train Epoch: 21 [7680/60000 (13%)]\tDiscriminator Loss: 0.507124\tGenerator Loss: 1.566722\n",
      "Train Epoch: 21 [8960/60000 (15%)]\tDiscriminator Loss: 0.575957\tGenerator Loss: 1.433703\n",
      "Train Epoch: 21 [10240/60000 (17%)]\tDiscriminator Loss: 0.550893\tGenerator Loss: 1.164100\n",
      "Train Epoch: 21 [11520/60000 (19%)]\tDiscriminator Loss: 0.575710\tGenerator Loss: 1.390140\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tDiscriminator Loss: 0.557745\tGenerator Loss: 1.128235\n",
      "Train Epoch: 21 [14080/60000 (23%)]\tDiscriminator Loss: 0.515336\tGenerator Loss: 1.320489\n",
      "Train Epoch: 21 [15360/60000 (26%)]\tDiscriminator Loss: 0.460353\tGenerator Loss: 1.749806\n",
      "Train Epoch: 21 [16640/60000 (28%)]\tDiscriminator Loss: 0.501382\tGenerator Loss: 1.016844\n",
      "Train Epoch: 21 [17920/60000 (30%)]\tDiscriminator Loss: 0.544471\tGenerator Loss: 1.041394\n",
      "Train Epoch: 21 [19200/60000 (32%)]\tDiscriminator Loss: 0.590289\tGenerator Loss: 1.024567\n",
      "Train Epoch: 21 [20480/60000 (34%)]\tDiscriminator Loss: 0.604532\tGenerator Loss: 0.854136\n",
      "Train Epoch: 21 [21760/60000 (36%)]\tDiscriminator Loss: 0.522095\tGenerator Loss: 1.316345\n",
      "Train Epoch: 21 [23040/60000 (38%)]\tDiscriminator Loss: 0.537822\tGenerator Loss: 1.348621\n",
      "Train Epoch: 21 [24320/60000 (41%)]\tDiscriminator Loss: 0.522228\tGenerator Loss: 1.295269\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tDiscriminator Loss: 0.530171\tGenerator Loss: 1.111910\n",
      "Train Epoch: 21 [26880/60000 (45%)]\tDiscriminator Loss: 0.511134\tGenerator Loss: 1.263749\n",
      "Train Epoch: 21 [28160/60000 (47%)]\tDiscriminator Loss: 0.574189\tGenerator Loss: 0.770234\n",
      "Train Epoch: 21 [29440/60000 (49%)]\tDiscriminator Loss: 0.581744\tGenerator Loss: 0.940727\n",
      "Train Epoch: 21 [30720/60000 (51%)]\tDiscriminator Loss: 0.471199\tGenerator Loss: 1.284759\n",
      "Train Epoch: 21 [32000/60000 (53%)]\tDiscriminator Loss: 0.524674\tGenerator Loss: 1.570064\n",
      "Train Epoch: 21 [33280/60000 (55%)]\tDiscriminator Loss: 0.496337\tGenerator Loss: 1.103434\n",
      "Train Epoch: 21 [34560/60000 (58%)]\tDiscriminator Loss: 0.475059\tGenerator Loss: 1.220084\n",
      "Train Epoch: 21 [35840/60000 (60%)]\tDiscriminator Loss: 0.491386\tGenerator Loss: 1.153739\n",
      "Train Epoch: 21 [37120/60000 (62%)]\tDiscriminator Loss: 0.453813\tGenerator Loss: 1.295387\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tDiscriminator Loss: 0.576222\tGenerator Loss: 0.873579\n",
      "Train Epoch: 21 [39680/60000 (66%)]\tDiscriminator Loss: 0.525040\tGenerator Loss: 1.080638\n",
      "Train Epoch: 21 [40960/60000 (68%)]\tDiscriminator Loss: 0.576781\tGenerator Loss: 1.463577\n",
      "Train Epoch: 21 [42240/60000 (70%)]\tDiscriminator Loss: 0.530754\tGenerator Loss: 1.080275\n",
      "Train Epoch: 21 [43520/60000 (72%)]\tDiscriminator Loss: 0.548472\tGenerator Loss: 0.964598\n",
      "Train Epoch: 21 [44800/60000 (75%)]\tDiscriminator Loss: 0.546952\tGenerator Loss: 1.187631\n",
      "Train Epoch: 21 [46080/60000 (77%)]\tDiscriminator Loss: 0.588287\tGenerator Loss: 1.101366\n",
      "Train Epoch: 21 [47360/60000 (79%)]\tDiscriminator Loss: 0.599752\tGenerator Loss: 0.836169\n",
      "Train Epoch: 21 [48640/60000 (81%)]\tDiscriminator Loss: 0.519963\tGenerator Loss: 1.618994\n",
      "Train Epoch: 21 [49920/60000 (83%)]\tDiscriminator Loss: 0.500874\tGenerator Loss: 1.329207\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tDiscriminator Loss: 0.572616\tGenerator Loss: 1.406522\n",
      "Train Epoch: 21 [52480/60000 (87%)]\tDiscriminator Loss: 0.528445\tGenerator Loss: 1.388243\n",
      "Train Epoch: 21 [53760/60000 (90%)]\tDiscriminator Loss: 0.639884\tGenerator Loss: 1.783150\n",
      "Train Epoch: 21 [55040/60000 (92%)]\tDiscriminator Loss: 0.533858\tGenerator Loss: 1.147235\n",
      "Train Epoch: 21 [56320/60000 (94%)]\tDiscriminator Loss: 0.607216\tGenerator Loss: 1.132963\n",
      "Train Epoch: 21 [57600/60000 (96%)]\tDiscriminator Loss: 0.551340\tGenerator Loss: 1.253971\n",
      "Train Epoch: 21 [58880/60000 (98%)]\tDiscriminator Loss: 0.508922\tGenerator Loss: 1.462193\n",
      "Train Epoch: 22 [0/60000 (0%)]\tDiscriminator Loss: 0.490964\tGenerator Loss: 1.164745\n",
      "Train Epoch: 22 [1280/60000 (2%)]\tDiscriminator Loss: 0.580402\tGenerator Loss: 0.819201\n",
      "Train Epoch: 22 [2560/60000 (4%)]\tDiscriminator Loss: 0.602983\tGenerator Loss: 0.981230\n",
      "Train Epoch: 22 [3840/60000 (6%)]\tDiscriminator Loss: 0.587365\tGenerator Loss: 1.052859\n",
      "Train Epoch: 22 [5120/60000 (9%)]\tDiscriminator Loss: 0.591346\tGenerator Loss: 1.437280\n",
      "Train Epoch: 22 [6400/60000 (11%)]\tDiscriminator Loss: 0.557335\tGenerator Loss: 0.980554\n",
      "Train Epoch: 22 [7680/60000 (13%)]\tDiscriminator Loss: 0.530212\tGenerator Loss: 1.325523\n",
      "Train Epoch: 22 [8960/60000 (15%)]\tDiscriminator Loss: 0.526419\tGenerator Loss: 1.289638\n",
      "Train Epoch: 22 [10240/60000 (17%)]\tDiscriminator Loss: 0.521561\tGenerator Loss: 1.226238\n",
      "Train Epoch: 22 [11520/60000 (19%)]\tDiscriminator Loss: 0.546576\tGenerator Loss: 1.014092\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tDiscriminator Loss: 0.487615\tGenerator Loss: 1.283106\n",
      "Train Epoch: 22 [14080/60000 (23%)]\tDiscriminator Loss: 0.503085\tGenerator Loss: 1.498568\n",
      "Train Epoch: 22 [15360/60000 (26%)]\tDiscriminator Loss: 0.529027\tGenerator Loss: 1.301724\n",
      "Train Epoch: 22 [16640/60000 (28%)]\tDiscriminator Loss: 0.524942\tGenerator Loss: 1.351688\n",
      "Train Epoch: 22 [17920/60000 (30%)]\tDiscriminator Loss: 0.565831\tGenerator Loss: 1.192837\n",
      "Train Epoch: 22 [19200/60000 (32%)]\tDiscriminator Loss: 0.563460\tGenerator Loss: 1.350665\n",
      "Train Epoch: 22 [20480/60000 (34%)]\tDiscriminator Loss: 0.549389\tGenerator Loss: 1.313135\n",
      "Train Epoch: 22 [21760/60000 (36%)]\tDiscriminator Loss: 0.461527\tGenerator Loss: 1.378394\n",
      "Train Epoch: 22 [23040/60000 (38%)]\tDiscriminator Loss: 0.492979\tGenerator Loss: 1.256154\n",
      "Train Epoch: 22 [24320/60000 (41%)]\tDiscriminator Loss: 0.569851\tGenerator Loss: 1.055197\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tDiscriminator Loss: 0.628601\tGenerator Loss: 1.035039\n",
      "Train Epoch: 22 [26880/60000 (45%)]\tDiscriminator Loss: 0.564257\tGenerator Loss: 1.035452\n",
      "Train Epoch: 22 [28160/60000 (47%)]\tDiscriminator Loss: 0.540603\tGenerator Loss: 1.032658\n",
      "Train Epoch: 22 [29440/60000 (49%)]\tDiscriminator Loss: 0.494581\tGenerator Loss: 1.247517\n",
      "Train Epoch: 22 [30720/60000 (51%)]\tDiscriminator Loss: 0.576640\tGenerator Loss: 0.821453\n",
      "Train Epoch: 22 [32000/60000 (53%)]\tDiscriminator Loss: 0.532669\tGenerator Loss: 1.087416\n",
      "Train Epoch: 22 [33280/60000 (55%)]\tDiscriminator Loss: 0.503853\tGenerator Loss: 1.215500\n",
      "Train Epoch: 22 [34560/60000 (58%)]\tDiscriminator Loss: 0.567760\tGenerator Loss: 0.961178\n",
      "Train Epoch: 22 [35840/60000 (60%)]\tDiscriminator Loss: 0.519023\tGenerator Loss: 1.311721\n",
      "Train Epoch: 22 [37120/60000 (62%)]\tDiscriminator Loss: 0.532194\tGenerator Loss: 0.937012\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tDiscriminator Loss: 0.569721\tGenerator Loss: 1.118360\n",
      "Train Epoch: 22 [39680/60000 (66%)]\tDiscriminator Loss: 0.545569\tGenerator Loss: 1.285392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 22 [40960/60000 (68%)]\tDiscriminator Loss: 0.508362\tGenerator Loss: 1.114753\n",
      "Train Epoch: 22 [42240/60000 (70%)]\tDiscriminator Loss: 0.531755\tGenerator Loss: 1.478179\n",
      "Train Epoch: 22 [43520/60000 (72%)]\tDiscriminator Loss: 0.483356\tGenerator Loss: 1.530605\n",
      "Train Epoch: 22 [44800/60000 (75%)]\tDiscriminator Loss: 0.516484\tGenerator Loss: 1.350527\n",
      "Train Epoch: 22 [46080/60000 (77%)]\tDiscriminator Loss: 0.547493\tGenerator Loss: 1.353058\n",
      "Train Epoch: 22 [47360/60000 (79%)]\tDiscriminator Loss: 0.612691\tGenerator Loss: 1.636241\n",
      "Train Epoch: 22 [48640/60000 (81%)]\tDiscriminator Loss: 0.629496\tGenerator Loss: 1.093213\n",
      "Train Epoch: 22 [49920/60000 (83%)]\tDiscriminator Loss: 0.649426\tGenerator Loss: 0.880557\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tDiscriminator Loss: 0.516156\tGenerator Loss: 0.950018\n",
      "Train Epoch: 22 [52480/60000 (87%)]\tDiscriminator Loss: 0.545559\tGenerator Loss: 1.099267\n",
      "Train Epoch: 22 [53760/60000 (90%)]\tDiscriminator Loss: 0.506814\tGenerator Loss: 1.342577\n",
      "Train Epoch: 22 [55040/60000 (92%)]\tDiscriminator Loss: 0.547674\tGenerator Loss: 1.274809\n",
      "Train Epoch: 22 [56320/60000 (94%)]\tDiscriminator Loss: 0.548858\tGenerator Loss: 1.218356\n",
      "Train Epoch: 22 [57600/60000 (96%)]\tDiscriminator Loss: 0.563145\tGenerator Loss: 0.734556\n",
      "Train Epoch: 22 [58880/60000 (98%)]\tDiscriminator Loss: 0.591210\tGenerator Loss: 1.025313\n",
      "Train Epoch: 23 [0/60000 (0%)]\tDiscriminator Loss: 0.555587\tGenerator Loss: 1.145449\n",
      "Train Epoch: 23 [1280/60000 (2%)]\tDiscriminator Loss: 0.521389\tGenerator Loss: 1.289469\n",
      "Train Epoch: 23 [2560/60000 (4%)]\tDiscriminator Loss: 0.525595\tGenerator Loss: 1.238534\n",
      "Train Epoch: 23 [3840/60000 (6%)]\tDiscriminator Loss: 0.518356\tGenerator Loss: 1.679738\n",
      "Train Epoch: 23 [5120/60000 (9%)]\tDiscriminator Loss: 0.500855\tGenerator Loss: 1.380080\n",
      "Train Epoch: 23 [6400/60000 (11%)]\tDiscriminator Loss: 0.494438\tGenerator Loss: 1.154323\n",
      "Train Epoch: 23 [7680/60000 (13%)]\tDiscriminator Loss: 0.570183\tGenerator Loss: 1.000124\n",
      "Train Epoch: 23 [8960/60000 (15%)]\tDiscriminator Loss: 0.607187\tGenerator Loss: 0.855550\n",
      "Train Epoch: 23 [10240/60000 (17%)]\tDiscriminator Loss: 0.567035\tGenerator Loss: 1.060252\n",
      "Train Epoch: 23 [11520/60000 (19%)]\tDiscriminator Loss: 0.575368\tGenerator Loss: 1.349612\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tDiscriminator Loss: 0.580208\tGenerator Loss: 0.879954\n",
      "Train Epoch: 23 [14080/60000 (23%)]\tDiscriminator Loss: 0.501972\tGenerator Loss: 1.244013\n",
      "Train Epoch: 23 [15360/60000 (26%)]\tDiscriminator Loss: 0.544413\tGenerator Loss: 1.217243\n",
      "Train Epoch: 23 [16640/60000 (28%)]\tDiscriminator Loss: 0.539573\tGenerator Loss: 1.461401\n",
      "Train Epoch: 23 [17920/60000 (30%)]\tDiscriminator Loss: 0.640729\tGenerator Loss: 0.647802\n",
      "Train Epoch: 23 [19200/60000 (32%)]\tDiscriminator Loss: 0.504600\tGenerator Loss: 0.972444\n",
      "Train Epoch: 23 [20480/60000 (34%)]\tDiscriminator Loss: 0.466389\tGenerator Loss: 1.241969\n",
      "Train Epoch: 23 [21760/60000 (36%)]\tDiscriminator Loss: 0.514282\tGenerator Loss: 1.115220\n",
      "Train Epoch: 23 [23040/60000 (38%)]\tDiscriminator Loss: 0.534222\tGenerator Loss: 1.275016\n",
      "Train Epoch: 23 [24320/60000 (41%)]\tDiscriminator Loss: 0.467955\tGenerator Loss: 1.452786\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tDiscriminator Loss: 0.605824\tGenerator Loss: 1.054113\n",
      "Train Epoch: 23 [26880/60000 (45%)]\tDiscriminator Loss: 0.543205\tGenerator Loss: 0.999039\n",
      "Train Epoch: 23 [28160/60000 (47%)]\tDiscriminator Loss: 0.503298\tGenerator Loss: 1.226562\n",
      "Train Epoch: 23 [29440/60000 (49%)]\tDiscriminator Loss: 0.540387\tGenerator Loss: 1.112986\n",
      "Train Epoch: 23 [30720/60000 (51%)]\tDiscriminator Loss: 0.518908\tGenerator Loss: 1.509427\n",
      "Train Epoch: 23 [32000/60000 (53%)]\tDiscriminator Loss: 0.539399\tGenerator Loss: 1.166236\n",
      "Train Epoch: 23 [33280/60000 (55%)]\tDiscriminator Loss: 0.588351\tGenerator Loss: 1.106911\n",
      "Train Epoch: 23 [34560/60000 (58%)]\tDiscriminator Loss: 0.567440\tGenerator Loss: 1.081056\n",
      "Train Epoch: 23 [35840/60000 (60%)]\tDiscriminator Loss: 0.520114\tGenerator Loss: 1.377215\n",
      "Train Epoch: 23 [37120/60000 (62%)]\tDiscriminator Loss: 0.521430\tGenerator Loss: 1.072794\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tDiscriminator Loss: 0.556590\tGenerator Loss: 0.979862\n",
      "Train Epoch: 23 [39680/60000 (66%)]\tDiscriminator Loss: 0.669987\tGenerator Loss: 0.662008\n",
      "Train Epoch: 23 [40960/60000 (68%)]\tDiscriminator Loss: 0.520426\tGenerator Loss: 1.275170\n",
      "Train Epoch: 23 [42240/60000 (70%)]\tDiscriminator Loss: 0.506612\tGenerator Loss: 1.284389\n",
      "Train Epoch: 23 [43520/60000 (72%)]\tDiscriminator Loss: 0.498079\tGenerator Loss: 1.273709\n",
      "Train Epoch: 23 [44800/60000 (75%)]\tDiscriminator Loss: 0.440060\tGenerator Loss: 1.247620\n",
      "Train Epoch: 23 [46080/60000 (77%)]\tDiscriminator Loss: 0.495114\tGenerator Loss: 1.210716\n",
      "Train Epoch: 23 [47360/60000 (79%)]\tDiscriminator Loss: 0.660823\tGenerator Loss: 0.795328\n",
      "Train Epoch: 23 [48640/60000 (81%)]\tDiscriminator Loss: 0.635786\tGenerator Loss: 1.255390\n",
      "Train Epoch: 23 [49920/60000 (83%)]\tDiscriminator Loss: 0.604475\tGenerator Loss: 1.271401\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tDiscriminator Loss: 0.520688\tGenerator Loss: 1.091567\n",
      "Train Epoch: 23 [52480/60000 (87%)]\tDiscriminator Loss: 0.514330\tGenerator Loss: 1.174850\n",
      "Train Epoch: 23 [53760/60000 (90%)]\tDiscriminator Loss: 0.532580\tGenerator Loss: 1.023748\n",
      "Train Epoch: 23 [55040/60000 (92%)]\tDiscriminator Loss: 0.578899\tGenerator Loss: 1.158733\n",
      "Train Epoch: 23 [56320/60000 (94%)]\tDiscriminator Loss: 0.575956\tGenerator Loss: 1.216013\n",
      "Train Epoch: 23 [57600/60000 (96%)]\tDiscriminator Loss: 0.525981\tGenerator Loss: 1.210659\n",
      "Train Epoch: 23 [58880/60000 (98%)]\tDiscriminator Loss: 0.467394\tGenerator Loss: 1.238762\n",
      "Train Epoch: 24 [0/60000 (0%)]\tDiscriminator Loss: 0.446737\tGenerator Loss: 1.218792\n",
      "Train Epoch: 24 [1280/60000 (2%)]\tDiscriminator Loss: 0.810406\tGenerator Loss: 0.514348\n",
      "Train Epoch: 24 [2560/60000 (4%)]\tDiscriminator Loss: 0.561475\tGenerator Loss: 1.295529\n",
      "Train Epoch: 24 [3840/60000 (6%)]\tDiscriminator Loss: 0.605479\tGenerator Loss: 0.735916\n",
      "Train Epoch: 24 [5120/60000 (9%)]\tDiscriminator Loss: 0.584390\tGenerator Loss: 1.043643\n",
      "Train Epoch: 24 [6400/60000 (11%)]\tDiscriminator Loss: 0.544575\tGenerator Loss: 1.267406\n",
      "Train Epoch: 24 [7680/60000 (13%)]\tDiscriminator Loss: 0.498982\tGenerator Loss: 1.285747\n",
      "Train Epoch: 24 [8960/60000 (15%)]\tDiscriminator Loss: 0.485990\tGenerator Loss: 1.128191\n",
      "Train Epoch: 24 [10240/60000 (17%)]\tDiscriminator Loss: 0.579439\tGenerator Loss: 0.918984\n",
      "Train Epoch: 24 [11520/60000 (19%)]\tDiscriminator Loss: 0.591441\tGenerator Loss: 0.996802\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tDiscriminator Loss: 0.528179\tGenerator Loss: 1.200952\n",
      "Train Epoch: 24 [14080/60000 (23%)]\tDiscriminator Loss: 0.544497\tGenerator Loss: 1.143618\n",
      "Train Epoch: 24 [15360/60000 (26%)]\tDiscriminator Loss: 0.524526\tGenerator Loss: 1.098934\n",
      "Train Epoch: 24 [16640/60000 (28%)]\tDiscriminator Loss: 0.539373\tGenerator Loss: 1.196385\n",
      "Train Epoch: 24 [17920/60000 (30%)]\tDiscriminator Loss: 0.561283\tGenerator Loss: 1.010527\n",
      "Train Epoch: 24 [19200/60000 (32%)]\tDiscriminator Loss: 0.538257\tGenerator Loss: 1.331327\n",
      "Train Epoch: 24 [20480/60000 (34%)]\tDiscriminator Loss: 0.533553\tGenerator Loss: 1.074230\n",
      "Train Epoch: 24 [21760/60000 (36%)]\tDiscriminator Loss: 0.604770\tGenerator Loss: 1.350672\n",
      "Train Epoch: 24 [23040/60000 (38%)]\tDiscriminator Loss: 0.532401\tGenerator Loss: 1.133969\n",
      "Train Epoch: 24 [24320/60000 (41%)]\tDiscriminator Loss: 0.579107\tGenerator Loss: 0.851087\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tDiscriminator Loss: 0.553700\tGenerator Loss: 1.130380\n",
      "Train Epoch: 24 [26880/60000 (45%)]\tDiscriminator Loss: 0.483983\tGenerator Loss: 1.324175\n",
      "Train Epoch: 24 [28160/60000 (47%)]\tDiscriminator Loss: 0.545157\tGenerator Loss: 1.772358\n",
      "Train Epoch: 24 [29440/60000 (49%)]\tDiscriminator Loss: 0.561333\tGenerator Loss: 0.961910\n",
      "Train Epoch: 24 [30720/60000 (51%)]\tDiscriminator Loss: 0.587378\tGenerator Loss: 1.284311\n",
      "Train Epoch: 24 [32000/60000 (53%)]\tDiscriminator Loss: 0.535424\tGenerator Loss: 1.331615\n",
      "Train Epoch: 24 [33280/60000 (55%)]\tDiscriminator Loss: 0.544080\tGenerator Loss: 1.007387\n",
      "Train Epoch: 24 [34560/60000 (58%)]\tDiscriminator Loss: 0.500834\tGenerator Loss: 1.287585\n",
      "Train Epoch: 24 [35840/60000 (60%)]\tDiscriminator Loss: 0.528895\tGenerator Loss: 1.166320\n",
      "Train Epoch: 24 [37120/60000 (62%)]\tDiscriminator Loss: 0.561085\tGenerator Loss: 1.084985\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tDiscriminator Loss: 0.596872\tGenerator Loss: 0.696911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 24 [39680/60000 (66%)]\tDiscriminator Loss: 0.543099\tGenerator Loss: 1.140139\n",
      "Train Epoch: 24 [40960/60000 (68%)]\tDiscriminator Loss: 0.627457\tGenerator Loss: 1.153838\n",
      "Train Epoch: 24 [42240/60000 (70%)]\tDiscriminator Loss: 0.569844\tGenerator Loss: 1.319558\n",
      "Train Epoch: 24 [43520/60000 (72%)]\tDiscriminator Loss: 0.533570\tGenerator Loss: 1.473654\n",
      "Train Epoch: 24 [44800/60000 (75%)]\tDiscriminator Loss: 0.516152\tGenerator Loss: 1.387116\n",
      "Train Epoch: 24 [46080/60000 (77%)]\tDiscriminator Loss: 0.572888\tGenerator Loss: 0.860438\n",
      "Train Epoch: 24 [47360/60000 (79%)]\tDiscriminator Loss: 0.505201\tGenerator Loss: 0.975592\n",
      "Train Epoch: 24 [48640/60000 (81%)]\tDiscriminator Loss: 0.645825\tGenerator Loss: 0.690302\n",
      "Train Epoch: 24 [49920/60000 (83%)]\tDiscriminator Loss: 0.521396\tGenerator Loss: 1.177091\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tDiscriminator Loss: 0.526183\tGenerator Loss: 1.042472\n",
      "Train Epoch: 24 [52480/60000 (87%)]\tDiscriminator Loss: 0.504212\tGenerator Loss: 1.148354\n",
      "Train Epoch: 24 [53760/60000 (90%)]\tDiscriminator Loss: 0.575439\tGenerator Loss: 0.929582\n",
      "Train Epoch: 24 [55040/60000 (92%)]\tDiscriminator Loss: 0.604857\tGenerator Loss: 1.174562\n",
      "Train Epoch: 24 [56320/60000 (94%)]\tDiscriminator Loss: 0.556366\tGenerator Loss: 1.141873\n",
      "Train Epoch: 24 [57600/60000 (96%)]\tDiscriminator Loss: 0.515069\tGenerator Loss: 1.287073\n",
      "Train Epoch: 24 [58880/60000 (98%)]\tDiscriminator Loss: 0.496497\tGenerator Loss: 1.300714\n",
      "Train Epoch: 25 [0/60000 (0%)]\tDiscriminator Loss: 0.519423\tGenerator Loss: 1.326987\n",
      "Train Epoch: 25 [1280/60000 (2%)]\tDiscriminator Loss: 0.508794\tGenerator Loss: 1.298389\n",
      "Train Epoch: 25 [2560/60000 (4%)]\tDiscriminator Loss: 0.563176\tGenerator Loss: 1.091078\n",
      "Train Epoch: 25 [3840/60000 (6%)]\tDiscriminator Loss: 0.559817\tGenerator Loss: 1.017813\n",
      "Train Epoch: 25 [5120/60000 (9%)]\tDiscriminator Loss: 0.536562\tGenerator Loss: 1.116494\n",
      "Train Epoch: 25 [6400/60000 (11%)]\tDiscriminator Loss: 0.543668\tGenerator Loss: 1.158578\n",
      "Train Epoch: 25 [7680/60000 (13%)]\tDiscriminator Loss: 0.494125\tGenerator Loss: 1.141521\n",
      "Train Epoch: 25 [8960/60000 (15%)]\tDiscriminator Loss: 0.546355\tGenerator Loss: 1.006662\n",
      "Train Epoch: 25 [10240/60000 (17%)]\tDiscriminator Loss: 0.569479\tGenerator Loss: 1.012740\n",
      "Train Epoch: 25 [11520/60000 (19%)]\tDiscriminator Loss: 0.578435\tGenerator Loss: 1.274688\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tDiscriminator Loss: 0.514425\tGenerator Loss: 1.525811\n",
      "Train Epoch: 25 [14080/60000 (23%)]\tDiscriminator Loss: 0.500445\tGenerator Loss: 1.316103\n",
      "Train Epoch: 25 [15360/60000 (26%)]\tDiscriminator Loss: 0.483759\tGenerator Loss: 1.257394\n",
      "Train Epoch: 25 [16640/60000 (28%)]\tDiscriminator Loss: 0.596743\tGenerator Loss: 1.207142\n",
      "Train Epoch: 25 [17920/60000 (30%)]\tDiscriminator Loss: 0.529896\tGenerator Loss: 1.132706\n",
      "Train Epoch: 25 [19200/60000 (32%)]\tDiscriminator Loss: 0.610061\tGenerator Loss: 1.516809\n",
      "Train Epoch: 25 [20480/60000 (34%)]\tDiscriminator Loss: 0.506002\tGenerator Loss: 1.466388\n",
      "Train Epoch: 25 [21760/60000 (36%)]\tDiscriminator Loss: 0.528640\tGenerator Loss: 1.560874\n",
      "Train Epoch: 25 [23040/60000 (38%)]\tDiscriminator Loss: 0.602741\tGenerator Loss: 0.900171\n",
      "Train Epoch: 25 [24320/60000 (41%)]\tDiscriminator Loss: 0.530584\tGenerator Loss: 1.255438\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tDiscriminator Loss: 0.536379\tGenerator Loss: 1.150975\n",
      "Train Epoch: 25 [26880/60000 (45%)]\tDiscriminator Loss: 0.508385\tGenerator Loss: 0.996843\n",
      "Train Epoch: 25 [28160/60000 (47%)]\tDiscriminator Loss: 0.551906\tGenerator Loss: 1.026140\n",
      "Train Epoch: 25 [29440/60000 (49%)]\tDiscriminator Loss: 0.567826\tGenerator Loss: 1.349903\n",
      "Train Epoch: 25 [30720/60000 (51%)]\tDiscriminator Loss: 0.553683\tGenerator Loss: 1.077870\n",
      "Train Epoch: 25 [32000/60000 (53%)]\tDiscriminator Loss: 0.505038\tGenerator Loss: 1.155318\n",
      "Train Epoch: 25 [33280/60000 (55%)]\tDiscriminator Loss: 0.487562\tGenerator Loss: 1.164496\n",
      "Train Epoch: 25 [34560/60000 (58%)]\tDiscriminator Loss: 0.889411\tGenerator Loss: 0.270753\n",
      "Train Epoch: 25 [35840/60000 (60%)]\tDiscriminator Loss: 0.560550\tGenerator Loss: 1.047592\n",
      "Train Epoch: 25 [37120/60000 (62%)]\tDiscriminator Loss: 0.532223\tGenerator Loss: 1.379586\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tDiscriminator Loss: 0.526290\tGenerator Loss: 1.113090\n",
      "Train Epoch: 25 [39680/60000 (66%)]\tDiscriminator Loss: 0.482900\tGenerator Loss: 1.389956\n",
      "Train Epoch: 25 [40960/60000 (68%)]\tDiscriminator Loss: 0.608919\tGenerator Loss: 1.506558\n",
      "Train Epoch: 25 [42240/60000 (70%)]\tDiscriminator Loss: 0.581639\tGenerator Loss: 0.983112\n",
      "Train Epoch: 25 [43520/60000 (72%)]\tDiscriminator Loss: 0.630344\tGenerator Loss: 1.159956\n",
      "Train Epoch: 25 [44800/60000 (75%)]\tDiscriminator Loss: 0.571136\tGenerator Loss: 1.235015\n",
      "Train Epoch: 25 [46080/60000 (77%)]\tDiscriminator Loss: 0.517021\tGenerator Loss: 1.221141\n",
      "Train Epoch: 25 [47360/60000 (79%)]\tDiscriminator Loss: 0.569432\tGenerator Loss: 1.209596\n",
      "Train Epoch: 25 [48640/60000 (81%)]\tDiscriminator Loss: 0.534515\tGenerator Loss: 1.279290\n",
      "Train Epoch: 25 [49920/60000 (83%)]\tDiscriminator Loss: 0.541910\tGenerator Loss: 1.145342\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tDiscriminator Loss: 0.563857\tGenerator Loss: 1.055352\n",
      "Train Epoch: 25 [52480/60000 (87%)]\tDiscriminator Loss: 0.562985\tGenerator Loss: 1.119231\n",
      "Train Epoch: 25 [53760/60000 (90%)]\tDiscriminator Loss: 0.543176\tGenerator Loss: 1.048775\n",
      "Train Epoch: 25 [55040/60000 (92%)]\tDiscriminator Loss: 0.598898\tGenerator Loss: 1.259933\n",
      "Train Epoch: 25 [56320/60000 (94%)]\tDiscriminator Loss: 0.552600\tGenerator Loss: 1.095280\n",
      "Train Epoch: 25 [57600/60000 (96%)]\tDiscriminator Loss: 0.620718\tGenerator Loss: 0.862054\n",
      "Train Epoch: 25 [58880/60000 (98%)]\tDiscriminator Loss: 0.590421\tGenerator Loss: 1.024246\n",
      "Train Epoch: 26 [0/60000 (0%)]\tDiscriminator Loss: 0.537945\tGenerator Loss: 0.988312\n",
      "Train Epoch: 26 [1280/60000 (2%)]\tDiscriminator Loss: 0.497068\tGenerator Loss: 1.360539\n",
      "Train Epoch: 26 [2560/60000 (4%)]\tDiscriminator Loss: 0.481347\tGenerator Loss: 1.320528\n",
      "Train Epoch: 26 [3840/60000 (6%)]\tDiscriminator Loss: 0.506109\tGenerator Loss: 1.495283\n",
      "Train Epoch: 26 [5120/60000 (9%)]\tDiscriminator Loss: 0.606111\tGenerator Loss: 0.774512\n",
      "Train Epoch: 26 [6400/60000 (11%)]\tDiscriminator Loss: 0.552093\tGenerator Loss: 1.305000\n",
      "Train Epoch: 26 [7680/60000 (13%)]\tDiscriminator Loss: 0.559241\tGenerator Loss: 1.401219\n",
      "Train Epoch: 26 [8960/60000 (15%)]\tDiscriminator Loss: 0.485390\tGenerator Loss: 1.448256\n",
      "Train Epoch: 26 [10240/60000 (17%)]\tDiscriminator Loss: 0.450133\tGenerator Loss: 1.435187\n",
      "Train Epoch: 26 [11520/60000 (19%)]\tDiscriminator Loss: 0.518977\tGenerator Loss: 1.460467\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tDiscriminator Loss: 0.524188\tGenerator Loss: 1.596258\n",
      "Train Epoch: 26 [14080/60000 (23%)]\tDiscriminator Loss: 0.550469\tGenerator Loss: 0.787298\n",
      "Train Epoch: 26 [15360/60000 (26%)]\tDiscriminator Loss: 0.537516\tGenerator Loss: 1.241541\n",
      "Train Epoch: 26 [16640/60000 (28%)]\tDiscriminator Loss: 0.470205\tGenerator Loss: 1.304064\n",
      "Train Epoch: 26 [17920/60000 (30%)]\tDiscriminator Loss: 0.455874\tGenerator Loss: 1.318086\n",
      "Train Epoch: 26 [19200/60000 (32%)]\tDiscriminator Loss: 0.522403\tGenerator Loss: 0.843581\n",
      "Train Epoch: 26 [20480/60000 (34%)]\tDiscriminator Loss: 0.581340\tGenerator Loss: 0.902713\n",
      "Train Epoch: 26 [21760/60000 (36%)]\tDiscriminator Loss: 0.620565\tGenerator Loss: 0.832052\n",
      "Train Epoch: 26 [23040/60000 (38%)]\tDiscriminator Loss: 0.469125\tGenerator Loss: 1.375663\n",
      "Train Epoch: 26 [24320/60000 (41%)]\tDiscriminator Loss: 0.487147\tGenerator Loss: 1.500778\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tDiscriminator Loss: 0.517172\tGenerator Loss: 1.374583\n",
      "Train Epoch: 26 [26880/60000 (45%)]\tDiscriminator Loss: 0.513254\tGenerator Loss: 1.006443\n",
      "Train Epoch: 26 [28160/60000 (47%)]\tDiscriminator Loss: 0.540141\tGenerator Loss: 1.020141\n",
      "Train Epoch: 26 [29440/60000 (49%)]\tDiscriminator Loss: 0.589105\tGenerator Loss: 1.041986\n",
      "Train Epoch: 26 [30720/60000 (51%)]\tDiscriminator Loss: 0.537792\tGenerator Loss: 1.152274\n",
      "Train Epoch: 26 [32000/60000 (53%)]\tDiscriminator Loss: 0.487129\tGenerator Loss: 1.157464\n",
      "Train Epoch: 26 [33280/60000 (55%)]\tDiscriminator Loss: 0.513502\tGenerator Loss: 0.908535\n",
      "Train Epoch: 26 [34560/60000 (58%)]\tDiscriminator Loss: 0.537183\tGenerator Loss: 1.185518\n",
      "Train Epoch: 26 [35840/60000 (60%)]\tDiscriminator Loss: 0.518917\tGenerator Loss: 0.941046\n",
      "Train Epoch: 26 [37120/60000 (62%)]\tDiscriminator Loss: 0.560856\tGenerator Loss: 1.352713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 26 [38400/60000 (64%)]\tDiscriminator Loss: 0.552525\tGenerator Loss: 1.250387\n",
      "Train Epoch: 26 [39680/60000 (66%)]\tDiscriminator Loss: 0.482783\tGenerator Loss: 1.340791\n",
      "Train Epoch: 26 [40960/60000 (68%)]\tDiscriminator Loss: 0.561437\tGenerator Loss: 1.802576\n",
      "Train Epoch: 26 [42240/60000 (70%)]\tDiscriminator Loss: 0.540410\tGenerator Loss: 1.255397\n",
      "Train Epoch: 26 [43520/60000 (72%)]\tDiscriminator Loss: 0.522881\tGenerator Loss: 1.221172\n",
      "Train Epoch: 26 [44800/60000 (75%)]\tDiscriminator Loss: 0.569780\tGenerator Loss: 1.563968\n",
      "Train Epoch: 26 [46080/60000 (77%)]\tDiscriminator Loss: 0.590928\tGenerator Loss: 1.616152\n",
      "Train Epoch: 26 [47360/60000 (79%)]\tDiscriminator Loss: 0.516644\tGenerator Loss: 1.271177\n",
      "Train Epoch: 26 [48640/60000 (81%)]\tDiscriminator Loss: 0.524681\tGenerator Loss: 1.106051\n",
      "Train Epoch: 26 [49920/60000 (83%)]\tDiscriminator Loss: 0.570055\tGenerator Loss: 1.143702\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tDiscriminator Loss: 0.556736\tGenerator Loss: 1.005980\n",
      "Train Epoch: 26 [52480/60000 (87%)]\tDiscriminator Loss: 0.574418\tGenerator Loss: 1.143676\n",
      "Train Epoch: 26 [53760/60000 (90%)]\tDiscriminator Loss: 0.534518\tGenerator Loss: 0.995940\n",
      "Train Epoch: 26 [55040/60000 (92%)]\tDiscriminator Loss: 0.554756\tGenerator Loss: 1.161282\n",
      "Train Epoch: 26 [56320/60000 (94%)]\tDiscriminator Loss: 0.499057\tGenerator Loss: 1.161586\n",
      "Train Epoch: 26 [57600/60000 (96%)]\tDiscriminator Loss: 0.613419\tGenerator Loss: 0.994403\n",
      "Train Epoch: 26 [58880/60000 (98%)]\tDiscriminator Loss: 0.565358\tGenerator Loss: 1.189580\n",
      "Train Epoch: 27 [0/60000 (0%)]\tDiscriminator Loss: 0.626125\tGenerator Loss: 1.432223\n",
      "Train Epoch: 27 [1280/60000 (2%)]\tDiscriminator Loss: 0.554094\tGenerator Loss: 0.946738\n",
      "Train Epoch: 27 [2560/60000 (4%)]\tDiscriminator Loss: 0.499482\tGenerator Loss: 1.471891\n",
      "Train Epoch: 27 [3840/60000 (6%)]\tDiscriminator Loss: 0.518132\tGenerator Loss: 1.095079\n",
      "Train Epoch: 27 [5120/60000 (9%)]\tDiscriminator Loss: 0.570203\tGenerator Loss: 1.031223\n",
      "Train Epoch: 27 [6400/60000 (11%)]\tDiscriminator Loss: 0.684613\tGenerator Loss: 0.956288\n",
      "Train Epoch: 27 [7680/60000 (13%)]\tDiscriminator Loss: 0.550910\tGenerator Loss: 1.591023\n",
      "Train Epoch: 27 [8960/60000 (15%)]\tDiscriminator Loss: 0.509229\tGenerator Loss: 1.130456\n",
      "Train Epoch: 27 [10240/60000 (17%)]\tDiscriminator Loss: 0.544613\tGenerator Loss: 0.964446\n",
      "Train Epoch: 27 [11520/60000 (19%)]\tDiscriminator Loss: 0.607614\tGenerator Loss: 1.754927\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tDiscriminator Loss: 0.512112\tGenerator Loss: 1.336033\n",
      "Train Epoch: 27 [14080/60000 (23%)]\tDiscriminator Loss: 0.512607\tGenerator Loss: 1.327447\n",
      "Train Epoch: 27 [15360/60000 (26%)]\tDiscriminator Loss: 0.540377\tGenerator Loss: 1.224144\n",
      "Train Epoch: 27 [16640/60000 (28%)]\tDiscriminator Loss: 0.616838\tGenerator Loss: 0.650327\n",
      "Train Epoch: 27 [17920/60000 (30%)]\tDiscriminator Loss: 0.510292\tGenerator Loss: 1.018265\n",
      "Train Epoch: 27 [19200/60000 (32%)]\tDiscriminator Loss: 0.588665\tGenerator Loss: 1.611481\n",
      "Train Epoch: 27 [20480/60000 (34%)]\tDiscriminator Loss: 0.484602\tGenerator Loss: 1.287922\n",
      "Train Epoch: 27 [21760/60000 (36%)]\tDiscriminator Loss: 0.513778\tGenerator Loss: 1.180096\n",
      "Train Epoch: 27 [23040/60000 (38%)]\tDiscriminator Loss: 0.549651\tGenerator Loss: 1.116544\n",
      "Train Epoch: 27 [24320/60000 (41%)]\tDiscriminator Loss: 0.535203\tGenerator Loss: 1.180547\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tDiscriminator Loss: 0.553284\tGenerator Loss: 1.155534\n",
      "Train Epoch: 27 [26880/60000 (45%)]\tDiscriminator Loss: 0.588566\tGenerator Loss: 0.766855\n",
      "Train Epoch: 27 [28160/60000 (47%)]\tDiscriminator Loss: 0.532470\tGenerator Loss: 1.364935\n",
      "Train Epoch: 27 [29440/60000 (49%)]\tDiscriminator Loss: 0.569126\tGenerator Loss: 1.342350\n",
      "Train Epoch: 27 [30720/60000 (51%)]\tDiscriminator Loss: 0.709816\tGenerator Loss: 2.288750\n",
      "Train Epoch: 27 [32000/60000 (53%)]\tDiscriminator Loss: 0.588189\tGenerator Loss: 1.035802\n",
      "Train Epoch: 27 [33280/60000 (55%)]\tDiscriminator Loss: 0.565782\tGenerator Loss: 1.161563\n",
      "Train Epoch: 27 [34560/60000 (58%)]\tDiscriminator Loss: 0.494080\tGenerator Loss: 1.163867\n",
      "Train Epoch: 27 [35840/60000 (60%)]\tDiscriminator Loss: 0.482784\tGenerator Loss: 1.511860\n",
      "Train Epoch: 27 [37120/60000 (62%)]\tDiscriminator Loss: 0.522852\tGenerator Loss: 0.949810\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tDiscriminator Loss: 0.540253\tGenerator Loss: 1.244091\n",
      "Train Epoch: 27 [39680/60000 (66%)]\tDiscriminator Loss: 0.535300\tGenerator Loss: 1.265198\n",
      "Train Epoch: 27 [40960/60000 (68%)]\tDiscriminator Loss: 0.513549\tGenerator Loss: 1.105837\n",
      "Train Epoch: 27 [42240/60000 (70%)]\tDiscriminator Loss: 0.534615\tGenerator Loss: 1.316538\n",
      "Train Epoch: 27 [43520/60000 (72%)]\tDiscriminator Loss: 0.585921\tGenerator Loss: 1.102270\n",
      "Train Epoch: 27 [44800/60000 (75%)]\tDiscriminator Loss: 0.609044\tGenerator Loss: 1.527232\n",
      "Train Epoch: 27 [46080/60000 (77%)]\tDiscriminator Loss: 0.535056\tGenerator Loss: 1.280494\n",
      "Train Epoch: 27 [47360/60000 (79%)]\tDiscriminator Loss: 0.528274\tGenerator Loss: 1.054200\n",
      "Train Epoch: 27 [48640/60000 (81%)]\tDiscriminator Loss: 0.531883\tGenerator Loss: 1.184319\n",
      "Train Epoch: 27 [49920/60000 (83%)]\tDiscriminator Loss: 0.591514\tGenerator Loss: 0.990238\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tDiscriminator Loss: 0.589867\tGenerator Loss: 1.416433\n",
      "Train Epoch: 27 [52480/60000 (87%)]\tDiscriminator Loss: 0.476592\tGenerator Loss: 1.275241\n",
      "Train Epoch: 27 [53760/60000 (90%)]\tDiscriminator Loss: 0.497581\tGenerator Loss: 1.194378\n",
      "Train Epoch: 27 [55040/60000 (92%)]\tDiscriminator Loss: 0.579716\tGenerator Loss: 1.128731\n",
      "Train Epoch: 27 [56320/60000 (94%)]\tDiscriminator Loss: 0.623022\tGenerator Loss: 1.355222\n",
      "Train Epoch: 27 [57600/60000 (96%)]\tDiscriminator Loss: 0.535429\tGenerator Loss: 1.216410\n",
      "Train Epoch: 27 [58880/60000 (98%)]\tDiscriminator Loss: 0.499806\tGenerator Loss: 1.453902\n",
      "Train Epoch: 28 [0/60000 (0%)]\tDiscriminator Loss: 0.502742\tGenerator Loss: 1.232126\n",
      "Train Epoch: 28 [1280/60000 (2%)]\tDiscriminator Loss: 0.491692\tGenerator Loss: 1.324138\n",
      "Train Epoch: 28 [2560/60000 (4%)]\tDiscriminator Loss: 0.551704\tGenerator Loss: 0.976403\n",
      "Train Epoch: 28 [3840/60000 (6%)]\tDiscriminator Loss: 0.567554\tGenerator Loss: 1.035306\n",
      "Train Epoch: 28 [5120/60000 (9%)]\tDiscriminator Loss: 0.502966\tGenerator Loss: 1.096887\n",
      "Train Epoch: 28 [6400/60000 (11%)]\tDiscriminator Loss: 0.536734\tGenerator Loss: 1.476347\n",
      "Train Epoch: 28 [7680/60000 (13%)]\tDiscriminator Loss: 0.537313\tGenerator Loss: 1.164511\n",
      "Train Epoch: 28 [8960/60000 (15%)]\tDiscriminator Loss: 0.630319\tGenerator Loss: 0.676658\n",
      "Train Epoch: 28 [10240/60000 (17%)]\tDiscriminator Loss: 0.528598\tGenerator Loss: 1.098893\n",
      "Train Epoch: 28 [11520/60000 (19%)]\tDiscriminator Loss: 0.533682\tGenerator Loss: 1.374314\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tDiscriminator Loss: 0.493875\tGenerator Loss: 1.325864\n",
      "Train Epoch: 28 [14080/60000 (23%)]\tDiscriminator Loss: 0.547326\tGenerator Loss: 1.173260\n",
      "Train Epoch: 28 [15360/60000 (26%)]\tDiscriminator Loss: 0.533251\tGenerator Loss: 1.037345\n",
      "Train Epoch: 28 [16640/60000 (28%)]\tDiscriminator Loss: 0.587139\tGenerator Loss: 1.213255\n",
      "Train Epoch: 28 [17920/60000 (30%)]\tDiscriminator Loss: 0.667351\tGenerator Loss: 1.480387\n",
      "Train Epoch: 28 [19200/60000 (32%)]\tDiscriminator Loss: 0.553603\tGenerator Loss: 1.076796\n",
      "Train Epoch: 28 [20480/60000 (34%)]\tDiscriminator Loss: 0.551005\tGenerator Loss: 0.995170\n",
      "Train Epoch: 28 [21760/60000 (36%)]\tDiscriminator Loss: 0.458626\tGenerator Loss: 1.423600\n",
      "Train Epoch: 28 [23040/60000 (38%)]\tDiscriminator Loss: 0.459111\tGenerator Loss: 1.344060\n",
      "Train Epoch: 28 [24320/60000 (41%)]\tDiscriminator Loss: 0.513218\tGenerator Loss: 1.449376\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tDiscriminator Loss: 0.476627\tGenerator Loss: 1.337363\n",
      "Train Epoch: 28 [26880/60000 (45%)]\tDiscriminator Loss: 0.588537\tGenerator Loss: 1.670245\n",
      "Train Epoch: 28 [28160/60000 (47%)]\tDiscriminator Loss: 0.606583\tGenerator Loss: 0.878934\n",
      "Train Epoch: 28 [29440/60000 (49%)]\tDiscriminator Loss: 0.543323\tGenerator Loss: 1.255407\n",
      "Train Epoch: 28 [30720/60000 (51%)]\tDiscriminator Loss: 0.515929\tGenerator Loss: 1.156691\n",
      "Train Epoch: 28 [32000/60000 (53%)]\tDiscriminator Loss: 0.524865\tGenerator Loss: 1.020212\n",
      "Train Epoch: 28 [33280/60000 (55%)]\tDiscriminator Loss: 0.534525\tGenerator Loss: 1.414488\n",
      "Train Epoch: 28 [34560/60000 (58%)]\tDiscriminator Loss: 0.562101\tGenerator Loss: 1.068250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 28 [35840/60000 (60%)]\tDiscriminator Loss: 0.572998\tGenerator Loss: 1.385406\n",
      "Train Epoch: 28 [37120/60000 (62%)]\tDiscriminator Loss: 0.520400\tGenerator Loss: 1.142832\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tDiscriminator Loss: 0.603466\tGenerator Loss: 0.957607\n",
      "Train Epoch: 28 [39680/60000 (66%)]\tDiscriminator Loss: 0.538265\tGenerator Loss: 1.479985\n",
      "Train Epoch: 28 [40960/60000 (68%)]\tDiscriminator Loss: 0.551278\tGenerator Loss: 1.348676\n",
      "Train Epoch: 28 [42240/60000 (70%)]\tDiscriminator Loss: 0.690855\tGenerator Loss: 0.540097\n",
      "Train Epoch: 28 [43520/60000 (72%)]\tDiscriminator Loss: 0.560156\tGenerator Loss: 1.065052\n",
      "Train Epoch: 28 [44800/60000 (75%)]\tDiscriminator Loss: 0.585336\tGenerator Loss: 1.289052\n",
      "Train Epoch: 28 [46080/60000 (77%)]\tDiscriminator Loss: 0.538268\tGenerator Loss: 1.354949\n",
      "Train Epoch: 28 [47360/60000 (79%)]\tDiscriminator Loss: 0.657329\tGenerator Loss: 0.881068\n",
      "Train Epoch: 28 [48640/60000 (81%)]\tDiscriminator Loss: 0.583891\tGenerator Loss: 1.437948\n",
      "Train Epoch: 28 [49920/60000 (83%)]\tDiscriminator Loss: 0.582437\tGenerator Loss: 1.134264\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tDiscriminator Loss: 0.560587\tGenerator Loss: 1.182417\n",
      "Train Epoch: 28 [52480/60000 (87%)]\tDiscriminator Loss: 0.533725\tGenerator Loss: 1.083338\n",
      "Train Epoch: 28 [53760/60000 (90%)]\tDiscriminator Loss: 0.567849\tGenerator Loss: 1.133740\n",
      "Train Epoch: 28 [55040/60000 (92%)]\tDiscriminator Loss: 0.552793\tGenerator Loss: 1.236978\n",
      "Train Epoch: 28 [56320/60000 (94%)]\tDiscriminator Loss: 0.527094\tGenerator Loss: 1.179027\n",
      "Train Epoch: 28 [57600/60000 (96%)]\tDiscriminator Loss: 0.507785\tGenerator Loss: 1.463295\n",
      "Train Epoch: 28 [58880/60000 (98%)]\tDiscriminator Loss: 0.552354\tGenerator Loss: 1.092105\n",
      "Train Epoch: 29 [0/60000 (0%)]\tDiscriminator Loss: 0.594574\tGenerator Loss: 0.936152\n",
      "Train Epoch: 29 [1280/60000 (2%)]\tDiscriminator Loss: 0.511905\tGenerator Loss: 1.161027\n",
      "Train Epoch: 29 [2560/60000 (4%)]\tDiscriminator Loss: 0.488250\tGenerator Loss: 1.565509\n",
      "Train Epoch: 29 [3840/60000 (6%)]\tDiscriminator Loss: 0.500696\tGenerator Loss: 1.159807\n",
      "Train Epoch: 29 [5120/60000 (9%)]\tDiscriminator Loss: 0.557808\tGenerator Loss: 1.215980\n",
      "Train Epoch: 29 [6400/60000 (11%)]\tDiscriminator Loss: 0.541614\tGenerator Loss: 1.260738\n",
      "Train Epoch: 29 [7680/60000 (13%)]\tDiscriminator Loss: 0.561111\tGenerator Loss: 1.226547\n",
      "Train Epoch: 29 [8960/60000 (15%)]\tDiscriminator Loss: 0.455702\tGenerator Loss: 1.233222\n",
      "Train Epoch: 29 [10240/60000 (17%)]\tDiscriminator Loss: 0.486959\tGenerator Loss: 1.361161\n",
      "Train Epoch: 29 [11520/60000 (19%)]\tDiscriminator Loss: 0.511580\tGenerator Loss: 1.413705\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tDiscriminator Loss: 0.604894\tGenerator Loss: 1.612326\n",
      "Train Epoch: 29 [14080/60000 (23%)]\tDiscriminator Loss: 0.551208\tGenerator Loss: 1.244856\n",
      "Train Epoch: 29 [15360/60000 (26%)]\tDiscriminator Loss: 0.564503\tGenerator Loss: 1.210831\n",
      "Train Epoch: 29 [16640/60000 (28%)]\tDiscriminator Loss: 0.512399\tGenerator Loss: 1.712594\n",
      "Train Epoch: 29 [17920/60000 (30%)]\tDiscriminator Loss: 0.466356\tGenerator Loss: 1.154875\n",
      "Train Epoch: 29 [19200/60000 (32%)]\tDiscriminator Loss: 0.512902\tGenerator Loss: 1.179058\n",
      "Train Epoch: 29 [20480/60000 (34%)]\tDiscriminator Loss: 0.515388\tGenerator Loss: 1.332012\n",
      "Train Epoch: 29 [21760/60000 (36%)]\tDiscriminator Loss: 0.553073\tGenerator Loss: 1.309059\n",
      "Train Epoch: 29 [23040/60000 (38%)]\tDiscriminator Loss: 0.587033\tGenerator Loss: 1.085479\n",
      "Train Epoch: 29 [24320/60000 (41%)]\tDiscriminator Loss: 0.554146\tGenerator Loss: 1.311309\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tDiscriminator Loss: 0.472031\tGenerator Loss: 1.345734\n",
      "Train Epoch: 29 [26880/60000 (45%)]\tDiscriminator Loss: 0.574246\tGenerator Loss: 1.087923\n",
      "Train Epoch: 29 [28160/60000 (47%)]\tDiscriminator Loss: 0.519305\tGenerator Loss: 1.303583\n",
      "Train Epoch: 29 [29440/60000 (49%)]\tDiscriminator Loss: 0.566457\tGenerator Loss: 1.451560\n",
      "Train Epoch: 29 [30720/60000 (51%)]\tDiscriminator Loss: 0.566423\tGenerator Loss: 1.269353\n",
      "Train Epoch: 29 [32000/60000 (53%)]\tDiscriminator Loss: 0.603859\tGenerator Loss: 0.925287\n",
      "Train Epoch: 29 [33280/60000 (55%)]\tDiscriminator Loss: 0.581913\tGenerator Loss: 1.005121\n",
      "Train Epoch: 29 [34560/60000 (58%)]\tDiscriminator Loss: 0.566246\tGenerator Loss: 1.063721\n",
      "Train Epoch: 29 [35840/60000 (60%)]\tDiscriminator Loss: 0.559476\tGenerator Loss: 1.081177\n",
      "Train Epoch: 29 [37120/60000 (62%)]\tDiscriminator Loss: 0.575827\tGenerator Loss: 1.253060\n",
      "Train Epoch: 29 [38400/60000 (64%)]\tDiscriminator Loss: 0.516619\tGenerator Loss: 1.181252\n",
      "Train Epoch: 29 [39680/60000 (66%)]\tDiscriminator Loss: 0.531236\tGenerator Loss: 0.968136\n",
      "Train Epoch: 29 [40960/60000 (68%)]\tDiscriminator Loss: 0.566495\tGenerator Loss: 1.308136\n",
      "Train Epoch: 29 [42240/60000 (70%)]\tDiscriminator Loss: 0.531815\tGenerator Loss: 1.329483\n",
      "Train Epoch: 29 [43520/60000 (72%)]\tDiscriminator Loss: 0.523457\tGenerator Loss: 1.556400\n",
      "Train Epoch: 29 [44800/60000 (75%)]\tDiscriminator Loss: 0.554237\tGenerator Loss: 1.056495\n",
      "Train Epoch: 29 [46080/60000 (77%)]\tDiscriminator Loss: 0.557509\tGenerator Loss: 1.170509\n",
      "Train Epoch: 29 [47360/60000 (79%)]\tDiscriminator Loss: 0.503230\tGenerator Loss: 1.122453\n",
      "Train Epoch: 29 [48640/60000 (81%)]\tDiscriminator Loss: 0.511166\tGenerator Loss: 1.370253\n",
      "Train Epoch: 29 [49920/60000 (83%)]\tDiscriminator Loss: 0.512339\tGenerator Loss: 1.339099\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tDiscriminator Loss: 0.556761\tGenerator Loss: 1.232303\n",
      "Train Epoch: 29 [52480/60000 (87%)]\tDiscriminator Loss: 0.533873\tGenerator Loss: 1.294867\n",
      "Train Epoch: 29 [53760/60000 (90%)]\tDiscriminator Loss: 0.599776\tGenerator Loss: 0.873167\n",
      "Train Epoch: 29 [55040/60000 (92%)]\tDiscriminator Loss: 0.541454\tGenerator Loss: 1.038847\n",
      "Train Epoch: 29 [56320/60000 (94%)]\tDiscriminator Loss: 0.499074\tGenerator Loss: 1.310857\n",
      "Train Epoch: 29 [57600/60000 (96%)]\tDiscriminator Loss: 0.607250\tGenerator Loss: 1.503714\n",
      "Train Epoch: 29 [58880/60000 (98%)]\tDiscriminator Loss: 0.510088\tGenerator Loss: 1.109938\n",
      "Train Epoch: 30 [0/60000 (0%)]\tDiscriminator Loss: 0.543609\tGenerator Loss: 1.085187\n",
      "Train Epoch: 30 [1280/60000 (2%)]\tDiscriminator Loss: 0.488716\tGenerator Loss: 0.989422\n",
      "Train Epoch: 30 [2560/60000 (4%)]\tDiscriminator Loss: 0.562025\tGenerator Loss: 1.522558\n",
      "Train Epoch: 30 [3840/60000 (6%)]\tDiscriminator Loss: 0.568735\tGenerator Loss: 1.152805\n",
      "Train Epoch: 30 [5120/60000 (9%)]\tDiscriminator Loss: 0.535718\tGenerator Loss: 1.424465\n",
      "Train Epoch: 30 [6400/60000 (11%)]\tDiscriminator Loss: 0.506463\tGenerator Loss: 1.453510\n",
      "Train Epoch: 30 [7680/60000 (13%)]\tDiscriminator Loss: 0.563508\tGenerator Loss: 1.303617\n",
      "Train Epoch: 30 [8960/60000 (15%)]\tDiscriminator Loss: 0.552827\tGenerator Loss: 1.012816\n",
      "Train Epoch: 30 [10240/60000 (17%)]\tDiscriminator Loss: 0.613880\tGenerator Loss: 1.331943\n",
      "Train Epoch: 30 [11520/60000 (19%)]\tDiscriminator Loss: 0.479432\tGenerator Loss: 1.311562\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tDiscriminator Loss: 0.570277\tGenerator Loss: 1.288424\n",
      "Train Epoch: 30 [14080/60000 (23%)]\tDiscriminator Loss: 0.536334\tGenerator Loss: 1.179474\n",
      "Train Epoch: 30 [15360/60000 (26%)]\tDiscriminator Loss: 0.551782\tGenerator Loss: 1.106962\n",
      "Train Epoch: 30 [16640/60000 (28%)]\tDiscriminator Loss: 0.535544\tGenerator Loss: 1.430964\n",
      "Train Epoch: 30 [17920/60000 (30%)]\tDiscriminator Loss: 0.507979\tGenerator Loss: 0.906848\n",
      "Train Epoch: 30 [19200/60000 (32%)]\tDiscriminator Loss: 0.554094\tGenerator Loss: 0.950742\n",
      "Train Epoch: 30 [20480/60000 (34%)]\tDiscriminator Loss: 0.537396\tGenerator Loss: 1.145942\n",
      "Train Epoch: 30 [21760/60000 (36%)]\tDiscriminator Loss: 0.504625\tGenerator Loss: 1.089936\n",
      "Train Epoch: 30 [23040/60000 (38%)]\tDiscriminator Loss: 0.523402\tGenerator Loss: 1.442057\n",
      "Train Epoch: 30 [24320/60000 (41%)]\tDiscriminator Loss: 0.488357\tGenerator Loss: 1.097411\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tDiscriminator Loss: 0.605115\tGenerator Loss: 1.413640\n",
      "Train Epoch: 30 [26880/60000 (45%)]\tDiscriminator Loss: 0.541201\tGenerator Loss: 1.002846\n",
      "Train Epoch: 30 [28160/60000 (47%)]\tDiscriminator Loss: 0.555156\tGenerator Loss: 1.399748\n",
      "Train Epoch: 30 [29440/60000 (49%)]\tDiscriminator Loss: 0.581527\tGenerator Loss: 1.089032\n",
      "Train Epoch: 30 [30720/60000 (51%)]\tDiscriminator Loss: 0.540909\tGenerator Loss: 1.095559\n",
      "Train Epoch: 30 [32000/60000 (53%)]\tDiscriminator Loss: 0.443854\tGenerator Loss: 1.513709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 30 [33280/60000 (55%)]\tDiscriminator Loss: 0.539260\tGenerator Loss: 1.231535\n",
      "Train Epoch: 30 [34560/60000 (58%)]\tDiscriminator Loss: 0.561144\tGenerator Loss: 1.309895\n",
      "Train Epoch: 30 [35840/60000 (60%)]\tDiscriminator Loss: 0.535997\tGenerator Loss: 1.282933\n",
      "Train Epoch: 30 [37120/60000 (62%)]\tDiscriminator Loss: 0.525155\tGenerator Loss: 1.093846\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tDiscriminator Loss: 0.605140\tGenerator Loss: 0.835522\n",
      "Train Epoch: 30 [39680/60000 (66%)]\tDiscriminator Loss: 0.543389\tGenerator Loss: 1.167717\n",
      "Train Epoch: 30 [40960/60000 (68%)]\tDiscriminator Loss: 0.566600\tGenerator Loss: 1.248925\n",
      "Train Epoch: 30 [42240/60000 (70%)]\tDiscriminator Loss: 0.476050\tGenerator Loss: 1.348094\n",
      "Train Epoch: 30 [43520/60000 (72%)]\tDiscriminator Loss: 0.543005\tGenerator Loss: 0.981116\n",
      "Train Epoch: 30 [44800/60000 (75%)]\tDiscriminator Loss: 0.614657\tGenerator Loss: 0.695819\n",
      "Train Epoch: 30 [46080/60000 (77%)]\tDiscriminator Loss: 0.526837\tGenerator Loss: 1.310696\n",
      "Train Epoch: 30 [47360/60000 (79%)]\tDiscriminator Loss: 0.538377\tGenerator Loss: 1.042654\n",
      "Train Epoch: 30 [48640/60000 (81%)]\tDiscriminator Loss: 0.600634\tGenerator Loss: 1.618790\n",
      "Train Epoch: 30 [49920/60000 (83%)]\tDiscriminator Loss: 0.552991\tGenerator Loss: 1.028904\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tDiscriminator Loss: 0.591597\tGenerator Loss: 1.212418\n",
      "Train Epoch: 30 [52480/60000 (87%)]\tDiscriminator Loss: 0.551361\tGenerator Loss: 1.151245\n",
      "Train Epoch: 30 [53760/60000 (90%)]\tDiscriminator Loss: 0.464462\tGenerator Loss: 1.007013\n",
      "Train Epoch: 30 [55040/60000 (92%)]\tDiscriminator Loss: 0.578183\tGenerator Loss: 1.063119\n",
      "Train Epoch: 30 [56320/60000 (94%)]\tDiscriminator Loss: 0.626861\tGenerator Loss: 1.473714\n",
      "Train Epoch: 30 [57600/60000 (96%)]\tDiscriminator Loss: 0.471178\tGenerator Loss: 1.225557\n",
      "Train Epoch: 30 [58880/60000 (98%)]\tDiscriminator Loss: 0.539019\tGenerator Loss: 1.163231\n",
      "Train Epoch: 31 [0/60000 (0%)]\tDiscriminator Loss: 0.565096\tGenerator Loss: 1.190369\n",
      "Train Epoch: 31 [1280/60000 (2%)]\tDiscriminator Loss: 0.558963\tGenerator Loss: 0.975244\n",
      "Train Epoch: 31 [2560/60000 (4%)]\tDiscriminator Loss: 0.508661\tGenerator Loss: 1.065191\n",
      "Train Epoch: 31 [3840/60000 (6%)]\tDiscriminator Loss: 0.517607\tGenerator Loss: 1.188079\n",
      "Train Epoch: 31 [5120/60000 (9%)]\tDiscriminator Loss: 0.573058\tGenerator Loss: 0.975050\n",
      "Train Epoch: 31 [6400/60000 (11%)]\tDiscriminator Loss: 0.580116\tGenerator Loss: 1.062179\n",
      "Train Epoch: 31 [7680/60000 (13%)]\tDiscriminator Loss: 0.517753\tGenerator Loss: 1.066447\n",
      "Train Epoch: 31 [8960/60000 (15%)]\tDiscriminator Loss: 0.482132\tGenerator Loss: 1.143202\n",
      "Train Epoch: 31 [10240/60000 (17%)]\tDiscriminator Loss: 0.464325\tGenerator Loss: 1.328208\n",
      "Train Epoch: 31 [11520/60000 (19%)]\tDiscriminator Loss: 0.543129\tGenerator Loss: 0.951663\n",
      "Train Epoch: 31 [12800/60000 (21%)]\tDiscriminator Loss: 0.529401\tGenerator Loss: 1.153932\n",
      "Train Epoch: 31 [14080/60000 (23%)]\tDiscriminator Loss: 0.473459\tGenerator Loss: 1.258143\n",
      "Train Epoch: 31 [15360/60000 (26%)]\tDiscriminator Loss: 0.489383\tGenerator Loss: 1.085250\n",
      "Train Epoch: 31 [16640/60000 (28%)]\tDiscriminator Loss: 0.521730\tGenerator Loss: 1.103258\n",
      "Train Epoch: 31 [17920/60000 (30%)]\tDiscriminator Loss: 0.566381\tGenerator Loss: 1.022426\n",
      "Train Epoch: 31 [19200/60000 (32%)]\tDiscriminator Loss: 0.520495\tGenerator Loss: 1.566786\n",
      "Train Epoch: 31 [20480/60000 (34%)]\tDiscriminator Loss: 0.531095\tGenerator Loss: 1.466766\n",
      "Train Epoch: 31 [21760/60000 (36%)]\tDiscriminator Loss: 0.550744\tGenerator Loss: 1.312894\n",
      "Train Epoch: 31 [23040/60000 (38%)]\tDiscriminator Loss: 0.567402\tGenerator Loss: 1.210004\n",
      "Train Epoch: 31 [24320/60000 (41%)]\tDiscriminator Loss: 0.500374\tGenerator Loss: 1.340726\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tDiscriminator Loss: 0.542817\tGenerator Loss: 0.913298\n",
      "Train Epoch: 31 [26880/60000 (45%)]\tDiscriminator Loss: 0.616061\tGenerator Loss: 1.109010\n",
      "Train Epoch: 31 [28160/60000 (47%)]\tDiscriminator Loss: 0.512573\tGenerator Loss: 1.171121\n",
      "Train Epoch: 31 [29440/60000 (49%)]\tDiscriminator Loss: 0.508516\tGenerator Loss: 1.277412\n",
      "Train Epoch: 31 [30720/60000 (51%)]\tDiscriminator Loss: 0.555254\tGenerator Loss: 1.339957\n",
      "Train Epoch: 31 [32000/60000 (53%)]\tDiscriminator Loss: 0.573690\tGenerator Loss: 0.890177\n",
      "Train Epoch: 31 [33280/60000 (55%)]\tDiscriminator Loss: 0.514072\tGenerator Loss: 1.191266\n",
      "Train Epoch: 31 [34560/60000 (58%)]\tDiscriminator Loss: 0.529963\tGenerator Loss: 1.520961\n",
      "Train Epoch: 31 [35840/60000 (60%)]\tDiscriminator Loss: 0.592620\tGenerator Loss: 1.070095\n",
      "Train Epoch: 31 [37120/60000 (62%)]\tDiscriminator Loss: 0.506897\tGenerator Loss: 1.264403\n",
      "Train Epoch: 31 [38400/60000 (64%)]\tDiscriminator Loss: 0.542515\tGenerator Loss: 1.037074\n",
      "Train Epoch: 31 [39680/60000 (66%)]\tDiscriminator Loss: 0.533194\tGenerator Loss: 1.085265\n",
      "Train Epoch: 31 [40960/60000 (68%)]\tDiscriminator Loss: 0.669494\tGenerator Loss: 0.582991\n",
      "Train Epoch: 31 [42240/60000 (70%)]\tDiscriminator Loss: 0.539940\tGenerator Loss: 1.462964\n",
      "Train Epoch: 31 [43520/60000 (72%)]\tDiscriminator Loss: 0.566706\tGenerator Loss: 1.041111\n",
      "Train Epoch: 31 [44800/60000 (75%)]\tDiscriminator Loss: 0.505840\tGenerator Loss: 1.149105\n",
      "Train Epoch: 31 [46080/60000 (77%)]\tDiscriminator Loss: 0.662700\tGenerator Loss: 0.635466\n",
      "Train Epoch: 31 [47360/60000 (79%)]\tDiscriminator Loss: 0.558681\tGenerator Loss: 1.350717\n",
      "Train Epoch: 31 [48640/60000 (81%)]\tDiscriminator Loss: 0.552832\tGenerator Loss: 1.342337\n",
      "Train Epoch: 31 [49920/60000 (83%)]\tDiscriminator Loss: 0.512859\tGenerator Loss: 1.240849\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tDiscriminator Loss: 0.515000\tGenerator Loss: 1.351884\n",
      "Train Epoch: 31 [52480/60000 (87%)]\tDiscriminator Loss: 0.637609\tGenerator Loss: 0.810379\n",
      "Train Epoch: 31 [53760/60000 (90%)]\tDiscriminator Loss: 0.582552\tGenerator Loss: 1.417708\n",
      "Train Epoch: 31 [55040/60000 (92%)]\tDiscriminator Loss: 0.545460\tGenerator Loss: 1.492455\n",
      "Train Epoch: 31 [56320/60000 (94%)]\tDiscriminator Loss: 0.474872\tGenerator Loss: 1.276491\n",
      "Train Epoch: 31 [57600/60000 (96%)]\tDiscriminator Loss: 0.505574\tGenerator Loss: 1.119512\n",
      "Train Epoch: 31 [58880/60000 (98%)]\tDiscriminator Loss: 0.613708\tGenerator Loss: 1.261017\n",
      "Train Epoch: 32 [0/60000 (0%)]\tDiscriminator Loss: 0.473142\tGenerator Loss: 1.253466\n",
      "Train Epoch: 32 [1280/60000 (2%)]\tDiscriminator Loss: 0.488700\tGenerator Loss: 1.356293\n",
      "Train Epoch: 32 [2560/60000 (4%)]\tDiscriminator Loss: 0.536658\tGenerator Loss: 1.075155\n",
      "Train Epoch: 32 [3840/60000 (6%)]\tDiscriminator Loss: 0.562621\tGenerator Loss: 0.940918\n",
      "Train Epoch: 32 [5120/60000 (9%)]\tDiscriminator Loss: 0.553455\tGenerator Loss: 0.985387\n",
      "Train Epoch: 32 [6400/60000 (11%)]\tDiscriminator Loss: 0.486787\tGenerator Loss: 1.216027\n",
      "Train Epoch: 32 [7680/60000 (13%)]\tDiscriminator Loss: 0.466467\tGenerator Loss: 1.210588\n",
      "Train Epoch: 32 [8960/60000 (15%)]\tDiscriminator Loss: 0.670032\tGenerator Loss: 0.695558\n",
      "Train Epoch: 32 [10240/60000 (17%)]\tDiscriminator Loss: 0.565642\tGenerator Loss: 0.922787\n",
      "Train Epoch: 32 [11520/60000 (19%)]\tDiscriminator Loss: 0.561979\tGenerator Loss: 1.243064\n",
      "Train Epoch: 32 [12800/60000 (21%)]\tDiscriminator Loss: 0.520935\tGenerator Loss: 1.438719\n",
      "Train Epoch: 32 [14080/60000 (23%)]\tDiscriminator Loss: 0.522171\tGenerator Loss: 1.158521\n",
      "Train Epoch: 32 [15360/60000 (26%)]\tDiscriminator Loss: 0.653610\tGenerator Loss: 0.793765\n",
      "Train Epoch: 32 [16640/60000 (28%)]\tDiscriminator Loss: 0.516923\tGenerator Loss: 1.484621\n",
      "Train Epoch: 32 [17920/60000 (30%)]\tDiscriminator Loss: 0.542645\tGenerator Loss: 1.352172\n",
      "Train Epoch: 32 [19200/60000 (32%)]\tDiscriminator Loss: 0.574188\tGenerator Loss: 0.899172\n",
      "Train Epoch: 32 [20480/60000 (34%)]\tDiscriminator Loss: 0.548367\tGenerator Loss: 1.250624\n",
      "Train Epoch: 32 [21760/60000 (36%)]\tDiscriminator Loss: 0.522056\tGenerator Loss: 1.056511\n",
      "Train Epoch: 32 [23040/60000 (38%)]\tDiscriminator Loss: 0.536625\tGenerator Loss: 1.449651\n",
      "Train Epoch: 32 [24320/60000 (41%)]\tDiscriminator Loss: 0.586991\tGenerator Loss: 1.144341\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tDiscriminator Loss: 0.538815\tGenerator Loss: 1.200793\n",
      "Train Epoch: 32 [26880/60000 (45%)]\tDiscriminator Loss: 0.473961\tGenerator Loss: 1.143445\n",
      "Train Epoch: 32 [28160/60000 (47%)]\tDiscriminator Loss: 0.525391\tGenerator Loss: 1.091366\n",
      "Train Epoch: 32 [29440/60000 (49%)]\tDiscriminator Loss: 0.615228\tGenerator Loss: 1.552750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 32 [30720/60000 (51%)]\tDiscriminator Loss: 0.542025\tGenerator Loss: 1.114966\n",
      "Train Epoch: 32 [32000/60000 (53%)]\tDiscriminator Loss: 0.541213\tGenerator Loss: 0.834356\n",
      "Train Epoch: 32 [33280/60000 (55%)]\tDiscriminator Loss: 0.629515\tGenerator Loss: 1.840779\n",
      "Train Epoch: 32 [34560/60000 (58%)]\tDiscriminator Loss: 0.576719\tGenerator Loss: 1.241719\n",
      "Train Epoch: 32 [35840/60000 (60%)]\tDiscriminator Loss: 0.445796\tGenerator Loss: 1.192606\n",
      "Train Epoch: 32 [37120/60000 (62%)]\tDiscriminator Loss: 0.514417\tGenerator Loss: 1.687549\n",
      "Train Epoch: 32 [38400/60000 (64%)]\tDiscriminator Loss: 0.541981\tGenerator Loss: 1.111431\n",
      "Train Epoch: 32 [39680/60000 (66%)]\tDiscriminator Loss: 0.549149\tGenerator Loss: 1.528762\n",
      "Train Epoch: 32 [40960/60000 (68%)]\tDiscriminator Loss: 0.576916\tGenerator Loss: 1.428858\n",
      "Train Epoch: 32 [42240/60000 (70%)]\tDiscriminator Loss: 0.564640\tGenerator Loss: 1.005221\n",
      "Train Epoch: 32 [43520/60000 (72%)]\tDiscriminator Loss: 0.523708\tGenerator Loss: 0.998854\n",
      "Train Epoch: 32 [44800/60000 (75%)]\tDiscriminator Loss: 0.539954\tGenerator Loss: 1.363874\n",
      "Train Epoch: 32 [46080/60000 (77%)]\tDiscriminator Loss: 0.530547\tGenerator Loss: 1.011664\n",
      "Train Epoch: 32 [47360/60000 (79%)]\tDiscriminator Loss: 0.597515\tGenerator Loss: 1.611937\n",
      "Train Epoch: 32 [48640/60000 (81%)]\tDiscriminator Loss: 0.531545\tGenerator Loss: 1.091923\n",
      "Train Epoch: 32 [49920/60000 (83%)]\tDiscriminator Loss: 0.529732\tGenerator Loss: 1.041033\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tDiscriminator Loss: 0.556163\tGenerator Loss: 1.383139\n",
      "Train Epoch: 32 [52480/60000 (87%)]\tDiscriminator Loss: 0.541805\tGenerator Loss: 1.272875\n",
      "Train Epoch: 32 [53760/60000 (90%)]\tDiscriminator Loss: 0.480103\tGenerator Loss: 1.316683\n",
      "Train Epoch: 32 [55040/60000 (92%)]\tDiscriminator Loss: 0.487296\tGenerator Loss: 1.275609\n",
      "Train Epoch: 32 [56320/60000 (94%)]\tDiscriminator Loss: 0.587142\tGenerator Loss: 1.148081\n",
      "Train Epoch: 32 [57600/60000 (96%)]\tDiscriminator Loss: 0.533808\tGenerator Loss: 1.251235\n",
      "Train Epoch: 32 [58880/60000 (98%)]\tDiscriminator Loss: 0.520222\tGenerator Loss: 1.068396\n",
      "Train Epoch: 33 [0/60000 (0%)]\tDiscriminator Loss: 0.480310\tGenerator Loss: 1.398658\n",
      "Train Epoch: 33 [1280/60000 (2%)]\tDiscriminator Loss: 0.597988\tGenerator Loss: 1.683772\n",
      "Train Epoch: 33 [2560/60000 (4%)]\tDiscriminator Loss: 0.555035\tGenerator Loss: 1.098804\n",
      "Train Epoch: 33 [3840/60000 (6%)]\tDiscriminator Loss: 0.589406\tGenerator Loss: 1.085539\n",
      "Train Epoch: 33 [5120/60000 (9%)]\tDiscriminator Loss: 0.526072\tGenerator Loss: 1.418752\n",
      "Train Epoch: 33 [6400/60000 (11%)]\tDiscriminator Loss: 0.516082\tGenerator Loss: 1.360873\n",
      "Train Epoch: 33 [7680/60000 (13%)]\tDiscriminator Loss: 0.508452\tGenerator Loss: 1.286225\n",
      "Train Epoch: 33 [8960/60000 (15%)]\tDiscriminator Loss: 0.528498\tGenerator Loss: 1.167176\n",
      "Train Epoch: 33 [10240/60000 (17%)]\tDiscriminator Loss: 0.568196\tGenerator Loss: 1.597341\n",
      "Train Epoch: 33 [11520/60000 (19%)]\tDiscriminator Loss: 0.518017\tGenerator Loss: 1.732804\n",
      "Train Epoch: 33 [12800/60000 (21%)]\tDiscriminator Loss: 0.540287\tGenerator Loss: 1.381583\n",
      "Train Epoch: 33 [14080/60000 (23%)]\tDiscriminator Loss: 0.534672\tGenerator Loss: 1.502810\n",
      "Train Epoch: 33 [15360/60000 (26%)]\tDiscriminator Loss: 0.572801\tGenerator Loss: 1.282839\n",
      "Train Epoch: 33 [16640/60000 (28%)]\tDiscriminator Loss: 0.502936\tGenerator Loss: 1.336650\n",
      "Train Epoch: 33 [17920/60000 (30%)]\tDiscriminator Loss: 0.513406\tGenerator Loss: 1.008248\n",
      "Train Epoch: 33 [19200/60000 (32%)]\tDiscriminator Loss: 0.504588\tGenerator Loss: 1.348330\n",
      "Train Epoch: 33 [20480/60000 (34%)]\tDiscriminator Loss: 0.549953\tGenerator Loss: 1.225670\n",
      "Train Epoch: 33 [21760/60000 (36%)]\tDiscriminator Loss: 0.459763\tGenerator Loss: 1.415846\n",
      "Train Epoch: 33 [23040/60000 (38%)]\tDiscriminator Loss: 0.600975\tGenerator Loss: 0.841673\n",
      "Train Epoch: 33 [24320/60000 (41%)]\tDiscriminator Loss: 0.522358\tGenerator Loss: 1.035566\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tDiscriminator Loss: 0.512511\tGenerator Loss: 1.035288\n",
      "Train Epoch: 33 [26880/60000 (45%)]\tDiscriminator Loss: 0.559598\tGenerator Loss: 1.209732\n",
      "Train Epoch: 33 [28160/60000 (47%)]\tDiscriminator Loss: 0.580347\tGenerator Loss: 1.253374\n",
      "Train Epoch: 33 [29440/60000 (49%)]\tDiscriminator Loss: 0.451652\tGenerator Loss: 1.394326\n",
      "Train Epoch: 33 [30720/60000 (51%)]\tDiscriminator Loss: 0.518107\tGenerator Loss: 1.488717\n",
      "Train Epoch: 33 [32000/60000 (53%)]\tDiscriminator Loss: 0.535524\tGenerator Loss: 1.167786\n",
      "Train Epoch: 33 [33280/60000 (55%)]\tDiscriminator Loss: 0.562302\tGenerator Loss: 0.916552\n",
      "Train Epoch: 33 [34560/60000 (58%)]\tDiscriminator Loss: 0.503749\tGenerator Loss: 1.554029\n",
      "Train Epoch: 33 [35840/60000 (60%)]\tDiscriminator Loss: 0.529532\tGenerator Loss: 0.942938\n",
      "Train Epoch: 33 [37120/60000 (62%)]\tDiscriminator Loss: 0.535138\tGenerator Loss: 1.218704\n",
      "Train Epoch: 33 [38400/60000 (64%)]\tDiscriminator Loss: 0.575986\tGenerator Loss: 1.371056\n",
      "Train Epoch: 33 [39680/60000 (66%)]\tDiscriminator Loss: 0.513692\tGenerator Loss: 1.111110\n",
      "Train Epoch: 33 [40960/60000 (68%)]\tDiscriminator Loss: 0.548786\tGenerator Loss: 1.261946\n",
      "Train Epoch: 33 [42240/60000 (70%)]\tDiscriminator Loss: 0.574263\tGenerator Loss: 1.014264\n",
      "Train Epoch: 33 [43520/60000 (72%)]\tDiscriminator Loss: 0.505486\tGenerator Loss: 1.515243\n",
      "Train Epoch: 33 [44800/60000 (75%)]\tDiscriminator Loss: 0.532906\tGenerator Loss: 1.385886\n",
      "Train Epoch: 33 [46080/60000 (77%)]\tDiscriminator Loss: 0.560594\tGenerator Loss: 1.202591\n",
      "Train Epoch: 33 [47360/60000 (79%)]\tDiscriminator Loss: 0.549948\tGenerator Loss: 1.050532\n",
      "Train Epoch: 33 [48640/60000 (81%)]\tDiscriminator Loss: 0.495818\tGenerator Loss: 1.305718\n",
      "Train Epoch: 33 [49920/60000 (83%)]\tDiscriminator Loss: 0.566581\tGenerator Loss: 1.070729\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tDiscriminator Loss: 0.679584\tGenerator Loss: 0.749687\n",
      "Train Epoch: 33 [52480/60000 (87%)]\tDiscriminator Loss: 0.511576\tGenerator Loss: 1.486063\n",
      "Train Epoch: 33 [53760/60000 (90%)]\tDiscriminator Loss: 0.477036\tGenerator Loss: 1.666476\n",
      "Train Epoch: 33 [55040/60000 (92%)]\tDiscriminator Loss: 0.556837\tGenerator Loss: 1.059953\n",
      "Train Epoch: 33 [56320/60000 (94%)]\tDiscriminator Loss: 0.576080\tGenerator Loss: 1.419702\n",
      "Train Epoch: 33 [57600/60000 (96%)]\tDiscriminator Loss: 0.543809\tGenerator Loss: 1.211780\n",
      "Train Epoch: 33 [58880/60000 (98%)]\tDiscriminator Loss: 0.604283\tGenerator Loss: 0.972364\n",
      "Train Epoch: 34 [0/60000 (0%)]\tDiscriminator Loss: 0.577216\tGenerator Loss: 1.238943\n",
      "Train Epoch: 34 [1280/60000 (2%)]\tDiscriminator Loss: 0.514544\tGenerator Loss: 1.441430\n",
      "Train Epoch: 34 [2560/60000 (4%)]\tDiscriminator Loss: 0.562713\tGenerator Loss: 1.014059\n",
      "Train Epoch: 34 [3840/60000 (6%)]\tDiscriminator Loss: 0.491501\tGenerator Loss: 1.247771\n",
      "Train Epoch: 34 [5120/60000 (9%)]\tDiscriminator Loss: 0.513246\tGenerator Loss: 1.422896\n",
      "Train Epoch: 34 [6400/60000 (11%)]\tDiscriminator Loss: 0.564805\tGenerator Loss: 1.139814\n",
      "Train Epoch: 34 [7680/60000 (13%)]\tDiscriminator Loss: 0.580999\tGenerator Loss: 1.231224\n",
      "Train Epoch: 34 [8960/60000 (15%)]\tDiscriminator Loss: 0.560166\tGenerator Loss: 1.476940\n",
      "Train Epoch: 34 [10240/60000 (17%)]\tDiscriminator Loss: 0.520213\tGenerator Loss: 1.238526\n",
      "Train Epoch: 34 [11520/60000 (19%)]\tDiscriminator Loss: 0.522352\tGenerator Loss: 1.122608\n",
      "Train Epoch: 34 [12800/60000 (21%)]\tDiscriminator Loss: 0.524880\tGenerator Loss: 1.345011\n",
      "Train Epoch: 34 [14080/60000 (23%)]\tDiscriminator Loss: 0.514134\tGenerator Loss: 1.364178\n",
      "Train Epoch: 34 [15360/60000 (26%)]\tDiscriminator Loss: 0.487440\tGenerator Loss: 1.168767\n",
      "Train Epoch: 34 [16640/60000 (28%)]\tDiscriminator Loss: 0.503852\tGenerator Loss: 1.243373\n",
      "Train Epoch: 34 [17920/60000 (30%)]\tDiscriminator Loss: 0.699840\tGenerator Loss: 0.604612\n",
      "Train Epoch: 34 [19200/60000 (32%)]\tDiscriminator Loss: 0.549039\tGenerator Loss: 1.393823\n",
      "Train Epoch: 34 [20480/60000 (34%)]\tDiscriminator Loss: 0.520886\tGenerator Loss: 1.342782\n",
      "Train Epoch: 34 [21760/60000 (36%)]\tDiscriminator Loss: 0.500611\tGenerator Loss: 1.156894\n",
      "Train Epoch: 34 [23040/60000 (38%)]\tDiscriminator Loss: 0.646459\tGenerator Loss: 1.356618\n",
      "Train Epoch: 34 [24320/60000 (41%)]\tDiscriminator Loss: 0.566417\tGenerator Loss: 1.086846\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tDiscriminator Loss: 0.579939\tGenerator Loss: 1.610527\n",
      "Train Epoch: 34 [26880/60000 (45%)]\tDiscriminator Loss: 0.466868\tGenerator Loss: 1.326372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 34 [28160/60000 (47%)]\tDiscriminator Loss: 0.485736\tGenerator Loss: 1.520571\n",
      "Train Epoch: 34 [29440/60000 (49%)]\tDiscriminator Loss: 0.515058\tGenerator Loss: 0.926463\n",
      "Train Epoch: 34 [30720/60000 (51%)]\tDiscriminator Loss: 0.549999\tGenerator Loss: 1.163723\n",
      "Train Epoch: 34 [32000/60000 (53%)]\tDiscriminator Loss: 0.591969\tGenerator Loss: 0.909732\n",
      "Train Epoch: 34 [33280/60000 (55%)]\tDiscriminator Loss: 0.529276\tGenerator Loss: 1.213880\n",
      "Train Epoch: 34 [34560/60000 (58%)]\tDiscriminator Loss: 0.532840\tGenerator Loss: 1.063561\n",
      "Train Epoch: 34 [35840/60000 (60%)]\tDiscriminator Loss: 0.580422\tGenerator Loss: 1.328761\n",
      "Train Epoch: 34 [37120/60000 (62%)]\tDiscriminator Loss: 0.538864\tGenerator Loss: 1.398855\n",
      "Train Epoch: 34 [38400/60000 (64%)]\tDiscriminator Loss: 0.524192\tGenerator Loss: 1.184090\n",
      "Train Epoch: 34 [39680/60000 (66%)]\tDiscriminator Loss: 0.544511\tGenerator Loss: 1.195964\n",
      "Train Epoch: 34 [40960/60000 (68%)]\tDiscriminator Loss: 0.541506\tGenerator Loss: 1.012971\n",
      "Train Epoch: 34 [42240/60000 (70%)]\tDiscriminator Loss: 0.519662\tGenerator Loss: 1.271695\n",
      "Train Epoch: 34 [43520/60000 (72%)]\tDiscriminator Loss: 0.504826\tGenerator Loss: 1.283344\n",
      "Train Epoch: 34 [44800/60000 (75%)]\tDiscriminator Loss: 0.543180\tGenerator Loss: 1.004450\n",
      "Train Epoch: 34 [46080/60000 (77%)]\tDiscriminator Loss: 0.574963\tGenerator Loss: 1.203118\n",
      "Train Epoch: 34 [47360/60000 (79%)]\tDiscriminator Loss: 0.496051\tGenerator Loss: 1.051477\n",
      "Train Epoch: 34 [48640/60000 (81%)]\tDiscriminator Loss: 0.533912\tGenerator Loss: 1.001793\n",
      "Train Epoch: 34 [49920/60000 (83%)]\tDiscriminator Loss: 0.573516\tGenerator Loss: 0.842465\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tDiscriminator Loss: 0.649105\tGenerator Loss: 0.829287\n",
      "Train Epoch: 34 [52480/60000 (87%)]\tDiscriminator Loss: 0.588359\tGenerator Loss: 1.402947\n",
      "Train Epoch: 34 [53760/60000 (90%)]\tDiscriminator Loss: 0.510172\tGenerator Loss: 1.525458\n",
      "Train Epoch: 34 [55040/60000 (92%)]\tDiscriminator Loss: 0.481493\tGenerator Loss: 1.534086\n",
      "Train Epoch: 34 [56320/60000 (94%)]\tDiscriminator Loss: 0.548403\tGenerator Loss: 1.034430\n",
      "Train Epoch: 34 [57600/60000 (96%)]\tDiscriminator Loss: 0.546553\tGenerator Loss: 1.286003\n",
      "Train Epoch: 34 [58880/60000 (98%)]\tDiscriminator Loss: 0.489014\tGenerator Loss: 1.493475\n",
      "Train Epoch: 35 [0/60000 (0%)]\tDiscriminator Loss: 0.603638\tGenerator Loss: 1.592548\n",
      "Train Epoch: 35 [1280/60000 (2%)]\tDiscriminator Loss: 0.529122\tGenerator Loss: 1.248867\n",
      "Train Epoch: 35 [2560/60000 (4%)]\tDiscriminator Loss: 0.525217\tGenerator Loss: 1.230718\n",
      "Train Epoch: 35 [3840/60000 (6%)]\tDiscriminator Loss: 0.539741\tGenerator Loss: 1.362280\n",
      "Train Epoch: 35 [5120/60000 (9%)]\tDiscriminator Loss: 0.577207\tGenerator Loss: 1.301013\n",
      "Train Epoch: 35 [6400/60000 (11%)]\tDiscriminator Loss: 0.482377\tGenerator Loss: 1.183221\n",
      "Train Epoch: 35 [7680/60000 (13%)]\tDiscriminator Loss: 0.522983\tGenerator Loss: 1.306010\n",
      "Train Epoch: 35 [8960/60000 (15%)]\tDiscriminator Loss: 0.549300\tGenerator Loss: 1.259021\n",
      "Train Epoch: 35 [10240/60000 (17%)]\tDiscriminator Loss: 0.503399\tGenerator Loss: 1.458664\n",
      "Train Epoch: 35 [11520/60000 (19%)]\tDiscriminator Loss: 0.544269\tGenerator Loss: 1.061503\n",
      "Train Epoch: 35 [12800/60000 (21%)]\tDiscriminator Loss: 0.525262\tGenerator Loss: 1.158687\n",
      "Train Epoch: 35 [14080/60000 (23%)]\tDiscriminator Loss: 0.541646\tGenerator Loss: 0.926147\n",
      "Train Epoch: 35 [15360/60000 (26%)]\tDiscriminator Loss: 0.499793\tGenerator Loss: 1.159605\n",
      "Train Epoch: 35 [16640/60000 (28%)]\tDiscriminator Loss: 0.585083\tGenerator Loss: 1.612394\n",
      "Train Epoch: 35 [17920/60000 (30%)]\tDiscriminator Loss: 0.535802\tGenerator Loss: 1.489423\n",
      "Train Epoch: 35 [19200/60000 (32%)]\tDiscriminator Loss: 0.549214\tGenerator Loss: 1.097696\n",
      "Train Epoch: 35 [20480/60000 (34%)]\tDiscriminator Loss: 0.497069\tGenerator Loss: 1.056307\n",
      "Train Epoch: 35 [21760/60000 (36%)]\tDiscriminator Loss: 0.597721\tGenerator Loss: 0.828230\n",
      "Train Epoch: 35 [23040/60000 (38%)]\tDiscriminator Loss: 0.553453\tGenerator Loss: 1.094920\n",
      "Train Epoch: 35 [24320/60000 (41%)]\tDiscriminator Loss: 0.493466\tGenerator Loss: 1.263450\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tDiscriminator Loss: 0.651308\tGenerator Loss: 0.780225\n",
      "Train Epoch: 35 [26880/60000 (45%)]\tDiscriminator Loss: 0.536173\tGenerator Loss: 1.411711\n",
      "Train Epoch: 35 [28160/60000 (47%)]\tDiscriminator Loss: 0.527402\tGenerator Loss: 1.191891\n",
      "Train Epoch: 35 [29440/60000 (49%)]\tDiscriminator Loss: 0.628135\tGenerator Loss: 0.778937\n",
      "Train Epoch: 35 [30720/60000 (51%)]\tDiscriminator Loss: 0.498181\tGenerator Loss: 1.206732\n",
      "Train Epoch: 35 [32000/60000 (53%)]\tDiscriminator Loss: 0.490142\tGenerator Loss: 1.293128\n",
      "Train Epoch: 35 [33280/60000 (55%)]\tDiscriminator Loss: 0.582208\tGenerator Loss: 1.052990\n",
      "Train Epoch: 35 [34560/60000 (58%)]\tDiscriminator Loss: 0.682534\tGenerator Loss: 1.061200\n",
      "Train Epoch: 35 [35840/60000 (60%)]\tDiscriminator Loss: 0.519065\tGenerator Loss: 1.395121\n",
      "Train Epoch: 35 [37120/60000 (62%)]\tDiscriminator Loss: 0.477183\tGenerator Loss: 1.436871\n",
      "Train Epoch: 35 [38400/60000 (64%)]\tDiscriminator Loss: 0.541927\tGenerator Loss: 1.404325\n",
      "Train Epoch: 35 [39680/60000 (66%)]\tDiscriminator Loss: 0.507792\tGenerator Loss: 1.318139\n",
      "Train Epoch: 35 [40960/60000 (68%)]\tDiscriminator Loss: 0.530075\tGenerator Loss: 1.099877\n",
      "Train Epoch: 35 [42240/60000 (70%)]\tDiscriminator Loss: 0.488635\tGenerator Loss: 1.566708\n",
      "Train Epoch: 35 [43520/60000 (72%)]\tDiscriminator Loss: 0.520389\tGenerator Loss: 1.551346\n",
      "Train Epoch: 35 [44800/60000 (75%)]\tDiscriminator Loss: 0.566169\tGenerator Loss: 1.544432\n",
      "Train Epoch: 35 [46080/60000 (77%)]\tDiscriminator Loss: 0.525811\tGenerator Loss: 1.541607\n",
      "Train Epoch: 35 [47360/60000 (79%)]\tDiscriminator Loss: 0.480385\tGenerator Loss: 1.432090\n",
      "Train Epoch: 35 [48640/60000 (81%)]\tDiscriminator Loss: 0.548889\tGenerator Loss: 1.240176\n",
      "Train Epoch: 35 [49920/60000 (83%)]\tDiscriminator Loss: 0.564256\tGenerator Loss: 1.342536\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tDiscriminator Loss: 0.516225\tGenerator Loss: 1.288320\n",
      "Train Epoch: 35 [52480/60000 (87%)]\tDiscriminator Loss: 0.573009\tGenerator Loss: 0.878280\n",
      "Train Epoch: 35 [53760/60000 (90%)]\tDiscriminator Loss: 0.555226\tGenerator Loss: 1.037075\n",
      "Train Epoch: 35 [55040/60000 (92%)]\tDiscriminator Loss: 0.535965\tGenerator Loss: 1.145168\n",
      "Train Epoch: 35 [56320/60000 (94%)]\tDiscriminator Loss: 0.565894\tGenerator Loss: 1.169444\n",
      "Train Epoch: 35 [57600/60000 (96%)]\tDiscriminator Loss: 0.595196\tGenerator Loss: 1.202005\n",
      "Train Epoch: 35 [58880/60000 (98%)]\tDiscriminator Loss: 0.491799\tGenerator Loss: 1.125324\n",
      "Train Epoch: 36 [0/60000 (0%)]\tDiscriminator Loss: 0.485864\tGenerator Loss: 1.498374\n",
      "Train Epoch: 36 [1280/60000 (2%)]\tDiscriminator Loss: 0.610534\tGenerator Loss: 0.979028\n",
      "Train Epoch: 36 [2560/60000 (4%)]\tDiscriminator Loss: 0.560057\tGenerator Loss: 1.130479\n",
      "Train Epoch: 36 [3840/60000 (6%)]\tDiscriminator Loss: 0.482642\tGenerator Loss: 1.401870\n",
      "Train Epoch: 36 [5120/60000 (9%)]\tDiscriminator Loss: 0.491443\tGenerator Loss: 1.400588\n",
      "Train Epoch: 36 [6400/60000 (11%)]\tDiscriminator Loss: 0.588103\tGenerator Loss: 1.412157\n",
      "Train Epoch: 36 [7680/60000 (13%)]\tDiscriminator Loss: 0.526762\tGenerator Loss: 1.143077\n",
      "Train Epoch: 36 [8960/60000 (15%)]\tDiscriminator Loss: 0.526569\tGenerator Loss: 1.618537\n",
      "Train Epoch: 36 [10240/60000 (17%)]\tDiscriminator Loss: 0.560578\tGenerator Loss: 1.538308\n",
      "Train Epoch: 36 [11520/60000 (19%)]\tDiscriminator Loss: 0.539649\tGenerator Loss: 1.526194\n",
      "Train Epoch: 36 [12800/60000 (21%)]\tDiscriminator Loss: 0.504262\tGenerator Loss: 1.161046\n",
      "Train Epoch: 36 [14080/60000 (23%)]\tDiscriminator Loss: 0.577178\tGenerator Loss: 1.124982\n",
      "Train Epoch: 36 [15360/60000 (26%)]\tDiscriminator Loss: 0.645582\tGenerator Loss: 1.209151\n",
      "Train Epoch: 36 [16640/60000 (28%)]\tDiscriminator Loss: 0.505926\tGenerator Loss: 1.299476\n",
      "Train Epoch: 36 [17920/60000 (30%)]\tDiscriminator Loss: 0.504927\tGenerator Loss: 1.216226\n",
      "Train Epoch: 36 [19200/60000 (32%)]\tDiscriminator Loss: 0.529681\tGenerator Loss: 1.451175\n",
      "Train Epoch: 36 [20480/60000 (34%)]\tDiscriminator Loss: 0.609867\tGenerator Loss: 0.800666\n",
      "Train Epoch: 36 [21760/60000 (36%)]\tDiscriminator Loss: 0.582961\tGenerator Loss: 0.727189\n",
      "Train Epoch: 36 [23040/60000 (38%)]\tDiscriminator Loss: 0.498038\tGenerator Loss: 1.290395\n",
      "Train Epoch: 36 [24320/60000 (41%)]\tDiscriminator Loss: 0.570235\tGenerator Loss: 1.686855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 36 [25600/60000 (43%)]\tDiscriminator Loss: 0.574801\tGenerator Loss: 1.120669\n",
      "Train Epoch: 36 [26880/60000 (45%)]\tDiscriminator Loss: 0.532130\tGenerator Loss: 1.151936\n",
      "Train Epoch: 36 [28160/60000 (47%)]\tDiscriminator Loss: 0.545866\tGenerator Loss: 1.072077\n",
      "Train Epoch: 36 [29440/60000 (49%)]\tDiscriminator Loss: 0.539783\tGenerator Loss: 1.432356\n",
      "Train Epoch: 36 [30720/60000 (51%)]\tDiscriminator Loss: 0.651303\tGenerator Loss: 0.942442\n",
      "Train Epoch: 36 [32000/60000 (53%)]\tDiscriminator Loss: 0.510616\tGenerator Loss: 1.151716\n",
      "Train Epoch: 36 [33280/60000 (55%)]\tDiscriminator Loss: 0.503060\tGenerator Loss: 1.208180\n",
      "Train Epoch: 36 [34560/60000 (58%)]\tDiscriminator Loss: 0.519907\tGenerator Loss: 1.185765\n",
      "Train Epoch: 36 [35840/60000 (60%)]\tDiscriminator Loss: 0.571610\tGenerator Loss: 1.069398\n",
      "Train Epoch: 36 [37120/60000 (62%)]\tDiscriminator Loss: 0.559675\tGenerator Loss: 1.081008\n",
      "Train Epoch: 36 [38400/60000 (64%)]\tDiscriminator Loss: 0.526467\tGenerator Loss: 1.613571\n",
      "Train Epoch: 36 [39680/60000 (66%)]\tDiscriminator Loss: 0.543779\tGenerator Loss: 1.193044\n",
      "Train Epoch: 36 [40960/60000 (68%)]\tDiscriminator Loss: 0.533802\tGenerator Loss: 1.315759\n",
      "Train Epoch: 36 [42240/60000 (70%)]\tDiscriminator Loss: 0.503166\tGenerator Loss: 1.214590\n",
      "Train Epoch: 36 [43520/60000 (72%)]\tDiscriminator Loss: 0.503497\tGenerator Loss: 1.447289\n",
      "Train Epoch: 36 [44800/60000 (75%)]\tDiscriminator Loss: 0.526590\tGenerator Loss: 1.599774\n",
      "Train Epoch: 36 [46080/60000 (77%)]\tDiscriminator Loss: 0.548322\tGenerator Loss: 1.104939\n",
      "Train Epoch: 36 [47360/60000 (79%)]\tDiscriminator Loss: 0.562196\tGenerator Loss: 1.035587\n",
      "Train Epoch: 36 [48640/60000 (81%)]\tDiscriminator Loss: 0.554600\tGenerator Loss: 1.129209\n",
      "Train Epoch: 36 [49920/60000 (83%)]\tDiscriminator Loss: 0.474120\tGenerator Loss: 1.389826\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tDiscriminator Loss: 0.459021\tGenerator Loss: 1.264131\n",
      "Train Epoch: 36 [52480/60000 (87%)]\tDiscriminator Loss: 0.539039\tGenerator Loss: 1.398007\n",
      "Train Epoch: 36 [53760/60000 (90%)]\tDiscriminator Loss: 0.526332\tGenerator Loss: 1.653446\n",
      "Train Epoch: 36 [55040/60000 (92%)]\tDiscriminator Loss: 0.515474\tGenerator Loss: 1.278958\n",
      "Train Epoch: 36 [56320/60000 (94%)]\tDiscriminator Loss: 0.554740\tGenerator Loss: 1.287178\n",
      "Train Epoch: 36 [57600/60000 (96%)]\tDiscriminator Loss: 0.522957\tGenerator Loss: 1.038206\n",
      "Train Epoch: 36 [58880/60000 (98%)]\tDiscriminator Loss: 0.548933\tGenerator Loss: 0.961775\n",
      "Train Epoch: 37 [0/60000 (0%)]\tDiscriminator Loss: 0.555336\tGenerator Loss: 0.844195\n",
      "Train Epoch: 37 [1280/60000 (2%)]\tDiscriminator Loss: 0.527810\tGenerator Loss: 1.470509\n",
      "Train Epoch: 37 [2560/60000 (4%)]\tDiscriminator Loss: 0.549167\tGenerator Loss: 1.069964\n",
      "Train Epoch: 37 [3840/60000 (6%)]\tDiscriminator Loss: 0.617129\tGenerator Loss: 0.773938\n",
      "Train Epoch: 37 [5120/60000 (9%)]\tDiscriminator Loss: 0.521588\tGenerator Loss: 1.337565\n",
      "Train Epoch: 37 [6400/60000 (11%)]\tDiscriminator Loss: 0.577638\tGenerator Loss: 0.805719\n",
      "Train Epoch: 37 [7680/60000 (13%)]\tDiscriminator Loss: 0.493016\tGenerator Loss: 1.101890\n",
      "Train Epoch: 37 [8960/60000 (15%)]\tDiscriminator Loss: 0.490587\tGenerator Loss: 1.115462\n",
      "Train Epoch: 37 [10240/60000 (17%)]\tDiscriminator Loss: 0.529333\tGenerator Loss: 0.974575\n",
      "Train Epoch: 37 [11520/60000 (19%)]\tDiscriminator Loss: 0.500810\tGenerator Loss: 1.055075\n",
      "Train Epoch: 37 [12800/60000 (21%)]\tDiscriminator Loss: 0.535333\tGenerator Loss: 1.369449\n",
      "Train Epoch: 37 [14080/60000 (23%)]\tDiscriminator Loss: 0.593544\tGenerator Loss: 1.473542\n",
      "Train Epoch: 37 [15360/60000 (26%)]\tDiscriminator Loss: 0.540781\tGenerator Loss: 1.704753\n",
      "Train Epoch: 37 [16640/60000 (28%)]\tDiscriminator Loss: 0.497402\tGenerator Loss: 1.118364\n",
      "Train Epoch: 37 [17920/60000 (30%)]\tDiscriminator Loss: 0.530018\tGenerator Loss: 1.465607\n",
      "Train Epoch: 37 [19200/60000 (32%)]\tDiscriminator Loss: 0.527291\tGenerator Loss: 1.563949\n",
      "Train Epoch: 37 [20480/60000 (34%)]\tDiscriminator Loss: 0.502283\tGenerator Loss: 1.345786\n",
      "Train Epoch: 37 [21760/60000 (36%)]\tDiscriminator Loss: 0.540676\tGenerator Loss: 1.052316\n",
      "Train Epoch: 37 [23040/60000 (38%)]\tDiscriminator Loss: 0.518883\tGenerator Loss: 1.003663\n",
      "Train Epoch: 37 [24320/60000 (41%)]\tDiscriminator Loss: 0.501364\tGenerator Loss: 1.425702\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tDiscriminator Loss: 0.547641\tGenerator Loss: 1.144006\n",
      "Train Epoch: 37 [26880/60000 (45%)]\tDiscriminator Loss: 0.624904\tGenerator Loss: 1.056922\n",
      "Train Epoch: 37 [28160/60000 (47%)]\tDiscriminator Loss: 0.586638\tGenerator Loss: 1.360369\n",
      "Train Epoch: 37 [29440/60000 (49%)]\tDiscriminator Loss: 0.507824\tGenerator Loss: 1.101896\n",
      "Train Epoch: 37 [30720/60000 (51%)]\tDiscriminator Loss: 0.528477\tGenerator Loss: 1.236228\n",
      "Train Epoch: 37 [32000/60000 (53%)]\tDiscriminator Loss: 0.495852\tGenerator Loss: 1.245661\n",
      "Train Epoch: 37 [33280/60000 (55%)]\tDiscriminator Loss: 0.530994\tGenerator Loss: 1.351995\n",
      "Train Epoch: 37 [34560/60000 (58%)]\tDiscriminator Loss: 0.541501\tGenerator Loss: 0.910702\n",
      "Train Epoch: 37 [35840/60000 (60%)]\tDiscriminator Loss: 0.558999\tGenerator Loss: 1.043931\n",
      "Train Epoch: 37 [37120/60000 (62%)]\tDiscriminator Loss: 0.564447\tGenerator Loss: 1.479272\n",
      "Train Epoch: 37 [38400/60000 (64%)]\tDiscriminator Loss: 0.501696\tGenerator Loss: 1.232601\n",
      "Train Epoch: 37 [39680/60000 (66%)]\tDiscriminator Loss: 0.577491\tGenerator Loss: 0.839303\n",
      "Train Epoch: 37 [40960/60000 (68%)]\tDiscriminator Loss: 0.556857\tGenerator Loss: 1.089452\n",
      "Train Epoch: 37 [42240/60000 (70%)]\tDiscriminator Loss: 0.532078\tGenerator Loss: 1.303168\n",
      "Train Epoch: 37 [43520/60000 (72%)]\tDiscriminator Loss: 0.554807\tGenerator Loss: 1.712383\n",
      "Train Epoch: 37 [44800/60000 (75%)]\tDiscriminator Loss: 0.538545\tGenerator Loss: 0.981343\n",
      "Train Epoch: 37 [46080/60000 (77%)]\tDiscriminator Loss: 0.535727\tGenerator Loss: 1.121599\n",
      "Train Epoch: 37 [47360/60000 (79%)]\tDiscriminator Loss: 0.599565\tGenerator Loss: 1.416089\n",
      "Train Epoch: 37 [48640/60000 (81%)]\tDiscriminator Loss: 0.501945\tGenerator Loss: 1.146343\n",
      "Train Epoch: 37 [49920/60000 (83%)]\tDiscriminator Loss: 0.567474\tGenerator Loss: 0.986172\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tDiscriminator Loss: 0.629968\tGenerator Loss: 1.153519\n",
      "Train Epoch: 37 [52480/60000 (87%)]\tDiscriminator Loss: 0.536630\tGenerator Loss: 1.332265\n",
      "Train Epoch: 37 [53760/60000 (90%)]\tDiscriminator Loss: 0.488077\tGenerator Loss: 1.426800\n",
      "Train Epoch: 37 [55040/60000 (92%)]\tDiscriminator Loss: 0.498333\tGenerator Loss: 1.145655\n",
      "Train Epoch: 37 [56320/60000 (94%)]\tDiscriminator Loss: 0.629036\tGenerator Loss: 1.308831\n",
      "Train Epoch: 37 [57600/60000 (96%)]\tDiscriminator Loss: 0.472040\tGenerator Loss: 1.203838\n",
      "Train Epoch: 37 [58880/60000 (98%)]\tDiscriminator Loss: 0.498373\tGenerator Loss: 1.171744\n",
      "Train Epoch: 38 [0/60000 (0%)]\tDiscriminator Loss: 0.534695\tGenerator Loss: 1.351550\n",
      "Train Epoch: 38 [1280/60000 (2%)]\tDiscriminator Loss: 0.561853\tGenerator Loss: 0.774524\n",
      "Train Epoch: 38 [2560/60000 (4%)]\tDiscriminator Loss: 0.547419\tGenerator Loss: 1.482381\n",
      "Train Epoch: 38 [3840/60000 (6%)]\tDiscriminator Loss: 0.517080\tGenerator Loss: 1.222098\n",
      "Train Epoch: 38 [5120/60000 (9%)]\tDiscriminator Loss: 0.532071\tGenerator Loss: 1.194101\n",
      "Train Epoch: 38 [6400/60000 (11%)]\tDiscriminator Loss: 0.466733\tGenerator Loss: 1.272708\n",
      "Train Epoch: 38 [7680/60000 (13%)]\tDiscriminator Loss: 0.482729\tGenerator Loss: 1.565799\n",
      "Train Epoch: 38 [8960/60000 (15%)]\tDiscriminator Loss: 0.613758\tGenerator Loss: 1.327130\n",
      "Train Epoch: 38 [10240/60000 (17%)]\tDiscriminator Loss: 0.497239\tGenerator Loss: 1.149046\n",
      "Train Epoch: 38 [11520/60000 (19%)]\tDiscriminator Loss: 0.512205\tGenerator Loss: 1.410489\n",
      "Train Epoch: 38 [12800/60000 (21%)]\tDiscriminator Loss: 0.523960\tGenerator Loss: 1.272072\n",
      "Train Epoch: 38 [14080/60000 (23%)]\tDiscriminator Loss: 0.561560\tGenerator Loss: 1.288001\n",
      "Train Epoch: 38 [15360/60000 (26%)]\tDiscriminator Loss: 0.556877\tGenerator Loss: 1.354822\n",
      "Train Epoch: 38 [16640/60000 (28%)]\tDiscriminator Loss: 0.511080\tGenerator Loss: 1.175597\n",
      "Train Epoch: 38 [17920/60000 (30%)]\tDiscriminator Loss: 0.506710\tGenerator Loss: 1.260813\n",
      "Train Epoch: 38 [19200/60000 (32%)]\tDiscriminator Loss: 0.598637\tGenerator Loss: 0.830864\n",
      "Train Epoch: 38 [20480/60000 (34%)]\tDiscriminator Loss: 0.535172\tGenerator Loss: 1.074879\n",
      "Train Epoch: 38 [21760/60000 (36%)]\tDiscriminator Loss: 0.478262\tGenerator Loss: 1.430283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 38 [23040/60000 (38%)]\tDiscriminator Loss: 0.541207\tGenerator Loss: 0.978309\n",
      "Train Epoch: 38 [24320/60000 (41%)]\tDiscriminator Loss: 0.495470\tGenerator Loss: 1.081087\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tDiscriminator Loss: 0.468695\tGenerator Loss: 1.315679\n",
      "Train Epoch: 38 [26880/60000 (45%)]\tDiscriminator Loss: 0.524727\tGenerator Loss: 1.491902\n",
      "Train Epoch: 38 [28160/60000 (47%)]\tDiscriminator Loss: 0.508819\tGenerator Loss: 1.471809\n",
      "Train Epoch: 38 [29440/60000 (49%)]\tDiscriminator Loss: 0.544438\tGenerator Loss: 1.123309\n",
      "Train Epoch: 38 [30720/60000 (51%)]\tDiscriminator Loss: 0.453888\tGenerator Loss: 1.284091\n",
      "Train Epoch: 38 [32000/60000 (53%)]\tDiscriminator Loss: 0.563134\tGenerator Loss: 1.185526\n",
      "Train Epoch: 38 [33280/60000 (55%)]\tDiscriminator Loss: 0.550307\tGenerator Loss: 1.491539\n",
      "Train Epoch: 38 [34560/60000 (58%)]\tDiscriminator Loss: 0.456753\tGenerator Loss: 1.236915\n",
      "Train Epoch: 38 [35840/60000 (60%)]\tDiscriminator Loss: 0.533721\tGenerator Loss: 1.147851\n",
      "Train Epoch: 38 [37120/60000 (62%)]\tDiscriminator Loss: 0.579305\tGenerator Loss: 1.122473\n",
      "Train Epoch: 38 [38400/60000 (64%)]\tDiscriminator Loss: 0.540592\tGenerator Loss: 1.650093\n",
      "Train Epoch: 38 [39680/60000 (66%)]\tDiscriminator Loss: 0.460158\tGenerator Loss: 1.202707\n",
      "Train Epoch: 38 [40960/60000 (68%)]\tDiscriminator Loss: 0.593429\tGenerator Loss: 0.902325\n",
      "Train Epoch: 38 [42240/60000 (70%)]\tDiscriminator Loss: 0.611547\tGenerator Loss: 0.942336\n",
      "Train Epoch: 38 [43520/60000 (72%)]\tDiscriminator Loss: 0.531161\tGenerator Loss: 1.167678\n",
      "Train Epoch: 38 [44800/60000 (75%)]\tDiscriminator Loss: 0.517863\tGenerator Loss: 1.239346\n",
      "Train Epoch: 38 [46080/60000 (77%)]\tDiscriminator Loss: 0.495832\tGenerator Loss: 1.154433\n",
      "Train Epoch: 38 [47360/60000 (79%)]\tDiscriminator Loss: 0.576717\tGenerator Loss: 1.198570\n",
      "Train Epoch: 38 [48640/60000 (81%)]\tDiscriminator Loss: 0.571505\tGenerator Loss: 1.160757\n",
      "Train Epoch: 38 [49920/60000 (83%)]\tDiscriminator Loss: 0.568818\tGenerator Loss: 1.373431\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tDiscriminator Loss: 0.504070\tGenerator Loss: 1.386753\n",
      "Train Epoch: 38 [52480/60000 (87%)]\tDiscriminator Loss: 0.499340\tGenerator Loss: 1.209106\n",
      "Train Epoch: 38 [53760/60000 (90%)]\tDiscriminator Loss: 0.554566\tGenerator Loss: 1.677435\n",
      "Train Epoch: 38 [55040/60000 (92%)]\tDiscriminator Loss: 0.467158\tGenerator Loss: 1.162974\n",
      "Train Epoch: 38 [56320/60000 (94%)]\tDiscriminator Loss: 0.541435\tGenerator Loss: 1.016429\n",
      "Train Epoch: 38 [57600/60000 (96%)]\tDiscriminator Loss: 0.533181\tGenerator Loss: 1.164733\n",
      "Train Epoch: 38 [58880/60000 (98%)]\tDiscriminator Loss: 0.578966\tGenerator Loss: 1.245866\n",
      "Train Epoch: 39 [0/60000 (0%)]\tDiscriminator Loss: 0.533385\tGenerator Loss: 1.149657\n",
      "Train Epoch: 39 [1280/60000 (2%)]\tDiscriminator Loss: 0.561127\tGenerator Loss: 0.935492\n",
      "Train Epoch: 39 [2560/60000 (4%)]\tDiscriminator Loss: 0.535057\tGenerator Loss: 1.226756\n",
      "Train Epoch: 39 [3840/60000 (6%)]\tDiscriminator Loss: 0.493943\tGenerator Loss: 1.341398\n",
      "Train Epoch: 39 [5120/60000 (9%)]\tDiscriminator Loss: 0.605423\tGenerator Loss: 1.835367\n",
      "Train Epoch: 39 [6400/60000 (11%)]\tDiscriminator Loss: 0.558475\tGenerator Loss: 1.221202\n",
      "Train Epoch: 39 [7680/60000 (13%)]\tDiscriminator Loss: 0.557037\tGenerator Loss: 1.181868\n",
      "Train Epoch: 39 [8960/60000 (15%)]\tDiscriminator Loss: 0.474664\tGenerator Loss: 1.494865\n",
      "Train Epoch: 39 [10240/60000 (17%)]\tDiscriminator Loss: 0.517273\tGenerator Loss: 1.183538\n",
      "Train Epoch: 39 [11520/60000 (19%)]\tDiscriminator Loss: 0.536412\tGenerator Loss: 1.188942\n",
      "Train Epoch: 39 [12800/60000 (21%)]\tDiscriminator Loss: 0.525539\tGenerator Loss: 1.270223\n",
      "Train Epoch: 39 [14080/60000 (23%)]\tDiscriminator Loss: 0.531654\tGenerator Loss: 1.026201\n",
      "Train Epoch: 39 [15360/60000 (26%)]\tDiscriminator Loss: 0.524069\tGenerator Loss: 1.207078\n",
      "Train Epoch: 39 [16640/60000 (28%)]\tDiscriminator Loss: 0.500088\tGenerator Loss: 1.446481\n",
      "Train Epoch: 39 [17920/60000 (30%)]\tDiscriminator Loss: 0.636315\tGenerator Loss: 0.870751\n",
      "Train Epoch: 39 [19200/60000 (32%)]\tDiscriminator Loss: 0.647426\tGenerator Loss: 0.670563\n",
      "Train Epoch: 39 [20480/60000 (34%)]\tDiscriminator Loss: 0.494231\tGenerator Loss: 1.234975\n",
      "Train Epoch: 39 [21760/60000 (36%)]\tDiscriminator Loss: 0.488275\tGenerator Loss: 1.131870\n",
      "Train Epoch: 39 [23040/60000 (38%)]\tDiscriminator Loss: 0.486138\tGenerator Loss: 1.194336\n",
      "Train Epoch: 39 [24320/60000 (41%)]\tDiscriminator Loss: 0.560247\tGenerator Loss: 1.160586\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tDiscriminator Loss: 0.548580\tGenerator Loss: 1.401961\n",
      "Train Epoch: 39 [26880/60000 (45%)]\tDiscriminator Loss: 0.492333\tGenerator Loss: 1.052745\n",
      "Train Epoch: 39 [28160/60000 (47%)]\tDiscriminator Loss: 0.519883\tGenerator Loss: 1.226156\n",
      "Train Epoch: 39 [29440/60000 (49%)]\tDiscriminator Loss: 0.517472\tGenerator Loss: 1.488595\n",
      "Train Epoch: 39 [30720/60000 (51%)]\tDiscriminator Loss: 0.565207\tGenerator Loss: 0.937050\n",
      "Train Epoch: 39 [32000/60000 (53%)]\tDiscriminator Loss: 0.528829\tGenerator Loss: 1.011858\n",
      "Train Epoch: 39 [33280/60000 (55%)]\tDiscriminator Loss: 0.549295\tGenerator Loss: 0.949407\n",
      "Train Epoch: 39 [34560/60000 (58%)]\tDiscriminator Loss: 0.465409\tGenerator Loss: 1.500201\n",
      "Train Epoch: 39 [35840/60000 (60%)]\tDiscriminator Loss: 0.587535\tGenerator Loss: 1.497206\n",
      "Train Epoch: 39 [37120/60000 (62%)]\tDiscriminator Loss: 0.476545\tGenerator Loss: 1.184321\n",
      "Train Epoch: 39 [38400/60000 (64%)]\tDiscriminator Loss: 0.481963\tGenerator Loss: 1.379725\n",
      "Train Epoch: 39 [39680/60000 (66%)]\tDiscriminator Loss: 0.526687\tGenerator Loss: 1.124628\n",
      "Train Epoch: 39 [40960/60000 (68%)]\tDiscriminator Loss: 0.522427\tGenerator Loss: 1.234528\n",
      "Train Epoch: 39 [42240/60000 (70%)]\tDiscriminator Loss: 0.563873\tGenerator Loss: 1.333176\n",
      "Train Epoch: 39 [43520/60000 (72%)]\tDiscriminator Loss: 0.571752\tGenerator Loss: 1.092802\n",
      "Train Epoch: 39 [44800/60000 (75%)]\tDiscriminator Loss: 0.452458\tGenerator Loss: 1.356199\n",
      "Train Epoch: 39 [46080/60000 (77%)]\tDiscriminator Loss: 0.491744\tGenerator Loss: 1.402242\n",
      "Train Epoch: 39 [47360/60000 (79%)]\tDiscriminator Loss: 0.559541\tGenerator Loss: 1.327797\n",
      "Train Epoch: 39 [48640/60000 (81%)]\tDiscriminator Loss: 0.533407\tGenerator Loss: 1.251407\n",
      "Train Epoch: 39 [49920/60000 (83%)]\tDiscriminator Loss: 0.502426\tGenerator Loss: 1.610123\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tDiscriminator Loss: 0.572506\tGenerator Loss: 1.293233\n",
      "Train Epoch: 39 [52480/60000 (87%)]\tDiscriminator Loss: 0.570109\tGenerator Loss: 1.075955\n",
      "Train Epoch: 39 [53760/60000 (90%)]\tDiscriminator Loss: 0.535304\tGenerator Loss: 1.315539\n",
      "Train Epoch: 39 [55040/60000 (92%)]\tDiscriminator Loss: 0.551531\tGenerator Loss: 0.965062\n",
      "Train Epoch: 39 [56320/60000 (94%)]\tDiscriminator Loss: 0.519654\tGenerator Loss: 1.193623\n",
      "Train Epoch: 39 [57600/60000 (96%)]\tDiscriminator Loss: 0.484271\tGenerator Loss: 1.164526\n",
      "Train Epoch: 39 [58880/60000 (98%)]\tDiscriminator Loss: 0.580114\tGenerator Loss: 1.627002\n",
      "Train Epoch: 40 [0/60000 (0%)]\tDiscriminator Loss: 0.490287\tGenerator Loss: 1.039407\n",
      "Train Epoch: 40 [1280/60000 (2%)]\tDiscriminator Loss: 0.534842\tGenerator Loss: 1.461690\n",
      "Train Epoch: 40 [2560/60000 (4%)]\tDiscriminator Loss: 0.557038\tGenerator Loss: 1.361801\n",
      "Train Epoch: 40 [3840/60000 (6%)]\tDiscriminator Loss: 0.533678\tGenerator Loss: 1.300775\n",
      "Train Epoch: 40 [5120/60000 (9%)]\tDiscriminator Loss: 0.544079\tGenerator Loss: 1.052897\n",
      "Train Epoch: 40 [6400/60000 (11%)]\tDiscriminator Loss: 0.542154\tGenerator Loss: 1.023092\n",
      "Train Epoch: 40 [7680/60000 (13%)]\tDiscriminator Loss: 0.608431\tGenerator Loss: 0.831845\n",
      "Train Epoch: 40 [8960/60000 (15%)]\tDiscriminator Loss: 0.497583\tGenerator Loss: 1.286468\n",
      "Train Epoch: 40 [10240/60000 (17%)]\tDiscriminator Loss: 0.488974\tGenerator Loss: 1.197204\n",
      "Train Epoch: 40 [11520/60000 (19%)]\tDiscriminator Loss: 0.539139\tGenerator Loss: 1.009127\n",
      "Train Epoch: 40 [12800/60000 (21%)]\tDiscriminator Loss: 0.544632\tGenerator Loss: 1.416490\n",
      "Train Epoch: 40 [14080/60000 (23%)]\tDiscriminator Loss: 0.481878\tGenerator Loss: 1.157007\n",
      "Train Epoch: 40 [15360/60000 (26%)]\tDiscriminator Loss: 0.509632\tGenerator Loss: 1.018821\n",
      "Train Epoch: 40 [16640/60000 (28%)]\tDiscriminator Loss: 0.576686\tGenerator Loss: 0.977365\n",
      "Train Epoch: 40 [17920/60000 (30%)]\tDiscriminator Loss: 0.561918\tGenerator Loss: 1.021667\n",
      "Train Epoch: 40 [19200/60000 (32%)]\tDiscriminator Loss: 0.500869\tGenerator Loss: 1.016790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 40 [20480/60000 (34%)]\tDiscriminator Loss: 0.499784\tGenerator Loss: 1.400152\n",
      "Train Epoch: 40 [21760/60000 (36%)]\tDiscriminator Loss: 0.549726\tGenerator Loss: 1.237000\n",
      "Train Epoch: 40 [23040/60000 (38%)]\tDiscriminator Loss: 0.547594\tGenerator Loss: 1.120533\n",
      "Train Epoch: 40 [24320/60000 (41%)]\tDiscriminator Loss: 0.612204\tGenerator Loss: 0.743107\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tDiscriminator Loss: 0.556117\tGenerator Loss: 0.939433\n",
      "Train Epoch: 40 [26880/60000 (45%)]\tDiscriminator Loss: 0.489509\tGenerator Loss: 1.493170\n",
      "Train Epoch: 40 [28160/60000 (47%)]\tDiscriminator Loss: 0.514806\tGenerator Loss: 1.756642\n",
      "Train Epoch: 40 [29440/60000 (49%)]\tDiscriminator Loss: 0.594796\tGenerator Loss: 1.003507\n",
      "Train Epoch: 40 [30720/60000 (51%)]\tDiscriminator Loss: 0.552098\tGenerator Loss: 0.991408\n",
      "Train Epoch: 40 [32000/60000 (53%)]\tDiscriminator Loss: 0.536288\tGenerator Loss: 1.000882\n",
      "Train Epoch: 40 [33280/60000 (55%)]\tDiscriminator Loss: 0.565545\tGenerator Loss: 1.128891\n",
      "Train Epoch: 40 [34560/60000 (58%)]\tDiscriminator Loss: 0.563189\tGenerator Loss: 1.048133\n",
      "Train Epoch: 40 [35840/60000 (60%)]\tDiscriminator Loss: 0.543847\tGenerator Loss: 1.233484\n",
      "Train Epoch: 40 [37120/60000 (62%)]\tDiscriminator Loss: 0.479802\tGenerator Loss: 1.162821\n",
      "Train Epoch: 40 [38400/60000 (64%)]\tDiscriminator Loss: 0.547679\tGenerator Loss: 1.147061\n",
      "Train Epoch: 40 [39680/60000 (66%)]\tDiscriminator Loss: 0.568504\tGenerator Loss: 1.080665\n",
      "Train Epoch: 40 [40960/60000 (68%)]\tDiscriminator Loss: 0.471248\tGenerator Loss: 1.161861\n",
      "Train Epoch: 40 [42240/60000 (70%)]\tDiscriminator Loss: 0.582231\tGenerator Loss: 0.814741\n",
      "Train Epoch: 40 [43520/60000 (72%)]\tDiscriminator Loss: 0.492658\tGenerator Loss: 1.471221\n",
      "Train Epoch: 40 [44800/60000 (75%)]\tDiscriminator Loss: 0.531071\tGenerator Loss: 1.467503\n",
      "Train Epoch: 40 [46080/60000 (77%)]\tDiscriminator Loss: 0.593586\tGenerator Loss: 1.181367\n",
      "Train Epoch: 40 [47360/60000 (79%)]\tDiscriminator Loss: 0.560142\tGenerator Loss: 1.472606\n",
      "Train Epoch: 40 [48640/60000 (81%)]\tDiscriminator Loss: 0.508530\tGenerator Loss: 1.203948\n",
      "Train Epoch: 40 [49920/60000 (83%)]\tDiscriminator Loss: 0.477326\tGenerator Loss: 1.433243\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tDiscriminator Loss: 0.562434\tGenerator Loss: 1.040511\n",
      "Train Epoch: 40 [52480/60000 (87%)]\tDiscriminator Loss: 0.594764\tGenerator Loss: 1.194945\n",
      "Train Epoch: 40 [53760/60000 (90%)]\tDiscriminator Loss: 0.518184\tGenerator Loss: 1.517148\n",
      "Train Epoch: 40 [55040/60000 (92%)]\tDiscriminator Loss: 0.495163\tGenerator Loss: 1.666656\n",
      "Train Epoch: 40 [56320/60000 (94%)]\tDiscriminator Loss: 0.588773\tGenerator Loss: 1.164446\n",
      "Train Epoch: 40 [57600/60000 (96%)]\tDiscriminator Loss: 0.552308\tGenerator Loss: 1.491769\n",
      "Train Epoch: 40 [58880/60000 (98%)]\tDiscriminator Loss: 0.551247\tGenerator Loss: 1.383387\n",
      "Train Epoch: 41 [0/60000 (0%)]\tDiscriminator Loss: 0.495878\tGenerator Loss: 1.225435\n",
      "Train Epoch: 41 [1280/60000 (2%)]\tDiscriminator Loss: 0.517320\tGenerator Loss: 1.234290\n",
      "Train Epoch: 41 [2560/60000 (4%)]\tDiscriminator Loss: 0.542420\tGenerator Loss: 1.530801\n",
      "Train Epoch: 41 [3840/60000 (6%)]\tDiscriminator Loss: 0.598393\tGenerator Loss: 1.126946\n",
      "Train Epoch: 41 [5120/60000 (9%)]\tDiscriminator Loss: 0.558441\tGenerator Loss: 1.109050\n",
      "Train Epoch: 41 [6400/60000 (11%)]\tDiscriminator Loss: 0.506357\tGenerator Loss: 1.133490\n",
      "Train Epoch: 41 [7680/60000 (13%)]\tDiscriminator Loss: 0.532929\tGenerator Loss: 1.077581\n",
      "Train Epoch: 41 [8960/60000 (15%)]\tDiscriminator Loss: 0.481690\tGenerator Loss: 1.068171\n",
      "Train Epoch: 41 [10240/60000 (17%)]\tDiscriminator Loss: 0.562401\tGenerator Loss: 1.449199\n",
      "Train Epoch: 41 [11520/60000 (19%)]\tDiscriminator Loss: 0.516988\tGenerator Loss: 1.116500\n",
      "Train Epoch: 41 [12800/60000 (21%)]\tDiscriminator Loss: 0.425512\tGenerator Loss: 1.154222\n",
      "Train Epoch: 41 [14080/60000 (23%)]\tDiscriminator Loss: 0.454810\tGenerator Loss: 1.288223\n",
      "Train Epoch: 41 [15360/60000 (26%)]\tDiscriminator Loss: 0.501801\tGenerator Loss: 1.998545\n",
      "Train Epoch: 41 [16640/60000 (28%)]\tDiscriminator Loss: 0.509564\tGenerator Loss: 1.234324\n",
      "Train Epoch: 41 [17920/60000 (30%)]\tDiscriminator Loss: 0.530669\tGenerator Loss: 1.209910\n",
      "Train Epoch: 41 [19200/60000 (32%)]\tDiscriminator Loss: 0.549086\tGenerator Loss: 1.182246\n",
      "Train Epoch: 41 [20480/60000 (34%)]\tDiscriminator Loss: 0.568617\tGenerator Loss: 1.441597\n",
      "Train Epoch: 41 [21760/60000 (36%)]\tDiscriminator Loss: 0.579722\tGenerator Loss: 1.678300\n",
      "Train Epoch: 41 [23040/60000 (38%)]\tDiscriminator Loss: 0.507175\tGenerator Loss: 1.470621\n",
      "Train Epoch: 41 [24320/60000 (41%)]\tDiscriminator Loss: 0.503489\tGenerator Loss: 1.246415\n",
      "Train Epoch: 41 [25600/60000 (43%)]\tDiscriminator Loss: 0.506583\tGenerator Loss: 0.973054\n",
      "Train Epoch: 41 [26880/60000 (45%)]\tDiscriminator Loss: 0.494830\tGenerator Loss: 1.309053\n",
      "Train Epoch: 41 [28160/60000 (47%)]\tDiscriminator Loss: 0.505509\tGenerator Loss: 1.034878\n",
      "Train Epoch: 41 [29440/60000 (49%)]\tDiscriminator Loss: 0.550387\tGenerator Loss: 1.253862\n",
      "Train Epoch: 41 [30720/60000 (51%)]\tDiscriminator Loss: 0.580661\tGenerator Loss: 0.951530\n",
      "Train Epoch: 41 [32000/60000 (53%)]\tDiscriminator Loss: 0.456423\tGenerator Loss: 1.242282\n",
      "Train Epoch: 41 [33280/60000 (55%)]\tDiscriminator Loss: 0.565558\tGenerator Loss: 0.939339\n",
      "Train Epoch: 41 [34560/60000 (58%)]\tDiscriminator Loss: 0.539203\tGenerator Loss: 0.987216\n",
      "Train Epoch: 41 [35840/60000 (60%)]\tDiscriminator Loss: 0.538223\tGenerator Loss: 2.094205\n",
      "Train Epoch: 41 [37120/60000 (62%)]\tDiscriminator Loss: 0.510952\tGenerator Loss: 1.373121\n",
      "Train Epoch: 41 [38400/60000 (64%)]\tDiscriminator Loss: 0.503942\tGenerator Loss: 1.522781\n",
      "Train Epoch: 41 [39680/60000 (66%)]\tDiscriminator Loss: 0.575303\tGenerator Loss: 1.159901\n",
      "Train Epoch: 41 [40960/60000 (68%)]\tDiscriminator Loss: 0.569905\tGenerator Loss: 1.173307\n",
      "Train Epoch: 41 [42240/60000 (70%)]\tDiscriminator Loss: 0.547529\tGenerator Loss: 1.283307\n",
      "Train Epoch: 41 [43520/60000 (72%)]\tDiscriminator Loss: 0.474843\tGenerator Loss: 1.391948\n",
      "Train Epoch: 41 [44800/60000 (75%)]\tDiscriminator Loss: 0.564571\tGenerator Loss: 1.657041\n",
      "Train Epoch: 41 [46080/60000 (77%)]\tDiscriminator Loss: 0.454591\tGenerator Loss: 1.075766\n",
      "Train Epoch: 41 [47360/60000 (79%)]\tDiscriminator Loss: 0.556706\tGenerator Loss: 1.189127\n",
      "Train Epoch: 41 [48640/60000 (81%)]\tDiscriminator Loss: 0.504527\tGenerator Loss: 1.201235\n",
      "Train Epoch: 41 [49920/60000 (83%)]\tDiscriminator Loss: 0.541820\tGenerator Loss: 1.167852\n",
      "Train Epoch: 41 [51200/60000 (85%)]\tDiscriminator Loss: 0.589483\tGenerator Loss: 1.227910\n",
      "Train Epoch: 41 [52480/60000 (87%)]\tDiscriminator Loss: 0.474572\tGenerator Loss: 1.228835\n",
      "Train Epoch: 41 [53760/60000 (90%)]\tDiscriminator Loss: 0.543244\tGenerator Loss: 1.422551\n",
      "Train Epoch: 41 [55040/60000 (92%)]\tDiscriminator Loss: 0.490265\tGenerator Loss: 1.218886\n",
      "Train Epoch: 41 [56320/60000 (94%)]\tDiscriminator Loss: 0.485512\tGenerator Loss: 1.114693\n",
      "Train Epoch: 41 [57600/60000 (96%)]\tDiscriminator Loss: 0.561586\tGenerator Loss: 1.259036\n",
      "Train Epoch: 41 [58880/60000 (98%)]\tDiscriminator Loss: 0.506597\tGenerator Loss: 1.439918\n",
      "Train Epoch: 42 [0/60000 (0%)]\tDiscriminator Loss: 0.529975\tGenerator Loss: 1.182202\n",
      "Train Epoch: 42 [1280/60000 (2%)]\tDiscriminator Loss: 0.478814\tGenerator Loss: 1.136954\n",
      "Train Epoch: 42 [2560/60000 (4%)]\tDiscriminator Loss: 0.533086\tGenerator Loss: 1.327935\n",
      "Train Epoch: 42 [3840/60000 (6%)]\tDiscriminator Loss: 0.572542\tGenerator Loss: 1.507930\n",
      "Train Epoch: 42 [5120/60000 (9%)]\tDiscriminator Loss: 0.510644\tGenerator Loss: 1.127106\n",
      "Train Epoch: 42 [6400/60000 (11%)]\tDiscriminator Loss: 0.540416\tGenerator Loss: 1.274007\n",
      "Train Epoch: 42 [7680/60000 (13%)]\tDiscriminator Loss: 0.514231\tGenerator Loss: 1.394507\n",
      "Train Epoch: 42 [8960/60000 (15%)]\tDiscriminator Loss: 0.495028\tGenerator Loss: 1.913971\n",
      "Train Epoch: 42 [10240/60000 (17%)]\tDiscriminator Loss: 0.512519\tGenerator Loss: 1.253593\n",
      "Train Epoch: 42 [11520/60000 (19%)]\tDiscriminator Loss: 0.535030\tGenerator Loss: 0.980296\n",
      "Train Epoch: 42 [12800/60000 (21%)]\tDiscriminator Loss: 0.570900\tGenerator Loss: 0.859600\n",
      "Train Epoch: 42 [14080/60000 (23%)]\tDiscriminator Loss: 0.549703\tGenerator Loss: 1.179161\n",
      "Train Epoch: 42 [15360/60000 (26%)]\tDiscriminator Loss: 0.480568\tGenerator Loss: 1.344751\n",
      "Train Epoch: 42 [16640/60000 (28%)]\tDiscriminator Loss: 0.535004\tGenerator Loss: 1.169082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 42 [17920/60000 (30%)]\tDiscriminator Loss: 0.543911\tGenerator Loss: 1.069014\n",
      "Train Epoch: 42 [19200/60000 (32%)]\tDiscriminator Loss: 0.516333\tGenerator Loss: 1.271488\n",
      "Train Epoch: 42 [20480/60000 (34%)]\tDiscriminator Loss: 0.479053\tGenerator Loss: 1.588053\n",
      "Train Epoch: 42 [21760/60000 (36%)]\tDiscriminator Loss: 0.580626\tGenerator Loss: 0.974771\n",
      "Train Epoch: 42 [23040/60000 (38%)]\tDiscriminator Loss: 0.587756\tGenerator Loss: 1.070580\n",
      "Train Epoch: 42 [24320/60000 (41%)]\tDiscriminator Loss: 0.546878\tGenerator Loss: 1.131310\n",
      "Train Epoch: 42 [25600/60000 (43%)]\tDiscriminator Loss: 0.515972\tGenerator Loss: 1.222519\n",
      "Train Epoch: 42 [26880/60000 (45%)]\tDiscriminator Loss: 0.574486\tGenerator Loss: 1.303862\n",
      "Train Epoch: 42 [28160/60000 (47%)]\tDiscriminator Loss: 0.499383\tGenerator Loss: 1.666761\n",
      "Train Epoch: 42 [29440/60000 (49%)]\tDiscriminator Loss: 0.562243\tGenerator Loss: 0.950249\n",
      "Train Epoch: 42 [30720/60000 (51%)]\tDiscriminator Loss: 0.512103\tGenerator Loss: 1.125949\n",
      "Train Epoch: 42 [32000/60000 (53%)]\tDiscriminator Loss: 0.523624\tGenerator Loss: 1.481891\n",
      "Train Epoch: 42 [33280/60000 (55%)]\tDiscriminator Loss: 0.527564\tGenerator Loss: 1.155946\n",
      "Train Epoch: 42 [34560/60000 (58%)]\tDiscriminator Loss: 0.524921\tGenerator Loss: 1.110983\n",
      "Train Epoch: 42 [35840/60000 (60%)]\tDiscriminator Loss: 0.531585\tGenerator Loss: 1.153394\n",
      "Train Epoch: 42 [37120/60000 (62%)]\tDiscriminator Loss: 0.542633\tGenerator Loss: 1.355083\n",
      "Train Epoch: 42 [38400/60000 (64%)]\tDiscriminator Loss: 0.455003\tGenerator Loss: 1.428421\n",
      "Train Epoch: 42 [39680/60000 (66%)]\tDiscriminator Loss: 0.573133\tGenerator Loss: 1.134213\n",
      "Train Epoch: 42 [40960/60000 (68%)]\tDiscriminator Loss: 0.547217\tGenerator Loss: 0.776932\n",
      "Train Epoch: 42 [42240/60000 (70%)]\tDiscriminator Loss: 0.557937\tGenerator Loss: 0.882972\n",
      "Train Epoch: 42 [43520/60000 (72%)]\tDiscriminator Loss: 0.543978\tGenerator Loss: 1.005248\n",
      "Train Epoch: 42 [44800/60000 (75%)]\tDiscriminator Loss: 0.568202\tGenerator Loss: 0.924857\n",
      "Train Epoch: 42 [46080/60000 (77%)]\tDiscriminator Loss: 0.521567\tGenerator Loss: 1.111095\n",
      "Train Epoch: 42 [47360/60000 (79%)]\tDiscriminator Loss: 0.528777\tGenerator Loss: 1.330220\n",
      "Train Epoch: 42 [48640/60000 (81%)]\tDiscriminator Loss: 0.555995\tGenerator Loss: 1.426522\n",
      "Train Epoch: 42 [49920/60000 (83%)]\tDiscriminator Loss: 0.525779\tGenerator Loss: 1.141207\n",
      "Train Epoch: 42 [51200/60000 (85%)]\tDiscriminator Loss: 0.514111\tGenerator Loss: 1.450179\n",
      "Train Epoch: 42 [52480/60000 (87%)]\tDiscriminator Loss: 0.561057\tGenerator Loss: 1.228908\n",
      "Train Epoch: 42 [53760/60000 (90%)]\tDiscriminator Loss: 0.531770\tGenerator Loss: 0.954094\n",
      "Train Epoch: 42 [55040/60000 (92%)]\tDiscriminator Loss: 0.516717\tGenerator Loss: 1.712343\n",
      "Train Epoch: 42 [56320/60000 (94%)]\tDiscriminator Loss: 0.547131\tGenerator Loss: 1.435423\n",
      "Train Epoch: 42 [57600/60000 (96%)]\tDiscriminator Loss: 0.507277\tGenerator Loss: 1.567818\n",
      "Train Epoch: 42 [58880/60000 (98%)]\tDiscriminator Loss: 0.519436\tGenerator Loss: 0.979139\n",
      "Train Epoch: 43 [0/60000 (0%)]\tDiscriminator Loss: 0.477523\tGenerator Loss: 1.393968\n",
      "Train Epoch: 43 [1280/60000 (2%)]\tDiscriminator Loss: 0.494565\tGenerator Loss: 1.409967\n",
      "Train Epoch: 43 [2560/60000 (4%)]\tDiscriminator Loss: 0.508933\tGenerator Loss: 1.386456\n",
      "Train Epoch: 43 [3840/60000 (6%)]\tDiscriminator Loss: 0.482241\tGenerator Loss: 1.174966\n",
      "Train Epoch: 43 [5120/60000 (9%)]\tDiscriminator Loss: 0.550617\tGenerator Loss: 0.971472\n",
      "Train Epoch: 43 [6400/60000 (11%)]\tDiscriminator Loss: 0.610332\tGenerator Loss: 1.171944\n",
      "Train Epoch: 43 [7680/60000 (13%)]\tDiscriminator Loss: 0.461821\tGenerator Loss: 1.609256\n",
      "Train Epoch: 43 [8960/60000 (15%)]\tDiscriminator Loss: 0.574120\tGenerator Loss: 1.335515\n",
      "Train Epoch: 43 [10240/60000 (17%)]\tDiscriminator Loss: 0.496149\tGenerator Loss: 1.080564\n",
      "Train Epoch: 43 [11520/60000 (19%)]\tDiscriminator Loss: 0.498001\tGenerator Loss: 1.389568\n",
      "Train Epoch: 43 [12800/60000 (21%)]\tDiscriminator Loss: 0.574295\tGenerator Loss: 1.202412\n",
      "Train Epoch: 43 [14080/60000 (23%)]\tDiscriminator Loss: 0.504831\tGenerator Loss: 1.351388\n",
      "Train Epoch: 43 [15360/60000 (26%)]\tDiscriminator Loss: 0.538189\tGenerator Loss: 1.256571\n",
      "Train Epoch: 43 [16640/60000 (28%)]\tDiscriminator Loss: 0.516137\tGenerator Loss: 1.094030\n",
      "Train Epoch: 43 [17920/60000 (30%)]\tDiscriminator Loss: 0.487868\tGenerator Loss: 1.329623\n",
      "Train Epoch: 43 [19200/60000 (32%)]\tDiscriminator Loss: 0.512507\tGenerator Loss: 1.094209\n",
      "Train Epoch: 43 [20480/60000 (34%)]\tDiscriminator Loss: 0.523718\tGenerator Loss: 1.474437\n",
      "Train Epoch: 43 [21760/60000 (36%)]\tDiscriminator Loss: 0.479181\tGenerator Loss: 1.539484\n",
      "Train Epoch: 43 [23040/60000 (38%)]\tDiscriminator Loss: 0.517195\tGenerator Loss: 0.985157\n",
      "Train Epoch: 43 [24320/60000 (41%)]\tDiscriminator Loss: 0.503426\tGenerator Loss: 1.454154\n",
      "Train Epoch: 43 [25600/60000 (43%)]\tDiscriminator Loss: 0.481646\tGenerator Loss: 1.547087\n",
      "Train Epoch: 43 [26880/60000 (45%)]\tDiscriminator Loss: 0.486292\tGenerator Loss: 1.366344\n",
      "Train Epoch: 43 [28160/60000 (47%)]\tDiscriminator Loss: 0.588014\tGenerator Loss: 1.366188\n",
      "Train Epoch: 43 [29440/60000 (49%)]\tDiscriminator Loss: 0.476912\tGenerator Loss: 1.366936\n",
      "Train Epoch: 43 [30720/60000 (51%)]\tDiscriminator Loss: 0.555699\tGenerator Loss: 1.043042\n",
      "Train Epoch: 43 [32000/60000 (53%)]\tDiscriminator Loss: 0.557754\tGenerator Loss: 0.987591\n",
      "Train Epoch: 43 [33280/60000 (55%)]\tDiscriminator Loss: 0.480449\tGenerator Loss: 1.295506\n",
      "Train Epoch: 43 [34560/60000 (58%)]\tDiscriminator Loss: 0.529146\tGenerator Loss: 1.220427\n",
      "Train Epoch: 43 [35840/60000 (60%)]\tDiscriminator Loss: 0.490778\tGenerator Loss: 1.647754\n",
      "Train Epoch: 43 [37120/60000 (62%)]\tDiscriminator Loss: 0.495126\tGenerator Loss: 1.449721\n",
      "Train Epoch: 43 [38400/60000 (64%)]\tDiscriminator Loss: 0.477939\tGenerator Loss: 1.208498\n",
      "Train Epoch: 43 [39680/60000 (66%)]\tDiscriminator Loss: 0.514530\tGenerator Loss: 1.451499\n",
      "Train Epoch: 43 [40960/60000 (68%)]\tDiscriminator Loss: 0.517588\tGenerator Loss: 1.101847\n",
      "Train Epoch: 43 [42240/60000 (70%)]\tDiscriminator Loss: 0.562770\tGenerator Loss: 0.988881\n",
      "Train Epoch: 43 [43520/60000 (72%)]\tDiscriminator Loss: 0.537811\tGenerator Loss: 1.166128\n",
      "Train Epoch: 43 [44800/60000 (75%)]\tDiscriminator Loss: 0.539756\tGenerator Loss: 1.082924\n",
      "Train Epoch: 43 [46080/60000 (77%)]\tDiscriminator Loss: 0.628903\tGenerator Loss: 0.700533\n",
      "Train Epoch: 43 [47360/60000 (79%)]\tDiscriminator Loss: 0.508724\tGenerator Loss: 1.177866\n",
      "Train Epoch: 43 [48640/60000 (81%)]\tDiscriminator Loss: 0.528001\tGenerator Loss: 1.031546\n",
      "Train Epoch: 43 [49920/60000 (83%)]\tDiscriminator Loss: 0.556190\tGenerator Loss: 1.240237\n",
      "Train Epoch: 43 [51200/60000 (85%)]\tDiscriminator Loss: 0.461631\tGenerator Loss: 1.232146\n",
      "Train Epoch: 43 [52480/60000 (87%)]\tDiscriminator Loss: 0.646665\tGenerator Loss: 0.619373\n",
      "Train Epoch: 43 [53760/60000 (90%)]\tDiscriminator Loss: 0.467258\tGenerator Loss: 1.543664\n",
      "Train Epoch: 43 [55040/60000 (92%)]\tDiscriminator Loss: 0.514023\tGenerator Loss: 1.224418\n",
      "Train Epoch: 43 [56320/60000 (94%)]\tDiscriminator Loss: 0.548780\tGenerator Loss: 0.977080\n",
      "Train Epoch: 43 [57600/60000 (96%)]\tDiscriminator Loss: 0.545096\tGenerator Loss: 1.030944\n",
      "Train Epoch: 43 [58880/60000 (98%)]\tDiscriminator Loss: 0.526712\tGenerator Loss: 1.569954\n",
      "Train Epoch: 44 [0/60000 (0%)]\tDiscriminator Loss: 0.491188\tGenerator Loss: 1.135719\n",
      "Train Epoch: 44 [1280/60000 (2%)]\tDiscriminator Loss: 0.406679\tGenerator Loss: 1.352636\n",
      "Train Epoch: 44 [2560/60000 (4%)]\tDiscriminator Loss: 0.549318\tGenerator Loss: 0.936395\n",
      "Train Epoch: 44 [3840/60000 (6%)]\tDiscriminator Loss: 0.528979\tGenerator Loss: 1.074228\n",
      "Train Epoch: 44 [5120/60000 (9%)]\tDiscriminator Loss: 0.488921\tGenerator Loss: 1.243462\n",
      "Train Epoch: 44 [6400/60000 (11%)]\tDiscriminator Loss: 0.482905\tGenerator Loss: 1.655814\n",
      "Train Epoch: 44 [7680/60000 (13%)]\tDiscriminator Loss: 0.572509\tGenerator Loss: 0.904864\n",
      "Train Epoch: 44 [8960/60000 (15%)]\tDiscriminator Loss: 0.515130\tGenerator Loss: 1.230770\n",
      "Train Epoch: 44 [10240/60000 (17%)]\tDiscriminator Loss: 0.459659\tGenerator Loss: 1.609570\n",
      "Train Epoch: 44 [11520/60000 (19%)]\tDiscriminator Loss: 0.530300\tGenerator Loss: 1.222446\n",
      "Train Epoch: 44 [12800/60000 (21%)]\tDiscriminator Loss: 0.537030\tGenerator Loss: 1.435081\n",
      "Train Epoch: 44 [14080/60000 (23%)]\tDiscriminator Loss: 0.484767\tGenerator Loss: 1.163342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 44 [15360/60000 (26%)]\tDiscriminator Loss: 0.593053\tGenerator Loss: 0.992024\n",
      "Train Epoch: 44 [16640/60000 (28%)]\tDiscriminator Loss: 0.547994\tGenerator Loss: 1.486863\n",
      "Train Epoch: 44 [17920/60000 (30%)]\tDiscriminator Loss: 0.521383\tGenerator Loss: 1.315795\n",
      "Train Epoch: 44 [19200/60000 (32%)]\tDiscriminator Loss: 0.503049\tGenerator Loss: 1.441637\n",
      "Train Epoch: 44 [20480/60000 (34%)]\tDiscriminator Loss: 0.525495\tGenerator Loss: 1.026776\n",
      "Train Epoch: 44 [21760/60000 (36%)]\tDiscriminator Loss: 0.550868\tGenerator Loss: 1.412510\n",
      "Train Epoch: 44 [23040/60000 (38%)]\tDiscriminator Loss: 0.534041\tGenerator Loss: 1.167388\n",
      "Train Epoch: 44 [24320/60000 (41%)]\tDiscriminator Loss: 0.545183\tGenerator Loss: 1.469138\n",
      "Train Epoch: 44 [25600/60000 (43%)]\tDiscriminator Loss: 0.512967\tGenerator Loss: 1.039950\n",
      "Train Epoch: 44 [26880/60000 (45%)]\tDiscriminator Loss: 0.600462\tGenerator Loss: 1.229820\n",
      "Train Epoch: 44 [28160/60000 (47%)]\tDiscriminator Loss: 0.521629\tGenerator Loss: 1.455184\n",
      "Train Epoch: 44 [29440/60000 (49%)]\tDiscriminator Loss: 0.494110\tGenerator Loss: 1.464153\n",
      "Train Epoch: 44 [30720/60000 (51%)]\tDiscriminator Loss: 0.579213\tGenerator Loss: 1.557466\n",
      "Train Epoch: 44 [32000/60000 (53%)]\tDiscriminator Loss: 0.537152\tGenerator Loss: 1.065764\n",
      "Train Epoch: 44 [33280/60000 (55%)]\tDiscriminator Loss: 0.542106\tGenerator Loss: 1.083737\n",
      "Train Epoch: 44 [34560/60000 (58%)]\tDiscriminator Loss: 0.545169\tGenerator Loss: 1.860479\n",
      "Train Epoch: 44 [35840/60000 (60%)]\tDiscriminator Loss: 0.542640\tGenerator Loss: 1.344533\n",
      "Train Epoch: 44 [37120/60000 (62%)]\tDiscriminator Loss: 0.518625\tGenerator Loss: 1.420360\n",
      "Train Epoch: 44 [38400/60000 (64%)]\tDiscriminator Loss: 0.477777\tGenerator Loss: 1.282206\n",
      "Train Epoch: 44 [39680/60000 (66%)]\tDiscriminator Loss: 0.592383\tGenerator Loss: 0.974867\n",
      "Train Epoch: 44 [40960/60000 (68%)]\tDiscriminator Loss: 0.579317\tGenerator Loss: 1.379539\n",
      "Train Epoch: 44 [42240/60000 (70%)]\tDiscriminator Loss: 0.553953\tGenerator Loss: 0.964638\n",
      "Train Epoch: 44 [43520/60000 (72%)]\tDiscriminator Loss: 0.537127\tGenerator Loss: 1.220986\n",
      "Train Epoch: 44 [44800/60000 (75%)]\tDiscriminator Loss: 0.496419\tGenerator Loss: 1.105433\n",
      "Train Epoch: 44 [46080/60000 (77%)]\tDiscriminator Loss: 0.526762\tGenerator Loss: 1.195575\n",
      "Train Epoch: 44 [47360/60000 (79%)]\tDiscriminator Loss: 0.578209\tGenerator Loss: 1.161741\n",
      "Train Epoch: 44 [48640/60000 (81%)]\tDiscriminator Loss: 0.550212\tGenerator Loss: 1.365921\n",
      "Train Epoch: 44 [49920/60000 (83%)]\tDiscriminator Loss: 0.573027\tGenerator Loss: 1.106654\n",
      "Train Epoch: 44 [51200/60000 (85%)]\tDiscriminator Loss: 0.500523\tGenerator Loss: 1.258156\n",
      "Train Epoch: 44 [52480/60000 (87%)]\tDiscriminator Loss: 0.509002\tGenerator Loss: 1.285323\n",
      "Train Epoch: 44 [53760/60000 (90%)]\tDiscriminator Loss: 0.623923\tGenerator Loss: 0.723789\n",
      "Train Epoch: 44 [55040/60000 (92%)]\tDiscriminator Loss: 0.601555\tGenerator Loss: 1.446258\n",
      "Train Epoch: 44 [56320/60000 (94%)]\tDiscriminator Loss: 0.482660\tGenerator Loss: 1.377619\n",
      "Train Epoch: 44 [57600/60000 (96%)]\tDiscriminator Loss: 0.522115\tGenerator Loss: 1.101813\n",
      "Train Epoch: 44 [58880/60000 (98%)]\tDiscriminator Loss: 0.529573\tGenerator Loss: 1.310874\n",
      "Train Epoch: 45 [0/60000 (0%)]\tDiscriminator Loss: 0.619563\tGenerator Loss: 1.401399\n",
      "Train Epoch: 45 [1280/60000 (2%)]\tDiscriminator Loss: 0.470032\tGenerator Loss: 1.418809\n",
      "Train Epoch: 45 [2560/60000 (4%)]\tDiscriminator Loss: 0.500354\tGenerator Loss: 1.201549\n",
      "Train Epoch: 45 [3840/60000 (6%)]\tDiscriminator Loss: 0.533988\tGenerator Loss: 1.489163\n",
      "Train Epoch: 45 [5120/60000 (9%)]\tDiscriminator Loss: 0.530135\tGenerator Loss: 1.015316\n",
      "Train Epoch: 45 [6400/60000 (11%)]\tDiscriminator Loss: 0.470669\tGenerator Loss: 1.285771\n",
      "Train Epoch: 45 [7680/60000 (13%)]\tDiscriminator Loss: 0.528479\tGenerator Loss: 1.111215\n",
      "Train Epoch: 45 [8960/60000 (15%)]\tDiscriminator Loss: 0.519906\tGenerator Loss: 1.046794\n",
      "Train Epoch: 45 [10240/60000 (17%)]\tDiscriminator Loss: 0.471442\tGenerator Loss: 1.397507\n",
      "Train Epoch: 45 [11520/60000 (19%)]\tDiscriminator Loss: 0.562484\tGenerator Loss: 1.460132\n",
      "Train Epoch: 45 [12800/60000 (21%)]\tDiscriminator Loss: 0.543942\tGenerator Loss: 1.125688\n",
      "Train Epoch: 45 [14080/60000 (23%)]\tDiscriminator Loss: 0.500264\tGenerator Loss: 1.107773\n",
      "Train Epoch: 45 [15360/60000 (26%)]\tDiscriminator Loss: 0.511705\tGenerator Loss: 1.430203\n",
      "Train Epoch: 45 [16640/60000 (28%)]\tDiscriminator Loss: 0.450690\tGenerator Loss: 1.468386\n",
      "Train Epoch: 45 [17920/60000 (30%)]\tDiscriminator Loss: 0.539480\tGenerator Loss: 1.842979\n",
      "Train Epoch: 45 [19200/60000 (32%)]\tDiscriminator Loss: 0.452733\tGenerator Loss: 1.562775\n",
      "Train Epoch: 45 [20480/60000 (34%)]\tDiscriminator Loss: 0.472222\tGenerator Loss: 1.425702\n",
      "Train Epoch: 45 [21760/60000 (36%)]\tDiscriminator Loss: 0.580023\tGenerator Loss: 0.857858\n",
      "Train Epoch: 45 [23040/60000 (38%)]\tDiscriminator Loss: 0.506422\tGenerator Loss: 1.175579\n",
      "Train Epoch: 45 [24320/60000 (41%)]\tDiscriminator Loss: 0.495854\tGenerator Loss: 1.157470\n",
      "Train Epoch: 45 [25600/60000 (43%)]\tDiscriminator Loss: 0.532385\tGenerator Loss: 1.184311\n",
      "Train Epoch: 45 [26880/60000 (45%)]\tDiscriminator Loss: 0.503720\tGenerator Loss: 1.422963\n",
      "Train Epoch: 45 [28160/60000 (47%)]\tDiscriminator Loss: 0.495529\tGenerator Loss: 1.346989\n",
      "Train Epoch: 45 [29440/60000 (49%)]\tDiscriminator Loss: 0.566131\tGenerator Loss: 0.968288\n",
      "Train Epoch: 45 [30720/60000 (51%)]\tDiscriminator Loss: 0.547560\tGenerator Loss: 1.196306\n",
      "Train Epoch: 45 [32000/60000 (53%)]\tDiscriminator Loss: 0.531694\tGenerator Loss: 1.653477\n",
      "Train Epoch: 45 [33280/60000 (55%)]\tDiscriminator Loss: 0.545607\tGenerator Loss: 1.666729\n",
      "Train Epoch: 45 [34560/60000 (58%)]\tDiscriminator Loss: 0.548357\tGenerator Loss: 1.340585\n",
      "Train Epoch: 45 [35840/60000 (60%)]\tDiscriminator Loss: 0.536982\tGenerator Loss: 1.276396\n",
      "Train Epoch: 45 [37120/60000 (62%)]\tDiscriminator Loss: 0.525034\tGenerator Loss: 1.256054\n",
      "Train Epoch: 45 [38400/60000 (64%)]\tDiscriminator Loss: 0.454757\tGenerator Loss: 1.312892\n",
      "Train Epoch: 45 [39680/60000 (66%)]\tDiscriminator Loss: 0.518027\tGenerator Loss: 1.285935\n",
      "Train Epoch: 45 [40960/60000 (68%)]\tDiscriminator Loss: 0.573682\tGenerator Loss: 0.898963\n",
      "Train Epoch: 45 [42240/60000 (70%)]\tDiscriminator Loss: 0.583576\tGenerator Loss: 0.837848\n",
      "Train Epoch: 45 [43520/60000 (72%)]\tDiscriminator Loss: 0.532339\tGenerator Loss: 1.151064\n",
      "Train Epoch: 45 [44800/60000 (75%)]\tDiscriminator Loss: 0.531255\tGenerator Loss: 1.130187\n",
      "Train Epoch: 45 [46080/60000 (77%)]\tDiscriminator Loss: 0.584140\tGenerator Loss: 1.105628\n",
      "Train Epoch: 45 [47360/60000 (79%)]\tDiscriminator Loss: 0.493586\tGenerator Loss: 1.046075\n",
      "Train Epoch: 45 [48640/60000 (81%)]\tDiscriminator Loss: 0.526106\tGenerator Loss: 1.366837\n",
      "Train Epoch: 45 [49920/60000 (83%)]\tDiscriminator Loss: 0.550474\tGenerator Loss: 1.202992\n",
      "Train Epoch: 45 [51200/60000 (85%)]\tDiscriminator Loss: 0.480878\tGenerator Loss: 1.634560\n",
      "Train Epoch: 45 [52480/60000 (87%)]\tDiscriminator Loss: 0.527083\tGenerator Loss: 1.581545\n",
      "Train Epoch: 45 [53760/60000 (90%)]\tDiscriminator Loss: 0.512492\tGenerator Loss: 1.211086\n",
      "Train Epoch: 45 [55040/60000 (92%)]\tDiscriminator Loss: 0.518180\tGenerator Loss: 1.353529\n",
      "Train Epoch: 45 [56320/60000 (94%)]\tDiscriminator Loss: 0.560645\tGenerator Loss: 1.097050\n",
      "Train Epoch: 45 [57600/60000 (96%)]\tDiscriminator Loss: 0.520618\tGenerator Loss: 1.215828\n",
      "Train Epoch: 45 [58880/60000 (98%)]\tDiscriminator Loss: 0.535668\tGenerator Loss: 1.164769\n",
      "Train Epoch: 46 [0/60000 (0%)]\tDiscriminator Loss: 0.526381\tGenerator Loss: 1.321323\n",
      "Train Epoch: 46 [1280/60000 (2%)]\tDiscriminator Loss: 0.533652\tGenerator Loss: 1.652277\n",
      "Train Epoch: 46 [2560/60000 (4%)]\tDiscriminator Loss: 0.453160\tGenerator Loss: 1.297646\n",
      "Train Epoch: 46 [3840/60000 (6%)]\tDiscriminator Loss: 0.517108\tGenerator Loss: 1.420350\n",
      "Train Epoch: 46 [5120/60000 (9%)]\tDiscriminator Loss: 0.571567\tGenerator Loss: 1.247843\n",
      "Train Epoch: 46 [6400/60000 (11%)]\tDiscriminator Loss: 0.581113\tGenerator Loss: 1.415181\n",
      "Train Epoch: 46 [7680/60000 (13%)]\tDiscriminator Loss: 0.459721\tGenerator Loss: 1.390464\n",
      "Train Epoch: 46 [8960/60000 (15%)]\tDiscriminator Loss: 0.439043\tGenerator Loss: 1.274275\n",
      "Train Epoch: 46 [10240/60000 (17%)]\tDiscriminator Loss: 0.570068\tGenerator Loss: 1.017355\n",
      "Train Epoch: 46 [11520/60000 (19%)]\tDiscriminator Loss: 0.542224\tGenerator Loss: 1.351997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 46 [12800/60000 (21%)]\tDiscriminator Loss: 0.531234\tGenerator Loss: 1.083840\n",
      "Train Epoch: 46 [14080/60000 (23%)]\tDiscriminator Loss: 0.590783\tGenerator Loss: 1.245051\n",
      "Train Epoch: 46 [15360/60000 (26%)]\tDiscriminator Loss: 0.475089\tGenerator Loss: 1.350879\n",
      "Train Epoch: 46 [16640/60000 (28%)]\tDiscriminator Loss: 0.563799\tGenerator Loss: 0.826924\n",
      "Train Epoch: 46 [17920/60000 (30%)]\tDiscriminator Loss: 0.552024\tGenerator Loss: 1.073656\n",
      "Train Epoch: 46 [19200/60000 (32%)]\tDiscriminator Loss: 0.486988\tGenerator Loss: 1.182279\n",
      "Train Epoch: 46 [20480/60000 (34%)]\tDiscriminator Loss: 0.519357\tGenerator Loss: 1.209350\n",
      "Train Epoch: 46 [21760/60000 (36%)]\tDiscriminator Loss: 0.568761\tGenerator Loss: 1.160123\n",
      "Train Epoch: 46 [23040/60000 (38%)]\tDiscriminator Loss: 0.497692\tGenerator Loss: 1.297575\n",
      "Train Epoch: 46 [24320/60000 (41%)]\tDiscriminator Loss: 0.507025\tGenerator Loss: 1.354336\n",
      "Train Epoch: 46 [25600/60000 (43%)]\tDiscriminator Loss: 0.504512\tGenerator Loss: 0.956061\n",
      "Train Epoch: 46 [26880/60000 (45%)]\tDiscriminator Loss: 0.561649\tGenerator Loss: 1.200228\n",
      "Train Epoch: 46 [28160/60000 (47%)]\tDiscriminator Loss: 0.518800\tGenerator Loss: 1.462049\n",
      "Train Epoch: 46 [29440/60000 (49%)]\tDiscriminator Loss: 0.479887\tGenerator Loss: 1.114950\n",
      "Train Epoch: 46 [30720/60000 (51%)]\tDiscriminator Loss: 0.521009\tGenerator Loss: 0.952821\n",
      "Train Epoch: 46 [32000/60000 (53%)]\tDiscriminator Loss: 0.512386\tGenerator Loss: 1.331974\n",
      "Train Epoch: 46 [33280/60000 (55%)]\tDiscriminator Loss: 0.587489\tGenerator Loss: 0.915498\n",
      "Train Epoch: 46 [34560/60000 (58%)]\tDiscriminator Loss: 0.506234\tGenerator Loss: 1.528345\n",
      "Train Epoch: 46 [35840/60000 (60%)]\tDiscriminator Loss: 0.534053\tGenerator Loss: 1.483805\n",
      "Train Epoch: 46 [37120/60000 (62%)]\tDiscriminator Loss: 0.564940\tGenerator Loss: 1.046621\n",
      "Train Epoch: 46 [38400/60000 (64%)]\tDiscriminator Loss: 0.557036\tGenerator Loss: 1.181596\n",
      "Train Epoch: 46 [39680/60000 (66%)]\tDiscriminator Loss: 0.458989\tGenerator Loss: 1.307370\n",
      "Train Epoch: 46 [40960/60000 (68%)]\tDiscriminator Loss: 0.577741\tGenerator Loss: 1.131400\n",
      "Train Epoch: 46 [42240/60000 (70%)]\tDiscriminator Loss: 0.514988\tGenerator Loss: 1.171484\n",
      "Train Epoch: 46 [43520/60000 (72%)]\tDiscriminator Loss: 0.494110\tGenerator Loss: 1.327749\n",
      "Train Epoch: 46 [44800/60000 (75%)]\tDiscriminator Loss: 0.508785\tGenerator Loss: 1.211299\n",
      "Train Epoch: 46 [46080/60000 (77%)]\tDiscriminator Loss: 0.534195\tGenerator Loss: 1.192866\n",
      "Train Epoch: 46 [47360/60000 (79%)]\tDiscriminator Loss: 0.532451\tGenerator Loss: 1.252872\n",
      "Train Epoch: 46 [48640/60000 (81%)]\tDiscriminator Loss: 0.536808\tGenerator Loss: 1.163105\n",
      "Train Epoch: 46 [49920/60000 (83%)]\tDiscriminator Loss: 0.486903\tGenerator Loss: 1.249287\n",
      "Train Epoch: 46 [51200/60000 (85%)]\tDiscriminator Loss: 0.585995\tGenerator Loss: 0.750697\n",
      "Train Epoch: 46 [52480/60000 (87%)]\tDiscriminator Loss: 0.500726\tGenerator Loss: 1.349185\n",
      "Train Epoch: 46 [53760/60000 (90%)]\tDiscriminator Loss: 0.489878\tGenerator Loss: 1.374621\n",
      "Train Epoch: 46 [55040/60000 (92%)]\tDiscriminator Loss: 0.570982\tGenerator Loss: 0.914762\n",
      "Train Epoch: 46 [56320/60000 (94%)]\tDiscriminator Loss: 0.476388\tGenerator Loss: 1.155800\n",
      "Train Epoch: 46 [57600/60000 (96%)]\tDiscriminator Loss: 0.468343\tGenerator Loss: 1.216322\n",
      "Train Epoch: 46 [58880/60000 (98%)]\tDiscriminator Loss: 0.575580\tGenerator Loss: 0.880187\n",
      "Train Epoch: 47 [0/60000 (0%)]\tDiscriminator Loss: 0.563367\tGenerator Loss: 1.341931\n",
      "Train Epoch: 47 [1280/60000 (2%)]\tDiscriminator Loss: 0.562096\tGenerator Loss: 1.355683\n",
      "Train Epoch: 47 [2560/60000 (4%)]\tDiscriminator Loss: 0.539413\tGenerator Loss: 1.358706\n",
      "Train Epoch: 47 [3840/60000 (6%)]\tDiscriminator Loss: 0.545811\tGenerator Loss: 0.862152\n",
      "Train Epoch: 47 [5120/60000 (9%)]\tDiscriminator Loss: 0.509136\tGenerator Loss: 1.309007\n",
      "Train Epoch: 47 [6400/60000 (11%)]\tDiscriminator Loss: 0.557046\tGenerator Loss: 1.336603\n",
      "Train Epoch: 47 [7680/60000 (13%)]\tDiscriminator Loss: 0.492432\tGenerator Loss: 1.460942\n",
      "Train Epoch: 47 [8960/60000 (15%)]\tDiscriminator Loss: 0.555310\tGenerator Loss: 1.045528\n",
      "Train Epoch: 47 [10240/60000 (17%)]\tDiscriminator Loss: 0.494972\tGenerator Loss: 1.192499\n",
      "Train Epoch: 47 [11520/60000 (19%)]\tDiscriminator Loss: 0.595693\tGenerator Loss: 1.401756\n",
      "Train Epoch: 47 [12800/60000 (21%)]\tDiscriminator Loss: 0.541741\tGenerator Loss: 1.249863\n",
      "Train Epoch: 47 [14080/60000 (23%)]\tDiscriminator Loss: 0.483679\tGenerator Loss: 0.981870\n",
      "Train Epoch: 47 [15360/60000 (26%)]\tDiscriminator Loss: 0.581292\tGenerator Loss: 1.338653\n",
      "Train Epoch: 47 [16640/60000 (28%)]\tDiscriminator Loss: 0.535058\tGenerator Loss: 1.087284\n",
      "Train Epoch: 47 [17920/60000 (30%)]\tDiscriminator Loss: 0.545423\tGenerator Loss: 0.991338\n",
      "Train Epoch: 47 [19200/60000 (32%)]\tDiscriminator Loss: 0.517354\tGenerator Loss: 0.946950\n",
      "Train Epoch: 47 [20480/60000 (34%)]\tDiscriminator Loss: 0.535180\tGenerator Loss: 1.040382\n",
      "Train Epoch: 47 [21760/60000 (36%)]\tDiscriminator Loss: 0.593327\tGenerator Loss: 1.060504\n",
      "Train Epoch: 47 [23040/60000 (38%)]\tDiscriminator Loss: 0.510561\tGenerator Loss: 1.039418\n",
      "Train Epoch: 47 [24320/60000 (41%)]\tDiscriminator Loss: 0.519003\tGenerator Loss: 1.308937\n",
      "Train Epoch: 47 [25600/60000 (43%)]\tDiscriminator Loss: 0.565129\tGenerator Loss: 1.308376\n",
      "Train Epoch: 47 [26880/60000 (45%)]\tDiscriminator Loss: 0.478160\tGenerator Loss: 1.223201\n",
      "Train Epoch: 47 [28160/60000 (47%)]\tDiscriminator Loss: 0.572872\tGenerator Loss: 1.327699\n",
      "Train Epoch: 47 [29440/60000 (49%)]\tDiscriminator Loss: 0.523158\tGenerator Loss: 1.308390\n",
      "Train Epoch: 47 [30720/60000 (51%)]\tDiscriminator Loss: 0.453631\tGenerator Loss: 1.286431\n",
      "Train Epoch: 47 [32000/60000 (53%)]\tDiscriminator Loss: 0.528313\tGenerator Loss: 1.383294\n",
      "Train Epoch: 47 [33280/60000 (55%)]\tDiscriminator Loss: 0.508141\tGenerator Loss: 1.350831\n",
      "Train Epoch: 47 [34560/60000 (58%)]\tDiscriminator Loss: 0.541926\tGenerator Loss: 1.149172\n",
      "Train Epoch: 47 [35840/60000 (60%)]\tDiscriminator Loss: 0.530148\tGenerator Loss: 1.357330\n",
      "Train Epoch: 47 [37120/60000 (62%)]\tDiscriminator Loss: 0.502797\tGenerator Loss: 1.124035\n",
      "Train Epoch: 47 [38400/60000 (64%)]\tDiscriminator Loss: 0.556286\tGenerator Loss: 1.135712\n",
      "Train Epoch: 47 [39680/60000 (66%)]\tDiscriminator Loss: 0.492739\tGenerator Loss: 1.181684\n",
      "Train Epoch: 47 [40960/60000 (68%)]\tDiscriminator Loss: 0.537306\tGenerator Loss: 1.233645\n",
      "Train Epoch: 47 [42240/60000 (70%)]\tDiscriminator Loss: 0.675840\tGenerator Loss: 0.553656\n",
      "Train Epoch: 47 [43520/60000 (72%)]\tDiscriminator Loss: 0.475006\tGenerator Loss: 1.309285\n",
      "Train Epoch: 47 [44800/60000 (75%)]\tDiscriminator Loss: 0.490752\tGenerator Loss: 1.111993\n",
      "Train Epoch: 47 [46080/60000 (77%)]\tDiscriminator Loss: 0.493669\tGenerator Loss: 1.277476\n",
      "Train Epoch: 47 [47360/60000 (79%)]\tDiscriminator Loss: 0.519920\tGenerator Loss: 1.577631\n",
      "Train Epoch: 47 [48640/60000 (81%)]\tDiscriminator Loss: 0.509518\tGenerator Loss: 1.362366\n",
      "Train Epoch: 47 [49920/60000 (83%)]\tDiscriminator Loss: 0.493759\tGenerator Loss: 1.406610\n",
      "Train Epoch: 47 [51200/60000 (85%)]\tDiscriminator Loss: 0.510151\tGenerator Loss: 1.455070\n",
      "Train Epoch: 47 [52480/60000 (87%)]\tDiscriminator Loss: 0.542693\tGenerator Loss: 1.369679\n",
      "Train Epoch: 47 [53760/60000 (90%)]\tDiscriminator Loss: 0.491944\tGenerator Loss: 1.295321\n",
      "Train Epoch: 47 [55040/60000 (92%)]\tDiscriminator Loss: 0.529051\tGenerator Loss: 1.266742\n",
      "Train Epoch: 47 [56320/60000 (94%)]\tDiscriminator Loss: 0.528252\tGenerator Loss: 1.312444\n",
      "Train Epoch: 47 [57600/60000 (96%)]\tDiscriminator Loss: 0.469392\tGenerator Loss: 1.334676\n",
      "Train Epoch: 47 [58880/60000 (98%)]\tDiscriminator Loss: 0.558749\tGenerator Loss: 1.702164\n",
      "Train Epoch: 48 [0/60000 (0%)]\tDiscriminator Loss: 0.521013\tGenerator Loss: 1.185899\n",
      "Train Epoch: 48 [1280/60000 (2%)]\tDiscriminator Loss: 0.506334\tGenerator Loss: 1.365105\n",
      "Train Epoch: 48 [2560/60000 (4%)]\tDiscriminator Loss: 0.566421\tGenerator Loss: 0.965568\n",
      "Train Epoch: 48 [3840/60000 (6%)]\tDiscriminator Loss: 0.514971\tGenerator Loss: 1.363601\n",
      "Train Epoch: 48 [5120/60000 (9%)]\tDiscriminator Loss: 0.529248\tGenerator Loss: 1.146664\n",
      "Train Epoch: 48 [6400/60000 (11%)]\tDiscriminator Loss: 0.535949\tGenerator Loss: 0.987962\n",
      "Train Epoch: 48 [7680/60000 (13%)]\tDiscriminator Loss: 0.510168\tGenerator Loss: 1.208029\n",
      "Train Epoch: 48 [8960/60000 (15%)]\tDiscriminator Loss: 0.570973\tGenerator Loss: 1.166491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 48 [10240/60000 (17%)]\tDiscriminator Loss: 0.504257\tGenerator Loss: 1.533974\n",
      "Train Epoch: 48 [11520/60000 (19%)]\tDiscriminator Loss: 0.566533\tGenerator Loss: 1.288071\n",
      "Train Epoch: 48 [12800/60000 (21%)]\tDiscriminator Loss: 0.591556\tGenerator Loss: 1.464260\n",
      "Train Epoch: 48 [14080/60000 (23%)]\tDiscriminator Loss: 0.498247\tGenerator Loss: 1.408768\n",
      "Train Epoch: 48 [15360/60000 (26%)]\tDiscriminator Loss: 0.530805\tGenerator Loss: 1.261598\n",
      "Train Epoch: 48 [16640/60000 (28%)]\tDiscriminator Loss: 0.548974\tGenerator Loss: 1.267482\n",
      "Train Epoch: 48 [17920/60000 (30%)]\tDiscriminator Loss: 0.556841\tGenerator Loss: 1.103515\n",
      "Train Epoch: 48 [19200/60000 (32%)]\tDiscriminator Loss: 0.531632\tGenerator Loss: 1.228012\n",
      "Train Epoch: 48 [20480/60000 (34%)]\tDiscriminator Loss: 0.536170\tGenerator Loss: 1.090255\n",
      "Train Epoch: 48 [21760/60000 (36%)]\tDiscriminator Loss: 0.496545\tGenerator Loss: 1.233026\n",
      "Train Epoch: 48 [23040/60000 (38%)]\tDiscriminator Loss: 0.499018\tGenerator Loss: 0.912305\n",
      "Train Epoch: 48 [24320/60000 (41%)]\tDiscriminator Loss: 0.597793\tGenerator Loss: 1.197691\n",
      "Train Epoch: 48 [25600/60000 (43%)]\tDiscriminator Loss: 0.552371\tGenerator Loss: 1.458154\n",
      "Train Epoch: 48 [26880/60000 (45%)]\tDiscriminator Loss: 0.508784\tGenerator Loss: 1.302620\n",
      "Train Epoch: 48 [28160/60000 (47%)]\tDiscriminator Loss: 0.462426\tGenerator Loss: 1.264072\n",
      "Train Epoch: 48 [29440/60000 (49%)]\tDiscriminator Loss: 0.471650\tGenerator Loss: 1.426858\n",
      "Train Epoch: 48 [30720/60000 (51%)]\tDiscriminator Loss: 0.519447\tGenerator Loss: 1.265847\n",
      "Train Epoch: 48 [32000/60000 (53%)]\tDiscriminator Loss: 0.436383\tGenerator Loss: 1.472283\n",
      "Train Epoch: 48 [33280/60000 (55%)]\tDiscriminator Loss: 0.494367\tGenerator Loss: 1.302838\n",
      "Train Epoch: 48 [34560/60000 (58%)]\tDiscriminator Loss: 0.563872\tGenerator Loss: 0.843823\n",
      "Train Epoch: 48 [35840/60000 (60%)]\tDiscriminator Loss: 0.485135\tGenerator Loss: 1.401921\n",
      "Train Epoch: 48 [37120/60000 (62%)]\tDiscriminator Loss: 0.503118\tGenerator Loss: 1.497225\n",
      "Train Epoch: 48 [38400/60000 (64%)]\tDiscriminator Loss: 0.516845\tGenerator Loss: 1.443181\n",
      "Train Epoch: 48 [39680/60000 (66%)]\tDiscriminator Loss: 0.528085\tGenerator Loss: 1.150793\n",
      "Train Epoch: 48 [40960/60000 (68%)]\tDiscriminator Loss: 0.453803\tGenerator Loss: 1.409668\n",
      "Train Epoch: 48 [42240/60000 (70%)]\tDiscriminator Loss: 0.558559\tGenerator Loss: 1.024079\n",
      "Train Epoch: 48 [43520/60000 (72%)]\tDiscriminator Loss: 0.449885\tGenerator Loss: 1.283757\n",
      "Train Epoch: 48 [44800/60000 (75%)]\tDiscriminator Loss: 0.603421\tGenerator Loss: 1.881275\n",
      "Train Epoch: 48 [46080/60000 (77%)]\tDiscriminator Loss: 0.552257\tGenerator Loss: 1.174144\n",
      "Train Epoch: 48 [47360/60000 (79%)]\tDiscriminator Loss: 0.547147\tGenerator Loss: 1.314805\n",
      "Train Epoch: 48 [48640/60000 (81%)]\tDiscriminator Loss: 0.491292\tGenerator Loss: 1.750954\n",
      "Train Epoch: 48 [49920/60000 (83%)]\tDiscriminator Loss: 0.526744\tGenerator Loss: 1.044601\n",
      "Train Epoch: 48 [51200/60000 (85%)]\tDiscriminator Loss: 0.562431\tGenerator Loss: 1.057322\n",
      "Train Epoch: 48 [52480/60000 (87%)]\tDiscriminator Loss: 0.509542\tGenerator Loss: 1.242486\n",
      "Train Epoch: 48 [53760/60000 (90%)]\tDiscriminator Loss: 0.511084\tGenerator Loss: 1.248470\n",
      "Train Epoch: 48 [55040/60000 (92%)]\tDiscriminator Loss: 0.539774\tGenerator Loss: 1.131409\n",
      "Train Epoch: 48 [56320/60000 (94%)]\tDiscriminator Loss: 0.571018\tGenerator Loss: 1.664252\n",
      "Train Epoch: 48 [57600/60000 (96%)]\tDiscriminator Loss: 0.574303\tGenerator Loss: 1.539838\n",
      "Train Epoch: 48 [58880/60000 (98%)]\tDiscriminator Loss: 0.643458\tGenerator Loss: 0.898699\n",
      "Train Epoch: 49 [0/60000 (0%)]\tDiscriminator Loss: 0.548095\tGenerator Loss: 1.058708\n",
      "Train Epoch: 49 [1280/60000 (2%)]\tDiscriminator Loss: 0.490239\tGenerator Loss: 1.287399\n",
      "Train Epoch: 49 [2560/60000 (4%)]\tDiscriminator Loss: 0.490548\tGenerator Loss: 1.658914\n",
      "Train Epoch: 49 [3840/60000 (6%)]\tDiscriminator Loss: 0.487792\tGenerator Loss: 1.748649\n",
      "Train Epoch: 49 [5120/60000 (9%)]\tDiscriminator Loss: 0.446335\tGenerator Loss: 1.371768\n",
      "Train Epoch: 49 [6400/60000 (11%)]\tDiscriminator Loss: 0.475912\tGenerator Loss: 1.977280\n",
      "Train Epoch: 49 [7680/60000 (13%)]\tDiscriminator Loss: 0.526551\tGenerator Loss: 0.947134\n",
      "Train Epoch: 49 [8960/60000 (15%)]\tDiscriminator Loss: 0.510581\tGenerator Loss: 1.433601\n",
      "Train Epoch: 49 [10240/60000 (17%)]\tDiscriminator Loss: 0.502249\tGenerator Loss: 1.242442\n",
      "Train Epoch: 49 [11520/60000 (19%)]\tDiscriminator Loss: 0.611273\tGenerator Loss: 0.865175\n",
      "Train Epoch: 49 [12800/60000 (21%)]\tDiscriminator Loss: 0.530823\tGenerator Loss: 1.267190\n",
      "Train Epoch: 49 [14080/60000 (23%)]\tDiscriminator Loss: 0.612705\tGenerator Loss: 0.982230\n",
      "Train Epoch: 49 [15360/60000 (26%)]\tDiscriminator Loss: 0.484275\tGenerator Loss: 1.369376\n",
      "Train Epoch: 49 [16640/60000 (28%)]\tDiscriminator Loss: 0.419712\tGenerator Loss: 1.324895\n",
      "Train Epoch: 49 [17920/60000 (30%)]\tDiscriminator Loss: 0.458051\tGenerator Loss: 1.399007\n",
      "Train Epoch: 49 [19200/60000 (32%)]\tDiscriminator Loss: 0.481153\tGenerator Loss: 1.258158\n",
      "Train Epoch: 49 [20480/60000 (34%)]\tDiscriminator Loss: 0.492418\tGenerator Loss: 1.063793\n",
      "Train Epoch: 49 [21760/60000 (36%)]\tDiscriminator Loss: 0.519357\tGenerator Loss: 1.199182\n",
      "Train Epoch: 49 [23040/60000 (38%)]\tDiscriminator Loss: 0.538193\tGenerator Loss: 1.247908\n",
      "Train Epoch: 49 [24320/60000 (41%)]\tDiscriminator Loss: 0.506048\tGenerator Loss: 1.207438\n",
      "Train Epoch: 49 [25600/60000 (43%)]\tDiscriminator Loss: 0.507336\tGenerator Loss: 1.111014\n",
      "Train Epoch: 49 [26880/60000 (45%)]\tDiscriminator Loss: 0.520234\tGenerator Loss: 0.994778\n",
      "Train Epoch: 49 [28160/60000 (47%)]\tDiscriminator Loss: 0.527874\tGenerator Loss: 1.217962\n",
      "Train Epoch: 49 [29440/60000 (49%)]\tDiscriminator Loss: 0.544864\tGenerator Loss: 0.862107\n",
      "Train Epoch: 49 [30720/60000 (51%)]\tDiscriminator Loss: 0.506190\tGenerator Loss: 1.108604\n",
      "Train Epoch: 49 [32000/60000 (53%)]\tDiscriminator Loss: 0.518468\tGenerator Loss: 1.154802\n",
      "Train Epoch: 49 [33280/60000 (55%)]\tDiscriminator Loss: 0.536137\tGenerator Loss: 1.534759\n",
      "Train Epoch: 49 [34560/60000 (58%)]\tDiscriminator Loss: 0.516934\tGenerator Loss: 1.205522\n",
      "Train Epoch: 49 [35840/60000 (60%)]\tDiscriminator Loss: 0.519501\tGenerator Loss: 1.398568\n",
      "Train Epoch: 49 [37120/60000 (62%)]\tDiscriminator Loss: 0.487582\tGenerator Loss: 1.105247\n",
      "Train Epoch: 49 [38400/60000 (64%)]\tDiscriminator Loss: 0.487399\tGenerator Loss: 1.293311\n",
      "Train Epoch: 49 [39680/60000 (66%)]\tDiscriminator Loss: 0.535583\tGenerator Loss: 1.107554\n",
      "Train Epoch: 49 [40960/60000 (68%)]\tDiscriminator Loss: 0.533104\tGenerator Loss: 1.120860\n",
      "Train Epoch: 49 [42240/60000 (70%)]\tDiscriminator Loss: 0.512053\tGenerator Loss: 1.209782\n",
      "Train Epoch: 49 [43520/60000 (72%)]\tDiscriminator Loss: 0.500699\tGenerator Loss: 1.060984\n",
      "Train Epoch: 49 [44800/60000 (75%)]\tDiscriminator Loss: 0.515460\tGenerator Loss: 1.460708\n",
      "Train Epoch: 49 [46080/60000 (77%)]\tDiscriminator Loss: 0.536416\tGenerator Loss: 1.074417\n",
      "Train Epoch: 49 [47360/60000 (79%)]\tDiscriminator Loss: 0.475236\tGenerator Loss: 1.506447\n",
      "Train Epoch: 49 [48640/60000 (81%)]\tDiscriminator Loss: 0.555062\tGenerator Loss: 0.999847\n",
      "Train Epoch: 49 [49920/60000 (83%)]\tDiscriminator Loss: 0.470068\tGenerator Loss: 1.303767\n",
      "Train Epoch: 49 [51200/60000 (85%)]\tDiscriminator Loss: 0.484907\tGenerator Loss: 1.071111\n",
      "Train Epoch: 49 [52480/60000 (87%)]\tDiscriminator Loss: 0.506282\tGenerator Loss: 1.321575\n",
      "Train Epoch: 49 [53760/60000 (90%)]\tDiscriminator Loss: 0.534862\tGenerator Loss: 1.254470\n",
      "Train Epoch: 49 [55040/60000 (92%)]\tDiscriminator Loss: 0.555255\tGenerator Loss: 1.338909\n",
      "Train Epoch: 49 [56320/60000 (94%)]\tDiscriminator Loss: 0.518165\tGenerator Loss: 1.322354\n",
      "Train Epoch: 49 [57600/60000 (96%)]\tDiscriminator Loss: 0.566230\tGenerator Loss: 0.956044\n",
      "Train Epoch: 49 [58880/60000 (98%)]\tDiscriminator Loss: 0.544595\tGenerator Loss: 0.939975\n",
      "Train Epoch: 50 [0/60000 (0%)]\tDiscriminator Loss: 0.439587\tGenerator Loss: 1.418424\n",
      "Train Epoch: 50 [1280/60000 (2%)]\tDiscriminator Loss: 0.510954\tGenerator Loss: 1.014086\n",
      "Train Epoch: 50 [2560/60000 (4%)]\tDiscriminator Loss: 0.556023\tGenerator Loss: 1.282165\n",
      "Train Epoch: 50 [3840/60000 (6%)]\tDiscriminator Loss: 0.498338\tGenerator Loss: 1.680194\n",
      "Train Epoch: 50 [5120/60000 (9%)]\tDiscriminator Loss: 0.538510\tGenerator Loss: 1.267570\n",
      "Train Epoch: 50 [6400/60000 (11%)]\tDiscriminator Loss: 0.519271\tGenerator Loss: 1.062700\n",
      "Train Epoch: 50 [7680/60000 (13%)]\tDiscriminator Loss: 0.475601\tGenerator Loss: 1.343140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 50 [8960/60000 (15%)]\tDiscriminator Loss: 0.593828\tGenerator Loss: 0.944900\n",
      "Train Epoch: 50 [10240/60000 (17%)]\tDiscriminator Loss: 0.529285\tGenerator Loss: 0.963428\n",
      "Train Epoch: 50 [11520/60000 (19%)]\tDiscriminator Loss: 0.481307\tGenerator Loss: 1.118222\n",
      "Train Epoch: 50 [12800/60000 (21%)]\tDiscriminator Loss: 0.523665\tGenerator Loss: 1.417525\n",
      "Train Epoch: 50 [14080/60000 (23%)]\tDiscriminator Loss: 0.609454\tGenerator Loss: 0.953505\n",
      "Train Epoch: 50 [15360/60000 (26%)]\tDiscriminator Loss: 0.479864\tGenerator Loss: 1.231209\n",
      "Train Epoch: 50 [16640/60000 (28%)]\tDiscriminator Loss: 0.507880\tGenerator Loss: 0.873292\n",
      "Train Epoch: 50 [17920/60000 (30%)]\tDiscriminator Loss: 0.506279\tGenerator Loss: 1.167096\n",
      "Train Epoch: 50 [19200/60000 (32%)]\tDiscriminator Loss: 0.520652\tGenerator Loss: 1.124692\n",
      "Train Epoch: 50 [20480/60000 (34%)]\tDiscriminator Loss: 0.542267\tGenerator Loss: 1.919296\n",
      "Train Epoch: 50 [21760/60000 (36%)]\tDiscriminator Loss: 0.564110\tGenerator Loss: 1.064357\n",
      "Train Epoch: 50 [23040/60000 (38%)]\tDiscriminator Loss: 0.493346\tGenerator Loss: 1.443218\n",
      "Train Epoch: 50 [24320/60000 (41%)]\tDiscriminator Loss: 0.509463\tGenerator Loss: 1.377994\n",
      "Train Epoch: 50 [25600/60000 (43%)]\tDiscriminator Loss: 0.538697\tGenerator Loss: 1.153263\n",
      "Train Epoch: 50 [26880/60000 (45%)]\tDiscriminator Loss: 0.539993\tGenerator Loss: 1.692070\n",
      "Train Epoch: 50 [28160/60000 (47%)]\tDiscriminator Loss: 0.491779\tGenerator Loss: 1.183472\n",
      "Train Epoch: 50 [29440/60000 (49%)]\tDiscriminator Loss: 0.564712\tGenerator Loss: 1.665951\n",
      "Train Epoch: 50 [30720/60000 (51%)]\tDiscriminator Loss: 0.438001\tGenerator Loss: 1.731826\n",
      "Train Epoch: 50 [32000/60000 (53%)]\tDiscriminator Loss: 0.512876\tGenerator Loss: 1.227000\n",
      "Train Epoch: 50 [33280/60000 (55%)]\tDiscriminator Loss: 0.503914\tGenerator Loss: 1.335608\n",
      "Train Epoch: 50 [34560/60000 (58%)]\tDiscriminator Loss: 0.473561\tGenerator Loss: 1.144797\n",
      "Train Epoch: 50 [35840/60000 (60%)]\tDiscriminator Loss: 0.535272\tGenerator Loss: 1.231052\n",
      "Train Epoch: 50 [37120/60000 (62%)]\tDiscriminator Loss: 0.491162\tGenerator Loss: 1.158147\n",
      "Train Epoch: 50 [38400/60000 (64%)]\tDiscriminator Loss: 0.520157\tGenerator Loss: 1.019663\n",
      "Train Epoch: 50 [39680/60000 (66%)]\tDiscriminator Loss: 0.459165\tGenerator Loss: 1.372793\n",
      "Train Epoch: 50 [40960/60000 (68%)]\tDiscriminator Loss: 0.479225\tGenerator Loss: 1.521454\n",
      "Train Epoch: 50 [42240/60000 (70%)]\tDiscriminator Loss: 0.540133\tGenerator Loss: 1.321130\n",
      "Train Epoch: 50 [43520/60000 (72%)]\tDiscriminator Loss: 0.542597\tGenerator Loss: 1.133064\n",
      "Train Epoch: 50 [44800/60000 (75%)]\tDiscriminator Loss: 0.588170\tGenerator Loss: 1.630713\n",
      "Train Epoch: 50 [46080/60000 (77%)]\tDiscriminator Loss: 0.500619\tGenerator Loss: 1.334839\n",
      "Train Epoch: 50 [47360/60000 (79%)]\tDiscriminator Loss: 0.576122\tGenerator Loss: 2.222064\n",
      "Train Epoch: 50 [48640/60000 (81%)]\tDiscriminator Loss: 0.504835\tGenerator Loss: 1.214587\n",
      "Train Epoch: 50 [49920/60000 (83%)]\tDiscriminator Loss: 0.463677\tGenerator Loss: 1.280842\n",
      "Train Epoch: 50 [51200/60000 (85%)]\tDiscriminator Loss: 0.567120\tGenerator Loss: 1.483435\n",
      "Train Epoch: 50 [52480/60000 (87%)]\tDiscriminator Loss: 0.471650\tGenerator Loss: 1.337475\n",
      "Train Epoch: 50 [53760/60000 (90%)]\tDiscriminator Loss: 0.506195\tGenerator Loss: 1.012765\n",
      "Train Epoch: 50 [55040/60000 (92%)]\tDiscriminator Loss: 0.576624\tGenerator Loss: 1.436365\n",
      "Train Epoch: 50 [56320/60000 (94%)]\tDiscriminator Loss: 0.429573\tGenerator Loss: 1.368368\n",
      "Train Epoch: 50 [57600/60000 (96%)]\tDiscriminator Loss: 0.532656\tGenerator Loss: 1.165448\n",
      "Train Epoch: 50 [58880/60000 (98%)]\tDiscriminator Loss: 0.521246\tGenerator Loss: 1.376578\n",
      "Train Epoch: 51 [0/60000 (0%)]\tDiscriminator Loss: 0.450827\tGenerator Loss: 1.402881\n",
      "Train Epoch: 51 [1280/60000 (2%)]\tDiscriminator Loss: 0.490675\tGenerator Loss: 1.393473\n",
      "Train Epoch: 51 [2560/60000 (4%)]\tDiscriminator Loss: 0.537700\tGenerator Loss: 1.275622\n",
      "Train Epoch: 51 [3840/60000 (6%)]\tDiscriminator Loss: 0.508712\tGenerator Loss: 1.358526\n",
      "Train Epoch: 51 [5120/60000 (9%)]\tDiscriminator Loss: 0.595040\tGenerator Loss: 1.369698\n",
      "Train Epoch: 51 [6400/60000 (11%)]\tDiscriminator Loss: 0.456703\tGenerator Loss: 1.919197\n",
      "Train Epoch: 51 [7680/60000 (13%)]\tDiscriminator Loss: 0.542413\tGenerator Loss: 0.957898\n",
      "Train Epoch: 51 [8960/60000 (15%)]\tDiscriminator Loss: 0.506910\tGenerator Loss: 1.184000\n",
      "Train Epoch: 51 [10240/60000 (17%)]\tDiscriminator Loss: 0.461228\tGenerator Loss: 1.392868\n",
      "Train Epoch: 51 [11520/60000 (19%)]\tDiscriminator Loss: 0.570771\tGenerator Loss: 0.963844\n",
      "Train Epoch: 51 [12800/60000 (21%)]\tDiscriminator Loss: 0.506106\tGenerator Loss: 1.206913\n",
      "Train Epoch: 51 [14080/60000 (23%)]\tDiscriminator Loss: 0.490952\tGenerator Loss: 1.149953\n",
      "Train Epoch: 51 [15360/60000 (26%)]\tDiscriminator Loss: 0.549395\tGenerator Loss: 1.103291\n",
      "Train Epoch: 51 [16640/60000 (28%)]\tDiscriminator Loss: 0.490441\tGenerator Loss: 1.358443\n",
      "Train Epoch: 51 [17920/60000 (30%)]\tDiscriminator Loss: 0.538787\tGenerator Loss: 1.030817\n",
      "Train Epoch: 51 [19200/60000 (32%)]\tDiscriminator Loss: 0.544629\tGenerator Loss: 1.409028\n",
      "Train Epoch: 51 [20480/60000 (34%)]\tDiscriminator Loss: 0.482857\tGenerator Loss: 1.194503\n",
      "Train Epoch: 51 [21760/60000 (36%)]\tDiscriminator Loss: 0.510848\tGenerator Loss: 1.100980\n",
      "Train Epoch: 51 [23040/60000 (38%)]\tDiscriminator Loss: 0.487520\tGenerator Loss: 1.395488\n",
      "Train Epoch: 51 [24320/60000 (41%)]\tDiscriminator Loss: 0.689365\tGenerator Loss: 1.662508\n",
      "Train Epoch: 51 [25600/60000 (43%)]\tDiscriminator Loss: 0.493817\tGenerator Loss: 1.183456\n",
      "Train Epoch: 51 [26880/60000 (45%)]\tDiscriminator Loss: 0.580099\tGenerator Loss: 1.703764\n",
      "Train Epoch: 51 [28160/60000 (47%)]\tDiscriminator Loss: 0.520441\tGenerator Loss: 1.352200\n",
      "Train Epoch: 51 [29440/60000 (49%)]\tDiscriminator Loss: 0.414086\tGenerator Loss: 1.544336\n",
      "Train Epoch: 51 [30720/60000 (51%)]\tDiscriminator Loss: 0.533953\tGenerator Loss: 1.100443\n",
      "Train Epoch: 51 [32000/60000 (53%)]\tDiscriminator Loss: 0.485608\tGenerator Loss: 1.153889\n",
      "Train Epoch: 51 [33280/60000 (55%)]\tDiscriminator Loss: 0.548530\tGenerator Loss: 1.432598\n",
      "Train Epoch: 51 [34560/60000 (58%)]\tDiscriminator Loss: 0.553805\tGenerator Loss: 1.087607\n",
      "Train Epoch: 51 [35840/60000 (60%)]\tDiscriminator Loss: 0.457587\tGenerator Loss: 1.441371\n",
      "Train Epoch: 51 [37120/60000 (62%)]\tDiscriminator Loss: 0.509425\tGenerator Loss: 1.039503\n",
      "Train Epoch: 51 [38400/60000 (64%)]\tDiscriminator Loss: 0.529898\tGenerator Loss: 1.206936\n",
      "Train Epoch: 51 [39680/60000 (66%)]\tDiscriminator Loss: 0.519444\tGenerator Loss: 1.488055\n",
      "Train Epoch: 51 [40960/60000 (68%)]\tDiscriminator Loss: 0.553371\tGenerator Loss: 1.078516\n",
      "Train Epoch: 51 [42240/60000 (70%)]\tDiscriminator Loss: 0.493506\tGenerator Loss: 1.152940\n",
      "Train Epoch: 51 [43520/60000 (72%)]\tDiscriminator Loss: 0.520275\tGenerator Loss: 1.251331\n",
      "Train Epoch: 51 [44800/60000 (75%)]\tDiscriminator Loss: 0.555095\tGenerator Loss: 1.291694\n",
      "Train Epoch: 51 [46080/60000 (77%)]\tDiscriminator Loss: 0.514013\tGenerator Loss: 1.236814\n",
      "Train Epoch: 51 [47360/60000 (79%)]\tDiscriminator Loss: 0.531316\tGenerator Loss: 1.248263\n",
      "Train Epoch: 51 [48640/60000 (81%)]\tDiscriminator Loss: 0.519836\tGenerator Loss: 1.186466\n",
      "Train Epoch: 51 [49920/60000 (83%)]\tDiscriminator Loss: 0.524350\tGenerator Loss: 1.344678\n",
      "Train Epoch: 51 [51200/60000 (85%)]\tDiscriminator Loss: 0.494718\tGenerator Loss: 1.156072\n",
      "Train Epoch: 51 [52480/60000 (87%)]\tDiscriminator Loss: 0.554695\tGenerator Loss: 1.480329\n",
      "Train Epoch: 51 [53760/60000 (90%)]\tDiscriminator Loss: 0.551914\tGenerator Loss: 0.900436\n",
      "Train Epoch: 51 [55040/60000 (92%)]\tDiscriminator Loss: 0.508390\tGenerator Loss: 1.542788\n",
      "Train Epoch: 51 [56320/60000 (94%)]\tDiscriminator Loss: 0.528461\tGenerator Loss: 1.132825\n",
      "Train Epoch: 51 [57600/60000 (96%)]\tDiscriminator Loss: 0.537141\tGenerator Loss: 1.369015\n",
      "Train Epoch: 51 [58880/60000 (98%)]\tDiscriminator Loss: 0.507047\tGenerator Loss: 1.593874\n",
      "Train Epoch: 52 [0/60000 (0%)]\tDiscriminator Loss: 0.495270\tGenerator Loss: 1.081058\n",
      "Train Epoch: 52 [1280/60000 (2%)]\tDiscriminator Loss: 0.529076\tGenerator Loss: 1.340678\n",
      "Train Epoch: 52 [2560/60000 (4%)]\tDiscriminator Loss: 0.511194\tGenerator Loss: 1.203939\n",
      "Train Epoch: 52 [3840/60000 (6%)]\tDiscriminator Loss: 0.553601\tGenerator Loss: 1.705379\n",
      "Train Epoch: 52 [5120/60000 (9%)]\tDiscriminator Loss: 0.535816\tGenerator Loss: 1.001176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 52 [6400/60000 (11%)]\tDiscriminator Loss: 0.563105\tGenerator Loss: 1.834571\n",
      "Train Epoch: 52 [7680/60000 (13%)]\tDiscriminator Loss: 0.461352\tGenerator Loss: 1.355463\n",
      "Train Epoch: 52 [8960/60000 (15%)]\tDiscriminator Loss: 0.517958\tGenerator Loss: 1.335838\n",
      "Train Epoch: 52 [10240/60000 (17%)]\tDiscriminator Loss: 0.454284\tGenerator Loss: 1.324137\n",
      "Train Epoch: 52 [11520/60000 (19%)]\tDiscriminator Loss: 0.548218\tGenerator Loss: 1.109543\n",
      "Train Epoch: 52 [12800/60000 (21%)]\tDiscriminator Loss: 0.520180\tGenerator Loss: 1.327124\n",
      "Train Epoch: 52 [14080/60000 (23%)]\tDiscriminator Loss: 0.504813\tGenerator Loss: 1.285892\n",
      "Train Epoch: 52 [15360/60000 (26%)]\tDiscriminator Loss: 0.514722\tGenerator Loss: 1.562829\n",
      "Train Epoch: 52 [16640/60000 (28%)]\tDiscriminator Loss: 0.526643\tGenerator Loss: 1.302794\n",
      "Train Epoch: 52 [17920/60000 (30%)]\tDiscriminator Loss: 0.493532\tGenerator Loss: 1.133038\n",
      "Train Epoch: 52 [19200/60000 (32%)]\tDiscriminator Loss: 0.505783\tGenerator Loss: 1.515149\n",
      "Train Epoch: 52 [20480/60000 (34%)]\tDiscriminator Loss: 0.514346\tGenerator Loss: 1.117510\n",
      "Train Epoch: 52 [21760/60000 (36%)]\tDiscriminator Loss: 0.507180\tGenerator Loss: 1.631459\n",
      "Train Epoch: 52 [23040/60000 (38%)]\tDiscriminator Loss: 0.560308\tGenerator Loss: 0.974884\n",
      "Train Epoch: 52 [24320/60000 (41%)]\tDiscriminator Loss: 0.541227\tGenerator Loss: 1.049711\n",
      "Train Epoch: 52 [25600/60000 (43%)]\tDiscriminator Loss: 0.575480\tGenerator Loss: 1.583024\n",
      "Train Epoch: 52 [26880/60000 (45%)]\tDiscriminator Loss: 0.583524\tGenerator Loss: 1.045627\n",
      "Train Epoch: 52 [28160/60000 (47%)]\tDiscriminator Loss: 0.510935\tGenerator Loss: 1.483228\n",
      "Train Epoch: 52 [29440/60000 (49%)]\tDiscriminator Loss: 0.523992\tGenerator Loss: 1.230795\n",
      "Train Epoch: 52 [30720/60000 (51%)]\tDiscriminator Loss: 0.572979\tGenerator Loss: 1.486353\n",
      "Train Epoch: 52 [32000/60000 (53%)]\tDiscriminator Loss: 0.505502\tGenerator Loss: 1.688223\n",
      "Train Epoch: 52 [33280/60000 (55%)]\tDiscriminator Loss: 0.515190\tGenerator Loss: 1.112380\n",
      "Train Epoch: 52 [34560/60000 (58%)]\tDiscriminator Loss: 0.499360\tGenerator Loss: 1.522956\n",
      "Train Epoch: 52 [35840/60000 (60%)]\tDiscriminator Loss: 0.613623\tGenerator Loss: 1.442447\n",
      "Train Epoch: 52 [37120/60000 (62%)]\tDiscriminator Loss: 0.486274\tGenerator Loss: 1.152054\n",
      "Train Epoch: 52 [38400/60000 (64%)]\tDiscriminator Loss: 0.512244\tGenerator Loss: 1.184645\n",
      "Train Epoch: 52 [39680/60000 (66%)]\tDiscriminator Loss: 0.577918\tGenerator Loss: 1.583290\n",
      "Train Epoch: 52 [40960/60000 (68%)]\tDiscriminator Loss: 0.531979\tGenerator Loss: 1.275498\n",
      "Train Epoch: 52 [42240/60000 (70%)]\tDiscriminator Loss: 0.544150\tGenerator Loss: 1.602196\n",
      "Train Epoch: 52 [43520/60000 (72%)]\tDiscriminator Loss: 0.526052\tGenerator Loss: 1.228714\n",
      "Train Epoch: 52 [44800/60000 (75%)]\tDiscriminator Loss: 0.657187\tGenerator Loss: 0.746495\n",
      "Train Epoch: 52 [46080/60000 (77%)]\tDiscriminator Loss: 0.526996\tGenerator Loss: 0.982744\n",
      "Train Epoch: 52 [47360/60000 (79%)]\tDiscriminator Loss: 0.461790\tGenerator Loss: 1.474609\n",
      "Train Epoch: 52 [48640/60000 (81%)]\tDiscriminator Loss: 0.541487\tGenerator Loss: 1.039052\n",
      "Train Epoch: 52 [49920/60000 (83%)]\tDiscriminator Loss: 0.503853\tGenerator Loss: 1.424028\n",
      "Train Epoch: 52 [51200/60000 (85%)]\tDiscriminator Loss: 0.529878\tGenerator Loss: 1.219697\n",
      "Train Epoch: 52 [52480/60000 (87%)]\tDiscriminator Loss: 0.590512\tGenerator Loss: 1.780957\n",
      "Train Epoch: 52 [53760/60000 (90%)]\tDiscriminator Loss: 0.464469\tGenerator Loss: 1.496428\n",
      "Train Epoch: 52 [55040/60000 (92%)]\tDiscriminator Loss: 0.497360\tGenerator Loss: 1.481816\n",
      "Train Epoch: 52 [56320/60000 (94%)]\tDiscriminator Loss: 0.544103\tGenerator Loss: 1.099161\n",
      "Train Epoch: 52 [57600/60000 (96%)]\tDiscriminator Loss: 0.528845\tGenerator Loss: 1.194557\n",
      "Train Epoch: 52 [58880/60000 (98%)]\tDiscriminator Loss: 0.449044\tGenerator Loss: 1.280290\n",
      "Train Epoch: 53 [0/60000 (0%)]\tDiscriminator Loss: 0.500460\tGenerator Loss: 1.352078\n",
      "Train Epoch: 53 [1280/60000 (2%)]\tDiscriminator Loss: 0.529448\tGenerator Loss: 1.299344\n",
      "Train Epoch: 53 [2560/60000 (4%)]\tDiscriminator Loss: 0.479448\tGenerator Loss: 1.268095\n",
      "Train Epoch: 53 [3840/60000 (6%)]\tDiscriminator Loss: 0.517309\tGenerator Loss: 1.128792\n",
      "Train Epoch: 53 [5120/60000 (9%)]\tDiscriminator Loss: 0.516119\tGenerator Loss: 1.416053\n",
      "Train Epoch: 53 [6400/60000 (11%)]\tDiscriminator Loss: 0.509143\tGenerator Loss: 1.022099\n",
      "Train Epoch: 53 [7680/60000 (13%)]\tDiscriminator Loss: 0.572409\tGenerator Loss: 1.160743\n",
      "Train Epoch: 53 [8960/60000 (15%)]\tDiscriminator Loss: 0.491995\tGenerator Loss: 1.564332\n",
      "Train Epoch: 53 [10240/60000 (17%)]\tDiscriminator Loss: 0.555362\tGenerator Loss: 0.954013\n",
      "Train Epoch: 53 [11520/60000 (19%)]\tDiscriminator Loss: 0.446770\tGenerator Loss: 1.678370\n",
      "Train Epoch: 53 [12800/60000 (21%)]\tDiscriminator Loss: 0.513700\tGenerator Loss: 1.431726\n",
      "Train Epoch: 53 [14080/60000 (23%)]\tDiscriminator Loss: 0.594136\tGenerator Loss: 1.437131\n",
      "Train Epoch: 53 [15360/60000 (26%)]\tDiscriminator Loss: 0.468855\tGenerator Loss: 1.271543\n",
      "Train Epoch: 53 [16640/60000 (28%)]\tDiscriminator Loss: 0.536088\tGenerator Loss: 1.190507\n",
      "Train Epoch: 53 [17920/60000 (30%)]\tDiscriminator Loss: 0.534901\tGenerator Loss: 1.795405\n",
      "Train Epoch: 53 [19200/60000 (32%)]\tDiscriminator Loss: 0.524334\tGenerator Loss: 0.974595\n",
      "Train Epoch: 53 [20480/60000 (34%)]\tDiscriminator Loss: 0.496040\tGenerator Loss: 1.369107\n",
      "Train Epoch: 53 [21760/60000 (36%)]\tDiscriminator Loss: 0.511729\tGenerator Loss: 1.306707\n",
      "Train Epoch: 53 [23040/60000 (38%)]\tDiscriminator Loss: 0.532426\tGenerator Loss: 1.019094\n",
      "Train Epoch: 53 [24320/60000 (41%)]\tDiscriminator Loss: 0.541222\tGenerator Loss: 1.305718\n",
      "Train Epoch: 53 [25600/60000 (43%)]\tDiscriminator Loss: 0.527662\tGenerator Loss: 1.374489\n",
      "Train Epoch: 53 [26880/60000 (45%)]\tDiscriminator Loss: 0.486817\tGenerator Loss: 1.403977\n",
      "Train Epoch: 53 [28160/60000 (47%)]\tDiscriminator Loss: 0.581875\tGenerator Loss: 0.868086\n",
      "Train Epoch: 53 [29440/60000 (49%)]\tDiscriminator Loss: 0.488417\tGenerator Loss: 1.381767\n",
      "Train Epoch: 53 [30720/60000 (51%)]\tDiscriminator Loss: 0.499400\tGenerator Loss: 1.397945\n",
      "Train Epoch: 53 [32000/60000 (53%)]\tDiscriminator Loss: 0.532244\tGenerator Loss: 1.095597\n",
      "Train Epoch: 53 [33280/60000 (55%)]\tDiscriminator Loss: 0.512695\tGenerator Loss: 1.712455\n",
      "Train Epoch: 53 [34560/60000 (58%)]\tDiscriminator Loss: 0.522346\tGenerator Loss: 1.282165\n",
      "Train Epoch: 53 [35840/60000 (60%)]\tDiscriminator Loss: 0.587389\tGenerator Loss: 1.189663\n",
      "Train Epoch: 53 [37120/60000 (62%)]\tDiscriminator Loss: 0.459911\tGenerator Loss: 1.205972\n",
      "Train Epoch: 53 [38400/60000 (64%)]\tDiscriminator Loss: 0.486508\tGenerator Loss: 1.229062\n",
      "Train Epoch: 53 [39680/60000 (66%)]\tDiscriminator Loss: 0.562565\tGenerator Loss: 1.678188\n",
      "Train Epoch: 53 [40960/60000 (68%)]\tDiscriminator Loss: 0.502153\tGenerator Loss: 1.668661\n",
      "Train Epoch: 53 [42240/60000 (70%)]\tDiscriminator Loss: 0.540175\tGenerator Loss: 0.962870\n",
      "Train Epoch: 53 [43520/60000 (72%)]\tDiscriminator Loss: 0.533718\tGenerator Loss: 1.297146\n",
      "Train Epoch: 53 [44800/60000 (75%)]\tDiscriminator Loss: 0.526794\tGenerator Loss: 1.518453\n",
      "Train Epoch: 53 [46080/60000 (77%)]\tDiscriminator Loss: 0.588369\tGenerator Loss: 0.917896\n",
      "Train Epoch: 53 [47360/60000 (79%)]\tDiscriminator Loss: 0.470484\tGenerator Loss: 1.408899\n",
      "Train Epoch: 53 [48640/60000 (81%)]\tDiscriminator Loss: 0.488697\tGenerator Loss: 1.265065\n",
      "Train Epoch: 53 [49920/60000 (83%)]\tDiscriminator Loss: 0.503262\tGenerator Loss: 1.598099\n",
      "Train Epoch: 53 [51200/60000 (85%)]\tDiscriminator Loss: 0.497229\tGenerator Loss: 1.249374\n",
      "Train Epoch: 53 [52480/60000 (87%)]\tDiscriminator Loss: 0.571545\tGenerator Loss: 1.370612\n",
      "Train Epoch: 53 [53760/60000 (90%)]\tDiscriminator Loss: 0.492145\tGenerator Loss: 1.020976\n",
      "Train Epoch: 53 [55040/60000 (92%)]\tDiscriminator Loss: 0.472649\tGenerator Loss: 1.269252\n",
      "Train Epoch: 53 [56320/60000 (94%)]\tDiscriminator Loss: 0.559882\tGenerator Loss: 1.028072\n",
      "Train Epoch: 53 [57600/60000 (96%)]\tDiscriminator Loss: 0.582745\tGenerator Loss: 1.104668\n",
      "Train Epoch: 53 [58880/60000 (98%)]\tDiscriminator Loss: 0.513500\tGenerator Loss: 1.475662\n",
      "Train Epoch: 54 [0/60000 (0%)]\tDiscriminator Loss: 0.518716\tGenerator Loss: 1.294364\n",
      "Train Epoch: 54 [1280/60000 (2%)]\tDiscriminator Loss: 0.567425\tGenerator Loss: 0.892309\n",
      "Train Epoch: 54 [2560/60000 (4%)]\tDiscriminator Loss: 0.489527\tGenerator Loss: 1.194165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 54 [3840/60000 (6%)]\tDiscriminator Loss: 0.490862\tGenerator Loss: 1.243849\n",
      "Train Epoch: 54 [5120/60000 (9%)]\tDiscriminator Loss: 0.524913\tGenerator Loss: 1.082061\n",
      "Train Epoch: 54 [6400/60000 (11%)]\tDiscriminator Loss: 0.502418\tGenerator Loss: 1.153010\n",
      "Train Epoch: 54 [7680/60000 (13%)]\tDiscriminator Loss: 0.500736\tGenerator Loss: 1.418671\n",
      "Train Epoch: 54 [8960/60000 (15%)]\tDiscriminator Loss: 0.553698\tGenerator Loss: 1.247026\n",
      "Train Epoch: 54 [10240/60000 (17%)]\tDiscriminator Loss: 0.500292\tGenerator Loss: 1.318398\n",
      "Train Epoch: 54 [11520/60000 (19%)]\tDiscriminator Loss: 0.508531\tGenerator Loss: 1.340472\n",
      "Train Epoch: 54 [12800/60000 (21%)]\tDiscriminator Loss: 0.540528\tGenerator Loss: 1.490043\n",
      "Train Epoch: 54 [14080/60000 (23%)]\tDiscriminator Loss: 0.490768\tGenerator Loss: 1.548845\n",
      "Train Epoch: 54 [15360/60000 (26%)]\tDiscriminator Loss: 0.528454\tGenerator Loss: 1.147866\n",
      "Train Epoch: 54 [16640/60000 (28%)]\tDiscriminator Loss: 0.541851\tGenerator Loss: 1.060798\n",
      "Train Epoch: 54 [17920/60000 (30%)]\tDiscriminator Loss: 0.523764\tGenerator Loss: 1.195659\n",
      "Train Epoch: 54 [19200/60000 (32%)]\tDiscriminator Loss: 0.510845\tGenerator Loss: 1.440016\n",
      "Train Epoch: 54 [20480/60000 (34%)]\tDiscriminator Loss: 0.497694\tGenerator Loss: 1.361554\n",
      "Train Epoch: 54 [21760/60000 (36%)]\tDiscriminator Loss: 0.500547\tGenerator Loss: 1.108144\n",
      "Train Epoch: 54 [23040/60000 (38%)]\tDiscriminator Loss: 0.459279\tGenerator Loss: 1.243242\n",
      "Train Epoch: 54 [24320/60000 (41%)]\tDiscriminator Loss: 0.513120\tGenerator Loss: 1.108578\n",
      "Train Epoch: 54 [25600/60000 (43%)]\tDiscriminator Loss: 0.514160\tGenerator Loss: 1.288831\n",
      "Train Epoch: 54 [26880/60000 (45%)]\tDiscriminator Loss: 0.516959\tGenerator Loss: 1.021209\n",
      "Train Epoch: 54 [28160/60000 (47%)]\tDiscriminator Loss: 0.518322\tGenerator Loss: 1.258524\n",
      "Train Epoch: 54 [29440/60000 (49%)]\tDiscriminator Loss: 0.510385\tGenerator Loss: 1.309645\n",
      "Train Epoch: 54 [30720/60000 (51%)]\tDiscriminator Loss: 0.516311\tGenerator Loss: 1.345230\n",
      "Train Epoch: 54 [32000/60000 (53%)]\tDiscriminator Loss: 0.531955\tGenerator Loss: 1.227692\n",
      "Train Epoch: 54 [33280/60000 (55%)]\tDiscriminator Loss: 0.553020\tGenerator Loss: 1.021542\n",
      "Train Epoch: 54 [34560/60000 (58%)]\tDiscriminator Loss: 0.617086\tGenerator Loss: 1.200325\n",
      "Train Epoch: 54 [35840/60000 (60%)]\tDiscriminator Loss: 0.586747\tGenerator Loss: 1.616364\n",
      "Train Epoch: 54 [37120/60000 (62%)]\tDiscriminator Loss: 0.462442\tGenerator Loss: 1.184463\n",
      "Train Epoch: 54 [38400/60000 (64%)]\tDiscriminator Loss: 0.444407\tGenerator Loss: 1.324855\n",
      "Train Epoch: 54 [39680/60000 (66%)]\tDiscriminator Loss: 0.545768\tGenerator Loss: 1.266170\n",
      "Train Epoch: 54 [40960/60000 (68%)]\tDiscriminator Loss: 0.502026\tGenerator Loss: 1.509923\n",
      "Train Epoch: 54 [42240/60000 (70%)]\tDiscriminator Loss: 0.543434\tGenerator Loss: 1.380999\n",
      "Train Epoch: 54 [43520/60000 (72%)]\tDiscriminator Loss: 0.485755\tGenerator Loss: 1.279438\n",
      "Train Epoch: 54 [44800/60000 (75%)]\tDiscriminator Loss: 0.485852\tGenerator Loss: 1.608454\n",
      "Train Epoch: 54 [46080/60000 (77%)]\tDiscriminator Loss: 0.637755\tGenerator Loss: 0.632964\n",
      "Train Epoch: 54 [47360/60000 (79%)]\tDiscriminator Loss: 0.492818\tGenerator Loss: 1.310075\n",
      "Train Epoch: 54 [48640/60000 (81%)]\tDiscriminator Loss: 0.525441\tGenerator Loss: 1.546582\n",
      "Train Epoch: 54 [49920/60000 (83%)]\tDiscriminator Loss: 0.541804\tGenerator Loss: 1.395398\n",
      "Train Epoch: 54 [51200/60000 (85%)]\tDiscriminator Loss: 0.510942\tGenerator Loss: 1.174587\n",
      "Train Epoch: 54 [52480/60000 (87%)]\tDiscriminator Loss: 0.526378\tGenerator Loss: 1.136707\n",
      "Train Epoch: 54 [53760/60000 (90%)]\tDiscriminator Loss: 0.555941\tGenerator Loss: 1.373663\n",
      "Train Epoch: 54 [55040/60000 (92%)]\tDiscriminator Loss: 0.530017\tGenerator Loss: 1.282736\n",
      "Train Epoch: 54 [56320/60000 (94%)]\tDiscriminator Loss: 0.502204\tGenerator Loss: 1.637493\n",
      "Train Epoch: 54 [57600/60000 (96%)]\tDiscriminator Loss: 0.602950\tGenerator Loss: 0.795961\n",
      "Train Epoch: 54 [58880/60000 (98%)]\tDiscriminator Loss: 0.474514\tGenerator Loss: 1.118380\n",
      "Train Epoch: 55 [0/60000 (0%)]\tDiscriminator Loss: 0.572921\tGenerator Loss: 0.867690\n",
      "Train Epoch: 55 [1280/60000 (2%)]\tDiscriminator Loss: 0.526020\tGenerator Loss: 1.200205\n",
      "Train Epoch: 55 [2560/60000 (4%)]\tDiscriminator Loss: 0.529541\tGenerator Loss: 1.072153\n",
      "Train Epoch: 55 [3840/60000 (6%)]\tDiscriminator Loss: 0.465771\tGenerator Loss: 1.531330\n",
      "Train Epoch: 55 [5120/60000 (9%)]\tDiscriminator Loss: 0.484681\tGenerator Loss: 1.146931\n",
      "Train Epoch: 55 [6400/60000 (11%)]\tDiscriminator Loss: 0.481636\tGenerator Loss: 1.262494\n",
      "Train Epoch: 55 [7680/60000 (13%)]\tDiscriminator Loss: 0.523094\tGenerator Loss: 1.094071\n",
      "Train Epoch: 55 [8960/60000 (15%)]\tDiscriminator Loss: 0.454713\tGenerator Loss: 1.466965\n",
      "Train Epoch: 55 [10240/60000 (17%)]\tDiscriminator Loss: 0.484943\tGenerator Loss: 1.139286\n",
      "Train Epoch: 55 [11520/60000 (19%)]\tDiscriminator Loss: 0.736184\tGenerator Loss: 0.501697\n",
      "Train Epoch: 55 [12800/60000 (21%)]\tDiscriminator Loss: 0.482710\tGenerator Loss: 1.375952\n",
      "Train Epoch: 55 [14080/60000 (23%)]\tDiscriminator Loss: 0.512923\tGenerator Loss: 1.943998\n",
      "Train Epoch: 55 [15360/60000 (26%)]\tDiscriminator Loss: 0.522610\tGenerator Loss: 1.316514\n",
      "Train Epoch: 55 [16640/60000 (28%)]\tDiscriminator Loss: 0.549220\tGenerator Loss: 1.517524\n",
      "Train Epoch: 55 [17920/60000 (30%)]\tDiscriminator Loss: 0.480459\tGenerator Loss: 1.402769\n",
      "Train Epoch: 55 [19200/60000 (32%)]\tDiscriminator Loss: 0.483919\tGenerator Loss: 1.211901\n",
      "Train Epoch: 55 [20480/60000 (34%)]\tDiscriminator Loss: 0.540076\tGenerator Loss: 1.136301\n",
      "Train Epoch: 55 [21760/60000 (36%)]\tDiscriminator Loss: 0.501398\tGenerator Loss: 1.682786\n",
      "Train Epoch: 55 [23040/60000 (38%)]\tDiscriminator Loss: 0.521312\tGenerator Loss: 1.281625\n",
      "Train Epoch: 55 [24320/60000 (41%)]\tDiscriminator Loss: 0.555356\tGenerator Loss: 1.098465\n",
      "Train Epoch: 55 [25600/60000 (43%)]\tDiscriminator Loss: 0.484383\tGenerator Loss: 1.093308\n",
      "Train Epoch: 55 [26880/60000 (45%)]\tDiscriminator Loss: 0.521511\tGenerator Loss: 1.261260\n",
      "Train Epoch: 55 [28160/60000 (47%)]\tDiscriminator Loss: 0.507149\tGenerator Loss: 1.408359\n",
      "Train Epoch: 55 [29440/60000 (49%)]\tDiscriminator Loss: 0.475906\tGenerator Loss: 1.439606\n",
      "Train Epoch: 55 [30720/60000 (51%)]\tDiscriminator Loss: 0.521856\tGenerator Loss: 1.662238\n",
      "Train Epoch: 55 [32000/60000 (53%)]\tDiscriminator Loss: 0.493918\tGenerator Loss: 1.321820\n",
      "Train Epoch: 55 [33280/60000 (55%)]\tDiscriminator Loss: 0.684187\tGenerator Loss: 0.569544\n",
      "Train Epoch: 55 [34560/60000 (58%)]\tDiscriminator Loss: 0.527089\tGenerator Loss: 1.462415\n",
      "Train Epoch: 55 [35840/60000 (60%)]\tDiscriminator Loss: 0.562878\tGenerator Loss: 1.453647\n",
      "Train Epoch: 55 [37120/60000 (62%)]\tDiscriminator Loss: 0.511998\tGenerator Loss: 1.378009\n",
      "Train Epoch: 55 [38400/60000 (64%)]\tDiscriminator Loss: 0.528088\tGenerator Loss: 1.719568\n",
      "Train Epoch: 55 [39680/60000 (66%)]\tDiscriminator Loss: 0.552968\tGenerator Loss: 1.177822\n",
      "Train Epoch: 55 [40960/60000 (68%)]\tDiscriminator Loss: 0.488491\tGenerator Loss: 1.320548\n",
      "Train Epoch: 55 [42240/60000 (70%)]\tDiscriminator Loss: 0.488392\tGenerator Loss: 1.147467\n",
      "Train Epoch: 55 [43520/60000 (72%)]\tDiscriminator Loss: 0.478046\tGenerator Loss: 1.389357\n",
      "Train Epoch: 55 [44800/60000 (75%)]\tDiscriminator Loss: 0.515272\tGenerator Loss: 1.208211\n",
      "Train Epoch: 55 [46080/60000 (77%)]\tDiscriminator Loss: 0.490299\tGenerator Loss: 1.578576\n",
      "Train Epoch: 55 [47360/60000 (79%)]\tDiscriminator Loss: 0.584357\tGenerator Loss: 1.507002\n",
      "Train Epoch: 55 [48640/60000 (81%)]\tDiscriminator Loss: 0.518999\tGenerator Loss: 1.154512\n",
      "Train Epoch: 55 [49920/60000 (83%)]\tDiscriminator Loss: 0.538512\tGenerator Loss: 0.952641\n",
      "Train Epoch: 55 [51200/60000 (85%)]\tDiscriminator Loss: 0.470272\tGenerator Loss: 1.201635\n",
      "Train Epoch: 55 [52480/60000 (87%)]\tDiscriminator Loss: 0.555908\tGenerator Loss: 0.993719\n",
      "Train Epoch: 55 [53760/60000 (90%)]\tDiscriminator Loss: 0.532798\tGenerator Loss: 1.389288\n",
      "Train Epoch: 55 [55040/60000 (92%)]\tDiscriminator Loss: 0.478464\tGenerator Loss: 1.406746\n",
      "Train Epoch: 55 [56320/60000 (94%)]\tDiscriminator Loss: 0.533764\tGenerator Loss: 1.055759\n",
      "Train Epoch: 55 [57600/60000 (96%)]\tDiscriminator Loss: 0.496699\tGenerator Loss: 1.132925\n",
      "Train Epoch: 55 [58880/60000 (98%)]\tDiscriminator Loss: 0.455797\tGenerator Loss: 1.307916\n",
      "Train Epoch: 56 [0/60000 (0%)]\tDiscriminator Loss: 0.458522\tGenerator Loss: 1.259267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 56 [1280/60000 (2%)]\tDiscriminator Loss: 0.544440\tGenerator Loss: 1.704585\n",
      "Train Epoch: 56 [2560/60000 (4%)]\tDiscriminator Loss: 0.496068\tGenerator Loss: 1.345796\n",
      "Train Epoch: 56 [3840/60000 (6%)]\tDiscriminator Loss: 0.470951\tGenerator Loss: 1.431653\n",
      "Train Epoch: 56 [5120/60000 (9%)]\tDiscriminator Loss: 0.525855\tGenerator Loss: 1.359481\n",
      "Train Epoch: 56 [6400/60000 (11%)]\tDiscriminator Loss: 0.460788\tGenerator Loss: 1.348487\n",
      "Train Epoch: 56 [7680/60000 (13%)]\tDiscriminator Loss: 0.590197\tGenerator Loss: 0.979263\n",
      "Train Epoch: 56 [8960/60000 (15%)]\tDiscriminator Loss: 0.503106\tGenerator Loss: 1.229435\n",
      "Train Epoch: 56 [10240/60000 (17%)]\tDiscriminator Loss: 0.521585\tGenerator Loss: 1.355930\n",
      "Train Epoch: 56 [11520/60000 (19%)]\tDiscriminator Loss: 0.478864\tGenerator Loss: 1.220316\n",
      "Train Epoch: 56 [12800/60000 (21%)]\tDiscriminator Loss: 0.508284\tGenerator Loss: 1.317184\n",
      "Train Epoch: 56 [14080/60000 (23%)]\tDiscriminator Loss: 0.487273\tGenerator Loss: 1.329406\n",
      "Train Epoch: 56 [15360/60000 (26%)]\tDiscriminator Loss: 0.532573\tGenerator Loss: 1.321116\n",
      "Train Epoch: 56 [16640/60000 (28%)]\tDiscriminator Loss: 0.545360\tGenerator Loss: 1.000190\n",
      "Train Epoch: 56 [17920/60000 (30%)]\tDiscriminator Loss: 0.481580\tGenerator Loss: 1.340694\n",
      "Train Epoch: 56 [19200/60000 (32%)]\tDiscriminator Loss: 0.502520\tGenerator Loss: 1.395706\n",
      "Train Epoch: 56 [20480/60000 (34%)]\tDiscriminator Loss: 0.558433\tGenerator Loss: 1.669010\n",
      "Train Epoch: 56 [21760/60000 (36%)]\tDiscriminator Loss: 0.522181\tGenerator Loss: 1.462799\n",
      "Train Epoch: 56 [23040/60000 (38%)]\tDiscriminator Loss: 0.527566\tGenerator Loss: 1.239543\n",
      "Train Epoch: 56 [24320/60000 (41%)]\tDiscriminator Loss: 0.546449\tGenerator Loss: 1.170530\n",
      "Train Epoch: 56 [25600/60000 (43%)]\tDiscriminator Loss: 0.522730\tGenerator Loss: 1.351149\n",
      "Train Epoch: 56 [26880/60000 (45%)]\tDiscriminator Loss: 0.586180\tGenerator Loss: 1.701921\n",
      "Train Epoch: 56 [28160/60000 (47%)]\tDiscriminator Loss: 0.546259\tGenerator Loss: 1.350362\n",
      "Train Epoch: 56 [29440/60000 (49%)]\tDiscriminator Loss: 0.528498\tGenerator Loss: 1.447873\n",
      "Train Epoch: 56 [30720/60000 (51%)]\tDiscriminator Loss: 0.549931\tGenerator Loss: 0.899067\n",
      "Train Epoch: 56 [32000/60000 (53%)]\tDiscriminator Loss: 0.532113\tGenerator Loss: 1.168769\n",
      "Train Epoch: 56 [33280/60000 (55%)]\tDiscriminator Loss: 0.474394\tGenerator Loss: 1.321393\n",
      "Train Epoch: 56 [34560/60000 (58%)]\tDiscriminator Loss: 0.494374\tGenerator Loss: 1.307292\n",
      "Train Epoch: 56 [35840/60000 (60%)]\tDiscriminator Loss: 0.524025\tGenerator Loss: 0.971079\n",
      "Train Epoch: 56 [37120/60000 (62%)]\tDiscriminator Loss: 0.550298\tGenerator Loss: 1.224457\n",
      "Train Epoch: 56 [38400/60000 (64%)]\tDiscriminator Loss: 0.519945\tGenerator Loss: 1.454888\n",
      "Train Epoch: 56 [39680/60000 (66%)]\tDiscriminator Loss: 0.546578\tGenerator Loss: 1.043215\n",
      "Train Epoch: 56 [40960/60000 (68%)]\tDiscriminator Loss: 0.596915\tGenerator Loss: 1.674730\n",
      "Train Epoch: 56 [42240/60000 (70%)]\tDiscriminator Loss: 0.492719\tGenerator Loss: 1.182407\n",
      "Train Epoch: 56 [43520/60000 (72%)]\tDiscriminator Loss: 0.594026\tGenerator Loss: 1.717397\n",
      "Train Epoch: 56 [44800/60000 (75%)]\tDiscriminator Loss: 0.491791\tGenerator Loss: 1.329377\n",
      "Train Epoch: 56 [46080/60000 (77%)]\tDiscriminator Loss: 0.514187\tGenerator Loss: 1.405230\n",
      "Train Epoch: 56 [47360/60000 (79%)]\tDiscriminator Loss: 0.574705\tGenerator Loss: 1.129608\n",
      "Train Epoch: 56 [48640/60000 (81%)]\tDiscriminator Loss: 0.568133\tGenerator Loss: 1.095013\n",
      "Train Epoch: 56 [49920/60000 (83%)]\tDiscriminator Loss: 0.519779\tGenerator Loss: 1.280335\n",
      "Train Epoch: 56 [51200/60000 (85%)]\tDiscriminator Loss: 0.551249\tGenerator Loss: 0.918078\n",
      "Train Epoch: 56 [52480/60000 (87%)]\tDiscriminator Loss: 0.573992\tGenerator Loss: 0.812806\n",
      "Train Epoch: 56 [53760/60000 (90%)]\tDiscriminator Loss: 0.565370\tGenerator Loss: 1.569110\n",
      "Train Epoch: 56 [55040/60000 (92%)]\tDiscriminator Loss: 0.503153\tGenerator Loss: 1.051986\n",
      "Train Epoch: 56 [56320/60000 (94%)]\tDiscriminator Loss: 0.484740\tGenerator Loss: 1.294586\n",
      "Train Epoch: 56 [57600/60000 (96%)]\tDiscriminator Loss: 0.534479\tGenerator Loss: 1.498653\n",
      "Train Epoch: 56 [58880/60000 (98%)]\tDiscriminator Loss: 0.510138\tGenerator Loss: 1.514246\n",
      "Train Epoch: 57 [0/60000 (0%)]\tDiscriminator Loss: 0.533059\tGenerator Loss: 1.158882\n",
      "Train Epoch: 57 [1280/60000 (2%)]\tDiscriminator Loss: 0.511610\tGenerator Loss: 1.447159\n",
      "Train Epoch: 57 [2560/60000 (4%)]\tDiscriminator Loss: 0.505324\tGenerator Loss: 1.373946\n",
      "Train Epoch: 57 [3840/60000 (6%)]\tDiscriminator Loss: 0.543139\tGenerator Loss: 1.326257\n",
      "Train Epoch: 57 [5120/60000 (9%)]\tDiscriminator Loss: 0.484632\tGenerator Loss: 1.458858\n",
      "Train Epoch: 57 [6400/60000 (11%)]\tDiscriminator Loss: 0.486463\tGenerator Loss: 1.401344\n",
      "Train Epoch: 57 [7680/60000 (13%)]\tDiscriminator Loss: 0.495165\tGenerator Loss: 1.377875\n",
      "Train Epoch: 57 [8960/60000 (15%)]\tDiscriminator Loss: 0.478068\tGenerator Loss: 1.472588\n",
      "Train Epoch: 57 [10240/60000 (17%)]\tDiscriminator Loss: 0.590139\tGenerator Loss: 1.511105\n",
      "Train Epoch: 57 [11520/60000 (19%)]\tDiscriminator Loss: 0.467816\tGenerator Loss: 1.505800\n",
      "Train Epoch: 57 [12800/60000 (21%)]\tDiscriminator Loss: 0.522584\tGenerator Loss: 1.271070\n",
      "Train Epoch: 57 [14080/60000 (23%)]\tDiscriminator Loss: 0.460524\tGenerator Loss: 1.369442\n",
      "Train Epoch: 57 [15360/60000 (26%)]\tDiscriminator Loss: 0.488207\tGenerator Loss: 1.556930\n",
      "Train Epoch: 57 [16640/60000 (28%)]\tDiscriminator Loss: 0.519264\tGenerator Loss: 1.636762\n",
      "Train Epoch: 57 [17920/60000 (30%)]\tDiscriminator Loss: 0.470949\tGenerator Loss: 1.201069\n",
      "Train Epoch: 57 [19200/60000 (32%)]\tDiscriminator Loss: 0.506693\tGenerator Loss: 1.301546\n",
      "Train Epoch: 57 [20480/60000 (34%)]\tDiscriminator Loss: 0.479957\tGenerator Loss: 1.380853\n",
      "Train Epoch: 57 [21760/60000 (36%)]\tDiscriminator Loss: 0.478490\tGenerator Loss: 1.521967\n",
      "Train Epoch: 57 [23040/60000 (38%)]\tDiscriminator Loss: 0.490744\tGenerator Loss: 1.282023\n",
      "Train Epoch: 57 [24320/60000 (41%)]\tDiscriminator Loss: 0.627810\tGenerator Loss: 0.726329\n",
      "Train Epoch: 57 [25600/60000 (43%)]\tDiscriminator Loss: 0.490656\tGenerator Loss: 1.201634\n",
      "Train Epoch: 57 [26880/60000 (45%)]\tDiscriminator Loss: 0.500702\tGenerator Loss: 1.400756\n",
      "Train Epoch: 57 [28160/60000 (47%)]\tDiscriminator Loss: 0.565846\tGenerator Loss: 1.172371\n",
      "Train Epoch: 57 [29440/60000 (49%)]\tDiscriminator Loss: 0.502982\tGenerator Loss: 1.184670\n",
      "Train Epoch: 57 [30720/60000 (51%)]\tDiscriminator Loss: 0.484840\tGenerator Loss: 1.483170\n",
      "Train Epoch: 57 [32000/60000 (53%)]\tDiscriminator Loss: 0.520709\tGenerator Loss: 1.534441\n",
      "Train Epoch: 57 [33280/60000 (55%)]\tDiscriminator Loss: 0.487984\tGenerator Loss: 1.360164\n",
      "Train Epoch: 57 [34560/60000 (58%)]\tDiscriminator Loss: 0.490176\tGenerator Loss: 1.361616\n",
      "Train Epoch: 57 [35840/60000 (60%)]\tDiscriminator Loss: 0.492992\tGenerator Loss: 1.438607\n",
      "Train Epoch: 57 [37120/60000 (62%)]\tDiscriminator Loss: 0.558304\tGenerator Loss: 1.016212\n",
      "Train Epoch: 57 [38400/60000 (64%)]\tDiscriminator Loss: 0.519931\tGenerator Loss: 1.306952\n",
      "Train Epoch: 57 [39680/60000 (66%)]\tDiscriminator Loss: 0.437345\tGenerator Loss: 1.142313\n",
      "Train Epoch: 57 [40960/60000 (68%)]\tDiscriminator Loss: 0.503428\tGenerator Loss: 1.544745\n",
      "Train Epoch: 57 [42240/60000 (70%)]\tDiscriminator Loss: 0.508206\tGenerator Loss: 1.183660\n",
      "Train Epoch: 57 [43520/60000 (72%)]\tDiscriminator Loss: 0.478306\tGenerator Loss: 1.451109\n",
      "Train Epoch: 57 [44800/60000 (75%)]\tDiscriminator Loss: 0.544750\tGenerator Loss: 1.226560\n",
      "Train Epoch: 57 [46080/60000 (77%)]\tDiscriminator Loss: 0.551207\tGenerator Loss: 1.042872\n",
      "Train Epoch: 57 [47360/60000 (79%)]\tDiscriminator Loss: 0.463300\tGenerator Loss: 1.297143\n",
      "Train Epoch: 57 [48640/60000 (81%)]\tDiscriminator Loss: 0.572574\tGenerator Loss: 1.188330\n",
      "Train Epoch: 57 [49920/60000 (83%)]\tDiscriminator Loss: 0.588504\tGenerator Loss: 1.058260\n",
      "Train Epoch: 57 [51200/60000 (85%)]\tDiscriminator Loss: 0.496140\tGenerator Loss: 1.353040\n",
      "Train Epoch: 57 [52480/60000 (87%)]\tDiscriminator Loss: 0.557261\tGenerator Loss: 1.653144\n",
      "Train Epoch: 57 [53760/60000 (90%)]\tDiscriminator Loss: 0.532824\tGenerator Loss: 1.049944\n",
      "Train Epoch: 57 [55040/60000 (92%)]\tDiscriminator Loss: 0.494795\tGenerator Loss: 1.205885\n",
      "Train Epoch: 57 [56320/60000 (94%)]\tDiscriminator Loss: 0.501207\tGenerator Loss: 1.336318\n",
      "Train Epoch: 57 [57600/60000 (96%)]\tDiscriminator Loss: 0.451918\tGenerator Loss: 1.245019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 57 [58880/60000 (98%)]\tDiscriminator Loss: 0.560466\tGenerator Loss: 0.968984\n",
      "Train Epoch: 58 [0/60000 (0%)]\tDiscriminator Loss: 0.526205\tGenerator Loss: 1.396667\n",
      "Train Epoch: 58 [1280/60000 (2%)]\tDiscriminator Loss: 0.494731\tGenerator Loss: 1.328496\n",
      "Train Epoch: 58 [2560/60000 (4%)]\tDiscriminator Loss: 0.470146\tGenerator Loss: 1.497778\n",
      "Train Epoch: 58 [3840/60000 (6%)]\tDiscriminator Loss: 0.525701\tGenerator Loss: 1.330944\n",
      "Train Epoch: 58 [5120/60000 (9%)]\tDiscriminator Loss: 0.458551\tGenerator Loss: 1.484965\n",
      "Train Epoch: 58 [6400/60000 (11%)]\tDiscriminator Loss: 0.524244\tGenerator Loss: 1.226898\n",
      "Train Epoch: 58 [7680/60000 (13%)]\tDiscriminator Loss: 0.436912\tGenerator Loss: 1.534099\n",
      "Train Epoch: 58 [8960/60000 (15%)]\tDiscriminator Loss: 0.478494\tGenerator Loss: 1.557101\n",
      "Train Epoch: 58 [10240/60000 (17%)]\tDiscriminator Loss: 0.539824\tGenerator Loss: 1.506844\n",
      "Train Epoch: 58 [11520/60000 (19%)]\tDiscriminator Loss: 0.478375\tGenerator Loss: 2.007497\n",
      "Train Epoch: 58 [12800/60000 (21%)]\tDiscriminator Loss: 0.491839\tGenerator Loss: 1.460270\n",
      "Train Epoch: 58 [14080/60000 (23%)]\tDiscriminator Loss: 0.535376\tGenerator Loss: 1.474549\n",
      "Train Epoch: 58 [15360/60000 (26%)]\tDiscriminator Loss: 0.456014\tGenerator Loss: 1.358301\n",
      "Train Epoch: 58 [16640/60000 (28%)]\tDiscriminator Loss: 0.512941\tGenerator Loss: 1.330910\n",
      "Train Epoch: 58 [17920/60000 (30%)]\tDiscriminator Loss: 0.486502\tGenerator Loss: 1.343298\n",
      "Train Epoch: 58 [19200/60000 (32%)]\tDiscriminator Loss: 0.513526\tGenerator Loss: 1.497970\n",
      "Train Epoch: 58 [20480/60000 (34%)]\tDiscriminator Loss: 0.504164\tGenerator Loss: 1.537157\n",
      "Train Epoch: 58 [21760/60000 (36%)]\tDiscriminator Loss: 0.493314\tGenerator Loss: 1.108700\n",
      "Train Epoch: 58 [23040/60000 (38%)]\tDiscriminator Loss: 0.467067\tGenerator Loss: 1.302593\n",
      "Train Epoch: 58 [24320/60000 (41%)]\tDiscriminator Loss: 0.540942\tGenerator Loss: 1.174702\n",
      "Train Epoch: 58 [25600/60000 (43%)]\tDiscriminator Loss: 0.441997\tGenerator Loss: 1.681075\n",
      "Train Epoch: 58 [26880/60000 (45%)]\tDiscriminator Loss: 0.491274\tGenerator Loss: 1.576364\n",
      "Train Epoch: 58 [28160/60000 (47%)]\tDiscriminator Loss: 0.589575\tGenerator Loss: 1.791654\n",
      "Train Epoch: 58 [29440/60000 (49%)]\tDiscriminator Loss: 0.484022\tGenerator Loss: 1.096183\n",
      "Train Epoch: 58 [30720/60000 (51%)]\tDiscriminator Loss: 0.539036\tGenerator Loss: 1.522159\n",
      "Train Epoch: 58 [32000/60000 (53%)]\tDiscriminator Loss: 0.519470\tGenerator Loss: 0.948870\n",
      "Train Epoch: 58 [33280/60000 (55%)]\tDiscriminator Loss: 0.466370\tGenerator Loss: 1.457193\n",
      "Train Epoch: 58 [34560/60000 (58%)]\tDiscriminator Loss: 0.463349\tGenerator Loss: 1.536598\n",
      "Train Epoch: 58 [35840/60000 (60%)]\tDiscriminator Loss: 0.505839\tGenerator Loss: 1.644578\n",
      "Train Epoch: 58 [37120/60000 (62%)]\tDiscriminator Loss: 0.529814\tGenerator Loss: 1.068177\n",
      "Train Epoch: 58 [38400/60000 (64%)]\tDiscriminator Loss: 0.479003\tGenerator Loss: 1.377332\n",
      "Train Epoch: 58 [39680/60000 (66%)]\tDiscriminator Loss: 0.514774\tGenerator Loss: 1.114750\n",
      "Train Epoch: 58 [40960/60000 (68%)]\tDiscriminator Loss: 0.527065\tGenerator Loss: 1.009727\n",
      "Train Epoch: 58 [42240/60000 (70%)]\tDiscriminator Loss: 0.505721\tGenerator Loss: 1.389578\n",
      "Train Epoch: 58 [43520/60000 (72%)]\tDiscriminator Loss: 0.461697\tGenerator Loss: 1.172390\n",
      "Train Epoch: 58 [44800/60000 (75%)]\tDiscriminator Loss: 0.502734\tGenerator Loss: 1.106553\n",
      "Train Epoch: 58 [46080/60000 (77%)]\tDiscriminator Loss: 0.495899\tGenerator Loss: 1.221703\n",
      "Train Epoch: 58 [47360/60000 (79%)]\tDiscriminator Loss: 0.461210\tGenerator Loss: 1.497620\n",
      "Train Epoch: 58 [48640/60000 (81%)]\tDiscriminator Loss: 0.477037\tGenerator Loss: 1.379250\n",
      "Train Epoch: 58 [49920/60000 (83%)]\tDiscriminator Loss: 0.529466\tGenerator Loss: 1.423017\n",
      "Train Epoch: 58 [51200/60000 (85%)]\tDiscriminator Loss: 0.511310\tGenerator Loss: 1.118562\n",
      "Train Epoch: 58 [52480/60000 (87%)]\tDiscriminator Loss: 0.496155\tGenerator Loss: 1.180576\n",
      "Train Epoch: 58 [53760/60000 (90%)]\tDiscriminator Loss: 0.523881\tGenerator Loss: 1.105989\n",
      "Train Epoch: 58 [55040/60000 (92%)]\tDiscriminator Loss: 0.515752\tGenerator Loss: 1.368605\n",
      "Train Epoch: 58 [56320/60000 (94%)]\tDiscriminator Loss: 0.485897\tGenerator Loss: 1.201590\n",
      "Train Epoch: 58 [57600/60000 (96%)]\tDiscriminator Loss: 0.548861\tGenerator Loss: 1.353694\n",
      "Train Epoch: 58 [58880/60000 (98%)]\tDiscriminator Loss: 0.495824\tGenerator Loss: 1.103139\n",
      "Train Epoch: 59 [0/60000 (0%)]\tDiscriminator Loss: 0.523905\tGenerator Loss: 1.550683\n",
      "Train Epoch: 59 [1280/60000 (2%)]\tDiscriminator Loss: 0.555152\tGenerator Loss: 1.328025\n",
      "Train Epoch: 59 [2560/60000 (4%)]\tDiscriminator Loss: 0.452491\tGenerator Loss: 1.351544\n",
      "Train Epoch: 59 [3840/60000 (6%)]\tDiscriminator Loss: 0.633327\tGenerator Loss: 1.595750\n",
      "Train Epoch: 59 [5120/60000 (9%)]\tDiscriminator Loss: 0.467218\tGenerator Loss: 1.174671\n",
      "Train Epoch: 59 [6400/60000 (11%)]\tDiscriminator Loss: 0.491014\tGenerator Loss: 1.193412\n",
      "Train Epoch: 59 [7680/60000 (13%)]\tDiscriminator Loss: 0.506523\tGenerator Loss: 1.339760\n",
      "Train Epoch: 59 [8960/60000 (15%)]\tDiscriminator Loss: 0.502806\tGenerator Loss: 1.286833\n",
      "Train Epoch: 59 [10240/60000 (17%)]\tDiscriminator Loss: 0.483477\tGenerator Loss: 1.401741\n",
      "Train Epoch: 59 [11520/60000 (19%)]\tDiscriminator Loss: 0.515200\tGenerator Loss: 0.978810\n",
      "Train Epoch: 59 [12800/60000 (21%)]\tDiscriminator Loss: 0.502547\tGenerator Loss: 1.188635\n",
      "Train Epoch: 59 [14080/60000 (23%)]\tDiscriminator Loss: 0.498477\tGenerator Loss: 1.273088\n",
      "Train Epoch: 59 [15360/60000 (26%)]\tDiscriminator Loss: 0.469484\tGenerator Loss: 1.215730\n",
      "Train Epoch: 59 [16640/60000 (28%)]\tDiscriminator Loss: 0.508304\tGenerator Loss: 1.268211\n",
      "Train Epoch: 59 [17920/60000 (30%)]\tDiscriminator Loss: 0.511434\tGenerator Loss: 1.293679\n",
      "Train Epoch: 59 [19200/60000 (32%)]\tDiscriminator Loss: 0.550419\tGenerator Loss: 1.005252\n",
      "Train Epoch: 59 [20480/60000 (34%)]\tDiscriminator Loss: 0.511076\tGenerator Loss: 1.096706\n",
      "Train Epoch: 59 [21760/60000 (36%)]\tDiscriminator Loss: 0.534478\tGenerator Loss: 1.346892\n",
      "Train Epoch: 59 [23040/60000 (38%)]\tDiscriminator Loss: 0.512814\tGenerator Loss: 1.444202\n",
      "Train Epoch: 59 [24320/60000 (41%)]\tDiscriminator Loss: 0.467123\tGenerator Loss: 1.397007\n",
      "Train Epoch: 59 [25600/60000 (43%)]\tDiscriminator Loss: 0.519478\tGenerator Loss: 1.962265\n",
      "Train Epoch: 59 [26880/60000 (45%)]\tDiscriminator Loss: 0.508312\tGenerator Loss: 1.053686\n",
      "Train Epoch: 59 [28160/60000 (47%)]\tDiscriminator Loss: 0.498225\tGenerator Loss: 1.552855\n",
      "Train Epoch: 59 [29440/60000 (49%)]\tDiscriminator Loss: 0.450203\tGenerator Loss: 1.498399\n",
      "Train Epoch: 59 [30720/60000 (51%)]\tDiscriminator Loss: 0.510875\tGenerator Loss: 1.097105\n",
      "Train Epoch: 59 [32000/60000 (53%)]\tDiscriminator Loss: 0.472433\tGenerator Loss: 1.187852\n",
      "Train Epoch: 59 [33280/60000 (55%)]\tDiscriminator Loss: 0.573817\tGenerator Loss: 1.788218\n",
      "Train Epoch: 59 [34560/60000 (58%)]\tDiscriminator Loss: 0.527473\tGenerator Loss: 1.202434\n",
      "Train Epoch: 59 [35840/60000 (60%)]\tDiscriminator Loss: 0.482863\tGenerator Loss: 1.017073\n",
      "Train Epoch: 59 [37120/60000 (62%)]\tDiscriminator Loss: 0.547969\tGenerator Loss: 1.260554\n",
      "Train Epoch: 59 [38400/60000 (64%)]\tDiscriminator Loss: 0.519394\tGenerator Loss: 1.172234\n",
      "Train Epoch: 59 [39680/60000 (66%)]\tDiscriminator Loss: 0.491428\tGenerator Loss: 1.441873\n",
      "Train Epoch: 59 [40960/60000 (68%)]\tDiscriminator Loss: 0.501723\tGenerator Loss: 1.275312\n",
      "Train Epoch: 59 [42240/60000 (70%)]\tDiscriminator Loss: 0.559977\tGenerator Loss: 1.715053\n",
      "Train Epoch: 59 [43520/60000 (72%)]\tDiscriminator Loss: 0.508997\tGenerator Loss: 1.343567\n",
      "Train Epoch: 59 [44800/60000 (75%)]\tDiscriminator Loss: 0.531444\tGenerator Loss: 1.298825\n",
      "Train Epoch: 59 [46080/60000 (77%)]\tDiscriminator Loss: 0.491175\tGenerator Loss: 1.252464\n",
      "Train Epoch: 59 [47360/60000 (79%)]\tDiscriminator Loss: 0.536572\tGenerator Loss: 1.360922\n",
      "Train Epoch: 59 [48640/60000 (81%)]\tDiscriminator Loss: 0.485067\tGenerator Loss: 1.259432\n",
      "Train Epoch: 59 [49920/60000 (83%)]\tDiscriminator Loss: 0.487550\tGenerator Loss: 1.163399\n",
      "Train Epoch: 59 [51200/60000 (85%)]\tDiscriminator Loss: 0.468388\tGenerator Loss: 0.983067\n",
      "Train Epoch: 59 [52480/60000 (87%)]\tDiscriminator Loss: 0.503470\tGenerator Loss: 1.165265\n",
      "Train Epoch: 59 [53760/60000 (90%)]\tDiscriminator Loss: 0.516952\tGenerator Loss: 0.951476\n",
      "Train Epoch: 59 [55040/60000 (92%)]\tDiscriminator Loss: 0.549945\tGenerator Loss: 1.334873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 59 [56320/60000 (94%)]\tDiscriminator Loss: 0.525583\tGenerator Loss: 1.403634\n",
      "Train Epoch: 59 [57600/60000 (96%)]\tDiscriminator Loss: 0.455604\tGenerator Loss: 1.324507\n",
      "Train Epoch: 59 [58880/60000 (98%)]\tDiscriminator Loss: 0.494760\tGenerator Loss: 1.428393\n",
      "Train Epoch: 60 [0/60000 (0%)]\tDiscriminator Loss: 0.464545\tGenerator Loss: 1.722207\n",
      "Train Epoch: 60 [1280/60000 (2%)]\tDiscriminator Loss: 0.470625\tGenerator Loss: 1.250023\n",
      "Train Epoch: 60 [2560/60000 (4%)]\tDiscriminator Loss: 0.525442\tGenerator Loss: 1.301245\n",
      "Train Epoch: 60 [3840/60000 (6%)]\tDiscriminator Loss: 0.422158\tGenerator Loss: 1.632810\n",
      "Train Epoch: 60 [5120/60000 (9%)]\tDiscriminator Loss: 0.482336\tGenerator Loss: 1.401218\n",
      "Train Epoch: 60 [6400/60000 (11%)]\tDiscriminator Loss: 0.478615\tGenerator Loss: 1.062677\n",
      "Train Epoch: 60 [7680/60000 (13%)]\tDiscriminator Loss: 0.510666\tGenerator Loss: 1.351460\n",
      "Train Epoch: 60 [8960/60000 (15%)]\tDiscriminator Loss: 0.522614\tGenerator Loss: 1.065965\n",
      "Train Epoch: 60 [10240/60000 (17%)]\tDiscriminator Loss: 0.542292\tGenerator Loss: 1.254158\n",
      "Train Epoch: 60 [11520/60000 (19%)]\tDiscriminator Loss: 0.511811\tGenerator Loss: 1.473186\n",
      "Train Epoch: 60 [12800/60000 (21%)]\tDiscriminator Loss: 0.501836\tGenerator Loss: 1.169257\n",
      "Train Epoch: 60 [14080/60000 (23%)]\tDiscriminator Loss: 0.544856\tGenerator Loss: 1.419112\n",
      "Train Epoch: 60 [15360/60000 (26%)]\tDiscriminator Loss: 0.491008\tGenerator Loss: 1.499149\n",
      "Train Epoch: 60 [16640/60000 (28%)]\tDiscriminator Loss: 0.501709\tGenerator Loss: 1.049721\n",
      "Train Epoch: 60 [17920/60000 (30%)]\tDiscriminator Loss: 0.593510\tGenerator Loss: 1.724576\n",
      "Train Epoch: 60 [19200/60000 (32%)]\tDiscriminator Loss: 0.603907\tGenerator Loss: 1.153146\n",
      "Train Epoch: 60 [20480/60000 (34%)]\tDiscriminator Loss: 0.518401\tGenerator Loss: 1.605873\n",
      "Train Epoch: 60 [21760/60000 (36%)]\tDiscriminator Loss: 0.498398\tGenerator Loss: 1.202392\n",
      "Train Epoch: 60 [23040/60000 (38%)]\tDiscriminator Loss: 0.571536\tGenerator Loss: 1.148888\n",
      "Train Epoch: 60 [24320/60000 (41%)]\tDiscriminator Loss: 0.510565\tGenerator Loss: 1.537496\n",
      "Train Epoch: 60 [25600/60000 (43%)]\tDiscriminator Loss: 0.589975\tGenerator Loss: 1.609696\n",
      "Train Epoch: 60 [26880/60000 (45%)]\tDiscriminator Loss: 0.532419\tGenerator Loss: 1.214784\n",
      "Train Epoch: 60 [28160/60000 (47%)]\tDiscriminator Loss: 0.516593\tGenerator Loss: 1.296454\n",
      "Train Epoch: 60 [29440/60000 (49%)]\tDiscriminator Loss: 0.494184\tGenerator Loss: 1.235587\n",
      "Train Epoch: 60 [30720/60000 (51%)]\tDiscriminator Loss: 0.544476\tGenerator Loss: 1.444280\n",
      "Train Epoch: 60 [32000/60000 (53%)]\tDiscriminator Loss: 0.547667\tGenerator Loss: 1.144122\n",
      "Train Epoch: 60 [33280/60000 (55%)]\tDiscriminator Loss: 0.487917\tGenerator Loss: 1.536911\n",
      "Train Epoch: 60 [34560/60000 (58%)]\tDiscriminator Loss: 0.486167\tGenerator Loss: 1.117426\n",
      "Train Epoch: 60 [35840/60000 (60%)]\tDiscriminator Loss: 0.559214\tGenerator Loss: 0.996446\n",
      "Train Epoch: 60 [37120/60000 (62%)]\tDiscriminator Loss: 0.453728\tGenerator Loss: 1.255203\n",
      "Train Epoch: 60 [38400/60000 (64%)]\tDiscriminator Loss: 0.549234\tGenerator Loss: 1.731865\n",
      "Train Epoch: 60 [39680/60000 (66%)]\tDiscriminator Loss: 0.499812\tGenerator Loss: 1.478089\n",
      "Train Epoch: 60 [40960/60000 (68%)]\tDiscriminator Loss: 0.517745\tGenerator Loss: 1.155551\n",
      "Train Epoch: 60 [42240/60000 (70%)]\tDiscriminator Loss: 0.525835\tGenerator Loss: 1.161187\n",
      "Train Epoch: 60 [43520/60000 (72%)]\tDiscriminator Loss: 0.559881\tGenerator Loss: 1.657325\n",
      "Train Epoch: 60 [44800/60000 (75%)]\tDiscriminator Loss: 0.538158\tGenerator Loss: 1.358477\n",
      "Train Epoch: 60 [46080/60000 (77%)]\tDiscriminator Loss: 0.511994\tGenerator Loss: 1.193229\n",
      "Train Epoch: 60 [47360/60000 (79%)]\tDiscriminator Loss: 0.468296\tGenerator Loss: 1.508447\n",
      "Train Epoch: 60 [48640/60000 (81%)]\tDiscriminator Loss: 0.476938\tGenerator Loss: 1.402493\n",
      "Train Epoch: 60 [49920/60000 (83%)]\tDiscriminator Loss: 0.534606\tGenerator Loss: 0.971789\n",
      "Train Epoch: 60 [51200/60000 (85%)]\tDiscriminator Loss: 0.470530\tGenerator Loss: 1.151874\n",
      "Train Epoch: 60 [52480/60000 (87%)]\tDiscriminator Loss: 0.498445\tGenerator Loss: 1.478035\n",
      "Train Epoch: 60 [53760/60000 (90%)]\tDiscriminator Loss: 0.498596\tGenerator Loss: 1.256502\n",
      "Train Epoch: 60 [55040/60000 (92%)]\tDiscriminator Loss: 0.470849\tGenerator Loss: 1.358459\n",
      "Train Epoch: 60 [56320/60000 (94%)]\tDiscriminator Loss: 0.532282\tGenerator Loss: 1.224925\n",
      "Train Epoch: 60 [57600/60000 (96%)]\tDiscriminator Loss: 0.523327\tGenerator Loss: 1.592261\n",
      "Train Epoch: 60 [58880/60000 (98%)]\tDiscriminator Loss: 0.444727\tGenerator Loss: 1.544407\n",
      "Train Epoch: 61 [0/60000 (0%)]\tDiscriminator Loss: 0.535973\tGenerator Loss: 0.926141\n",
      "Train Epoch: 61 [1280/60000 (2%)]\tDiscriminator Loss: 0.531025\tGenerator Loss: 1.683737\n",
      "Train Epoch: 61 [2560/60000 (4%)]\tDiscriminator Loss: 0.520011\tGenerator Loss: 1.416955\n",
      "Train Epoch: 61 [3840/60000 (6%)]\tDiscriminator Loss: 0.536204\tGenerator Loss: 1.562340\n",
      "Train Epoch: 61 [5120/60000 (9%)]\tDiscriminator Loss: 0.498058\tGenerator Loss: 1.229955\n",
      "Train Epoch: 61 [6400/60000 (11%)]\tDiscriminator Loss: 0.534416\tGenerator Loss: 1.233641\n",
      "Train Epoch: 61 [7680/60000 (13%)]\tDiscriminator Loss: 0.535733\tGenerator Loss: 1.290508\n",
      "Train Epoch: 61 [8960/60000 (15%)]\tDiscriminator Loss: 0.533033\tGenerator Loss: 1.249643\n",
      "Train Epoch: 61 [10240/60000 (17%)]\tDiscriminator Loss: 0.502228\tGenerator Loss: 1.141571\n",
      "Train Epoch: 61 [11520/60000 (19%)]\tDiscriminator Loss: 0.471645\tGenerator Loss: 1.193250\n",
      "Train Epoch: 61 [12800/60000 (21%)]\tDiscriminator Loss: 0.535220\tGenerator Loss: 1.214683\n",
      "Train Epoch: 61 [14080/60000 (23%)]\tDiscriminator Loss: 0.502571\tGenerator Loss: 1.687330\n",
      "Train Epoch: 61 [15360/60000 (26%)]\tDiscriminator Loss: 0.553479\tGenerator Loss: 1.499896\n",
      "Train Epoch: 61 [16640/60000 (28%)]\tDiscriminator Loss: 0.498217\tGenerator Loss: 1.708444\n",
      "Train Epoch: 61 [17920/60000 (30%)]\tDiscriminator Loss: 0.454749\tGenerator Loss: 1.406165\n",
      "Train Epoch: 61 [19200/60000 (32%)]\tDiscriminator Loss: 0.490777\tGenerator Loss: 1.396673\n",
      "Train Epoch: 61 [20480/60000 (34%)]\tDiscriminator Loss: 0.487004\tGenerator Loss: 1.451256\n",
      "Train Epoch: 61 [21760/60000 (36%)]\tDiscriminator Loss: 0.583538\tGenerator Loss: 1.401772\n",
      "Train Epoch: 61 [23040/60000 (38%)]\tDiscriminator Loss: 0.480951\tGenerator Loss: 1.225888\n",
      "Train Epoch: 61 [24320/60000 (41%)]\tDiscriminator Loss: 0.481147\tGenerator Loss: 1.264434\n",
      "Train Epoch: 61 [25600/60000 (43%)]\tDiscriminator Loss: 0.500745\tGenerator Loss: 1.421555\n",
      "Train Epoch: 61 [26880/60000 (45%)]\tDiscriminator Loss: 0.519260\tGenerator Loss: 1.177635\n",
      "Train Epoch: 61 [28160/60000 (47%)]\tDiscriminator Loss: 0.471846\tGenerator Loss: 1.374493\n",
      "Train Epoch: 61 [29440/60000 (49%)]\tDiscriminator Loss: 0.502115\tGenerator Loss: 1.623471\n",
      "Train Epoch: 61 [30720/60000 (51%)]\tDiscriminator Loss: 0.550554\tGenerator Loss: 1.422681\n",
      "Train Epoch: 61 [32000/60000 (53%)]\tDiscriminator Loss: 0.508660\tGenerator Loss: 1.290832\n",
      "Train Epoch: 61 [33280/60000 (55%)]\tDiscriminator Loss: 0.436721\tGenerator Loss: 1.437541\n",
      "Train Epoch: 61 [34560/60000 (58%)]\tDiscriminator Loss: 0.518361\tGenerator Loss: 1.505032\n",
      "Train Epoch: 61 [35840/60000 (60%)]\tDiscriminator Loss: 0.487690\tGenerator Loss: 1.297414\n",
      "Train Epoch: 61 [37120/60000 (62%)]\tDiscriminator Loss: 0.528608\tGenerator Loss: 1.290081\n",
      "Train Epoch: 61 [38400/60000 (64%)]\tDiscriminator Loss: 0.552385\tGenerator Loss: 1.245294\n",
      "Train Epoch: 61 [39680/60000 (66%)]\tDiscriminator Loss: 0.484492\tGenerator Loss: 1.588440\n",
      "Train Epoch: 61 [40960/60000 (68%)]\tDiscriminator Loss: 0.477286\tGenerator Loss: 1.488445\n",
      "Train Epoch: 61 [42240/60000 (70%)]\tDiscriminator Loss: 0.564339\tGenerator Loss: 1.388748\n",
      "Train Epoch: 61 [43520/60000 (72%)]\tDiscriminator Loss: 0.590444\tGenerator Loss: 1.702585\n",
      "Train Epoch: 61 [44800/60000 (75%)]\tDiscriminator Loss: 0.465136\tGenerator Loss: 1.292310\n",
      "Train Epoch: 61 [46080/60000 (77%)]\tDiscriminator Loss: 0.576501\tGenerator Loss: 1.446349\n",
      "Train Epoch: 61 [47360/60000 (79%)]\tDiscriminator Loss: 0.444125\tGenerator Loss: 1.323101\n",
      "Train Epoch: 61 [48640/60000 (81%)]\tDiscriminator Loss: 0.487234\tGenerator Loss: 1.192208\n",
      "Train Epoch: 61 [49920/60000 (83%)]\tDiscriminator Loss: 0.490056\tGenerator Loss: 1.248852\n",
      "Train Epoch: 61 [51200/60000 (85%)]\tDiscriminator Loss: 0.557879\tGenerator Loss: 1.330153\n",
      "Train Epoch: 61 [52480/60000 (87%)]\tDiscriminator Loss: 0.597795\tGenerator Loss: 1.551251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 61 [53760/60000 (90%)]\tDiscriminator Loss: 0.529071\tGenerator Loss: 1.334166\n",
      "Train Epoch: 61 [55040/60000 (92%)]\tDiscriminator Loss: 0.576421\tGenerator Loss: 1.531509\n",
      "Train Epoch: 61 [56320/60000 (94%)]\tDiscriminator Loss: 0.532820\tGenerator Loss: 1.120541\n",
      "Train Epoch: 61 [57600/60000 (96%)]\tDiscriminator Loss: 0.580038\tGenerator Loss: 2.101710\n",
      "Train Epoch: 61 [58880/60000 (98%)]\tDiscriminator Loss: 0.528820\tGenerator Loss: 1.268677\n",
      "Train Epoch: 62 [0/60000 (0%)]\tDiscriminator Loss: 0.493501\tGenerator Loss: 1.608101\n",
      "Train Epoch: 62 [1280/60000 (2%)]\tDiscriminator Loss: 0.492648\tGenerator Loss: 1.629464\n",
      "Train Epoch: 62 [2560/60000 (4%)]\tDiscriminator Loss: 0.516368\tGenerator Loss: 0.983482\n",
      "Train Epoch: 62 [3840/60000 (6%)]\tDiscriminator Loss: 0.520740\tGenerator Loss: 0.974915\n",
      "Train Epoch: 62 [5120/60000 (9%)]\tDiscriminator Loss: 0.559143\tGenerator Loss: 1.274275\n",
      "Train Epoch: 62 [6400/60000 (11%)]\tDiscriminator Loss: 0.469657\tGenerator Loss: 1.373250\n",
      "Train Epoch: 62 [7680/60000 (13%)]\tDiscriminator Loss: 0.546364\tGenerator Loss: 1.594002\n",
      "Train Epoch: 62 [8960/60000 (15%)]\tDiscriminator Loss: 0.509382\tGenerator Loss: 1.539720\n",
      "Train Epoch: 62 [10240/60000 (17%)]\tDiscriminator Loss: 0.487941\tGenerator Loss: 1.232655\n",
      "Train Epoch: 62 [11520/60000 (19%)]\tDiscriminator Loss: 0.524814\tGenerator Loss: 1.139037\n",
      "Train Epoch: 62 [12800/60000 (21%)]\tDiscriminator Loss: 0.507680\tGenerator Loss: 1.159078\n",
      "Train Epoch: 62 [14080/60000 (23%)]\tDiscriminator Loss: 0.558028\tGenerator Loss: 1.637969\n",
      "Train Epoch: 62 [15360/60000 (26%)]\tDiscriminator Loss: 0.535025\tGenerator Loss: 1.294623\n",
      "Train Epoch: 62 [16640/60000 (28%)]\tDiscriminator Loss: 0.479957\tGenerator Loss: 1.051311\n",
      "Train Epoch: 62 [17920/60000 (30%)]\tDiscriminator Loss: 0.538551\tGenerator Loss: 1.339503\n",
      "Train Epoch: 62 [19200/60000 (32%)]\tDiscriminator Loss: 0.522218\tGenerator Loss: 1.050107\n",
      "Train Epoch: 62 [20480/60000 (34%)]\tDiscriminator Loss: 0.532236\tGenerator Loss: 1.326186\n",
      "Train Epoch: 62 [21760/60000 (36%)]\tDiscriminator Loss: 0.495467\tGenerator Loss: 1.364833\n",
      "Train Epoch: 62 [23040/60000 (38%)]\tDiscriminator Loss: 0.574333\tGenerator Loss: 1.642590\n",
      "Train Epoch: 62 [24320/60000 (41%)]\tDiscriminator Loss: 0.498464\tGenerator Loss: 1.383969\n",
      "Train Epoch: 62 [25600/60000 (43%)]\tDiscriminator Loss: 0.534326\tGenerator Loss: 1.448124\n",
      "Train Epoch: 62 [26880/60000 (45%)]\tDiscriminator Loss: 0.542303\tGenerator Loss: 1.431410\n",
      "Train Epoch: 62 [28160/60000 (47%)]\tDiscriminator Loss: 0.514562\tGenerator Loss: 1.170118\n",
      "Train Epoch: 62 [29440/60000 (49%)]\tDiscriminator Loss: 0.496852\tGenerator Loss: 1.126406\n",
      "Train Epoch: 62 [30720/60000 (51%)]\tDiscriminator Loss: 0.568187\tGenerator Loss: 1.552613\n",
      "Train Epoch: 62 [32000/60000 (53%)]\tDiscriminator Loss: 0.486209\tGenerator Loss: 1.258410\n",
      "Train Epoch: 62 [33280/60000 (55%)]\tDiscriminator Loss: 0.504595\tGenerator Loss: 1.356134\n",
      "Train Epoch: 62 [34560/60000 (58%)]\tDiscriminator Loss: 0.529989\tGenerator Loss: 1.651805\n",
      "Train Epoch: 62 [35840/60000 (60%)]\tDiscriminator Loss: 0.439952\tGenerator Loss: 1.207700\n",
      "Train Epoch: 62 [37120/60000 (62%)]\tDiscriminator Loss: 0.463582\tGenerator Loss: 1.610357\n",
      "Train Epoch: 62 [38400/60000 (64%)]\tDiscriminator Loss: 0.561511\tGenerator Loss: 1.099813\n",
      "Train Epoch: 62 [39680/60000 (66%)]\tDiscriminator Loss: 0.493726\tGenerator Loss: 1.291212\n",
      "Train Epoch: 62 [40960/60000 (68%)]\tDiscriminator Loss: 0.521006\tGenerator Loss: 0.959674\n",
      "Train Epoch: 62 [42240/60000 (70%)]\tDiscriminator Loss: 0.489180\tGenerator Loss: 1.199275\n",
      "Train Epoch: 62 [43520/60000 (72%)]\tDiscriminator Loss: 0.489624\tGenerator Loss: 1.149313\n",
      "Train Epoch: 62 [44800/60000 (75%)]\tDiscriminator Loss: 0.477216\tGenerator Loss: 1.305053\n",
      "Train Epoch: 62 [46080/60000 (77%)]\tDiscriminator Loss: 0.514999\tGenerator Loss: 1.089940\n",
      "Train Epoch: 62 [47360/60000 (79%)]\tDiscriminator Loss: 0.512218\tGenerator Loss: 1.158004\n",
      "Train Epoch: 62 [48640/60000 (81%)]\tDiscriminator Loss: 0.505852\tGenerator Loss: 1.006580\n",
      "Train Epoch: 62 [49920/60000 (83%)]\tDiscriminator Loss: 0.550213\tGenerator Loss: 0.953879\n",
      "Train Epoch: 62 [51200/60000 (85%)]\tDiscriminator Loss: 0.457300\tGenerator Loss: 1.485284\n",
      "Train Epoch: 62 [52480/60000 (87%)]\tDiscriminator Loss: 0.508518\tGenerator Loss: 1.327427\n",
      "Train Epoch: 62 [53760/60000 (90%)]\tDiscriminator Loss: 0.480169\tGenerator Loss: 1.437247\n",
      "Train Epoch: 62 [55040/60000 (92%)]\tDiscriminator Loss: 0.515913\tGenerator Loss: 1.036086\n",
      "Train Epoch: 62 [56320/60000 (94%)]\tDiscriminator Loss: 0.518509\tGenerator Loss: 1.016276\n",
      "Train Epoch: 62 [57600/60000 (96%)]\tDiscriminator Loss: 0.476761\tGenerator Loss: 1.553891\n",
      "Train Epoch: 62 [58880/60000 (98%)]\tDiscriminator Loss: 0.531364\tGenerator Loss: 1.207768\n",
      "Train Epoch: 63 [0/60000 (0%)]\tDiscriminator Loss: 0.529687\tGenerator Loss: 1.473011\n",
      "Train Epoch: 63 [1280/60000 (2%)]\tDiscriminator Loss: 0.557786\tGenerator Loss: 1.728223\n",
      "Train Epoch: 63 [2560/60000 (4%)]\tDiscriminator Loss: 0.431187\tGenerator Loss: 1.327384\n",
      "Train Epoch: 63 [3840/60000 (6%)]\tDiscriminator Loss: 0.430035\tGenerator Loss: 1.332453\n",
      "Train Epoch: 63 [5120/60000 (9%)]\tDiscriminator Loss: 0.489047\tGenerator Loss: 1.255112\n",
      "Train Epoch: 63 [6400/60000 (11%)]\tDiscriminator Loss: 0.523316\tGenerator Loss: 1.341288\n",
      "Train Epoch: 63 [7680/60000 (13%)]\tDiscriminator Loss: 0.494048\tGenerator Loss: 1.010694\n",
      "Train Epoch: 63 [8960/60000 (15%)]\tDiscriminator Loss: 0.482903\tGenerator Loss: 1.768088\n",
      "Train Epoch: 63 [10240/60000 (17%)]\tDiscriminator Loss: 0.450984\tGenerator Loss: 1.541438\n",
      "Train Epoch: 63 [11520/60000 (19%)]\tDiscriminator Loss: 0.490169\tGenerator Loss: 1.979727\n",
      "Train Epoch: 63 [12800/60000 (21%)]\tDiscriminator Loss: 0.521013\tGenerator Loss: 1.084968\n",
      "Train Epoch: 63 [14080/60000 (23%)]\tDiscriminator Loss: 0.556609\tGenerator Loss: 1.581079\n",
      "Train Epoch: 63 [15360/60000 (26%)]\tDiscriminator Loss: 0.529392\tGenerator Loss: 1.083745\n",
      "Train Epoch: 63 [16640/60000 (28%)]\tDiscriminator Loss: 0.513608\tGenerator Loss: 0.978142\n",
      "Train Epoch: 63 [17920/60000 (30%)]\tDiscriminator Loss: 0.488631\tGenerator Loss: 1.286083\n",
      "Train Epoch: 63 [19200/60000 (32%)]\tDiscriminator Loss: 0.496234\tGenerator Loss: 1.354087\n",
      "Train Epoch: 63 [20480/60000 (34%)]\tDiscriminator Loss: 0.459290\tGenerator Loss: 1.243568\n",
      "Train Epoch: 63 [21760/60000 (36%)]\tDiscriminator Loss: 0.537594\tGenerator Loss: 1.226740\n",
      "Train Epoch: 63 [23040/60000 (38%)]\tDiscriminator Loss: 0.532761\tGenerator Loss: 1.231722\n",
      "Train Epoch: 63 [24320/60000 (41%)]\tDiscriminator Loss: 0.550747\tGenerator Loss: 1.454432\n",
      "Train Epoch: 63 [25600/60000 (43%)]\tDiscriminator Loss: 0.510764\tGenerator Loss: 1.613642\n",
      "Train Epoch: 63 [26880/60000 (45%)]\tDiscriminator Loss: 0.503547\tGenerator Loss: 1.266382\n",
      "Train Epoch: 63 [28160/60000 (47%)]\tDiscriminator Loss: 0.488369\tGenerator Loss: 1.386721\n",
      "Train Epoch: 63 [29440/60000 (49%)]\tDiscriminator Loss: 0.479359\tGenerator Loss: 1.960983\n",
      "Train Epoch: 63 [30720/60000 (51%)]\tDiscriminator Loss: 0.519410\tGenerator Loss: 1.820748\n",
      "Train Epoch: 63 [32000/60000 (53%)]\tDiscriminator Loss: 0.515692\tGenerator Loss: 1.064757\n",
      "Train Epoch: 63 [33280/60000 (55%)]\tDiscriminator Loss: 0.494220\tGenerator Loss: 1.152787\n",
      "Train Epoch: 63 [34560/60000 (58%)]\tDiscriminator Loss: 0.489532\tGenerator Loss: 1.117356\n",
      "Train Epoch: 63 [35840/60000 (60%)]\tDiscriminator Loss: 0.528483\tGenerator Loss: 1.193180\n",
      "Train Epoch: 63 [37120/60000 (62%)]\tDiscriminator Loss: 0.519644\tGenerator Loss: 1.324904\n",
      "Train Epoch: 63 [38400/60000 (64%)]\tDiscriminator Loss: 0.524485\tGenerator Loss: 1.110166\n",
      "Train Epoch: 63 [39680/60000 (66%)]\tDiscriminator Loss: 0.487104\tGenerator Loss: 1.479158\n",
      "Train Epoch: 63 [40960/60000 (68%)]\tDiscriminator Loss: 0.489601\tGenerator Loss: 1.391734\n",
      "Train Epoch: 63 [42240/60000 (70%)]\tDiscriminator Loss: 0.526387\tGenerator Loss: 1.348084\n",
      "Train Epoch: 63 [43520/60000 (72%)]\tDiscriminator Loss: 0.526647\tGenerator Loss: 1.329944\n",
      "Train Epoch: 63 [44800/60000 (75%)]\tDiscriminator Loss: 0.536696\tGenerator Loss: 1.491063\n",
      "Train Epoch: 63 [46080/60000 (77%)]\tDiscriminator Loss: 0.506809\tGenerator Loss: 1.238487\n",
      "Train Epoch: 63 [47360/60000 (79%)]\tDiscriminator Loss: 0.509412\tGenerator Loss: 1.097298\n",
      "Train Epoch: 63 [48640/60000 (81%)]\tDiscriminator Loss: 0.571112\tGenerator Loss: 1.183559\n",
      "Train Epoch: 63 [49920/60000 (83%)]\tDiscriminator Loss: 0.493935\tGenerator Loss: 1.036751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 63 [51200/60000 (85%)]\tDiscriminator Loss: 0.555990\tGenerator Loss: 1.400274\n",
      "Train Epoch: 63 [52480/60000 (87%)]\tDiscriminator Loss: 0.539379\tGenerator Loss: 1.264536\n",
      "Train Epoch: 63 [53760/60000 (90%)]\tDiscriminator Loss: 0.520208\tGenerator Loss: 1.178504\n",
      "Train Epoch: 63 [55040/60000 (92%)]\tDiscriminator Loss: 0.551433\tGenerator Loss: 1.218852\n",
      "Train Epoch: 63 [56320/60000 (94%)]\tDiscriminator Loss: 0.543368\tGenerator Loss: 1.449411\n",
      "Train Epoch: 63 [57600/60000 (96%)]\tDiscriminator Loss: 0.468203\tGenerator Loss: 1.559607\n",
      "Train Epoch: 63 [58880/60000 (98%)]\tDiscriminator Loss: 0.512846\tGenerator Loss: 1.265140\n",
      "Train Epoch: 64 [0/60000 (0%)]\tDiscriminator Loss: 0.504244\tGenerator Loss: 1.284831\n",
      "Train Epoch: 64 [1280/60000 (2%)]\tDiscriminator Loss: 0.430116\tGenerator Loss: 1.542160\n",
      "Train Epoch: 64 [2560/60000 (4%)]\tDiscriminator Loss: 0.557203\tGenerator Loss: 0.960615\n",
      "Train Epoch: 64 [3840/60000 (6%)]\tDiscriminator Loss: 0.511381\tGenerator Loss: 1.506017\n",
      "Train Epoch: 64 [5120/60000 (9%)]\tDiscriminator Loss: 0.454807\tGenerator Loss: 1.412437\n",
      "Train Epoch: 64 [6400/60000 (11%)]\tDiscriminator Loss: 0.491812\tGenerator Loss: 1.350415\n",
      "Train Epoch: 64 [7680/60000 (13%)]\tDiscriminator Loss: 0.501704\tGenerator Loss: 1.387522\n",
      "Train Epoch: 64 [8960/60000 (15%)]\tDiscriminator Loss: 0.495513\tGenerator Loss: 1.454192\n",
      "Train Epoch: 64 [10240/60000 (17%)]\tDiscriminator Loss: 0.477820\tGenerator Loss: 1.151046\n",
      "Train Epoch: 64 [11520/60000 (19%)]\tDiscriminator Loss: 0.488182\tGenerator Loss: 1.292180\n",
      "Train Epoch: 64 [12800/60000 (21%)]\tDiscriminator Loss: 0.558025\tGenerator Loss: 0.857366\n",
      "Train Epoch: 64 [14080/60000 (23%)]\tDiscriminator Loss: 0.539854\tGenerator Loss: 1.225993\n",
      "Train Epoch: 64 [15360/60000 (26%)]\tDiscriminator Loss: 0.546528\tGenerator Loss: 1.504407\n",
      "Train Epoch: 64 [16640/60000 (28%)]\tDiscriminator Loss: 0.503657\tGenerator Loss: 1.361696\n",
      "Train Epoch: 64 [17920/60000 (30%)]\tDiscriminator Loss: 0.483475\tGenerator Loss: 1.131248\n",
      "Train Epoch: 64 [19200/60000 (32%)]\tDiscriminator Loss: 0.506969\tGenerator Loss: 1.577033\n",
      "Train Epoch: 64 [20480/60000 (34%)]\tDiscriminator Loss: 0.514287\tGenerator Loss: 1.479694\n",
      "Train Epoch: 64 [21760/60000 (36%)]\tDiscriminator Loss: 0.470182\tGenerator Loss: 1.361842\n",
      "Train Epoch: 64 [23040/60000 (38%)]\tDiscriminator Loss: 0.480833\tGenerator Loss: 1.396481\n",
      "Train Epoch: 64 [24320/60000 (41%)]\tDiscriminator Loss: 0.597558\tGenerator Loss: 0.669905\n",
      "Train Epoch: 64 [25600/60000 (43%)]\tDiscriminator Loss: 0.495981\tGenerator Loss: 1.363277\n",
      "Train Epoch: 64 [26880/60000 (45%)]\tDiscriminator Loss: 0.525966\tGenerator Loss: 1.154452\n",
      "Train Epoch: 64 [28160/60000 (47%)]\tDiscriminator Loss: 0.534737\tGenerator Loss: 1.353740\n",
      "Train Epoch: 64 [29440/60000 (49%)]\tDiscriminator Loss: 0.510733\tGenerator Loss: 1.097725\n",
      "Train Epoch: 64 [30720/60000 (51%)]\tDiscriminator Loss: 0.494234\tGenerator Loss: 1.239460\n",
      "Train Epoch: 64 [32000/60000 (53%)]\tDiscriminator Loss: 0.563448\tGenerator Loss: 1.542601\n",
      "Train Epoch: 64 [33280/60000 (55%)]\tDiscriminator Loss: 0.512096\tGenerator Loss: 1.268236\n",
      "Train Epoch: 64 [34560/60000 (58%)]\tDiscriminator Loss: 0.517804\tGenerator Loss: 1.415354\n",
      "Train Epoch: 64 [35840/60000 (60%)]\tDiscriminator Loss: 0.490700\tGenerator Loss: 0.874781\n",
      "Train Epoch: 64 [37120/60000 (62%)]\tDiscriminator Loss: 0.484128\tGenerator Loss: 1.283744\n",
      "Train Epoch: 64 [38400/60000 (64%)]\tDiscriminator Loss: 0.552136\tGenerator Loss: 1.432559\n",
      "Train Epoch: 64 [39680/60000 (66%)]\tDiscriminator Loss: 0.475303\tGenerator Loss: 1.372057\n",
      "Train Epoch: 64 [40960/60000 (68%)]\tDiscriminator Loss: 0.489989\tGenerator Loss: 1.263160\n",
      "Train Epoch: 64 [42240/60000 (70%)]\tDiscriminator Loss: 0.558295\tGenerator Loss: 1.352062\n",
      "Train Epoch: 64 [43520/60000 (72%)]\tDiscriminator Loss: 0.560873\tGenerator Loss: 1.466082\n",
      "Train Epoch: 64 [44800/60000 (75%)]\tDiscriminator Loss: 0.493503\tGenerator Loss: 1.600243\n",
      "Train Epoch: 64 [46080/60000 (77%)]\tDiscriminator Loss: 0.568414\tGenerator Loss: 1.025353\n",
      "Train Epoch: 64 [47360/60000 (79%)]\tDiscriminator Loss: 0.427530\tGenerator Loss: 1.270890\n",
      "Train Epoch: 64 [48640/60000 (81%)]\tDiscriminator Loss: 0.552204\tGenerator Loss: 1.465196\n",
      "Train Epoch: 64 [49920/60000 (83%)]\tDiscriminator Loss: 0.492339\tGenerator Loss: 1.127333\n",
      "Train Epoch: 64 [51200/60000 (85%)]\tDiscriminator Loss: 0.488771\tGenerator Loss: 1.725818\n",
      "Train Epoch: 64 [52480/60000 (87%)]\tDiscriminator Loss: 0.520918\tGenerator Loss: 1.257175\n",
      "Train Epoch: 64 [53760/60000 (90%)]\tDiscriminator Loss: 0.462208\tGenerator Loss: 1.451294\n",
      "Train Epoch: 64 [55040/60000 (92%)]\tDiscriminator Loss: 0.516999\tGenerator Loss: 1.099146\n",
      "Train Epoch: 64 [56320/60000 (94%)]\tDiscriminator Loss: 0.536407\tGenerator Loss: 1.233715\n",
      "Train Epoch: 64 [57600/60000 (96%)]\tDiscriminator Loss: 0.508204\tGenerator Loss: 1.308387\n",
      "Train Epoch: 64 [58880/60000 (98%)]\tDiscriminator Loss: 0.536219\tGenerator Loss: 1.227110\n",
      "Train Epoch: 65 [0/60000 (0%)]\tDiscriminator Loss: 0.494607\tGenerator Loss: 1.117674\n",
      "Train Epoch: 65 [1280/60000 (2%)]\tDiscriminator Loss: 0.458528\tGenerator Loss: 1.430097\n",
      "Train Epoch: 65 [2560/60000 (4%)]\tDiscriminator Loss: 0.573511\tGenerator Loss: 1.138058\n",
      "Train Epoch: 65 [3840/60000 (6%)]\tDiscriminator Loss: 0.479911\tGenerator Loss: 1.607279\n",
      "Train Epoch: 65 [5120/60000 (9%)]\tDiscriminator Loss: 0.510583\tGenerator Loss: 1.129200\n",
      "Train Epoch: 65 [6400/60000 (11%)]\tDiscriminator Loss: 0.463467\tGenerator Loss: 1.208884\n",
      "Train Epoch: 65 [7680/60000 (13%)]\tDiscriminator Loss: 0.506698\tGenerator Loss: 1.212552\n",
      "Train Epoch: 65 [8960/60000 (15%)]\tDiscriminator Loss: 0.518675\tGenerator Loss: 1.014403\n",
      "Train Epoch: 65 [10240/60000 (17%)]\tDiscriminator Loss: 0.509871\tGenerator Loss: 1.265591\n",
      "Train Epoch: 65 [11520/60000 (19%)]\tDiscriminator Loss: 0.518460\tGenerator Loss: 1.178290\n",
      "Train Epoch: 65 [12800/60000 (21%)]\tDiscriminator Loss: 0.474720\tGenerator Loss: 1.388322\n",
      "Train Epoch: 65 [14080/60000 (23%)]\tDiscriminator Loss: 0.454423\tGenerator Loss: 1.540258\n",
      "Train Epoch: 65 [15360/60000 (26%)]\tDiscriminator Loss: 0.498156\tGenerator Loss: 1.100302\n",
      "Train Epoch: 65 [16640/60000 (28%)]\tDiscriminator Loss: 0.446294\tGenerator Loss: 1.253431\n",
      "Train Epoch: 65 [17920/60000 (30%)]\tDiscriminator Loss: 0.455734\tGenerator Loss: 1.660807\n",
      "Train Epoch: 65 [19200/60000 (32%)]\tDiscriminator Loss: 0.486760\tGenerator Loss: 1.250665\n",
      "Train Epoch: 65 [20480/60000 (34%)]\tDiscriminator Loss: 0.454480\tGenerator Loss: 1.553084\n",
      "Train Epoch: 65 [21760/60000 (36%)]\tDiscriminator Loss: 0.506059\tGenerator Loss: 1.341532\n",
      "Train Epoch: 65 [23040/60000 (38%)]\tDiscriminator Loss: 0.508476\tGenerator Loss: 1.592930\n",
      "Train Epoch: 65 [24320/60000 (41%)]\tDiscriminator Loss: 0.552499\tGenerator Loss: 1.556035\n",
      "Train Epoch: 65 [25600/60000 (43%)]\tDiscriminator Loss: 0.483648\tGenerator Loss: 1.166168\n",
      "Train Epoch: 65 [26880/60000 (45%)]\tDiscriminator Loss: 0.486780\tGenerator Loss: 1.279459\n",
      "Train Epoch: 65 [28160/60000 (47%)]\tDiscriminator Loss: 0.511289\tGenerator Loss: 1.430864\n",
      "Train Epoch: 65 [29440/60000 (49%)]\tDiscriminator Loss: 0.481863\tGenerator Loss: 1.102291\n",
      "Train Epoch: 65 [30720/60000 (51%)]\tDiscriminator Loss: 0.488646\tGenerator Loss: 1.436844\n",
      "Train Epoch: 65 [32000/60000 (53%)]\tDiscriminator Loss: 0.530494\tGenerator Loss: 1.260969\n",
      "Train Epoch: 65 [33280/60000 (55%)]\tDiscriminator Loss: 0.522979\tGenerator Loss: 1.126645\n",
      "Train Epoch: 65 [34560/60000 (58%)]\tDiscriminator Loss: 0.514804\tGenerator Loss: 0.967306\n",
      "Train Epoch: 65 [35840/60000 (60%)]\tDiscriminator Loss: 0.464139\tGenerator Loss: 1.236588\n",
      "Train Epoch: 65 [37120/60000 (62%)]\tDiscriminator Loss: 0.487021\tGenerator Loss: 1.191471\n",
      "Train Epoch: 65 [38400/60000 (64%)]\tDiscriminator Loss: 0.520728\tGenerator Loss: 1.403354\n",
      "Train Epoch: 65 [39680/60000 (66%)]\tDiscriminator Loss: 0.517985\tGenerator Loss: 1.083468\n",
      "Train Epoch: 65 [40960/60000 (68%)]\tDiscriminator Loss: 0.492554\tGenerator Loss: 1.477843\n",
      "Train Epoch: 65 [42240/60000 (70%)]\tDiscriminator Loss: 0.621159\tGenerator Loss: 0.898382\n",
      "Train Epoch: 65 [43520/60000 (72%)]\tDiscriminator Loss: 0.530233\tGenerator Loss: 1.389705\n",
      "Train Epoch: 65 [44800/60000 (75%)]\tDiscriminator Loss: 0.503800\tGenerator Loss: 1.753952\n",
      "Train Epoch: 65 [46080/60000 (77%)]\tDiscriminator Loss: 0.474037\tGenerator Loss: 1.422843\n",
      "Train Epoch: 65 [47360/60000 (79%)]\tDiscriminator Loss: 0.531034\tGenerator Loss: 1.277706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 65 [48640/60000 (81%)]\tDiscriminator Loss: 0.504111\tGenerator Loss: 0.978135\n",
      "Train Epoch: 65 [49920/60000 (83%)]\tDiscriminator Loss: 0.489316\tGenerator Loss: 1.543360\n",
      "Train Epoch: 65 [51200/60000 (85%)]\tDiscriminator Loss: 0.456612\tGenerator Loss: 1.683332\n",
      "Train Epoch: 65 [52480/60000 (87%)]\tDiscriminator Loss: 0.651675\tGenerator Loss: 0.718435\n",
      "Train Epoch: 65 [53760/60000 (90%)]\tDiscriminator Loss: 0.496741\tGenerator Loss: 1.717315\n",
      "Train Epoch: 65 [55040/60000 (92%)]\tDiscriminator Loss: 0.522052\tGenerator Loss: 1.219236\n",
      "Train Epoch: 65 [56320/60000 (94%)]\tDiscriminator Loss: 0.524064\tGenerator Loss: 1.017294\n",
      "Train Epoch: 65 [57600/60000 (96%)]\tDiscriminator Loss: 0.544893\tGenerator Loss: 1.164451\n",
      "Train Epoch: 65 [58880/60000 (98%)]\tDiscriminator Loss: 0.512459\tGenerator Loss: 1.183123\n",
      "Train Epoch: 66 [0/60000 (0%)]\tDiscriminator Loss: 0.563551\tGenerator Loss: 0.959424\n",
      "Train Epoch: 66 [1280/60000 (2%)]\tDiscriminator Loss: 0.468848\tGenerator Loss: 1.375806\n",
      "Train Epoch: 66 [2560/60000 (4%)]\tDiscriminator Loss: 0.522615\tGenerator Loss: 1.460217\n",
      "Train Epoch: 66 [3840/60000 (6%)]\tDiscriminator Loss: 0.507423\tGenerator Loss: 1.216636\n",
      "Train Epoch: 66 [5120/60000 (9%)]\tDiscriminator Loss: 0.469051\tGenerator Loss: 1.186861\n",
      "Train Epoch: 66 [6400/60000 (11%)]\tDiscriminator Loss: 0.510280\tGenerator Loss: 1.690013\n",
      "Train Epoch: 66 [7680/60000 (13%)]\tDiscriminator Loss: 0.531504\tGenerator Loss: 1.576003\n",
      "Train Epoch: 66 [8960/60000 (15%)]\tDiscriminator Loss: 0.514268\tGenerator Loss: 1.352227\n",
      "Train Epoch: 66 [10240/60000 (17%)]\tDiscriminator Loss: 0.492063\tGenerator Loss: 1.271096\n",
      "Train Epoch: 66 [11520/60000 (19%)]\tDiscriminator Loss: 0.498640\tGenerator Loss: 1.384308\n",
      "Train Epoch: 66 [12800/60000 (21%)]\tDiscriminator Loss: 0.502413\tGenerator Loss: 1.322616\n",
      "Train Epoch: 66 [14080/60000 (23%)]\tDiscriminator Loss: 0.541654\tGenerator Loss: 1.379892\n",
      "Train Epoch: 66 [15360/60000 (26%)]\tDiscriminator Loss: 0.502237\tGenerator Loss: 1.378385\n",
      "Train Epoch: 66 [16640/60000 (28%)]\tDiscriminator Loss: 0.511592\tGenerator Loss: 1.088922\n",
      "Train Epoch: 66 [17920/60000 (30%)]\tDiscriminator Loss: 0.543767\tGenerator Loss: 1.364528\n",
      "Train Epoch: 66 [19200/60000 (32%)]\tDiscriminator Loss: 0.508661\tGenerator Loss: 1.006334\n",
      "Train Epoch: 66 [20480/60000 (34%)]\tDiscriminator Loss: 0.486706\tGenerator Loss: 1.082568\n",
      "Train Epoch: 66 [21760/60000 (36%)]\tDiscriminator Loss: 0.495346\tGenerator Loss: 1.371058\n",
      "Train Epoch: 66 [23040/60000 (38%)]\tDiscriminator Loss: 0.515197\tGenerator Loss: 1.710969\n",
      "Train Epoch: 66 [24320/60000 (41%)]\tDiscriminator Loss: 0.492072\tGenerator Loss: 1.219208\n",
      "Train Epoch: 66 [25600/60000 (43%)]\tDiscriminator Loss: 0.483987\tGenerator Loss: 1.307789\n",
      "Train Epoch: 66 [26880/60000 (45%)]\tDiscriminator Loss: 0.548629\tGenerator Loss: 1.367191\n",
      "Train Epoch: 66 [28160/60000 (47%)]\tDiscriminator Loss: 0.487191\tGenerator Loss: 1.607197\n",
      "Train Epoch: 66 [29440/60000 (49%)]\tDiscriminator Loss: 0.527508\tGenerator Loss: 1.202996\n",
      "Train Epoch: 66 [30720/60000 (51%)]\tDiscriminator Loss: 0.467596\tGenerator Loss: 1.545217\n",
      "Train Epoch: 66 [32000/60000 (53%)]\tDiscriminator Loss: 0.551945\tGenerator Loss: 1.023717\n",
      "Train Epoch: 66 [33280/60000 (55%)]\tDiscriminator Loss: 0.486379\tGenerator Loss: 1.191892\n",
      "Train Epoch: 66 [34560/60000 (58%)]\tDiscriminator Loss: 0.535367\tGenerator Loss: 1.564290\n",
      "Train Epoch: 66 [35840/60000 (60%)]\tDiscriminator Loss: 0.574523\tGenerator Loss: 1.502913\n",
      "Train Epoch: 66 [37120/60000 (62%)]\tDiscriminator Loss: 0.516064\tGenerator Loss: 1.450301\n",
      "Train Epoch: 66 [38400/60000 (64%)]\tDiscriminator Loss: 0.471410\tGenerator Loss: 1.881502\n",
      "Train Epoch: 66 [39680/60000 (66%)]\tDiscriminator Loss: 0.522685\tGenerator Loss: 1.604676\n",
      "Train Epoch: 66 [40960/60000 (68%)]\tDiscriminator Loss: 0.661161\tGenerator Loss: 0.549382\n",
      "Train Epoch: 66 [42240/60000 (70%)]\tDiscriminator Loss: 0.494282\tGenerator Loss: 1.415060\n",
      "Train Epoch: 66 [43520/60000 (72%)]\tDiscriminator Loss: 0.520897\tGenerator Loss: 1.555176\n",
      "Train Epoch: 66 [44800/60000 (75%)]\tDiscriminator Loss: 0.545603\tGenerator Loss: 2.013194\n",
      "Train Epoch: 66 [46080/60000 (77%)]\tDiscriminator Loss: 0.570558\tGenerator Loss: 0.788391\n",
      "Train Epoch: 66 [47360/60000 (79%)]\tDiscriminator Loss: 0.485588\tGenerator Loss: 1.326379\n",
      "Train Epoch: 66 [48640/60000 (81%)]\tDiscriminator Loss: 0.524952\tGenerator Loss: 1.241768\n",
      "Train Epoch: 66 [49920/60000 (83%)]\tDiscriminator Loss: 0.521252\tGenerator Loss: 1.344692\n",
      "Train Epoch: 66 [51200/60000 (85%)]\tDiscriminator Loss: 0.522433\tGenerator Loss: 1.142208\n",
      "Train Epoch: 66 [52480/60000 (87%)]\tDiscriminator Loss: 0.472531\tGenerator Loss: 1.368084\n",
      "Train Epoch: 66 [53760/60000 (90%)]\tDiscriminator Loss: 0.527776\tGenerator Loss: 1.105649\n",
      "Train Epoch: 66 [55040/60000 (92%)]\tDiscriminator Loss: 0.461739\tGenerator Loss: 1.469441\n",
      "Train Epoch: 66 [56320/60000 (94%)]\tDiscriminator Loss: 0.491239\tGenerator Loss: 1.267822\n",
      "Train Epoch: 66 [57600/60000 (96%)]\tDiscriminator Loss: 0.496609\tGenerator Loss: 1.456666\n",
      "Train Epoch: 66 [58880/60000 (98%)]\tDiscriminator Loss: 0.551334\tGenerator Loss: 1.089232\n",
      "Train Epoch: 67 [0/60000 (0%)]\tDiscriminator Loss: 0.489073\tGenerator Loss: 1.393013\n",
      "Train Epoch: 67 [1280/60000 (2%)]\tDiscriminator Loss: 0.484695\tGenerator Loss: 1.505633\n",
      "Train Epoch: 67 [2560/60000 (4%)]\tDiscriminator Loss: 0.479592\tGenerator Loss: 1.700968\n",
      "Train Epoch: 67 [3840/60000 (6%)]\tDiscriminator Loss: 0.423771\tGenerator Loss: 1.341157\n",
      "Train Epoch: 67 [5120/60000 (9%)]\tDiscriminator Loss: 0.540698\tGenerator Loss: 0.966267\n",
      "Train Epoch: 67 [6400/60000 (11%)]\tDiscriminator Loss: 0.522401\tGenerator Loss: 1.400171\n",
      "Train Epoch: 67 [7680/60000 (13%)]\tDiscriminator Loss: 0.505589\tGenerator Loss: 1.518974\n",
      "Train Epoch: 67 [8960/60000 (15%)]\tDiscriminator Loss: 0.476280\tGenerator Loss: 1.313266\n",
      "Train Epoch: 67 [10240/60000 (17%)]\tDiscriminator Loss: 0.501817\tGenerator Loss: 1.196849\n",
      "Train Epoch: 67 [11520/60000 (19%)]\tDiscriminator Loss: 0.461683\tGenerator Loss: 1.568465\n",
      "Train Epoch: 67 [12800/60000 (21%)]\tDiscriminator Loss: 0.452598\tGenerator Loss: 1.108173\n",
      "Train Epoch: 67 [14080/60000 (23%)]\tDiscriminator Loss: 0.465404\tGenerator Loss: 1.026021\n",
      "Train Epoch: 67 [15360/60000 (26%)]\tDiscriminator Loss: 0.545248\tGenerator Loss: 1.453253\n",
      "Train Epoch: 67 [16640/60000 (28%)]\tDiscriminator Loss: 0.475265\tGenerator Loss: 1.407896\n",
      "Train Epoch: 67 [17920/60000 (30%)]\tDiscriminator Loss: 0.453986\tGenerator Loss: 1.475312\n",
      "Train Epoch: 67 [19200/60000 (32%)]\tDiscriminator Loss: 0.459101\tGenerator Loss: 1.251231\n",
      "Train Epoch: 67 [20480/60000 (34%)]\tDiscriminator Loss: 0.499985\tGenerator Loss: 1.275893\n",
      "Train Epoch: 67 [21760/60000 (36%)]\tDiscriminator Loss: 0.507106\tGenerator Loss: 1.088126\n",
      "Train Epoch: 67 [23040/60000 (38%)]\tDiscriminator Loss: 0.461291\tGenerator Loss: 1.913911\n",
      "Train Epoch: 67 [24320/60000 (41%)]\tDiscriminator Loss: 0.507780\tGenerator Loss: 1.308492\n",
      "Train Epoch: 67 [25600/60000 (43%)]\tDiscriminator Loss: 0.494951\tGenerator Loss: 1.241576\n",
      "Train Epoch: 67 [26880/60000 (45%)]\tDiscriminator Loss: 0.497948\tGenerator Loss: 1.203033\n",
      "Train Epoch: 67 [28160/60000 (47%)]\tDiscriminator Loss: 0.541282\tGenerator Loss: 0.865325\n",
      "Train Epoch: 67 [29440/60000 (49%)]\tDiscriminator Loss: 0.538492\tGenerator Loss: 1.535123\n",
      "Train Epoch: 67 [30720/60000 (51%)]\tDiscriminator Loss: 0.489394\tGenerator Loss: 1.061565\n",
      "Train Epoch: 67 [32000/60000 (53%)]\tDiscriminator Loss: 0.442330\tGenerator Loss: 1.294934\n",
      "Train Epoch: 67 [33280/60000 (55%)]\tDiscriminator Loss: 0.466989\tGenerator Loss: 1.441062\n",
      "Train Epoch: 67 [34560/60000 (58%)]\tDiscriminator Loss: 0.511625\tGenerator Loss: 1.254960\n",
      "Train Epoch: 67 [35840/60000 (60%)]\tDiscriminator Loss: 0.482493\tGenerator Loss: 1.354196\n",
      "Train Epoch: 67 [37120/60000 (62%)]\tDiscriminator Loss: 0.525688\tGenerator Loss: 1.206453\n",
      "Train Epoch: 67 [38400/60000 (64%)]\tDiscriminator Loss: 0.495164\tGenerator Loss: 1.263131\n",
      "Train Epoch: 67 [39680/60000 (66%)]\tDiscriminator Loss: 0.528234\tGenerator Loss: 1.423183\n",
      "Train Epoch: 67 [40960/60000 (68%)]\tDiscriminator Loss: 0.511979\tGenerator Loss: 1.081614\n",
      "Train Epoch: 67 [42240/60000 (70%)]\tDiscriminator Loss: 0.557346\tGenerator Loss: 2.070620\n",
      "Train Epoch: 67 [43520/60000 (72%)]\tDiscriminator Loss: 0.519388\tGenerator Loss: 1.282003\n",
      "Train Epoch: 67 [44800/60000 (75%)]\tDiscriminator Loss: 0.467227\tGenerator Loss: 1.104393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 67 [46080/60000 (77%)]\tDiscriminator Loss: 0.501949\tGenerator Loss: 1.749670\n",
      "Train Epoch: 67 [47360/60000 (79%)]\tDiscriminator Loss: 0.542064\tGenerator Loss: 1.029919\n",
      "Train Epoch: 67 [48640/60000 (81%)]\tDiscriminator Loss: 0.508608\tGenerator Loss: 1.431596\n",
      "Train Epoch: 67 [49920/60000 (83%)]\tDiscriminator Loss: 0.535911\tGenerator Loss: 1.140053\n",
      "Train Epoch: 67 [51200/60000 (85%)]\tDiscriminator Loss: 0.407305\tGenerator Loss: 1.283350\n",
      "Train Epoch: 67 [52480/60000 (87%)]\tDiscriminator Loss: 0.522158\tGenerator Loss: 1.321192\n",
      "Train Epoch: 67 [53760/60000 (90%)]\tDiscriminator Loss: 0.483731\tGenerator Loss: 1.276742\n",
      "Train Epoch: 67 [55040/60000 (92%)]\tDiscriminator Loss: 0.514287\tGenerator Loss: 1.347682\n",
      "Train Epoch: 67 [56320/60000 (94%)]\tDiscriminator Loss: 0.535686\tGenerator Loss: 1.191533\n",
      "Train Epoch: 67 [57600/60000 (96%)]\tDiscriminator Loss: 0.521076\tGenerator Loss: 1.377980\n",
      "Train Epoch: 67 [58880/60000 (98%)]\tDiscriminator Loss: 0.461803\tGenerator Loss: 1.387184\n",
      "Train Epoch: 68 [0/60000 (0%)]\tDiscriminator Loss: 0.460704\tGenerator Loss: 1.304649\n",
      "Train Epoch: 68 [1280/60000 (2%)]\tDiscriminator Loss: 0.536305\tGenerator Loss: 1.594285\n",
      "Train Epoch: 68 [2560/60000 (4%)]\tDiscriminator Loss: 0.550815\tGenerator Loss: 1.724988\n",
      "Train Epoch: 68 [3840/60000 (6%)]\tDiscriminator Loss: 0.518002\tGenerator Loss: 1.542711\n",
      "Train Epoch: 68 [5120/60000 (9%)]\tDiscriminator Loss: 0.503660\tGenerator Loss: 1.465454\n",
      "Train Epoch: 68 [6400/60000 (11%)]\tDiscriminator Loss: 0.479618\tGenerator Loss: 1.207689\n",
      "Train Epoch: 68 [7680/60000 (13%)]\tDiscriminator Loss: 0.487761\tGenerator Loss: 1.211167\n",
      "Train Epoch: 68 [8960/60000 (15%)]\tDiscriminator Loss: 0.516245\tGenerator Loss: 1.230770\n",
      "Train Epoch: 68 [10240/60000 (17%)]\tDiscriminator Loss: 0.473354\tGenerator Loss: 1.583781\n",
      "Train Epoch: 68 [11520/60000 (19%)]\tDiscriminator Loss: 0.554608\tGenerator Loss: 1.473974\n",
      "Train Epoch: 68 [12800/60000 (21%)]\tDiscriminator Loss: 0.479363\tGenerator Loss: 1.157716\n",
      "Train Epoch: 68 [14080/60000 (23%)]\tDiscriminator Loss: 0.451065\tGenerator Loss: 1.492516\n",
      "Train Epoch: 68 [15360/60000 (26%)]\tDiscriminator Loss: 0.494248\tGenerator Loss: 1.631483\n",
      "Train Epoch: 68 [16640/60000 (28%)]\tDiscriminator Loss: 0.481532\tGenerator Loss: 1.449138\n",
      "Train Epoch: 68 [17920/60000 (30%)]\tDiscriminator Loss: 0.528666\tGenerator Loss: 1.252655\n",
      "Train Epoch: 68 [19200/60000 (32%)]\tDiscriminator Loss: 0.571941\tGenerator Loss: 0.966779\n",
      "Train Epoch: 68 [20480/60000 (34%)]\tDiscriminator Loss: 0.471370\tGenerator Loss: 1.065662\n",
      "Train Epoch: 68 [21760/60000 (36%)]\tDiscriminator Loss: 0.478948\tGenerator Loss: 1.526963\n",
      "Train Epoch: 68 [23040/60000 (38%)]\tDiscriminator Loss: 0.531107\tGenerator Loss: 1.733758\n",
      "Train Epoch: 68 [24320/60000 (41%)]\tDiscriminator Loss: 0.537988\tGenerator Loss: 1.053741\n",
      "Train Epoch: 68 [25600/60000 (43%)]\tDiscriminator Loss: 0.450127\tGenerator Loss: 1.426450\n",
      "Train Epoch: 68 [26880/60000 (45%)]\tDiscriminator Loss: 0.506850\tGenerator Loss: 1.255713\n",
      "Train Epoch: 68 [28160/60000 (47%)]\tDiscriminator Loss: 0.528289\tGenerator Loss: 1.302953\n",
      "Train Epoch: 68 [29440/60000 (49%)]\tDiscriminator Loss: 0.508215\tGenerator Loss: 1.452552\n",
      "Train Epoch: 68 [30720/60000 (51%)]\tDiscriminator Loss: 0.470059\tGenerator Loss: 1.335392\n",
      "Train Epoch: 68 [32000/60000 (53%)]\tDiscriminator Loss: 0.491278\tGenerator Loss: 1.616310\n",
      "Train Epoch: 68 [33280/60000 (55%)]\tDiscriminator Loss: 0.497896\tGenerator Loss: 1.472747\n",
      "Train Epoch: 68 [34560/60000 (58%)]\tDiscriminator Loss: 0.532123\tGenerator Loss: 1.249831\n",
      "Train Epoch: 68 [35840/60000 (60%)]\tDiscriminator Loss: 0.512297\tGenerator Loss: 1.139661\n",
      "Train Epoch: 68 [37120/60000 (62%)]\tDiscriminator Loss: 0.487011\tGenerator Loss: 1.136995\n",
      "Train Epoch: 68 [38400/60000 (64%)]\tDiscriminator Loss: 0.506031\tGenerator Loss: 1.616850\n",
      "Train Epoch: 68 [39680/60000 (66%)]\tDiscriminator Loss: 0.524415\tGenerator Loss: 1.464886\n",
      "Train Epoch: 68 [40960/60000 (68%)]\tDiscriminator Loss: 0.551867\tGenerator Loss: 1.214462\n",
      "Train Epoch: 68 [42240/60000 (70%)]\tDiscriminator Loss: 0.544655\tGenerator Loss: 1.393936\n",
      "Train Epoch: 68 [43520/60000 (72%)]\tDiscriminator Loss: 0.462776\tGenerator Loss: 1.294785\n",
      "Train Epoch: 68 [44800/60000 (75%)]\tDiscriminator Loss: 0.503987\tGenerator Loss: 1.071131\n",
      "Train Epoch: 68 [46080/60000 (77%)]\tDiscriminator Loss: 0.502169\tGenerator Loss: 1.390736\n",
      "Train Epoch: 68 [47360/60000 (79%)]\tDiscriminator Loss: 0.475188\tGenerator Loss: 1.288114\n",
      "Train Epoch: 68 [48640/60000 (81%)]\tDiscriminator Loss: 0.532823\tGenerator Loss: 1.629854\n",
      "Train Epoch: 68 [49920/60000 (83%)]\tDiscriminator Loss: 0.537395\tGenerator Loss: 1.482186\n",
      "Train Epoch: 68 [51200/60000 (85%)]\tDiscriminator Loss: 0.510720\tGenerator Loss: 1.361716\n",
      "Train Epoch: 68 [52480/60000 (87%)]\tDiscriminator Loss: 0.504938\tGenerator Loss: 1.379846\n",
      "Train Epoch: 68 [53760/60000 (90%)]\tDiscriminator Loss: 0.510256\tGenerator Loss: 1.289255\n",
      "Train Epoch: 68 [55040/60000 (92%)]\tDiscriminator Loss: 0.537388\tGenerator Loss: 1.353298\n",
      "Train Epoch: 68 [56320/60000 (94%)]\tDiscriminator Loss: 0.605300\tGenerator Loss: 1.679747\n",
      "Train Epoch: 68 [57600/60000 (96%)]\tDiscriminator Loss: 0.526656\tGenerator Loss: 1.472205\n",
      "Train Epoch: 68 [58880/60000 (98%)]\tDiscriminator Loss: 0.484915\tGenerator Loss: 1.291470\n",
      "Train Epoch: 69 [0/60000 (0%)]\tDiscriminator Loss: 0.559280\tGenerator Loss: 0.954739\n",
      "Train Epoch: 69 [1280/60000 (2%)]\tDiscriminator Loss: 0.537976\tGenerator Loss: 0.869720\n",
      "Train Epoch: 69 [2560/60000 (4%)]\tDiscriminator Loss: 0.508887\tGenerator Loss: 1.179550\n",
      "Train Epoch: 69 [3840/60000 (6%)]\tDiscriminator Loss: 0.567477\tGenerator Loss: 1.144584\n",
      "Train Epoch: 69 [5120/60000 (9%)]\tDiscriminator Loss: 0.474311\tGenerator Loss: 1.292711\n",
      "Train Epoch: 69 [6400/60000 (11%)]\tDiscriminator Loss: 0.491466\tGenerator Loss: 1.425311\n",
      "Train Epoch: 69 [7680/60000 (13%)]\tDiscriminator Loss: 0.526031\tGenerator Loss: 1.436393\n",
      "Train Epoch: 69 [8960/60000 (15%)]\tDiscriminator Loss: 0.474724\tGenerator Loss: 1.288963\n",
      "Train Epoch: 69 [10240/60000 (17%)]\tDiscriminator Loss: 0.507194\tGenerator Loss: 1.292513\n",
      "Train Epoch: 69 [11520/60000 (19%)]\tDiscriminator Loss: 0.455835\tGenerator Loss: 1.330685\n",
      "Train Epoch: 69 [12800/60000 (21%)]\tDiscriminator Loss: 0.502216\tGenerator Loss: 1.438198\n",
      "Train Epoch: 69 [14080/60000 (23%)]\tDiscriminator Loss: 0.506224\tGenerator Loss: 1.148280\n",
      "Train Epoch: 69 [15360/60000 (26%)]\tDiscriminator Loss: 0.579359\tGenerator Loss: 0.817775\n",
      "Train Epoch: 69 [16640/60000 (28%)]\tDiscriminator Loss: 0.488746\tGenerator Loss: 1.258911\n",
      "Train Epoch: 69 [17920/60000 (30%)]\tDiscriminator Loss: 0.514736\tGenerator Loss: 1.472686\n",
      "Train Epoch: 69 [19200/60000 (32%)]\tDiscriminator Loss: 0.473779\tGenerator Loss: 1.352949\n",
      "Train Epoch: 69 [20480/60000 (34%)]\tDiscriminator Loss: 0.553562\tGenerator Loss: 0.851231\n",
      "Train Epoch: 69 [21760/60000 (36%)]\tDiscriminator Loss: 0.549541\tGenerator Loss: 1.108657\n",
      "Train Epoch: 69 [23040/60000 (38%)]\tDiscriminator Loss: 0.484295\tGenerator Loss: 1.429961\n",
      "Train Epoch: 69 [24320/60000 (41%)]\tDiscriminator Loss: 0.559954\tGenerator Loss: 0.980294\n",
      "Train Epoch: 69 [25600/60000 (43%)]\tDiscriminator Loss: 0.524771\tGenerator Loss: 1.461340\n",
      "Train Epoch: 69 [26880/60000 (45%)]\tDiscriminator Loss: 0.511141\tGenerator Loss: 1.186788\n",
      "Train Epoch: 69 [28160/60000 (47%)]\tDiscriminator Loss: 0.520914\tGenerator Loss: 1.059450\n",
      "Train Epoch: 69 [29440/60000 (49%)]\tDiscriminator Loss: 0.502442\tGenerator Loss: 1.343584\n",
      "Train Epoch: 69 [30720/60000 (51%)]\tDiscriminator Loss: 0.548955\tGenerator Loss: 1.318116\n",
      "Train Epoch: 69 [32000/60000 (53%)]\tDiscriminator Loss: 0.485382\tGenerator Loss: 1.161529\n",
      "Train Epoch: 69 [33280/60000 (55%)]\tDiscriminator Loss: 0.478117\tGenerator Loss: 1.339025\n",
      "Train Epoch: 69 [34560/60000 (58%)]\tDiscriminator Loss: 0.505381\tGenerator Loss: 1.118031\n",
      "Train Epoch: 69 [35840/60000 (60%)]\tDiscriminator Loss: 0.520398\tGenerator Loss: 1.771993\n",
      "Train Epoch: 69 [37120/60000 (62%)]\tDiscriminator Loss: 0.508898\tGenerator Loss: 1.112768\n",
      "Train Epoch: 69 [38400/60000 (64%)]\tDiscriminator Loss: 0.473440\tGenerator Loss: 1.191584\n",
      "Train Epoch: 69 [39680/60000 (66%)]\tDiscriminator Loss: 0.577920\tGenerator Loss: 1.439476\n",
      "Train Epoch: 69 [40960/60000 (68%)]\tDiscriminator Loss: 0.619384\tGenerator Loss: 1.970347\n",
      "Train Epoch: 69 [42240/60000 (70%)]\tDiscriminator Loss: 0.475990\tGenerator Loss: 1.299758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 69 [43520/60000 (72%)]\tDiscriminator Loss: 0.509784\tGenerator Loss: 1.507351\n",
      "Train Epoch: 69 [44800/60000 (75%)]\tDiscriminator Loss: 0.516313\tGenerator Loss: 1.208720\n",
      "Train Epoch: 69 [46080/60000 (77%)]\tDiscriminator Loss: 0.516609\tGenerator Loss: 1.466347\n",
      "Train Epoch: 69 [47360/60000 (79%)]\tDiscriminator Loss: 0.500274\tGenerator Loss: 1.316094\n",
      "Train Epoch: 69 [48640/60000 (81%)]\tDiscriminator Loss: 0.480250\tGenerator Loss: 2.107382\n",
      "Train Epoch: 69 [49920/60000 (83%)]\tDiscriminator Loss: 0.534769\tGenerator Loss: 0.965601\n",
      "Train Epoch: 69 [51200/60000 (85%)]\tDiscriminator Loss: 0.482142\tGenerator Loss: 1.448389\n",
      "Train Epoch: 69 [52480/60000 (87%)]\tDiscriminator Loss: 0.547988\tGenerator Loss: 1.672321\n",
      "Train Epoch: 69 [53760/60000 (90%)]\tDiscriminator Loss: 0.584139\tGenerator Loss: 1.615140\n",
      "Train Epoch: 69 [55040/60000 (92%)]\tDiscriminator Loss: 0.457250\tGenerator Loss: 1.368920\n",
      "Train Epoch: 69 [56320/60000 (94%)]\tDiscriminator Loss: 0.535028\tGenerator Loss: 1.302708\n",
      "Train Epoch: 69 [57600/60000 (96%)]\tDiscriminator Loss: 0.459662\tGenerator Loss: 1.510308\n",
      "Train Epoch: 69 [58880/60000 (98%)]\tDiscriminator Loss: 0.537133\tGenerator Loss: 1.268206\n",
      "Train Epoch: 70 [0/60000 (0%)]\tDiscriminator Loss: 0.491393\tGenerator Loss: 1.431558\n",
      "Train Epoch: 70 [1280/60000 (2%)]\tDiscriminator Loss: 0.495442\tGenerator Loss: 1.267510\n",
      "Train Epoch: 70 [2560/60000 (4%)]\tDiscriminator Loss: 0.498936\tGenerator Loss: 1.216088\n",
      "Train Epoch: 70 [3840/60000 (6%)]\tDiscriminator Loss: 0.490350\tGenerator Loss: 1.353478\n",
      "Train Epoch: 70 [5120/60000 (9%)]\tDiscriminator Loss: 0.515359\tGenerator Loss: 1.436687\n",
      "Train Epoch: 70 [6400/60000 (11%)]\tDiscriminator Loss: 0.496235\tGenerator Loss: 1.261261\n",
      "Train Epoch: 70 [7680/60000 (13%)]\tDiscriminator Loss: 0.488199\tGenerator Loss: 1.321947\n",
      "Train Epoch: 70 [8960/60000 (15%)]\tDiscriminator Loss: 0.474368\tGenerator Loss: 1.535845\n",
      "Train Epoch: 70 [10240/60000 (17%)]\tDiscriminator Loss: 0.451348\tGenerator Loss: 1.341052\n",
      "Train Epoch: 70 [11520/60000 (19%)]\tDiscriminator Loss: 0.585247\tGenerator Loss: 1.549578\n",
      "Train Epoch: 70 [12800/60000 (21%)]\tDiscriminator Loss: 0.479550\tGenerator Loss: 2.083272\n",
      "Train Epoch: 70 [14080/60000 (23%)]\tDiscriminator Loss: 0.453008\tGenerator Loss: 1.399313\n",
      "Train Epoch: 70 [15360/60000 (26%)]\tDiscriminator Loss: 0.520341\tGenerator Loss: 1.166792\n",
      "Train Epoch: 70 [16640/60000 (28%)]\tDiscriminator Loss: 0.500120\tGenerator Loss: 1.294420\n",
      "Train Epoch: 70 [17920/60000 (30%)]\tDiscriminator Loss: 0.513286\tGenerator Loss: 1.313203\n",
      "Train Epoch: 70 [19200/60000 (32%)]\tDiscriminator Loss: 0.545980\tGenerator Loss: 0.776960\n",
      "Train Epoch: 70 [20480/60000 (34%)]\tDiscriminator Loss: 0.551973\tGenerator Loss: 1.112865\n",
      "Train Epoch: 70 [21760/60000 (36%)]\tDiscriminator Loss: 0.527353\tGenerator Loss: 1.125066\n",
      "Train Epoch: 70 [23040/60000 (38%)]\tDiscriminator Loss: 0.521807\tGenerator Loss: 1.448594\n",
      "Train Epoch: 70 [24320/60000 (41%)]\tDiscriminator Loss: 0.432550\tGenerator Loss: 1.676917\n",
      "Train Epoch: 70 [25600/60000 (43%)]\tDiscriminator Loss: 0.472504\tGenerator Loss: 1.296763\n",
      "Train Epoch: 70 [26880/60000 (45%)]\tDiscriminator Loss: 0.483478\tGenerator Loss: 1.757266\n",
      "Train Epoch: 70 [28160/60000 (47%)]\tDiscriminator Loss: 0.509019\tGenerator Loss: 1.606518\n",
      "Train Epoch: 70 [29440/60000 (49%)]\tDiscriminator Loss: 0.537437\tGenerator Loss: 1.239704\n",
      "Train Epoch: 70 [30720/60000 (51%)]\tDiscriminator Loss: 0.469797\tGenerator Loss: 1.504996\n",
      "Train Epoch: 70 [32000/60000 (53%)]\tDiscriminator Loss: 0.467884\tGenerator Loss: 1.620145\n",
      "Train Epoch: 70 [33280/60000 (55%)]\tDiscriminator Loss: 0.504858\tGenerator Loss: 1.159056\n",
      "Train Epoch: 70 [34560/60000 (58%)]\tDiscriminator Loss: 0.542070\tGenerator Loss: 1.311116\n",
      "Train Epoch: 70 [35840/60000 (60%)]\tDiscriminator Loss: 0.520072\tGenerator Loss: 1.404010\n",
      "Train Epoch: 70 [37120/60000 (62%)]\tDiscriminator Loss: 0.513566\tGenerator Loss: 1.058419\n",
      "Train Epoch: 70 [38400/60000 (64%)]\tDiscriminator Loss: 0.528357\tGenerator Loss: 1.793865\n",
      "Train Epoch: 70 [39680/60000 (66%)]\tDiscriminator Loss: 0.485735\tGenerator Loss: 1.345046\n",
      "Train Epoch: 70 [40960/60000 (68%)]\tDiscriminator Loss: 0.484641\tGenerator Loss: 1.349669\n",
      "Train Epoch: 70 [42240/60000 (70%)]\tDiscriminator Loss: 0.570920\tGenerator Loss: 0.920775\n",
      "Train Epoch: 70 [43520/60000 (72%)]\tDiscriminator Loss: 0.510682\tGenerator Loss: 1.314029\n",
      "Train Epoch: 70 [44800/60000 (75%)]\tDiscriminator Loss: 0.495533\tGenerator Loss: 1.303215\n",
      "Train Epoch: 70 [46080/60000 (77%)]\tDiscriminator Loss: 0.454493\tGenerator Loss: 1.567750\n",
      "Train Epoch: 70 [47360/60000 (79%)]\tDiscriminator Loss: 0.486716\tGenerator Loss: 1.715295\n",
      "Train Epoch: 70 [48640/60000 (81%)]\tDiscriminator Loss: 0.452924\tGenerator Loss: 1.302781\n",
      "Train Epoch: 70 [49920/60000 (83%)]\tDiscriminator Loss: 0.471104\tGenerator Loss: 1.154885\n",
      "Train Epoch: 70 [51200/60000 (85%)]\tDiscriminator Loss: 0.480476\tGenerator Loss: 1.314209\n",
      "Train Epoch: 70 [52480/60000 (87%)]\tDiscriminator Loss: 0.508014\tGenerator Loss: 1.375709\n",
      "Train Epoch: 70 [53760/60000 (90%)]\tDiscriminator Loss: 0.450056\tGenerator Loss: 1.429594\n",
      "Train Epoch: 70 [55040/60000 (92%)]\tDiscriminator Loss: 0.480555\tGenerator Loss: 1.918403\n",
      "Train Epoch: 70 [56320/60000 (94%)]\tDiscriminator Loss: 0.554047\tGenerator Loss: 0.827901\n",
      "Train Epoch: 70 [57600/60000 (96%)]\tDiscriminator Loss: 0.470746\tGenerator Loss: 1.596515\n",
      "Train Epoch: 70 [58880/60000 (98%)]\tDiscriminator Loss: 0.503467\tGenerator Loss: 1.484601\n",
      "Train Epoch: 71 [0/60000 (0%)]\tDiscriminator Loss: 0.532502\tGenerator Loss: 0.920195\n",
      "Train Epoch: 71 [1280/60000 (2%)]\tDiscriminator Loss: 0.437596\tGenerator Loss: 1.725093\n",
      "Train Epoch: 71 [2560/60000 (4%)]\tDiscriminator Loss: 0.480852\tGenerator Loss: 1.855525\n",
      "Train Epoch: 71 [3840/60000 (6%)]\tDiscriminator Loss: 0.496592\tGenerator Loss: 1.548840\n",
      "Train Epoch: 71 [5120/60000 (9%)]\tDiscriminator Loss: 0.483689\tGenerator Loss: 1.476675\n",
      "Train Epoch: 71 [6400/60000 (11%)]\tDiscriminator Loss: 0.505848\tGenerator Loss: 1.361660\n",
      "Train Epoch: 71 [7680/60000 (13%)]\tDiscriminator Loss: 0.501769\tGenerator Loss: 1.507769\n",
      "Train Epoch: 71 [8960/60000 (15%)]\tDiscriminator Loss: 0.458451\tGenerator Loss: 1.785838\n",
      "Train Epoch: 71 [10240/60000 (17%)]\tDiscriminator Loss: 0.517658\tGenerator Loss: 0.933296\n",
      "Train Epoch: 71 [11520/60000 (19%)]\tDiscriminator Loss: 0.429806\tGenerator Loss: 1.379369\n",
      "Train Epoch: 71 [12800/60000 (21%)]\tDiscriminator Loss: 0.515206\tGenerator Loss: 1.072802\n",
      "Train Epoch: 71 [14080/60000 (23%)]\tDiscriminator Loss: 0.446085\tGenerator Loss: 1.457341\n",
      "Train Epoch: 71 [15360/60000 (26%)]\tDiscriminator Loss: 0.500398\tGenerator Loss: 1.600547\n",
      "Train Epoch: 71 [16640/60000 (28%)]\tDiscriminator Loss: 0.459665\tGenerator Loss: 1.600464\n",
      "Train Epoch: 71 [17920/60000 (30%)]\tDiscriminator Loss: 0.496917\tGenerator Loss: 1.292546\n",
      "Train Epoch: 71 [19200/60000 (32%)]\tDiscriminator Loss: 0.501376\tGenerator Loss: 1.693305\n",
      "Train Epoch: 71 [20480/60000 (34%)]\tDiscriminator Loss: 0.493681\tGenerator Loss: 1.720788\n",
      "Train Epoch: 71 [21760/60000 (36%)]\tDiscriminator Loss: 0.478350\tGenerator Loss: 1.404069\n",
      "Train Epoch: 71 [23040/60000 (38%)]\tDiscriminator Loss: 0.508227\tGenerator Loss: 1.353137\n",
      "Train Epoch: 71 [24320/60000 (41%)]\tDiscriminator Loss: 0.491576\tGenerator Loss: 1.081134\n",
      "Train Epoch: 71 [25600/60000 (43%)]\tDiscriminator Loss: 0.462909\tGenerator Loss: 1.404739\n",
      "Train Epoch: 71 [26880/60000 (45%)]\tDiscriminator Loss: 0.486936\tGenerator Loss: 1.143514\n",
      "Train Epoch: 71 [28160/60000 (47%)]\tDiscriminator Loss: 0.514658\tGenerator Loss: 1.618087\n",
      "Train Epoch: 71 [29440/60000 (49%)]\tDiscriminator Loss: 0.490216\tGenerator Loss: 1.007847\n",
      "Train Epoch: 71 [30720/60000 (51%)]\tDiscriminator Loss: 0.487528\tGenerator Loss: 1.545369\n",
      "Train Epoch: 71 [32000/60000 (53%)]\tDiscriminator Loss: 0.502756\tGenerator Loss: 0.968972\n",
      "Train Epoch: 71 [33280/60000 (55%)]\tDiscriminator Loss: 0.491475\tGenerator Loss: 1.474657\n",
      "Train Epoch: 71 [34560/60000 (58%)]\tDiscriminator Loss: 0.516757\tGenerator Loss: 1.677680\n",
      "Train Epoch: 71 [35840/60000 (60%)]\tDiscriminator Loss: 0.508323\tGenerator Loss: 1.546586\n",
      "Train Epoch: 71 [37120/60000 (62%)]\tDiscriminator Loss: 0.494528\tGenerator Loss: 1.259396\n",
      "Train Epoch: 71 [38400/60000 (64%)]\tDiscriminator Loss: 0.516670\tGenerator Loss: 1.315926\n",
      "Train Epoch: 71 [39680/60000 (66%)]\tDiscriminator Loss: 0.424999\tGenerator Loss: 1.560460\n",
      "Train Epoch: 71 [40960/60000 (68%)]\tDiscriminator Loss: 0.463007\tGenerator Loss: 1.086060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 71 [42240/60000 (70%)]\tDiscriminator Loss: 0.506418\tGenerator Loss: 1.527002\n",
      "Train Epoch: 71 [43520/60000 (72%)]\tDiscriminator Loss: 0.463976\tGenerator Loss: 1.286348\n",
      "Train Epoch: 71 [44800/60000 (75%)]\tDiscriminator Loss: 0.515544\tGenerator Loss: 1.311400\n",
      "Train Epoch: 71 [46080/60000 (77%)]\tDiscriminator Loss: 0.521263\tGenerator Loss: 0.918853\n",
      "Train Epoch: 71 [47360/60000 (79%)]\tDiscriminator Loss: 0.471254\tGenerator Loss: 1.449451\n",
      "Train Epoch: 71 [48640/60000 (81%)]\tDiscriminator Loss: 0.430890\tGenerator Loss: 1.463004\n",
      "Train Epoch: 71 [49920/60000 (83%)]\tDiscriminator Loss: 0.511628\tGenerator Loss: 1.792959\n",
      "Train Epoch: 71 [51200/60000 (85%)]\tDiscriminator Loss: 0.491496\tGenerator Loss: 1.066139\n",
      "Train Epoch: 71 [52480/60000 (87%)]\tDiscriminator Loss: 0.503669\tGenerator Loss: 1.088224\n",
      "Train Epoch: 71 [53760/60000 (90%)]\tDiscriminator Loss: 0.452894\tGenerator Loss: 1.264893\n",
      "Train Epoch: 71 [55040/60000 (92%)]\tDiscriminator Loss: 0.523564\tGenerator Loss: 0.859464\n",
      "Train Epoch: 71 [56320/60000 (94%)]\tDiscriminator Loss: 0.479226\tGenerator Loss: 1.070301\n",
      "Train Epoch: 71 [57600/60000 (96%)]\tDiscriminator Loss: 0.497622\tGenerator Loss: 1.319178\n",
      "Train Epoch: 71 [58880/60000 (98%)]\tDiscriminator Loss: 0.512675\tGenerator Loss: 1.291384\n",
      "Train Epoch: 72 [0/60000 (0%)]\tDiscriminator Loss: 0.533570\tGenerator Loss: 0.996542\n",
      "Train Epoch: 72 [1280/60000 (2%)]\tDiscriminator Loss: 0.487237\tGenerator Loss: 1.362414\n",
      "Train Epoch: 72 [2560/60000 (4%)]\tDiscriminator Loss: 0.492550\tGenerator Loss: 1.437720\n",
      "Train Epoch: 72 [3840/60000 (6%)]\tDiscriminator Loss: 0.481589\tGenerator Loss: 1.241922\n",
      "Train Epoch: 72 [5120/60000 (9%)]\tDiscriminator Loss: 0.498111\tGenerator Loss: 1.157534\n",
      "Train Epoch: 72 [6400/60000 (11%)]\tDiscriminator Loss: 0.458750\tGenerator Loss: 1.582502\n",
      "Train Epoch: 72 [7680/60000 (13%)]\tDiscriminator Loss: 0.558512\tGenerator Loss: 1.739583\n",
      "Train Epoch: 72 [8960/60000 (15%)]\tDiscriminator Loss: 0.510033\tGenerator Loss: 1.219587\n",
      "Train Epoch: 72 [10240/60000 (17%)]\tDiscriminator Loss: 0.517739\tGenerator Loss: 1.525187\n",
      "Train Epoch: 72 [11520/60000 (19%)]\tDiscriminator Loss: 0.455376\tGenerator Loss: 1.679032\n",
      "Train Epoch: 72 [12800/60000 (21%)]\tDiscriminator Loss: 0.521662\tGenerator Loss: 1.515775\n",
      "Train Epoch: 72 [14080/60000 (23%)]\tDiscriminator Loss: 0.572626\tGenerator Loss: 0.890281\n",
      "Train Epoch: 72 [15360/60000 (26%)]\tDiscriminator Loss: 0.557556\tGenerator Loss: 1.220959\n",
      "Train Epoch: 72 [16640/60000 (28%)]\tDiscriminator Loss: 0.513770\tGenerator Loss: 0.857458\n",
      "Train Epoch: 72 [17920/60000 (30%)]\tDiscriminator Loss: 0.482983\tGenerator Loss: 1.301865\n",
      "Train Epoch: 72 [19200/60000 (32%)]\tDiscriminator Loss: 0.504959\tGenerator Loss: 1.499834\n",
      "Train Epoch: 72 [20480/60000 (34%)]\tDiscriminator Loss: 0.517212\tGenerator Loss: 1.466141\n",
      "Train Epoch: 72 [21760/60000 (36%)]\tDiscriminator Loss: 0.500091\tGenerator Loss: 1.481306\n",
      "Train Epoch: 72 [23040/60000 (38%)]\tDiscriminator Loss: 0.506867\tGenerator Loss: 1.155757\n",
      "Train Epoch: 72 [24320/60000 (41%)]\tDiscriminator Loss: 0.517879\tGenerator Loss: 1.426231\n",
      "Train Epoch: 72 [25600/60000 (43%)]\tDiscriminator Loss: 0.453352\tGenerator Loss: 1.584400\n",
      "Train Epoch: 72 [26880/60000 (45%)]\tDiscriminator Loss: 0.522071\tGenerator Loss: 1.341721\n",
      "Train Epoch: 72 [28160/60000 (47%)]\tDiscriminator Loss: 0.526408\tGenerator Loss: 1.462097\n",
      "Train Epoch: 72 [29440/60000 (49%)]\tDiscriminator Loss: 0.501010\tGenerator Loss: 1.674966\n",
      "Train Epoch: 72 [30720/60000 (51%)]\tDiscriminator Loss: 0.483778\tGenerator Loss: 1.353322\n",
      "Train Epoch: 72 [32000/60000 (53%)]\tDiscriminator Loss: 0.507210\tGenerator Loss: 1.216110\n",
      "Train Epoch: 72 [33280/60000 (55%)]\tDiscriminator Loss: 0.494350\tGenerator Loss: 1.183951\n",
      "Train Epoch: 72 [34560/60000 (58%)]\tDiscriminator Loss: 0.568274\tGenerator Loss: 1.013299\n",
      "Train Epoch: 72 [35840/60000 (60%)]\tDiscriminator Loss: 0.497600\tGenerator Loss: 1.343879\n",
      "Train Epoch: 72 [37120/60000 (62%)]\tDiscriminator Loss: 0.496121\tGenerator Loss: 1.224241\n",
      "Train Epoch: 72 [38400/60000 (64%)]\tDiscriminator Loss: 0.543858\tGenerator Loss: 1.641541\n",
      "Train Epoch: 72 [39680/60000 (66%)]\tDiscriminator Loss: 0.501709\tGenerator Loss: 1.100775\n",
      "Train Epoch: 72 [40960/60000 (68%)]\tDiscriminator Loss: 0.535257\tGenerator Loss: 1.193082\n",
      "Train Epoch: 72 [42240/60000 (70%)]\tDiscriminator Loss: 0.502702\tGenerator Loss: 1.172850\n",
      "Train Epoch: 72 [43520/60000 (72%)]\tDiscriminator Loss: 0.506813\tGenerator Loss: 1.665369\n",
      "Train Epoch: 72 [44800/60000 (75%)]\tDiscriminator Loss: 0.472529\tGenerator Loss: 1.146009\n",
      "Train Epoch: 72 [46080/60000 (77%)]\tDiscriminator Loss: 0.533365\tGenerator Loss: 1.597798\n",
      "Train Epoch: 72 [47360/60000 (79%)]\tDiscriminator Loss: 0.527115\tGenerator Loss: 1.262339\n",
      "Train Epoch: 72 [48640/60000 (81%)]\tDiscriminator Loss: 0.507179\tGenerator Loss: 1.364729\n",
      "Train Epoch: 72 [49920/60000 (83%)]\tDiscriminator Loss: 0.527051\tGenerator Loss: 1.628483\n",
      "Train Epoch: 72 [51200/60000 (85%)]\tDiscriminator Loss: 0.475605\tGenerator Loss: 1.544390\n",
      "Train Epoch: 72 [52480/60000 (87%)]\tDiscriminator Loss: 0.482328\tGenerator Loss: 1.091239\n",
      "Train Epoch: 72 [53760/60000 (90%)]\tDiscriminator Loss: 0.494118\tGenerator Loss: 1.064140\n",
      "Train Epoch: 72 [55040/60000 (92%)]\tDiscriminator Loss: 0.505787\tGenerator Loss: 1.301404\n",
      "Train Epoch: 72 [56320/60000 (94%)]\tDiscriminator Loss: 0.458329\tGenerator Loss: 1.159662\n",
      "Train Epoch: 72 [57600/60000 (96%)]\tDiscriminator Loss: 0.464004\tGenerator Loss: 1.138550\n",
      "Train Epoch: 72 [58880/60000 (98%)]\tDiscriminator Loss: 0.446346\tGenerator Loss: 1.833769\n",
      "Train Epoch: 73 [0/60000 (0%)]\tDiscriminator Loss: 0.457718\tGenerator Loss: 1.528731\n",
      "Train Epoch: 73 [1280/60000 (2%)]\tDiscriminator Loss: 0.517325\tGenerator Loss: 1.287851\n",
      "Train Epoch: 73 [2560/60000 (4%)]\tDiscriminator Loss: 0.481077\tGenerator Loss: 1.169533\n",
      "Train Epoch: 73 [3840/60000 (6%)]\tDiscriminator Loss: 0.534254\tGenerator Loss: 1.075205\n",
      "Train Epoch: 73 [5120/60000 (9%)]\tDiscriminator Loss: 0.586882\tGenerator Loss: 1.220062\n",
      "Train Epoch: 73 [6400/60000 (11%)]\tDiscriminator Loss: 0.452437\tGenerator Loss: 1.564280\n",
      "Train Epoch: 73 [7680/60000 (13%)]\tDiscriminator Loss: 0.510181\tGenerator Loss: 1.274752\n",
      "Train Epoch: 73 [8960/60000 (15%)]\tDiscriminator Loss: 0.538575\tGenerator Loss: 1.039587\n",
      "Train Epoch: 73 [10240/60000 (17%)]\tDiscriminator Loss: 0.492079\tGenerator Loss: 1.117054\n",
      "Train Epoch: 73 [11520/60000 (19%)]\tDiscriminator Loss: 0.480567\tGenerator Loss: 1.562792\n",
      "Train Epoch: 73 [12800/60000 (21%)]\tDiscriminator Loss: 0.512458\tGenerator Loss: 1.286403\n",
      "Train Epoch: 73 [14080/60000 (23%)]\tDiscriminator Loss: 0.498655\tGenerator Loss: 1.175314\n",
      "Train Epoch: 73 [15360/60000 (26%)]\tDiscriminator Loss: 0.533847\tGenerator Loss: 1.680974\n",
      "Train Epoch: 73 [16640/60000 (28%)]\tDiscriminator Loss: 0.483209\tGenerator Loss: 1.572847\n",
      "Train Epoch: 73 [17920/60000 (30%)]\tDiscriminator Loss: 0.485559\tGenerator Loss: 1.470510\n",
      "Train Epoch: 73 [19200/60000 (32%)]\tDiscriminator Loss: 0.554797\tGenerator Loss: 1.080676\n",
      "Train Epoch: 73 [20480/60000 (34%)]\tDiscriminator Loss: 0.474283\tGenerator Loss: 1.804766\n",
      "Train Epoch: 73 [21760/60000 (36%)]\tDiscriminator Loss: 0.578374\tGenerator Loss: 0.973757\n",
      "Train Epoch: 73 [23040/60000 (38%)]\tDiscriminator Loss: 0.480114\tGenerator Loss: 1.268288\n",
      "Train Epoch: 73 [24320/60000 (41%)]\tDiscriminator Loss: 0.496832\tGenerator Loss: 1.140715\n",
      "Train Epoch: 73 [25600/60000 (43%)]\tDiscriminator Loss: 0.457858\tGenerator Loss: 1.718462\n",
      "Train Epoch: 73 [26880/60000 (45%)]\tDiscriminator Loss: 0.534128\tGenerator Loss: 1.002869\n",
      "Train Epoch: 73 [28160/60000 (47%)]\tDiscriminator Loss: 0.471682\tGenerator Loss: 1.460870\n",
      "Train Epoch: 73 [29440/60000 (49%)]\tDiscriminator Loss: 0.499327\tGenerator Loss: 1.351616\n",
      "Train Epoch: 73 [30720/60000 (51%)]\tDiscriminator Loss: 0.552472\tGenerator Loss: 1.673218\n",
      "Train Epoch: 73 [32000/60000 (53%)]\tDiscriminator Loss: 0.414186\tGenerator Loss: 1.375608\n",
      "Train Epoch: 73 [33280/60000 (55%)]\tDiscriminator Loss: 0.493170\tGenerator Loss: 1.014997\n",
      "Train Epoch: 73 [34560/60000 (58%)]\tDiscriminator Loss: 0.537101\tGenerator Loss: 1.174416\n",
      "Train Epoch: 73 [35840/60000 (60%)]\tDiscriminator Loss: 0.536030\tGenerator Loss: 1.765911\n",
      "Train Epoch: 73 [37120/60000 (62%)]\tDiscriminator Loss: 0.541459\tGenerator Loss: 1.333635\n",
      "Train Epoch: 73 [38400/60000 (64%)]\tDiscriminator Loss: 0.462259\tGenerator Loss: 1.622391\n",
      "Train Epoch: 73 [39680/60000 (66%)]\tDiscriminator Loss: 0.495720\tGenerator Loss: 1.025195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 73 [40960/60000 (68%)]\tDiscriminator Loss: 0.445426\tGenerator Loss: 1.194995\n",
      "Train Epoch: 73 [42240/60000 (70%)]\tDiscriminator Loss: 0.440178\tGenerator Loss: 1.176605\n",
      "Train Epoch: 73 [43520/60000 (72%)]\tDiscriminator Loss: 0.539369\tGenerator Loss: 1.463175\n",
      "Train Epoch: 73 [44800/60000 (75%)]\tDiscriminator Loss: 0.495186\tGenerator Loss: 1.277467\n",
      "Train Epoch: 73 [46080/60000 (77%)]\tDiscriminator Loss: 0.475759\tGenerator Loss: 1.173818\n",
      "Train Epoch: 73 [47360/60000 (79%)]\tDiscriminator Loss: 0.497132\tGenerator Loss: 1.476646\n",
      "Train Epoch: 73 [48640/60000 (81%)]\tDiscriminator Loss: 0.508080\tGenerator Loss: 1.741988\n",
      "Train Epoch: 73 [49920/60000 (83%)]\tDiscriminator Loss: 0.484207\tGenerator Loss: 1.257526\n",
      "Train Epoch: 73 [51200/60000 (85%)]\tDiscriminator Loss: 0.569178\tGenerator Loss: 1.130508\n",
      "Train Epoch: 73 [52480/60000 (87%)]\tDiscriminator Loss: 0.520724\tGenerator Loss: 1.332186\n",
      "Train Epoch: 73 [53760/60000 (90%)]\tDiscriminator Loss: 0.449478\tGenerator Loss: 1.485891\n",
      "Train Epoch: 73 [55040/60000 (92%)]\tDiscriminator Loss: 0.464413\tGenerator Loss: 1.530010\n",
      "Train Epoch: 73 [56320/60000 (94%)]\tDiscriminator Loss: 0.471936\tGenerator Loss: 1.310852\n",
      "Train Epoch: 73 [57600/60000 (96%)]\tDiscriminator Loss: 0.495931\tGenerator Loss: 1.160170\n",
      "Train Epoch: 73 [58880/60000 (98%)]\tDiscriminator Loss: 0.490370\tGenerator Loss: 1.416888\n",
      "Train Epoch: 74 [0/60000 (0%)]\tDiscriminator Loss: 0.523618\tGenerator Loss: 1.470552\n",
      "Train Epoch: 74 [1280/60000 (2%)]\tDiscriminator Loss: 0.531428\tGenerator Loss: 1.169880\n",
      "Train Epoch: 74 [2560/60000 (4%)]\tDiscriminator Loss: 0.464589\tGenerator Loss: 1.182733\n",
      "Train Epoch: 74 [3840/60000 (6%)]\tDiscriminator Loss: 0.459879\tGenerator Loss: 1.602417\n",
      "Train Epoch: 74 [5120/60000 (9%)]\tDiscriminator Loss: 0.463270\tGenerator Loss: 1.311750\n",
      "Train Epoch: 74 [6400/60000 (11%)]\tDiscriminator Loss: 0.506986\tGenerator Loss: 1.259907\n",
      "Train Epoch: 74 [7680/60000 (13%)]\tDiscriminator Loss: 0.521481\tGenerator Loss: 1.384975\n",
      "Train Epoch: 74 [8960/60000 (15%)]\tDiscriminator Loss: 0.456466\tGenerator Loss: 1.518878\n",
      "Train Epoch: 74 [10240/60000 (17%)]\tDiscriminator Loss: 0.475746\tGenerator Loss: 1.796752\n",
      "Train Epoch: 74 [11520/60000 (19%)]\tDiscriminator Loss: 0.452597\tGenerator Loss: 1.407533\n",
      "Train Epoch: 74 [12800/60000 (21%)]\tDiscriminator Loss: 0.478305\tGenerator Loss: 1.261017\n",
      "Train Epoch: 74 [14080/60000 (23%)]\tDiscriminator Loss: 0.541169\tGenerator Loss: 1.146729\n",
      "Train Epoch: 74 [15360/60000 (26%)]\tDiscriminator Loss: 0.543105\tGenerator Loss: 1.270057\n",
      "Train Epoch: 74 [16640/60000 (28%)]\tDiscriminator Loss: 0.434886\tGenerator Loss: 1.381241\n",
      "Train Epoch: 74 [17920/60000 (30%)]\tDiscriminator Loss: 0.491489\tGenerator Loss: 1.147709\n",
      "Train Epoch: 74 [19200/60000 (32%)]\tDiscriminator Loss: 0.462574\tGenerator Loss: 1.322617\n",
      "Train Epoch: 74 [20480/60000 (34%)]\tDiscriminator Loss: 0.526202\tGenerator Loss: 1.410894\n",
      "Train Epoch: 74 [21760/60000 (36%)]\tDiscriminator Loss: 0.641760\tGenerator Loss: 0.672920\n",
      "Train Epoch: 74 [23040/60000 (38%)]\tDiscriminator Loss: 0.486060\tGenerator Loss: 1.096866\n",
      "Train Epoch: 74 [24320/60000 (41%)]\tDiscriminator Loss: 0.560834\tGenerator Loss: 0.891418\n",
      "Train Epoch: 74 [25600/60000 (43%)]\tDiscriminator Loss: 0.544531\tGenerator Loss: 1.690867\n",
      "Train Epoch: 74 [26880/60000 (45%)]\tDiscriminator Loss: 0.513216\tGenerator Loss: 1.324067\n",
      "Train Epoch: 74 [28160/60000 (47%)]\tDiscriminator Loss: 0.517213\tGenerator Loss: 1.082710\n",
      "Train Epoch: 74 [29440/60000 (49%)]\tDiscriminator Loss: 0.514484\tGenerator Loss: 1.396174\n",
      "Train Epoch: 74 [30720/60000 (51%)]\tDiscriminator Loss: 0.502652\tGenerator Loss: 1.134103\n",
      "Train Epoch: 74 [32000/60000 (53%)]\tDiscriminator Loss: 0.483307\tGenerator Loss: 1.537088\n",
      "Train Epoch: 74 [33280/60000 (55%)]\tDiscriminator Loss: 0.496120\tGenerator Loss: 1.510302\n",
      "Train Epoch: 74 [34560/60000 (58%)]\tDiscriminator Loss: 0.532678\tGenerator Loss: 0.973579\n",
      "Train Epoch: 74 [35840/60000 (60%)]\tDiscriminator Loss: 0.498783\tGenerator Loss: 1.681945\n",
      "Train Epoch: 74 [37120/60000 (62%)]\tDiscriminator Loss: 0.457137\tGenerator Loss: 1.527788\n",
      "Train Epoch: 74 [38400/60000 (64%)]\tDiscriminator Loss: 0.468998\tGenerator Loss: 1.609693\n",
      "Train Epoch: 74 [39680/60000 (66%)]\tDiscriminator Loss: 0.440922\tGenerator Loss: 1.353245\n",
      "Train Epoch: 74 [40960/60000 (68%)]\tDiscriminator Loss: 0.527641\tGenerator Loss: 1.081578\n",
      "Train Epoch: 74 [42240/60000 (70%)]\tDiscriminator Loss: 0.455224\tGenerator Loss: 1.386724\n",
      "Train Epoch: 74 [43520/60000 (72%)]\tDiscriminator Loss: 0.547302\tGenerator Loss: 1.378650\n",
      "Train Epoch: 74 [44800/60000 (75%)]\tDiscriminator Loss: 0.451713\tGenerator Loss: 1.547143\n",
      "Train Epoch: 74 [46080/60000 (77%)]\tDiscriminator Loss: 0.505356\tGenerator Loss: 1.051103\n",
      "Train Epoch: 74 [47360/60000 (79%)]\tDiscriminator Loss: 0.507387\tGenerator Loss: 1.752542\n",
      "Train Epoch: 74 [48640/60000 (81%)]\tDiscriminator Loss: 0.569691\tGenerator Loss: 1.195755\n",
      "Train Epoch: 74 [49920/60000 (83%)]\tDiscriminator Loss: 0.474304\tGenerator Loss: 1.457973\n",
      "Train Epoch: 74 [51200/60000 (85%)]\tDiscriminator Loss: 0.457232\tGenerator Loss: 1.152862\n",
      "Train Epoch: 74 [52480/60000 (87%)]\tDiscriminator Loss: 0.499638\tGenerator Loss: 1.303270\n",
      "Train Epoch: 74 [53760/60000 (90%)]\tDiscriminator Loss: 0.453242\tGenerator Loss: 1.433155\n",
      "Train Epoch: 74 [55040/60000 (92%)]\tDiscriminator Loss: 0.538007\tGenerator Loss: 1.391822\n",
      "Train Epoch: 74 [56320/60000 (94%)]\tDiscriminator Loss: 0.491992\tGenerator Loss: 1.679847\n",
      "Train Epoch: 74 [57600/60000 (96%)]\tDiscriminator Loss: 0.422133\tGenerator Loss: 1.306669\n",
      "Train Epoch: 74 [58880/60000 (98%)]\tDiscriminator Loss: 0.532640\tGenerator Loss: 0.908838\n",
      "Train Epoch: 75 [0/60000 (0%)]\tDiscriminator Loss: 0.534067\tGenerator Loss: 0.922232\n",
      "Train Epoch: 75 [1280/60000 (2%)]\tDiscriminator Loss: 0.491279\tGenerator Loss: 1.302931\n",
      "Train Epoch: 75 [2560/60000 (4%)]\tDiscriminator Loss: 0.532878\tGenerator Loss: 1.469919\n",
      "Train Epoch: 75 [3840/60000 (6%)]\tDiscriminator Loss: 0.496523\tGenerator Loss: 1.384865\n",
      "Train Epoch: 75 [5120/60000 (9%)]\tDiscriminator Loss: 0.515112\tGenerator Loss: 0.971784\n",
      "Train Epoch: 75 [6400/60000 (11%)]\tDiscriminator Loss: 0.515199\tGenerator Loss: 1.625286\n",
      "Train Epoch: 75 [7680/60000 (13%)]\tDiscriminator Loss: 0.509575\tGenerator Loss: 1.291082\n",
      "Train Epoch: 75 [8960/60000 (15%)]\tDiscriminator Loss: 0.487991\tGenerator Loss: 1.233627\n",
      "Train Epoch: 75 [10240/60000 (17%)]\tDiscriminator Loss: 0.470278\tGenerator Loss: 1.292356\n",
      "Train Epoch: 75 [11520/60000 (19%)]\tDiscriminator Loss: 0.553433\tGenerator Loss: 1.343260\n",
      "Train Epoch: 75 [12800/60000 (21%)]\tDiscriminator Loss: 0.508761\tGenerator Loss: 1.642694\n",
      "Train Epoch: 75 [14080/60000 (23%)]\tDiscriminator Loss: 0.600719\tGenerator Loss: 0.958671\n",
      "Train Epoch: 75 [15360/60000 (26%)]\tDiscriminator Loss: 0.530082\tGenerator Loss: 1.887656\n",
      "Train Epoch: 75 [16640/60000 (28%)]\tDiscriminator Loss: 0.474769\tGenerator Loss: 1.466546\n",
      "Train Epoch: 75 [17920/60000 (30%)]\tDiscriminator Loss: 0.507269\tGenerator Loss: 1.550076\n",
      "Train Epoch: 75 [19200/60000 (32%)]\tDiscriminator Loss: 0.441692\tGenerator Loss: 1.258475\n",
      "Train Epoch: 75 [20480/60000 (34%)]\tDiscriminator Loss: 0.525287\tGenerator Loss: 1.375409\n",
      "Train Epoch: 75 [21760/60000 (36%)]\tDiscriminator Loss: 0.470166\tGenerator Loss: 1.414186\n",
      "Train Epoch: 75 [23040/60000 (38%)]\tDiscriminator Loss: 0.510916\tGenerator Loss: 1.448235\n",
      "Train Epoch: 75 [24320/60000 (41%)]\tDiscriminator Loss: 0.491054\tGenerator Loss: 1.279462\n",
      "Train Epoch: 75 [25600/60000 (43%)]\tDiscriminator Loss: 0.447435\tGenerator Loss: 1.765758\n",
      "Train Epoch: 75 [26880/60000 (45%)]\tDiscriminator Loss: 0.482028\tGenerator Loss: 1.226934\n",
      "Train Epoch: 75 [28160/60000 (47%)]\tDiscriminator Loss: 0.457266\tGenerator Loss: 1.514109\n",
      "Train Epoch: 75 [29440/60000 (49%)]\tDiscriminator Loss: 0.510200\tGenerator Loss: 1.612052\n",
      "Train Epoch: 75 [30720/60000 (51%)]\tDiscriminator Loss: 0.395548\tGenerator Loss: 1.264831\n",
      "Train Epoch: 75 [32000/60000 (53%)]\tDiscriminator Loss: 0.452893\tGenerator Loss: 1.710010\n",
      "Train Epoch: 75 [33280/60000 (55%)]\tDiscriminator Loss: 0.504804\tGenerator Loss: 1.023614\n",
      "Train Epoch: 75 [34560/60000 (58%)]\tDiscriminator Loss: 0.502006\tGenerator Loss: 1.171514\n",
      "Train Epoch: 75 [35840/60000 (60%)]\tDiscriminator Loss: 0.495373\tGenerator Loss: 1.028611\n",
      "Train Epoch: 75 [37120/60000 (62%)]\tDiscriminator Loss: 0.474152\tGenerator Loss: 1.329167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 75 [38400/60000 (64%)]\tDiscriminator Loss: 0.460575\tGenerator Loss: 1.558229\n",
      "Train Epoch: 75 [39680/60000 (66%)]\tDiscriminator Loss: 0.505933\tGenerator Loss: 1.341697\n",
      "Train Epoch: 75 [40960/60000 (68%)]\tDiscriminator Loss: 0.482195\tGenerator Loss: 1.229230\n",
      "Train Epoch: 75 [42240/60000 (70%)]\tDiscriminator Loss: 0.490233\tGenerator Loss: 1.425265\n",
      "Train Epoch: 75 [43520/60000 (72%)]\tDiscriminator Loss: 0.519735\tGenerator Loss: 1.497548\n",
      "Train Epoch: 75 [44800/60000 (75%)]\tDiscriminator Loss: 0.498340\tGenerator Loss: 1.224604\n",
      "Train Epoch: 75 [46080/60000 (77%)]\tDiscriminator Loss: 0.439184\tGenerator Loss: 1.362850\n",
      "Train Epoch: 75 [47360/60000 (79%)]\tDiscriminator Loss: 0.523073\tGenerator Loss: 1.250691\n",
      "Train Epoch: 75 [48640/60000 (81%)]\tDiscriminator Loss: 0.520571\tGenerator Loss: 1.186652\n",
      "Train Epoch: 75 [49920/60000 (83%)]\tDiscriminator Loss: 0.481229\tGenerator Loss: 1.565365\n",
      "Train Epoch: 75 [51200/60000 (85%)]\tDiscriminator Loss: 0.467715\tGenerator Loss: 1.203426\n",
      "Train Epoch: 75 [52480/60000 (87%)]\tDiscriminator Loss: 0.489170\tGenerator Loss: 1.544212\n",
      "Train Epoch: 75 [53760/60000 (90%)]\tDiscriminator Loss: 0.522416\tGenerator Loss: 1.643056\n",
      "Train Epoch: 75 [55040/60000 (92%)]\tDiscriminator Loss: 0.551574\tGenerator Loss: 1.901460\n",
      "Train Epoch: 75 [56320/60000 (94%)]\tDiscriminator Loss: 0.447273\tGenerator Loss: 1.484389\n",
      "Train Epoch: 75 [57600/60000 (96%)]\tDiscriminator Loss: 0.510450\tGenerator Loss: 1.347563\n",
      "Train Epoch: 75 [58880/60000 (98%)]\tDiscriminator Loss: 0.479704\tGenerator Loss: 1.402568\n",
      "Train Epoch: 76 [0/60000 (0%)]\tDiscriminator Loss: 0.483628\tGenerator Loss: 1.161021\n",
      "Train Epoch: 76 [1280/60000 (2%)]\tDiscriminator Loss: 0.501534\tGenerator Loss: 1.193794\n",
      "Train Epoch: 76 [2560/60000 (4%)]\tDiscriminator Loss: 0.550509\tGenerator Loss: 1.093264\n",
      "Train Epoch: 76 [3840/60000 (6%)]\tDiscriminator Loss: 0.556436\tGenerator Loss: 1.613988\n",
      "Train Epoch: 76 [5120/60000 (9%)]\tDiscriminator Loss: 0.465469\tGenerator Loss: 1.416945\n",
      "Train Epoch: 76 [6400/60000 (11%)]\tDiscriminator Loss: 0.569078\tGenerator Loss: 1.543124\n",
      "Train Epoch: 76 [7680/60000 (13%)]\tDiscriminator Loss: 0.488620\tGenerator Loss: 1.303215\n",
      "Train Epoch: 76 [8960/60000 (15%)]\tDiscriminator Loss: 0.418831\tGenerator Loss: 1.544567\n",
      "Train Epoch: 76 [10240/60000 (17%)]\tDiscriminator Loss: 0.490993\tGenerator Loss: 1.403594\n",
      "Train Epoch: 76 [11520/60000 (19%)]\tDiscriminator Loss: 0.465274\tGenerator Loss: 1.409465\n",
      "Train Epoch: 76 [12800/60000 (21%)]\tDiscriminator Loss: 0.449455\tGenerator Loss: 1.384145\n",
      "Train Epoch: 76 [14080/60000 (23%)]\tDiscriminator Loss: 0.458739\tGenerator Loss: 1.469754\n",
      "Train Epoch: 76 [15360/60000 (26%)]\tDiscriminator Loss: 0.445669\tGenerator Loss: 1.479848\n",
      "Train Epoch: 76 [16640/60000 (28%)]\tDiscriminator Loss: 0.483017\tGenerator Loss: 1.385841\n",
      "Train Epoch: 76 [17920/60000 (30%)]\tDiscriminator Loss: 0.457284\tGenerator Loss: 1.319245\n",
      "Train Epoch: 76 [19200/60000 (32%)]\tDiscriminator Loss: 0.529326\tGenerator Loss: 1.452344\n",
      "Train Epoch: 76 [20480/60000 (34%)]\tDiscriminator Loss: 0.459827\tGenerator Loss: 1.315863\n",
      "Train Epoch: 76 [21760/60000 (36%)]\tDiscriminator Loss: 0.497867\tGenerator Loss: 1.374666\n",
      "Train Epoch: 76 [23040/60000 (38%)]\tDiscriminator Loss: 0.479352\tGenerator Loss: 1.701476\n",
      "Train Epoch: 76 [24320/60000 (41%)]\tDiscriminator Loss: 0.454900\tGenerator Loss: 1.206807\n",
      "Train Epoch: 76 [25600/60000 (43%)]\tDiscriminator Loss: 0.463876\tGenerator Loss: 1.231306\n",
      "Train Epoch: 76 [26880/60000 (45%)]\tDiscriminator Loss: 0.497031\tGenerator Loss: 1.479267\n",
      "Train Epoch: 76 [28160/60000 (47%)]\tDiscriminator Loss: 0.428334\tGenerator Loss: 1.338089\n",
      "Train Epoch: 76 [29440/60000 (49%)]\tDiscriminator Loss: 0.486717\tGenerator Loss: 1.221154\n",
      "Train Epoch: 76 [30720/60000 (51%)]\tDiscriminator Loss: 0.504121\tGenerator Loss: 1.692841\n",
      "Train Epoch: 76 [32000/60000 (53%)]\tDiscriminator Loss: 0.508725\tGenerator Loss: 1.073025\n",
      "Train Epoch: 76 [33280/60000 (55%)]\tDiscriminator Loss: 0.466920\tGenerator Loss: 1.724832\n",
      "Train Epoch: 76 [34560/60000 (58%)]\tDiscriminator Loss: 0.476698\tGenerator Loss: 1.481939\n",
      "Train Epoch: 76 [35840/60000 (60%)]\tDiscriminator Loss: 0.444787\tGenerator Loss: 1.355480\n",
      "Train Epoch: 76 [37120/60000 (62%)]\tDiscriminator Loss: 0.488817\tGenerator Loss: 1.027023\n",
      "Train Epoch: 76 [38400/60000 (64%)]\tDiscriminator Loss: 0.503088\tGenerator Loss: 1.255072\n",
      "Train Epoch: 76 [39680/60000 (66%)]\tDiscriminator Loss: 0.519282\tGenerator Loss: 1.313950\n",
      "Train Epoch: 76 [40960/60000 (68%)]\tDiscriminator Loss: 0.515391\tGenerator Loss: 1.236432\n",
      "Train Epoch: 76 [42240/60000 (70%)]\tDiscriminator Loss: 0.500432\tGenerator Loss: 1.603541\n",
      "Train Epoch: 76 [43520/60000 (72%)]\tDiscriminator Loss: 0.503451\tGenerator Loss: 1.348685\n",
      "Train Epoch: 76 [44800/60000 (75%)]\tDiscriminator Loss: 0.524300\tGenerator Loss: 1.724540\n",
      "Train Epoch: 76 [46080/60000 (77%)]\tDiscriminator Loss: 0.466607\tGenerator Loss: 1.475370\n",
      "Train Epoch: 76 [47360/60000 (79%)]\tDiscriminator Loss: 0.693875\tGenerator Loss: 0.511510\n",
      "Train Epoch: 76 [48640/60000 (81%)]\tDiscriminator Loss: 0.463462\tGenerator Loss: 1.606271\n",
      "Train Epoch: 76 [49920/60000 (83%)]\tDiscriminator Loss: 0.453662\tGenerator Loss: 1.463532\n",
      "Train Epoch: 76 [51200/60000 (85%)]\tDiscriminator Loss: 0.530249\tGenerator Loss: 1.412685\n",
      "Train Epoch: 76 [52480/60000 (87%)]\tDiscriminator Loss: 0.594613\tGenerator Loss: 1.694226\n",
      "Train Epoch: 76 [53760/60000 (90%)]\tDiscriminator Loss: 0.455683\tGenerator Loss: 1.376326\n",
      "Train Epoch: 76 [55040/60000 (92%)]\tDiscriminator Loss: 0.467382\tGenerator Loss: 1.880947\n",
      "Train Epoch: 76 [56320/60000 (94%)]\tDiscriminator Loss: 0.438961\tGenerator Loss: 1.652358\n",
      "Train Epoch: 76 [57600/60000 (96%)]\tDiscriminator Loss: 0.490321\tGenerator Loss: 1.327455\n",
      "Train Epoch: 76 [58880/60000 (98%)]\tDiscriminator Loss: 0.586637\tGenerator Loss: 1.311934\n",
      "Train Epoch: 77 [0/60000 (0%)]\tDiscriminator Loss: 0.540530\tGenerator Loss: 1.690585\n",
      "Train Epoch: 77 [1280/60000 (2%)]\tDiscriminator Loss: 0.465866\tGenerator Loss: 1.330196\n",
      "Train Epoch: 77 [2560/60000 (4%)]\tDiscriminator Loss: 0.571655\tGenerator Loss: 1.574064\n",
      "Train Epoch: 77 [3840/60000 (6%)]\tDiscriminator Loss: 0.480510\tGenerator Loss: 1.455446\n",
      "Train Epoch: 77 [5120/60000 (9%)]\tDiscriminator Loss: 0.475615\tGenerator Loss: 1.230997\n",
      "Train Epoch: 77 [6400/60000 (11%)]\tDiscriminator Loss: 0.442279\tGenerator Loss: 1.283649\n",
      "Train Epoch: 77 [7680/60000 (13%)]\tDiscriminator Loss: 0.525084\tGenerator Loss: 1.386909\n",
      "Train Epoch: 77 [8960/60000 (15%)]\tDiscriminator Loss: 0.465974\tGenerator Loss: 1.430741\n",
      "Train Epoch: 77 [10240/60000 (17%)]\tDiscriminator Loss: 0.500342\tGenerator Loss: 1.447829\n",
      "Train Epoch: 77 [11520/60000 (19%)]\tDiscriminator Loss: 0.475802\tGenerator Loss: 1.569599\n",
      "Train Epoch: 77 [12800/60000 (21%)]\tDiscriminator Loss: 0.548791\tGenerator Loss: 1.510001\n",
      "Train Epoch: 77 [14080/60000 (23%)]\tDiscriminator Loss: 0.488831\tGenerator Loss: 1.262797\n",
      "Train Epoch: 77 [15360/60000 (26%)]\tDiscriminator Loss: 0.465157\tGenerator Loss: 1.429984\n",
      "Train Epoch: 77 [16640/60000 (28%)]\tDiscriminator Loss: 0.521571\tGenerator Loss: 1.296292\n",
      "Train Epoch: 77 [17920/60000 (30%)]\tDiscriminator Loss: 0.481721\tGenerator Loss: 1.556773\n",
      "Train Epoch: 77 [19200/60000 (32%)]\tDiscriminator Loss: 0.478044\tGenerator Loss: 1.537414\n",
      "Train Epoch: 77 [20480/60000 (34%)]\tDiscriminator Loss: 0.465392\tGenerator Loss: 1.412179\n",
      "Train Epoch: 77 [21760/60000 (36%)]\tDiscriminator Loss: 0.492641\tGenerator Loss: 1.209789\n",
      "Train Epoch: 77 [23040/60000 (38%)]\tDiscriminator Loss: 0.465205\tGenerator Loss: 1.203209\n",
      "Train Epoch: 77 [24320/60000 (41%)]\tDiscriminator Loss: 0.466250\tGenerator Loss: 1.274889\n",
      "Train Epoch: 77 [25600/60000 (43%)]\tDiscriminator Loss: 0.444713\tGenerator Loss: 1.400512\n",
      "Train Epoch: 77 [26880/60000 (45%)]\tDiscriminator Loss: 0.532091\tGenerator Loss: 1.262172\n",
      "Train Epoch: 77 [28160/60000 (47%)]\tDiscriminator Loss: 0.487987\tGenerator Loss: 1.317899\n",
      "Train Epoch: 77 [29440/60000 (49%)]\tDiscriminator Loss: 0.489018\tGenerator Loss: 1.467040\n",
      "Train Epoch: 77 [30720/60000 (51%)]\tDiscriminator Loss: 0.497920\tGenerator Loss: 1.085063\n",
      "Train Epoch: 77 [32000/60000 (53%)]\tDiscriminator Loss: 0.483460\tGenerator Loss: 1.464776\n",
      "Train Epoch: 77 [33280/60000 (55%)]\tDiscriminator Loss: 0.512878\tGenerator Loss: 1.659730\n",
      "Train Epoch: 77 [34560/60000 (58%)]\tDiscriminator Loss: 0.482566\tGenerator Loss: 1.508426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 77 [35840/60000 (60%)]\tDiscriminator Loss: 0.454077\tGenerator Loss: 1.257729\n",
      "Train Epoch: 77 [37120/60000 (62%)]\tDiscriminator Loss: 0.455728\tGenerator Loss: 1.415550\n",
      "Train Epoch: 77 [38400/60000 (64%)]\tDiscriminator Loss: 0.483114\tGenerator Loss: 1.601356\n",
      "Train Epoch: 77 [39680/60000 (66%)]\tDiscriminator Loss: 0.493104\tGenerator Loss: 1.234153\n",
      "Train Epoch: 77 [40960/60000 (68%)]\tDiscriminator Loss: 0.534686\tGenerator Loss: 1.794950\n",
      "Train Epoch: 77 [42240/60000 (70%)]\tDiscriminator Loss: 0.481539\tGenerator Loss: 1.143142\n",
      "Train Epoch: 77 [43520/60000 (72%)]\tDiscriminator Loss: 0.528325\tGenerator Loss: 1.302638\n",
      "Train Epoch: 77 [44800/60000 (75%)]\tDiscriminator Loss: 0.463985\tGenerator Loss: 1.261417\n",
      "Train Epoch: 77 [46080/60000 (77%)]\tDiscriminator Loss: 0.502002\tGenerator Loss: 1.239282\n",
      "Train Epoch: 77 [47360/60000 (79%)]\tDiscriminator Loss: 0.466818\tGenerator Loss: 1.492147\n",
      "Train Epoch: 77 [48640/60000 (81%)]\tDiscriminator Loss: 0.493776\tGenerator Loss: 1.552722\n",
      "Train Epoch: 77 [49920/60000 (83%)]\tDiscriminator Loss: 0.499561\tGenerator Loss: 1.228887\n",
      "Train Epoch: 77 [51200/60000 (85%)]\tDiscriminator Loss: 0.518887\tGenerator Loss: 0.989462\n",
      "Train Epoch: 77 [52480/60000 (87%)]\tDiscriminator Loss: 0.537042\tGenerator Loss: 1.598727\n",
      "Train Epoch: 77 [53760/60000 (90%)]\tDiscriminator Loss: 0.496236\tGenerator Loss: 1.285348\n",
      "Train Epoch: 77 [55040/60000 (92%)]\tDiscriminator Loss: 0.491497\tGenerator Loss: 1.450516\n",
      "Train Epoch: 77 [56320/60000 (94%)]\tDiscriminator Loss: 0.502083\tGenerator Loss: 1.205782\n",
      "Train Epoch: 77 [57600/60000 (96%)]\tDiscriminator Loss: 0.494121\tGenerator Loss: 1.240649\n",
      "Train Epoch: 77 [58880/60000 (98%)]\tDiscriminator Loss: 0.451472\tGenerator Loss: 1.266450\n",
      "Train Epoch: 78 [0/60000 (0%)]\tDiscriminator Loss: 0.488157\tGenerator Loss: 1.530501\n",
      "Train Epoch: 78 [1280/60000 (2%)]\tDiscriminator Loss: 0.485294\tGenerator Loss: 1.424858\n",
      "Train Epoch: 78 [2560/60000 (4%)]\tDiscriminator Loss: 0.468246\tGenerator Loss: 1.347515\n",
      "Train Epoch: 78 [3840/60000 (6%)]\tDiscriminator Loss: 0.513661\tGenerator Loss: 1.064499\n",
      "Train Epoch: 78 [5120/60000 (9%)]\tDiscriminator Loss: 0.426288\tGenerator Loss: 1.341712\n",
      "Train Epoch: 78 [6400/60000 (11%)]\tDiscriminator Loss: 0.544437\tGenerator Loss: 1.425314\n",
      "Train Epoch: 78 [7680/60000 (13%)]\tDiscriminator Loss: 0.465135\tGenerator Loss: 1.441607\n",
      "Train Epoch: 78 [8960/60000 (15%)]\tDiscriminator Loss: 0.481394\tGenerator Loss: 1.337423\n",
      "Train Epoch: 78 [10240/60000 (17%)]\tDiscriminator Loss: 0.497963\tGenerator Loss: 1.644950\n",
      "Train Epoch: 78 [11520/60000 (19%)]\tDiscriminator Loss: 0.454945\tGenerator Loss: 1.203863\n",
      "Train Epoch: 78 [12800/60000 (21%)]\tDiscriminator Loss: 0.477281\tGenerator Loss: 1.473245\n",
      "Train Epoch: 78 [14080/60000 (23%)]\tDiscriminator Loss: 0.500015\tGenerator Loss: 1.532748\n",
      "Train Epoch: 78 [15360/60000 (26%)]\tDiscriminator Loss: 0.500559\tGenerator Loss: 1.435344\n",
      "Train Epoch: 78 [16640/60000 (28%)]\tDiscriminator Loss: 0.479975\tGenerator Loss: 1.435318\n",
      "Train Epoch: 78 [17920/60000 (30%)]\tDiscriminator Loss: 0.479072\tGenerator Loss: 1.335024\n",
      "Train Epoch: 78 [19200/60000 (32%)]\tDiscriminator Loss: 0.474261\tGenerator Loss: 1.188371\n",
      "Train Epoch: 78 [20480/60000 (34%)]\tDiscriminator Loss: 0.517118\tGenerator Loss: 1.114312\n",
      "Train Epoch: 78 [21760/60000 (36%)]\tDiscriminator Loss: 0.508145\tGenerator Loss: 1.479410\n",
      "Train Epoch: 78 [23040/60000 (38%)]\tDiscriminator Loss: 0.488210\tGenerator Loss: 1.457961\n",
      "Train Epoch: 78 [24320/60000 (41%)]\tDiscriminator Loss: 0.456779\tGenerator Loss: 1.455319\n",
      "Train Epoch: 78 [25600/60000 (43%)]\tDiscriminator Loss: 0.479572\tGenerator Loss: 1.464511\n",
      "Train Epoch: 78 [26880/60000 (45%)]\tDiscriminator Loss: 0.501093\tGenerator Loss: 1.686351\n",
      "Train Epoch: 78 [28160/60000 (47%)]\tDiscriminator Loss: 0.418268\tGenerator Loss: 1.417020\n",
      "Train Epoch: 78 [29440/60000 (49%)]\tDiscriminator Loss: 0.441131\tGenerator Loss: 1.413511\n",
      "Train Epoch: 78 [30720/60000 (51%)]\tDiscriminator Loss: 0.477037\tGenerator Loss: 1.386311\n",
      "Train Epoch: 78 [32000/60000 (53%)]\tDiscriminator Loss: 0.559664\tGenerator Loss: 0.894078\n",
      "Train Epoch: 78 [33280/60000 (55%)]\tDiscriminator Loss: 0.531619\tGenerator Loss: 1.762413\n",
      "Train Epoch: 78 [34560/60000 (58%)]\tDiscriminator Loss: 0.448106\tGenerator Loss: 1.252513\n",
      "Train Epoch: 78 [35840/60000 (60%)]\tDiscriminator Loss: 0.503689\tGenerator Loss: 1.424405\n",
      "Train Epoch: 78 [37120/60000 (62%)]\tDiscriminator Loss: 0.481404\tGenerator Loss: 1.468769\n",
      "Train Epoch: 78 [38400/60000 (64%)]\tDiscriminator Loss: 0.510786\tGenerator Loss: 1.112919\n",
      "Train Epoch: 78 [39680/60000 (66%)]\tDiscriminator Loss: 0.484089\tGenerator Loss: 1.230775\n",
      "Train Epoch: 78 [40960/60000 (68%)]\tDiscriminator Loss: 0.449591\tGenerator Loss: 1.292640\n",
      "Train Epoch: 78 [42240/60000 (70%)]\tDiscriminator Loss: 0.531028\tGenerator Loss: 1.226139\n",
      "Train Epoch: 78 [43520/60000 (72%)]\tDiscriminator Loss: 0.514713\tGenerator Loss: 1.242549\n",
      "Train Epoch: 78 [44800/60000 (75%)]\tDiscriminator Loss: 0.511539\tGenerator Loss: 1.388409\n",
      "Train Epoch: 78 [46080/60000 (77%)]\tDiscriminator Loss: 0.515124\tGenerator Loss: 1.315522\n",
      "Train Epoch: 78 [47360/60000 (79%)]\tDiscriminator Loss: 0.432387\tGenerator Loss: 1.532970\n",
      "Train Epoch: 78 [48640/60000 (81%)]\tDiscriminator Loss: 0.518430\tGenerator Loss: 1.488283\n",
      "Train Epoch: 78 [49920/60000 (83%)]\tDiscriminator Loss: 0.478882\tGenerator Loss: 1.377005\n",
      "Train Epoch: 78 [51200/60000 (85%)]\tDiscriminator Loss: 0.446327\tGenerator Loss: 1.150998\n",
      "Train Epoch: 78 [52480/60000 (87%)]\tDiscriminator Loss: 0.493077\tGenerator Loss: 1.473956\n",
      "Train Epoch: 78 [53760/60000 (90%)]\tDiscriminator Loss: 0.497329\tGenerator Loss: 1.263602\n",
      "Train Epoch: 78 [55040/60000 (92%)]\tDiscriminator Loss: 0.452859\tGenerator Loss: 1.372251\n",
      "Train Epoch: 78 [56320/60000 (94%)]\tDiscriminator Loss: 0.483413\tGenerator Loss: 1.627999\n",
      "Train Epoch: 78 [57600/60000 (96%)]\tDiscriminator Loss: 0.498520\tGenerator Loss: 1.209067\n",
      "Train Epoch: 78 [58880/60000 (98%)]\tDiscriminator Loss: 0.485035\tGenerator Loss: 1.463418\n",
      "Train Epoch: 79 [0/60000 (0%)]\tDiscriminator Loss: 0.461821\tGenerator Loss: 1.121207\n",
      "Train Epoch: 79 [1280/60000 (2%)]\tDiscriminator Loss: 0.474745\tGenerator Loss: 1.285265\n",
      "Train Epoch: 79 [2560/60000 (4%)]\tDiscriminator Loss: 0.570792\tGenerator Loss: 1.275876\n",
      "Train Epoch: 79 [3840/60000 (6%)]\tDiscriminator Loss: 0.553958\tGenerator Loss: 1.023084\n",
      "Train Epoch: 79 [5120/60000 (9%)]\tDiscriminator Loss: 0.488855\tGenerator Loss: 1.570557\n",
      "Train Epoch: 79 [6400/60000 (11%)]\tDiscriminator Loss: 0.541810\tGenerator Loss: 1.842000\n",
      "Train Epoch: 79 [7680/60000 (13%)]\tDiscriminator Loss: 0.497116\tGenerator Loss: 1.279324\n",
      "Train Epoch: 79 [8960/60000 (15%)]\tDiscriminator Loss: 0.532347\tGenerator Loss: 1.289721\n",
      "Train Epoch: 79 [10240/60000 (17%)]\tDiscriminator Loss: 0.518647\tGenerator Loss: 1.582733\n",
      "Train Epoch: 79 [11520/60000 (19%)]\tDiscriminator Loss: 0.479826\tGenerator Loss: 1.178632\n",
      "Train Epoch: 79 [12800/60000 (21%)]\tDiscriminator Loss: 0.493364\tGenerator Loss: 1.248234\n",
      "Train Epoch: 79 [14080/60000 (23%)]\tDiscriminator Loss: 0.416167\tGenerator Loss: 1.623363\n",
      "Train Epoch: 79 [15360/60000 (26%)]\tDiscriminator Loss: 0.463611\tGenerator Loss: 1.715565\n",
      "Train Epoch: 79 [16640/60000 (28%)]\tDiscriminator Loss: 0.546776\tGenerator Loss: 1.212657\n",
      "Train Epoch: 79 [17920/60000 (30%)]\tDiscriminator Loss: 0.494256\tGenerator Loss: 1.251963\n",
      "Train Epoch: 79 [19200/60000 (32%)]\tDiscriminator Loss: 0.478074\tGenerator Loss: 1.575392\n",
      "Train Epoch: 79 [20480/60000 (34%)]\tDiscriminator Loss: 0.439406\tGenerator Loss: 1.395819\n",
      "Train Epoch: 79 [21760/60000 (36%)]\tDiscriminator Loss: 0.527880\tGenerator Loss: 1.246604\n",
      "Train Epoch: 79 [23040/60000 (38%)]\tDiscriminator Loss: 0.516829\tGenerator Loss: 1.145548\n",
      "Train Epoch: 79 [24320/60000 (41%)]\tDiscriminator Loss: 0.452214\tGenerator Loss: 1.284971\n",
      "Train Epoch: 79 [25600/60000 (43%)]\tDiscriminator Loss: 0.450204\tGenerator Loss: 1.177802\n",
      "Train Epoch: 79 [26880/60000 (45%)]\tDiscriminator Loss: 0.552869\tGenerator Loss: 1.274023\n",
      "Train Epoch: 79 [28160/60000 (47%)]\tDiscriminator Loss: 0.436243\tGenerator Loss: 1.722903\n",
      "Train Epoch: 79 [29440/60000 (49%)]\tDiscriminator Loss: 0.479447\tGenerator Loss: 1.155183\n",
      "Train Epoch: 79 [30720/60000 (51%)]\tDiscriminator Loss: 0.460063\tGenerator Loss: 1.396460\n",
      "Train Epoch: 79 [32000/60000 (53%)]\tDiscriminator Loss: 0.493331\tGenerator Loss: 1.368964\n",
      "Train Epoch: 79 [33280/60000 (55%)]\tDiscriminator Loss: 0.491453\tGenerator Loss: 1.268751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 79 [34560/60000 (58%)]\tDiscriminator Loss: 0.472292\tGenerator Loss: 1.194018\n",
      "Train Epoch: 79 [35840/60000 (60%)]\tDiscriminator Loss: 0.433355\tGenerator Loss: 1.382726\n",
      "Train Epoch: 79 [37120/60000 (62%)]\tDiscriminator Loss: 0.465272\tGenerator Loss: 1.430607\n",
      "Train Epoch: 79 [38400/60000 (64%)]\tDiscriminator Loss: 0.496805\tGenerator Loss: 1.709141\n",
      "Train Epoch: 79 [39680/60000 (66%)]\tDiscriminator Loss: 0.506019\tGenerator Loss: 1.060012\n",
      "Train Epoch: 79 [40960/60000 (68%)]\tDiscriminator Loss: 0.463524\tGenerator Loss: 1.344836\n",
      "Train Epoch: 79 [42240/60000 (70%)]\tDiscriminator Loss: 0.507458\tGenerator Loss: 1.639348\n",
      "Train Epoch: 79 [43520/60000 (72%)]\tDiscriminator Loss: 0.476845\tGenerator Loss: 1.603607\n",
      "Train Epoch: 79 [44800/60000 (75%)]\tDiscriminator Loss: 0.470301\tGenerator Loss: 1.373668\n",
      "Train Epoch: 79 [46080/60000 (77%)]\tDiscriminator Loss: 0.552041\tGenerator Loss: 1.808296\n",
      "Train Epoch: 79 [47360/60000 (79%)]\tDiscriminator Loss: 0.535654\tGenerator Loss: 1.237153\n",
      "Train Epoch: 79 [48640/60000 (81%)]\tDiscriminator Loss: 0.530944\tGenerator Loss: 1.446327\n",
      "Train Epoch: 79 [49920/60000 (83%)]\tDiscriminator Loss: 0.484945\tGenerator Loss: 1.654482\n",
      "Train Epoch: 79 [51200/60000 (85%)]\tDiscriminator Loss: 0.512045\tGenerator Loss: 1.129771\n",
      "Train Epoch: 79 [52480/60000 (87%)]\tDiscriminator Loss: 0.475325\tGenerator Loss: 1.645315\n",
      "Train Epoch: 79 [53760/60000 (90%)]\tDiscriminator Loss: 0.460060\tGenerator Loss: 1.165942\n",
      "Train Epoch: 79 [55040/60000 (92%)]\tDiscriminator Loss: 0.437804\tGenerator Loss: 1.600336\n",
      "Train Epoch: 79 [56320/60000 (94%)]\tDiscriminator Loss: 0.518268\tGenerator Loss: 1.158958\n",
      "Train Epoch: 79 [57600/60000 (96%)]\tDiscriminator Loss: 0.465552\tGenerator Loss: 1.517192\n",
      "Train Epoch: 79 [58880/60000 (98%)]\tDiscriminator Loss: 0.449414\tGenerator Loss: 1.553699\n",
      "Train Epoch: 80 [0/60000 (0%)]\tDiscriminator Loss: 0.577217\tGenerator Loss: 1.083704\n",
      "Train Epoch: 80 [1280/60000 (2%)]\tDiscriminator Loss: 0.506694\tGenerator Loss: 1.589890\n",
      "Train Epoch: 80 [2560/60000 (4%)]\tDiscriminator Loss: 0.459451\tGenerator Loss: 1.287518\n",
      "Train Epoch: 80 [3840/60000 (6%)]\tDiscriminator Loss: 0.467060\tGenerator Loss: 1.534061\n",
      "Train Epoch: 80 [5120/60000 (9%)]\tDiscriminator Loss: 0.497713\tGenerator Loss: 1.070348\n",
      "Train Epoch: 80 [6400/60000 (11%)]\tDiscriminator Loss: 0.451849\tGenerator Loss: 1.606536\n",
      "Train Epoch: 80 [7680/60000 (13%)]\tDiscriminator Loss: 0.464999\tGenerator Loss: 1.418496\n",
      "Train Epoch: 80 [8960/60000 (15%)]\tDiscriminator Loss: 0.456307\tGenerator Loss: 1.724469\n",
      "Train Epoch: 80 [10240/60000 (17%)]\tDiscriminator Loss: 0.493812\tGenerator Loss: 1.238394\n",
      "Train Epoch: 80 [11520/60000 (19%)]\tDiscriminator Loss: 0.442554\tGenerator Loss: 1.695472\n",
      "Train Epoch: 80 [12800/60000 (21%)]\tDiscriminator Loss: 0.456604\tGenerator Loss: 1.299924\n",
      "Train Epoch: 80 [14080/60000 (23%)]\tDiscriminator Loss: 0.467509\tGenerator Loss: 1.757106\n",
      "Train Epoch: 80 [15360/60000 (26%)]\tDiscriminator Loss: 0.505072\tGenerator Loss: 1.002664\n",
      "Train Epoch: 80 [16640/60000 (28%)]\tDiscriminator Loss: 0.428060\tGenerator Loss: 1.349688\n",
      "Train Epoch: 80 [17920/60000 (30%)]\tDiscriminator Loss: 0.450006\tGenerator Loss: 1.410683\n",
      "Train Epoch: 80 [19200/60000 (32%)]\tDiscriminator Loss: 0.471490\tGenerator Loss: 1.469716\n",
      "Train Epoch: 80 [20480/60000 (34%)]\tDiscriminator Loss: 0.472039\tGenerator Loss: 1.390664\n",
      "Train Epoch: 80 [21760/60000 (36%)]\tDiscriminator Loss: 0.528001\tGenerator Loss: 1.323515\n",
      "Train Epoch: 80 [23040/60000 (38%)]\tDiscriminator Loss: 0.440886\tGenerator Loss: 1.773801\n",
      "Train Epoch: 80 [24320/60000 (41%)]\tDiscriminator Loss: 0.488164\tGenerator Loss: 1.860692\n",
      "Train Epoch: 80 [25600/60000 (43%)]\tDiscriminator Loss: 0.497846\tGenerator Loss: 1.048632\n",
      "Train Epoch: 80 [26880/60000 (45%)]\tDiscriminator Loss: 0.468787\tGenerator Loss: 1.642966\n",
      "Train Epoch: 80 [28160/60000 (47%)]\tDiscriminator Loss: 0.529943\tGenerator Loss: 1.542541\n",
      "Train Epoch: 80 [29440/60000 (49%)]\tDiscriminator Loss: 0.493567\tGenerator Loss: 1.338344\n",
      "Train Epoch: 80 [30720/60000 (51%)]\tDiscriminator Loss: 0.452535\tGenerator Loss: 1.326862\n",
      "Train Epoch: 80 [32000/60000 (53%)]\tDiscriminator Loss: 0.490702\tGenerator Loss: 1.372493\n",
      "Train Epoch: 80 [33280/60000 (55%)]\tDiscriminator Loss: 0.468337\tGenerator Loss: 1.558776\n",
      "Train Epoch: 80 [34560/60000 (58%)]\tDiscriminator Loss: 0.468608\tGenerator Loss: 1.325114\n",
      "Train Epoch: 80 [35840/60000 (60%)]\tDiscriminator Loss: 0.578871\tGenerator Loss: 0.886854\n",
      "Train Epoch: 80 [37120/60000 (62%)]\tDiscriminator Loss: 0.460030\tGenerator Loss: 1.372363\n",
      "Train Epoch: 80 [38400/60000 (64%)]\tDiscriminator Loss: 0.492962\tGenerator Loss: 1.369980\n",
      "Train Epoch: 80 [39680/60000 (66%)]\tDiscriminator Loss: 0.535064\tGenerator Loss: 1.674890\n",
      "Train Epoch: 80 [40960/60000 (68%)]\tDiscriminator Loss: 0.452973\tGenerator Loss: 1.425258\n",
      "Train Epoch: 80 [42240/60000 (70%)]\tDiscriminator Loss: 0.530032\tGenerator Loss: 1.301033\n",
      "Train Epoch: 80 [43520/60000 (72%)]\tDiscriminator Loss: 0.483782\tGenerator Loss: 1.394372\n",
      "Train Epoch: 80 [44800/60000 (75%)]\tDiscriminator Loss: 0.548703\tGenerator Loss: 1.471412\n",
      "Train Epoch: 80 [46080/60000 (77%)]\tDiscriminator Loss: 0.464769\tGenerator Loss: 2.361986\n",
      "Train Epoch: 80 [47360/60000 (79%)]\tDiscriminator Loss: 0.480157\tGenerator Loss: 1.698408\n",
      "Train Epoch: 80 [48640/60000 (81%)]\tDiscriminator Loss: 0.453933\tGenerator Loss: 1.476463\n",
      "Train Epoch: 80 [49920/60000 (83%)]\tDiscriminator Loss: 0.465334\tGenerator Loss: 1.421765\n",
      "Train Epoch: 80 [51200/60000 (85%)]\tDiscriminator Loss: 0.494665\tGenerator Loss: 1.476005\n",
      "Train Epoch: 80 [52480/60000 (87%)]\tDiscriminator Loss: 0.467236\tGenerator Loss: 1.278543\n",
      "Train Epoch: 80 [53760/60000 (90%)]\tDiscriminator Loss: 0.512359\tGenerator Loss: 1.504596\n",
      "Train Epoch: 80 [55040/60000 (92%)]\tDiscriminator Loss: 0.485807\tGenerator Loss: 1.484614\n",
      "Train Epoch: 80 [56320/60000 (94%)]\tDiscriminator Loss: 0.480856\tGenerator Loss: 1.222343\n",
      "Train Epoch: 80 [57600/60000 (96%)]\tDiscriminator Loss: 0.545252\tGenerator Loss: 1.197284\n",
      "Train Epoch: 80 [58880/60000 (98%)]\tDiscriminator Loss: 0.472048\tGenerator Loss: 1.638500\n",
      "Train Epoch: 81 [0/60000 (0%)]\tDiscriminator Loss: 0.443939\tGenerator Loss: 1.460804\n",
      "Train Epoch: 81 [1280/60000 (2%)]\tDiscriminator Loss: 0.463438\tGenerator Loss: 1.446782\n",
      "Train Epoch: 81 [2560/60000 (4%)]\tDiscriminator Loss: 0.466090\tGenerator Loss: 1.473340\n",
      "Train Epoch: 81 [3840/60000 (6%)]\tDiscriminator Loss: 0.422221\tGenerator Loss: 1.567160\n",
      "Train Epoch: 81 [5120/60000 (9%)]\tDiscriminator Loss: 0.503145\tGenerator Loss: 2.107393\n",
      "Train Epoch: 81 [6400/60000 (11%)]\tDiscriminator Loss: 0.462001\tGenerator Loss: 1.656161\n",
      "Train Epoch: 81 [7680/60000 (13%)]\tDiscriminator Loss: 0.475094\tGenerator Loss: 1.268992\n",
      "Train Epoch: 81 [8960/60000 (15%)]\tDiscriminator Loss: 0.446822\tGenerator Loss: 1.555205\n",
      "Train Epoch: 81 [10240/60000 (17%)]\tDiscriminator Loss: 0.496922\tGenerator Loss: 1.058551\n",
      "Train Epoch: 81 [11520/60000 (19%)]\tDiscriminator Loss: 0.507184\tGenerator Loss: 1.182852\n",
      "Train Epoch: 81 [12800/60000 (21%)]\tDiscriminator Loss: 0.416918\tGenerator Loss: 1.385172\n",
      "Train Epoch: 81 [14080/60000 (23%)]\tDiscriminator Loss: 0.476672\tGenerator Loss: 1.516896\n",
      "Train Epoch: 81 [15360/60000 (26%)]\tDiscriminator Loss: 0.501661\tGenerator Loss: 1.587818\n",
      "Train Epoch: 81 [16640/60000 (28%)]\tDiscriminator Loss: 0.482212\tGenerator Loss: 1.564922\n",
      "Train Epoch: 81 [17920/60000 (30%)]\tDiscriminator Loss: 0.494717\tGenerator Loss: 1.071038\n",
      "Train Epoch: 81 [19200/60000 (32%)]\tDiscriminator Loss: 0.492339\tGenerator Loss: 1.241733\n",
      "Train Epoch: 81 [20480/60000 (34%)]\tDiscriminator Loss: 0.405968\tGenerator Loss: 1.207978\n",
      "Train Epoch: 81 [21760/60000 (36%)]\tDiscriminator Loss: 0.465659\tGenerator Loss: 1.740841\n",
      "Train Epoch: 81 [23040/60000 (38%)]\tDiscriminator Loss: 0.479968\tGenerator Loss: 1.539225\n",
      "Train Epoch: 81 [24320/60000 (41%)]\tDiscriminator Loss: 0.448308\tGenerator Loss: 1.223983\n",
      "Train Epoch: 81 [25600/60000 (43%)]\tDiscriminator Loss: 0.505265\tGenerator Loss: 1.623025\n",
      "Train Epoch: 81 [26880/60000 (45%)]\tDiscriminator Loss: 0.453201\tGenerator Loss: 1.510806\n",
      "Train Epoch: 81 [28160/60000 (47%)]\tDiscriminator Loss: 0.518848\tGenerator Loss: 1.313236\n",
      "Train Epoch: 81 [29440/60000 (49%)]\tDiscriminator Loss: 0.463849\tGenerator Loss: 1.429843\n",
      "Train Epoch: 81 [30720/60000 (51%)]\tDiscriminator Loss: 0.481998\tGenerator Loss: 1.380075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 81 [32000/60000 (53%)]\tDiscriminator Loss: 0.491040\tGenerator Loss: 1.642537\n",
      "Train Epoch: 81 [33280/60000 (55%)]\tDiscriminator Loss: 0.456409\tGenerator Loss: 1.478991\n",
      "Train Epoch: 81 [34560/60000 (58%)]\tDiscriminator Loss: 0.486158\tGenerator Loss: 1.210760\n",
      "Train Epoch: 81 [35840/60000 (60%)]\tDiscriminator Loss: 0.457879\tGenerator Loss: 1.465111\n",
      "Train Epoch: 81 [37120/60000 (62%)]\tDiscriminator Loss: 0.429086\tGenerator Loss: 1.493471\n",
      "Train Epoch: 81 [38400/60000 (64%)]\tDiscriminator Loss: 0.478221\tGenerator Loss: 1.292549\n",
      "Train Epoch: 81 [39680/60000 (66%)]\tDiscriminator Loss: 0.440252\tGenerator Loss: 1.752720\n",
      "Train Epoch: 81 [40960/60000 (68%)]\tDiscriminator Loss: 0.517478\tGenerator Loss: 1.112567\n",
      "Train Epoch: 81 [42240/60000 (70%)]\tDiscriminator Loss: 0.493131\tGenerator Loss: 1.440408\n",
      "Train Epoch: 81 [43520/60000 (72%)]\tDiscriminator Loss: 0.494724\tGenerator Loss: 1.501139\n",
      "Train Epoch: 81 [44800/60000 (75%)]\tDiscriminator Loss: 0.490061\tGenerator Loss: 1.494745\n",
      "Train Epoch: 81 [46080/60000 (77%)]\tDiscriminator Loss: 0.460937\tGenerator Loss: 1.315185\n",
      "Train Epoch: 81 [47360/60000 (79%)]\tDiscriminator Loss: 0.508746\tGenerator Loss: 1.470148\n",
      "Train Epoch: 81 [48640/60000 (81%)]\tDiscriminator Loss: 0.490414\tGenerator Loss: 1.784772\n",
      "Train Epoch: 81 [49920/60000 (83%)]\tDiscriminator Loss: 0.455272\tGenerator Loss: 1.188487\n",
      "Train Epoch: 81 [51200/60000 (85%)]\tDiscriminator Loss: 0.500776\tGenerator Loss: 1.315696\n",
      "Train Epoch: 81 [52480/60000 (87%)]\tDiscriminator Loss: 0.530258\tGenerator Loss: 1.441180\n",
      "Train Epoch: 81 [53760/60000 (90%)]\tDiscriminator Loss: 0.484703\tGenerator Loss: 1.192699\n",
      "Train Epoch: 81 [55040/60000 (92%)]\tDiscriminator Loss: 0.470426\tGenerator Loss: 1.447953\n",
      "Train Epoch: 81 [56320/60000 (94%)]\tDiscriminator Loss: 0.508956\tGenerator Loss: 1.156734\n",
      "Train Epoch: 81 [57600/60000 (96%)]\tDiscriminator Loss: 0.486169\tGenerator Loss: 1.375271\n",
      "Train Epoch: 81 [58880/60000 (98%)]\tDiscriminator Loss: 0.466538\tGenerator Loss: 1.150084\n",
      "Train Epoch: 82 [0/60000 (0%)]\tDiscriminator Loss: 0.397231\tGenerator Loss: 1.397572\n",
      "Train Epoch: 82 [1280/60000 (2%)]\tDiscriminator Loss: 0.472663\tGenerator Loss: 1.529364\n",
      "Train Epoch: 82 [2560/60000 (4%)]\tDiscriminator Loss: 0.473374\tGenerator Loss: 0.991090\n",
      "Train Epoch: 82 [3840/60000 (6%)]\tDiscriminator Loss: 0.476022\tGenerator Loss: 1.396774\n",
      "Train Epoch: 82 [5120/60000 (9%)]\tDiscriminator Loss: 0.493211\tGenerator Loss: 1.435057\n",
      "Train Epoch: 82 [6400/60000 (11%)]\tDiscriminator Loss: 0.460475\tGenerator Loss: 1.632580\n",
      "Train Epoch: 82 [7680/60000 (13%)]\tDiscriminator Loss: 0.454775\tGenerator Loss: 1.540520\n",
      "Train Epoch: 82 [8960/60000 (15%)]\tDiscriminator Loss: 0.506338\tGenerator Loss: 1.532826\n",
      "Train Epoch: 82 [10240/60000 (17%)]\tDiscriminator Loss: 0.528608\tGenerator Loss: 1.882121\n",
      "Train Epoch: 82 [11520/60000 (19%)]\tDiscriminator Loss: 0.497704\tGenerator Loss: 1.336140\n",
      "Train Epoch: 82 [12800/60000 (21%)]\tDiscriminator Loss: 0.507252\tGenerator Loss: 1.246074\n",
      "Train Epoch: 82 [14080/60000 (23%)]\tDiscriminator Loss: 0.534676\tGenerator Loss: 1.442500\n",
      "Train Epoch: 82 [15360/60000 (26%)]\tDiscriminator Loss: 0.481093\tGenerator Loss: 1.640948\n",
      "Train Epoch: 82 [16640/60000 (28%)]\tDiscriminator Loss: 0.490787\tGenerator Loss: 1.300978\n",
      "Train Epoch: 82 [17920/60000 (30%)]\tDiscriminator Loss: 0.514846\tGenerator Loss: 1.242056\n",
      "Train Epoch: 82 [19200/60000 (32%)]\tDiscriminator Loss: 0.489589\tGenerator Loss: 1.452779\n",
      "Train Epoch: 82 [20480/60000 (34%)]\tDiscriminator Loss: 0.463093\tGenerator Loss: 1.365393\n",
      "Train Epoch: 82 [21760/60000 (36%)]\tDiscriminator Loss: 0.452638\tGenerator Loss: 1.249579\n",
      "Train Epoch: 82 [23040/60000 (38%)]\tDiscriminator Loss: 0.462709\tGenerator Loss: 1.359128\n",
      "Train Epoch: 82 [24320/60000 (41%)]\tDiscriminator Loss: 0.544333\tGenerator Loss: 0.788784\n",
      "Train Epoch: 82 [25600/60000 (43%)]\tDiscriminator Loss: 0.456050\tGenerator Loss: 1.487663\n",
      "Train Epoch: 82 [26880/60000 (45%)]\tDiscriminator Loss: 0.526352\tGenerator Loss: 1.080343\n",
      "Train Epoch: 82 [28160/60000 (47%)]\tDiscriminator Loss: 0.455492\tGenerator Loss: 1.274477\n",
      "Train Epoch: 82 [29440/60000 (49%)]\tDiscriminator Loss: 0.436626\tGenerator Loss: 1.281142\n",
      "Train Epoch: 82 [30720/60000 (51%)]\tDiscriminator Loss: 0.509224\tGenerator Loss: 1.771448\n",
      "Train Epoch: 82 [32000/60000 (53%)]\tDiscriminator Loss: 0.509502\tGenerator Loss: 1.903619\n",
      "Train Epoch: 82 [33280/60000 (55%)]\tDiscriminator Loss: 0.493703\tGenerator Loss: 1.283719\n",
      "Train Epoch: 82 [34560/60000 (58%)]\tDiscriminator Loss: 0.496576\tGenerator Loss: 0.928721\n",
      "Train Epoch: 82 [35840/60000 (60%)]\tDiscriminator Loss: 0.480345\tGenerator Loss: 1.505489\n",
      "Train Epoch: 82 [37120/60000 (62%)]\tDiscriminator Loss: 0.462337\tGenerator Loss: 1.363850\n",
      "Train Epoch: 82 [38400/60000 (64%)]\tDiscriminator Loss: 0.474631\tGenerator Loss: 1.418111\n",
      "Train Epoch: 82 [39680/60000 (66%)]\tDiscriminator Loss: 0.482954\tGenerator Loss: 1.651318\n",
      "Train Epoch: 82 [40960/60000 (68%)]\tDiscriminator Loss: 0.464252\tGenerator Loss: 1.481818\n",
      "Train Epoch: 82 [42240/60000 (70%)]\tDiscriminator Loss: 0.461236\tGenerator Loss: 1.325699\n",
      "Train Epoch: 82 [43520/60000 (72%)]\tDiscriminator Loss: 0.502828\tGenerator Loss: 1.372612\n",
      "Train Epoch: 82 [44800/60000 (75%)]\tDiscriminator Loss: 0.519323\tGenerator Loss: 1.576823\n",
      "Train Epoch: 82 [46080/60000 (77%)]\tDiscriminator Loss: 0.512914\tGenerator Loss: 1.574843\n",
      "Train Epoch: 82 [47360/60000 (79%)]\tDiscriminator Loss: 0.482666\tGenerator Loss: 1.096332\n",
      "Train Epoch: 82 [48640/60000 (81%)]\tDiscriminator Loss: 0.545558\tGenerator Loss: 1.477825\n",
      "Train Epoch: 82 [49920/60000 (83%)]\tDiscriminator Loss: 0.469007\tGenerator Loss: 1.524049\n",
      "Train Epoch: 82 [51200/60000 (85%)]\tDiscriminator Loss: 0.502741\tGenerator Loss: 1.320795\n",
      "Train Epoch: 82 [52480/60000 (87%)]\tDiscriminator Loss: 0.473341\tGenerator Loss: 1.663015\n",
      "Train Epoch: 82 [53760/60000 (90%)]\tDiscriminator Loss: 0.450366\tGenerator Loss: 1.450509\n",
      "Train Epoch: 82 [55040/60000 (92%)]\tDiscriminator Loss: 0.463751\tGenerator Loss: 1.345772\n",
      "Train Epoch: 82 [56320/60000 (94%)]\tDiscriminator Loss: 0.522268\tGenerator Loss: 1.827558\n",
      "Train Epoch: 82 [57600/60000 (96%)]\tDiscriminator Loss: 0.517810\tGenerator Loss: 1.580418\n",
      "Train Epoch: 82 [58880/60000 (98%)]\tDiscriminator Loss: 0.454354\tGenerator Loss: 1.661675\n",
      "Train Epoch: 83 [0/60000 (0%)]\tDiscriminator Loss: 0.455978\tGenerator Loss: 1.233011\n",
      "Train Epoch: 83 [1280/60000 (2%)]\tDiscriminator Loss: 0.459904\tGenerator Loss: 1.530874\n",
      "Train Epoch: 83 [2560/60000 (4%)]\tDiscriminator Loss: 0.568058\tGenerator Loss: 0.812884\n",
      "Train Epoch: 83 [3840/60000 (6%)]\tDiscriminator Loss: 0.610551\tGenerator Loss: 0.819402\n",
      "Train Epoch: 83 [5120/60000 (9%)]\tDiscriminator Loss: 0.414974\tGenerator Loss: 1.581194\n",
      "Train Epoch: 83 [6400/60000 (11%)]\tDiscriminator Loss: 0.492303\tGenerator Loss: 1.416096\n",
      "Train Epoch: 83 [7680/60000 (13%)]\tDiscriminator Loss: 0.466137\tGenerator Loss: 1.619320\n",
      "Train Epoch: 83 [8960/60000 (15%)]\tDiscriminator Loss: 0.491883\tGenerator Loss: 1.209783\n",
      "Train Epoch: 83 [10240/60000 (17%)]\tDiscriminator Loss: 0.531684\tGenerator Loss: 1.820696\n",
      "Train Epoch: 83 [11520/60000 (19%)]\tDiscriminator Loss: 0.532237\tGenerator Loss: 1.157643\n",
      "Train Epoch: 83 [12800/60000 (21%)]\tDiscriminator Loss: 0.453021\tGenerator Loss: 1.245167\n",
      "Train Epoch: 83 [14080/60000 (23%)]\tDiscriminator Loss: 0.511082\tGenerator Loss: 1.036968\n",
      "Train Epoch: 83 [15360/60000 (26%)]\tDiscriminator Loss: 0.406573\tGenerator Loss: 1.465785\n",
      "Train Epoch: 83 [16640/60000 (28%)]\tDiscriminator Loss: 0.505808\tGenerator Loss: 1.252113\n",
      "Train Epoch: 83 [17920/60000 (30%)]\tDiscriminator Loss: 0.474390\tGenerator Loss: 1.248351\n",
      "Train Epoch: 83 [19200/60000 (32%)]\tDiscriminator Loss: 0.508052\tGenerator Loss: 1.145638\n",
      "Train Epoch: 83 [20480/60000 (34%)]\tDiscriminator Loss: 0.488754\tGenerator Loss: 1.744737\n",
      "Train Epoch: 83 [21760/60000 (36%)]\tDiscriminator Loss: 0.500523\tGenerator Loss: 1.557266\n",
      "Train Epoch: 83 [23040/60000 (38%)]\tDiscriminator Loss: 0.517062\tGenerator Loss: 1.210590\n",
      "Train Epoch: 83 [24320/60000 (41%)]\tDiscriminator Loss: 0.458364\tGenerator Loss: 1.078901\n",
      "Train Epoch: 83 [25600/60000 (43%)]\tDiscriminator Loss: 0.527972\tGenerator Loss: 1.008815\n",
      "Train Epoch: 83 [26880/60000 (45%)]\tDiscriminator Loss: 0.470605\tGenerator Loss: 1.264012\n",
      "Train Epoch: 83 [28160/60000 (47%)]\tDiscriminator Loss: 0.581348\tGenerator Loss: 2.022471\n",
      "Train Epoch: 83 [29440/60000 (49%)]\tDiscriminator Loss: 0.453020\tGenerator Loss: 1.317355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 83 [30720/60000 (51%)]\tDiscriminator Loss: 0.487528\tGenerator Loss: 1.402652\n",
      "Train Epoch: 83 [32000/60000 (53%)]\tDiscriminator Loss: 0.406151\tGenerator Loss: 1.568547\n",
      "Train Epoch: 83 [33280/60000 (55%)]\tDiscriminator Loss: 0.493537\tGenerator Loss: 1.371369\n",
      "Train Epoch: 83 [34560/60000 (58%)]\tDiscriminator Loss: 0.414734\tGenerator Loss: 1.653687\n",
      "Train Epoch: 83 [35840/60000 (60%)]\tDiscriminator Loss: 0.520176\tGenerator Loss: 1.125806\n",
      "Train Epoch: 83 [37120/60000 (62%)]\tDiscriminator Loss: 0.505721\tGenerator Loss: 1.190194\n",
      "Train Epoch: 83 [38400/60000 (64%)]\tDiscriminator Loss: 0.463071\tGenerator Loss: 1.447146\n",
      "Train Epoch: 83 [39680/60000 (66%)]\tDiscriminator Loss: 0.557135\tGenerator Loss: 0.874794\n",
      "Train Epoch: 83 [40960/60000 (68%)]\tDiscriminator Loss: 0.558219\tGenerator Loss: 1.011983\n",
      "Train Epoch: 83 [42240/60000 (70%)]\tDiscriminator Loss: 0.429462\tGenerator Loss: 1.753641\n",
      "Train Epoch: 83 [43520/60000 (72%)]\tDiscriminator Loss: 0.434371\tGenerator Loss: 1.366093\n",
      "Train Epoch: 83 [44800/60000 (75%)]\tDiscriminator Loss: 0.440923\tGenerator Loss: 1.441037\n",
      "Train Epoch: 83 [46080/60000 (77%)]\tDiscriminator Loss: 0.472909\tGenerator Loss: 1.475977\n",
      "Train Epoch: 83 [47360/60000 (79%)]\tDiscriminator Loss: 0.531242\tGenerator Loss: 1.697366\n",
      "Train Epoch: 83 [48640/60000 (81%)]\tDiscriminator Loss: 0.461416\tGenerator Loss: 1.583293\n",
      "Train Epoch: 83 [49920/60000 (83%)]\tDiscriminator Loss: 0.511807\tGenerator Loss: 1.068482\n",
      "Train Epoch: 83 [51200/60000 (85%)]\tDiscriminator Loss: 0.456688\tGenerator Loss: 1.393446\n",
      "Train Epoch: 83 [52480/60000 (87%)]\tDiscriminator Loss: 0.541417\tGenerator Loss: 0.988642\n",
      "Train Epoch: 83 [53760/60000 (90%)]\tDiscriminator Loss: 0.525363\tGenerator Loss: 1.285971\n",
      "Train Epoch: 83 [55040/60000 (92%)]\tDiscriminator Loss: 0.444499\tGenerator Loss: 1.468676\n",
      "Train Epoch: 83 [56320/60000 (94%)]\tDiscriminator Loss: 0.477544\tGenerator Loss: 1.226108\n",
      "Train Epoch: 83 [57600/60000 (96%)]\tDiscriminator Loss: 0.534318\tGenerator Loss: 1.209106\n",
      "Train Epoch: 83 [58880/60000 (98%)]\tDiscriminator Loss: 0.458199\tGenerator Loss: 1.296486\n",
      "Train Epoch: 84 [0/60000 (0%)]\tDiscriminator Loss: 0.504211\tGenerator Loss: 1.037982\n",
      "Train Epoch: 84 [1280/60000 (2%)]\tDiscriminator Loss: 0.503619\tGenerator Loss: 1.276828\n",
      "Train Epoch: 84 [2560/60000 (4%)]\tDiscriminator Loss: 0.471344\tGenerator Loss: 1.648686\n",
      "Train Epoch: 84 [3840/60000 (6%)]\tDiscriminator Loss: 0.497080\tGenerator Loss: 1.595880\n",
      "Train Epoch: 84 [5120/60000 (9%)]\tDiscriminator Loss: 0.447961\tGenerator Loss: 1.507706\n",
      "Train Epoch: 84 [6400/60000 (11%)]\tDiscriminator Loss: 0.494725\tGenerator Loss: 1.504204\n",
      "Train Epoch: 84 [7680/60000 (13%)]\tDiscriminator Loss: 0.497315\tGenerator Loss: 1.577913\n",
      "Train Epoch: 84 [8960/60000 (15%)]\tDiscriminator Loss: 0.548786\tGenerator Loss: 1.252079\n",
      "Train Epoch: 84 [10240/60000 (17%)]\tDiscriminator Loss: 0.432564\tGenerator Loss: 1.456782\n",
      "Train Epoch: 84 [11520/60000 (19%)]\tDiscriminator Loss: 0.480661\tGenerator Loss: 1.070625\n",
      "Train Epoch: 84 [12800/60000 (21%)]\tDiscriminator Loss: 0.550310\tGenerator Loss: 1.498521\n",
      "Train Epoch: 84 [14080/60000 (23%)]\tDiscriminator Loss: 0.507104\tGenerator Loss: 1.675267\n",
      "Train Epoch: 84 [15360/60000 (26%)]\tDiscriminator Loss: 0.491571\tGenerator Loss: 1.280957\n",
      "Train Epoch: 84 [16640/60000 (28%)]\tDiscriminator Loss: 0.493712\tGenerator Loss: 1.119506\n",
      "Train Epoch: 84 [17920/60000 (30%)]\tDiscriminator Loss: 0.568469\tGenerator Loss: 0.868008\n",
      "Train Epoch: 84 [19200/60000 (32%)]\tDiscriminator Loss: 0.403279\tGenerator Loss: 1.650382\n",
      "Train Epoch: 84 [20480/60000 (34%)]\tDiscriminator Loss: 0.513952\tGenerator Loss: 1.042771\n",
      "Train Epoch: 84 [21760/60000 (36%)]\tDiscriminator Loss: 0.481552\tGenerator Loss: 1.302225\n",
      "Train Epoch: 84 [23040/60000 (38%)]\tDiscriminator Loss: 0.458865\tGenerator Loss: 1.607226\n",
      "Train Epoch: 84 [24320/60000 (41%)]\tDiscriminator Loss: 0.484494\tGenerator Loss: 1.219739\n",
      "Train Epoch: 84 [25600/60000 (43%)]\tDiscriminator Loss: 0.467310\tGenerator Loss: 1.642826\n",
      "Train Epoch: 84 [26880/60000 (45%)]\tDiscriminator Loss: 0.478457\tGenerator Loss: 1.275678\n",
      "Train Epoch: 84 [28160/60000 (47%)]\tDiscriminator Loss: 0.461328\tGenerator Loss: 1.405993\n",
      "Train Epoch: 84 [29440/60000 (49%)]\tDiscriminator Loss: 0.494920\tGenerator Loss: 1.213290\n",
      "Train Epoch: 84 [30720/60000 (51%)]\tDiscriminator Loss: 0.449065\tGenerator Loss: 1.497213\n",
      "Train Epoch: 84 [32000/60000 (53%)]\tDiscriminator Loss: 0.403218\tGenerator Loss: 1.742145\n",
      "Train Epoch: 84 [33280/60000 (55%)]\tDiscriminator Loss: 0.456232\tGenerator Loss: 1.156630\n",
      "Train Epoch: 84 [34560/60000 (58%)]\tDiscriminator Loss: 0.470473\tGenerator Loss: 1.313899\n",
      "Train Epoch: 84 [35840/60000 (60%)]\tDiscriminator Loss: 0.525885\tGenerator Loss: 1.447125\n",
      "Train Epoch: 84 [37120/60000 (62%)]\tDiscriminator Loss: 0.500092\tGenerator Loss: 1.985486\n",
      "Train Epoch: 84 [38400/60000 (64%)]\tDiscriminator Loss: 0.500917\tGenerator Loss: 1.085656\n",
      "Train Epoch: 84 [39680/60000 (66%)]\tDiscriminator Loss: 0.507680\tGenerator Loss: 1.214383\n",
      "Train Epoch: 84 [40960/60000 (68%)]\tDiscriminator Loss: 0.499873\tGenerator Loss: 1.286957\n",
      "Train Epoch: 84 [42240/60000 (70%)]\tDiscriminator Loss: 0.461814\tGenerator Loss: 1.752190\n",
      "Train Epoch: 84 [43520/60000 (72%)]\tDiscriminator Loss: 0.497300\tGenerator Loss: 1.411578\n",
      "Train Epoch: 84 [44800/60000 (75%)]\tDiscriminator Loss: 0.559340\tGenerator Loss: 0.932248\n",
      "Train Epoch: 84 [46080/60000 (77%)]\tDiscriminator Loss: 0.464350\tGenerator Loss: 1.385295\n",
      "Train Epoch: 84 [47360/60000 (79%)]\tDiscriminator Loss: 0.466568\tGenerator Loss: 1.073604\n",
      "Train Epoch: 84 [48640/60000 (81%)]\tDiscriminator Loss: 0.475205\tGenerator Loss: 1.492022\n",
      "Train Epoch: 84 [49920/60000 (83%)]\tDiscriminator Loss: 0.444266\tGenerator Loss: 1.753446\n",
      "Train Epoch: 84 [51200/60000 (85%)]\tDiscriminator Loss: 0.504714\tGenerator Loss: 1.493425\n",
      "Train Epoch: 84 [52480/60000 (87%)]\tDiscriminator Loss: 0.488301\tGenerator Loss: 1.445160\n",
      "Train Epoch: 84 [53760/60000 (90%)]\tDiscriminator Loss: 0.482317\tGenerator Loss: 1.791026\n",
      "Train Epoch: 84 [55040/60000 (92%)]\tDiscriminator Loss: 0.458119\tGenerator Loss: 1.284649\n",
      "Train Epoch: 84 [56320/60000 (94%)]\tDiscriminator Loss: 0.579289\tGenerator Loss: 1.915570\n",
      "Train Epoch: 84 [57600/60000 (96%)]\tDiscriminator Loss: 0.454808\tGenerator Loss: 1.285425\n",
      "Train Epoch: 84 [58880/60000 (98%)]\tDiscriminator Loss: 0.493358\tGenerator Loss: 1.737999\n",
      "Train Epoch: 85 [0/60000 (0%)]\tDiscriminator Loss: 0.472188\tGenerator Loss: 1.321558\n",
      "Train Epoch: 85 [1280/60000 (2%)]\tDiscriminator Loss: 0.481763\tGenerator Loss: 1.438514\n",
      "Train Epoch: 85 [2560/60000 (4%)]\tDiscriminator Loss: 0.457023\tGenerator Loss: 1.469481\n",
      "Train Epoch: 85 [3840/60000 (6%)]\tDiscriminator Loss: 0.419384\tGenerator Loss: 1.468886\n",
      "Train Epoch: 85 [5120/60000 (9%)]\tDiscriminator Loss: 0.495149\tGenerator Loss: 1.577985\n",
      "Train Epoch: 85 [6400/60000 (11%)]\tDiscriminator Loss: 0.495219\tGenerator Loss: 1.253590\n",
      "Train Epoch: 85 [7680/60000 (13%)]\tDiscriminator Loss: 0.431437\tGenerator Loss: 1.292599\n",
      "Train Epoch: 85 [8960/60000 (15%)]\tDiscriminator Loss: 0.485795\tGenerator Loss: 1.478263\n",
      "Train Epoch: 85 [10240/60000 (17%)]\tDiscriminator Loss: 0.461782\tGenerator Loss: 1.221249\n",
      "Train Epoch: 85 [11520/60000 (19%)]\tDiscriminator Loss: 0.444807\tGenerator Loss: 1.260375\n",
      "Train Epoch: 85 [12800/60000 (21%)]\tDiscriminator Loss: 0.513974\tGenerator Loss: 1.134653\n",
      "Train Epoch: 85 [14080/60000 (23%)]\tDiscriminator Loss: 0.446532\tGenerator Loss: 1.344712\n",
      "Train Epoch: 85 [15360/60000 (26%)]\tDiscriminator Loss: 0.428492\tGenerator Loss: 1.281779\n",
      "Train Epoch: 85 [16640/60000 (28%)]\tDiscriminator Loss: 0.449816\tGenerator Loss: 1.217845\n",
      "Train Epoch: 85 [17920/60000 (30%)]\tDiscriminator Loss: 0.485437\tGenerator Loss: 1.590089\n",
      "Train Epoch: 85 [19200/60000 (32%)]\tDiscriminator Loss: 0.443125\tGenerator Loss: 1.438882\n",
      "Train Epoch: 85 [20480/60000 (34%)]\tDiscriminator Loss: 0.466482\tGenerator Loss: 1.357000\n",
      "Train Epoch: 85 [21760/60000 (36%)]\tDiscriminator Loss: 0.491241\tGenerator Loss: 1.480671\n",
      "Train Epoch: 85 [23040/60000 (38%)]\tDiscriminator Loss: 0.442106\tGenerator Loss: 1.581750\n",
      "Train Epoch: 85 [24320/60000 (41%)]\tDiscriminator Loss: 0.450717\tGenerator Loss: 1.205006\n",
      "Train Epoch: 85 [25600/60000 (43%)]\tDiscriminator Loss: 0.425080\tGenerator Loss: 1.562325\n",
      "Train Epoch: 85 [26880/60000 (45%)]\tDiscriminator Loss: 0.482569\tGenerator Loss: 1.138771\n",
      "Train Epoch: 85 [28160/60000 (47%)]\tDiscriminator Loss: 0.415824\tGenerator Loss: 1.508674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 85 [29440/60000 (49%)]\tDiscriminator Loss: 0.465043\tGenerator Loss: 1.519263\n",
      "Train Epoch: 85 [30720/60000 (51%)]\tDiscriminator Loss: 0.401483\tGenerator Loss: 1.574716\n",
      "Train Epoch: 85 [32000/60000 (53%)]\tDiscriminator Loss: 0.514861\tGenerator Loss: 1.162711\n",
      "Train Epoch: 85 [33280/60000 (55%)]\tDiscriminator Loss: 0.557599\tGenerator Loss: 1.204335\n",
      "Train Epoch: 85 [34560/60000 (58%)]\tDiscriminator Loss: 0.471638\tGenerator Loss: 1.399120\n",
      "Train Epoch: 85 [35840/60000 (60%)]\tDiscriminator Loss: 0.469299\tGenerator Loss: 1.366794\n",
      "Train Epoch: 85 [37120/60000 (62%)]\tDiscriminator Loss: 0.416934\tGenerator Loss: 1.423418\n",
      "Train Epoch: 85 [38400/60000 (64%)]\tDiscriminator Loss: 0.479797\tGenerator Loss: 1.342777\n",
      "Train Epoch: 85 [39680/60000 (66%)]\tDiscriminator Loss: 0.434549\tGenerator Loss: 1.471617\n",
      "Train Epoch: 85 [40960/60000 (68%)]\tDiscriminator Loss: 0.517871\tGenerator Loss: 0.977439\n",
      "Train Epoch: 85 [42240/60000 (70%)]\tDiscriminator Loss: 0.529529\tGenerator Loss: 1.569741\n",
      "Train Epoch: 85 [43520/60000 (72%)]\tDiscriminator Loss: 0.507564\tGenerator Loss: 1.580888\n",
      "Train Epoch: 85 [44800/60000 (75%)]\tDiscriminator Loss: 0.478804\tGenerator Loss: 1.311007\n",
      "Train Epoch: 85 [46080/60000 (77%)]\tDiscriminator Loss: 0.484215\tGenerator Loss: 1.294995\n",
      "Train Epoch: 85 [47360/60000 (79%)]\tDiscriminator Loss: 0.579721\tGenerator Loss: 1.544084\n",
      "Train Epoch: 85 [48640/60000 (81%)]\tDiscriminator Loss: 0.392512\tGenerator Loss: 1.520104\n",
      "Train Epoch: 85 [49920/60000 (83%)]\tDiscriminator Loss: 0.491370\tGenerator Loss: 1.469030\n",
      "Train Epoch: 85 [51200/60000 (85%)]\tDiscriminator Loss: 0.497917\tGenerator Loss: 1.109955\n",
      "Train Epoch: 85 [52480/60000 (87%)]\tDiscriminator Loss: 0.447948\tGenerator Loss: 1.418492\n",
      "Train Epoch: 85 [53760/60000 (90%)]\tDiscriminator Loss: 0.472004\tGenerator Loss: 1.448143\n",
      "Train Epoch: 85 [55040/60000 (92%)]\tDiscriminator Loss: 0.490424\tGenerator Loss: 1.524970\n",
      "Train Epoch: 85 [56320/60000 (94%)]\tDiscriminator Loss: 0.481802\tGenerator Loss: 1.294971\n",
      "Train Epoch: 85 [57600/60000 (96%)]\tDiscriminator Loss: 0.468980\tGenerator Loss: 1.136507\n",
      "Train Epoch: 85 [58880/60000 (98%)]\tDiscriminator Loss: 0.508512\tGenerator Loss: 1.388855\n",
      "Train Epoch: 86 [0/60000 (0%)]\tDiscriminator Loss: 0.466350\tGenerator Loss: 1.499240\n",
      "Train Epoch: 86 [1280/60000 (2%)]\tDiscriminator Loss: 0.462734\tGenerator Loss: 1.444314\n",
      "Train Epoch: 86 [2560/60000 (4%)]\tDiscriminator Loss: 0.475671\tGenerator Loss: 1.172171\n",
      "Train Epoch: 86 [3840/60000 (6%)]\tDiscriminator Loss: 0.460496\tGenerator Loss: 1.076654\n",
      "Train Epoch: 86 [5120/60000 (9%)]\tDiscriminator Loss: 0.495400\tGenerator Loss: 1.825592\n",
      "Train Epoch: 86 [6400/60000 (11%)]\tDiscriminator Loss: 0.531492\tGenerator Loss: 1.149902\n",
      "Train Epoch: 86 [7680/60000 (13%)]\tDiscriminator Loss: 0.482463\tGenerator Loss: 1.258895\n",
      "Train Epoch: 86 [8960/60000 (15%)]\tDiscriminator Loss: 0.542280\tGenerator Loss: 0.963956\n",
      "Train Epoch: 86 [10240/60000 (17%)]\tDiscriminator Loss: 0.435313\tGenerator Loss: 1.354565\n",
      "Train Epoch: 86 [11520/60000 (19%)]\tDiscriminator Loss: 0.561530\tGenerator Loss: 1.107787\n",
      "Train Epoch: 86 [12800/60000 (21%)]\tDiscriminator Loss: 0.434603\tGenerator Loss: 1.714694\n",
      "Train Epoch: 86 [14080/60000 (23%)]\tDiscriminator Loss: 0.590489\tGenerator Loss: 1.292395\n",
      "Train Epoch: 86 [15360/60000 (26%)]\tDiscriminator Loss: 0.446597\tGenerator Loss: 1.248443\n",
      "Train Epoch: 86 [16640/60000 (28%)]\tDiscriminator Loss: 0.496510\tGenerator Loss: 1.087225\n",
      "Train Epoch: 86 [17920/60000 (30%)]\tDiscriminator Loss: 0.454389\tGenerator Loss: 1.518851\n",
      "Train Epoch: 86 [19200/60000 (32%)]\tDiscriminator Loss: 0.460457\tGenerator Loss: 1.294163\n",
      "Train Epoch: 86 [20480/60000 (34%)]\tDiscriminator Loss: 0.551934\tGenerator Loss: 0.972867\n",
      "Train Epoch: 86 [21760/60000 (36%)]\tDiscriminator Loss: 0.513488\tGenerator Loss: 1.139840\n",
      "Train Epoch: 86 [23040/60000 (38%)]\tDiscriminator Loss: 0.426693\tGenerator Loss: 1.117844\n",
      "Train Epoch: 86 [24320/60000 (41%)]\tDiscriminator Loss: 0.556773\tGenerator Loss: 1.913213\n",
      "Train Epoch: 86 [25600/60000 (43%)]\tDiscriminator Loss: 0.543622\tGenerator Loss: 1.475021\n",
      "Train Epoch: 86 [26880/60000 (45%)]\tDiscriminator Loss: 0.515558\tGenerator Loss: 1.374599\n",
      "Train Epoch: 86 [28160/60000 (47%)]\tDiscriminator Loss: 0.449462\tGenerator Loss: 1.351258\n",
      "Train Epoch: 86 [29440/60000 (49%)]\tDiscriminator Loss: 0.497527\tGenerator Loss: 1.663583\n",
      "Train Epoch: 86 [30720/60000 (51%)]\tDiscriminator Loss: 0.493301\tGenerator Loss: 1.722919\n",
      "Train Epoch: 86 [32000/60000 (53%)]\tDiscriminator Loss: 0.434783\tGenerator Loss: 1.393573\n",
      "Train Epoch: 86 [33280/60000 (55%)]\tDiscriminator Loss: 0.462442\tGenerator Loss: 1.382882\n",
      "Train Epoch: 86 [34560/60000 (58%)]\tDiscriminator Loss: 0.462346\tGenerator Loss: 1.212178\n",
      "Train Epoch: 86 [35840/60000 (60%)]\tDiscriminator Loss: 0.498353\tGenerator Loss: 1.502746\n",
      "Train Epoch: 86 [37120/60000 (62%)]\tDiscriminator Loss: 0.495230\tGenerator Loss: 1.463832\n",
      "Train Epoch: 86 [38400/60000 (64%)]\tDiscriminator Loss: 0.474370\tGenerator Loss: 1.719020\n",
      "Train Epoch: 86 [39680/60000 (66%)]\tDiscriminator Loss: 0.471121\tGenerator Loss: 1.318479\n",
      "Train Epoch: 86 [40960/60000 (68%)]\tDiscriminator Loss: 0.465369\tGenerator Loss: 1.384889\n",
      "Train Epoch: 86 [42240/60000 (70%)]\tDiscriminator Loss: 0.456046\tGenerator Loss: 1.338488\n",
      "Train Epoch: 86 [43520/60000 (72%)]\tDiscriminator Loss: 0.500855\tGenerator Loss: 1.223043\n",
      "Train Epoch: 86 [44800/60000 (75%)]\tDiscriminator Loss: 0.442835\tGenerator Loss: 1.327738\n",
      "Train Epoch: 86 [46080/60000 (77%)]\tDiscriminator Loss: 0.468932\tGenerator Loss: 1.536354\n",
      "Train Epoch: 86 [47360/60000 (79%)]\tDiscriminator Loss: 0.472913\tGenerator Loss: 1.179391\n",
      "Train Epoch: 86 [48640/60000 (81%)]\tDiscriminator Loss: 0.475180\tGenerator Loss: 1.614155\n",
      "Train Epoch: 86 [49920/60000 (83%)]\tDiscriminator Loss: 0.496883\tGenerator Loss: 0.978808\n",
      "Train Epoch: 86 [51200/60000 (85%)]\tDiscriminator Loss: 0.435521\tGenerator Loss: 1.293590\n",
      "Train Epoch: 86 [52480/60000 (87%)]\tDiscriminator Loss: 0.512744\tGenerator Loss: 1.488843\n",
      "Train Epoch: 86 [53760/60000 (90%)]\tDiscriminator Loss: 0.442977\tGenerator Loss: 1.432420\n",
      "Train Epoch: 86 [55040/60000 (92%)]\tDiscriminator Loss: 0.531530\tGenerator Loss: 1.042178\n",
      "Train Epoch: 86 [56320/60000 (94%)]\tDiscriminator Loss: 0.505870\tGenerator Loss: 1.301456\n",
      "Train Epoch: 86 [57600/60000 (96%)]\tDiscriminator Loss: 0.491604\tGenerator Loss: 1.291937\n",
      "Train Epoch: 86 [58880/60000 (98%)]\tDiscriminator Loss: 0.470292\tGenerator Loss: 1.486846\n",
      "Train Epoch: 87 [0/60000 (0%)]\tDiscriminator Loss: 0.447443\tGenerator Loss: 1.386686\n",
      "Train Epoch: 87 [1280/60000 (2%)]\tDiscriminator Loss: 0.444711\tGenerator Loss: 1.681053\n",
      "Train Epoch: 87 [2560/60000 (4%)]\tDiscriminator Loss: 0.445813\tGenerator Loss: 1.314742\n",
      "Train Epoch: 87 [3840/60000 (6%)]\tDiscriminator Loss: 0.503568\tGenerator Loss: 1.002017\n",
      "Train Epoch: 87 [5120/60000 (9%)]\tDiscriminator Loss: 0.411401\tGenerator Loss: 1.442354\n",
      "Train Epoch: 87 [6400/60000 (11%)]\tDiscriminator Loss: 0.471611\tGenerator Loss: 1.152505\n",
      "Train Epoch: 87 [7680/60000 (13%)]\tDiscriminator Loss: 0.488183\tGenerator Loss: 1.636245\n",
      "Train Epoch: 87 [8960/60000 (15%)]\tDiscriminator Loss: 0.476621\tGenerator Loss: 1.377506\n",
      "Train Epoch: 87 [10240/60000 (17%)]\tDiscriminator Loss: 0.452050\tGenerator Loss: 1.365035\n",
      "Train Epoch: 87 [11520/60000 (19%)]\tDiscriminator Loss: 0.435249\tGenerator Loss: 1.584278\n",
      "Train Epoch: 87 [12800/60000 (21%)]\tDiscriminator Loss: 0.483953\tGenerator Loss: 1.233217\n",
      "Train Epoch: 87 [14080/60000 (23%)]\tDiscriminator Loss: 0.486160\tGenerator Loss: 1.222586\n",
      "Train Epoch: 87 [15360/60000 (26%)]\tDiscriminator Loss: 0.451153\tGenerator Loss: 1.401531\n",
      "Train Epoch: 87 [16640/60000 (28%)]\tDiscriminator Loss: 0.484392\tGenerator Loss: 1.287314\n",
      "Train Epoch: 87 [17920/60000 (30%)]\tDiscriminator Loss: 0.510531\tGenerator Loss: 1.566596\n",
      "Train Epoch: 87 [19200/60000 (32%)]\tDiscriminator Loss: 0.517299\tGenerator Loss: 1.478468\n",
      "Train Epoch: 87 [20480/60000 (34%)]\tDiscriminator Loss: 0.477067\tGenerator Loss: 1.211757\n",
      "Train Epoch: 87 [21760/60000 (36%)]\tDiscriminator Loss: 0.441062\tGenerator Loss: 1.592256\n",
      "Train Epoch: 87 [23040/60000 (38%)]\tDiscriminator Loss: 0.497686\tGenerator Loss: 1.064135\n",
      "Train Epoch: 87 [24320/60000 (41%)]\tDiscriminator Loss: 0.433107\tGenerator Loss: 1.482866\n",
      "Train Epoch: 87 [25600/60000 (43%)]\tDiscriminator Loss: 0.473339\tGenerator Loss: 1.323134\n",
      "Train Epoch: 87 [26880/60000 (45%)]\tDiscriminator Loss: 0.497806\tGenerator Loss: 1.397419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 87 [28160/60000 (47%)]\tDiscriminator Loss: 0.514887\tGenerator Loss: 1.402285\n",
      "Train Epoch: 87 [29440/60000 (49%)]\tDiscriminator Loss: 0.533745\tGenerator Loss: 1.114276\n",
      "Train Epoch: 87 [30720/60000 (51%)]\tDiscriminator Loss: 0.478643\tGenerator Loss: 1.101867\n",
      "Train Epoch: 87 [32000/60000 (53%)]\tDiscriminator Loss: 0.411198\tGenerator Loss: 1.279488\n",
      "Train Epoch: 87 [33280/60000 (55%)]\tDiscriminator Loss: 0.518944\tGenerator Loss: 1.581740\n",
      "Train Epoch: 87 [34560/60000 (58%)]\tDiscriminator Loss: 0.490716\tGenerator Loss: 1.151150\n",
      "Train Epoch: 87 [35840/60000 (60%)]\tDiscriminator Loss: 0.433381\tGenerator Loss: 1.678450\n",
      "Train Epoch: 87 [37120/60000 (62%)]\tDiscriminator Loss: 0.520619\tGenerator Loss: 1.479387\n",
      "Train Epoch: 87 [38400/60000 (64%)]\tDiscriminator Loss: 0.480389\tGenerator Loss: 1.283334\n",
      "Train Epoch: 87 [39680/60000 (66%)]\tDiscriminator Loss: 0.405883\tGenerator Loss: 1.655272\n",
      "Train Epoch: 87 [40960/60000 (68%)]\tDiscriminator Loss: 0.516776\tGenerator Loss: 1.318604\n",
      "Train Epoch: 87 [42240/60000 (70%)]\tDiscriminator Loss: 0.502106\tGenerator Loss: 1.484261\n",
      "Train Epoch: 87 [43520/60000 (72%)]\tDiscriminator Loss: 0.478119\tGenerator Loss: 1.166811\n",
      "Train Epoch: 87 [44800/60000 (75%)]\tDiscriminator Loss: 0.509400\tGenerator Loss: 1.758891\n",
      "Train Epoch: 87 [46080/60000 (77%)]\tDiscriminator Loss: 0.506038\tGenerator Loss: 1.051997\n",
      "Train Epoch: 87 [47360/60000 (79%)]\tDiscriminator Loss: 0.461974\tGenerator Loss: 1.414099\n",
      "Train Epoch: 87 [48640/60000 (81%)]\tDiscriminator Loss: 0.467582\tGenerator Loss: 1.374460\n",
      "Train Epoch: 87 [49920/60000 (83%)]\tDiscriminator Loss: 0.540149\tGenerator Loss: 1.477848\n",
      "Train Epoch: 87 [51200/60000 (85%)]\tDiscriminator Loss: 0.479604\tGenerator Loss: 1.423941\n",
      "Train Epoch: 87 [52480/60000 (87%)]\tDiscriminator Loss: 0.451292\tGenerator Loss: 1.512408\n",
      "Train Epoch: 87 [53760/60000 (90%)]\tDiscriminator Loss: 0.458284\tGenerator Loss: 1.443976\n",
      "Train Epoch: 87 [55040/60000 (92%)]\tDiscriminator Loss: 0.522752\tGenerator Loss: 1.491590\n",
      "Train Epoch: 87 [56320/60000 (94%)]\tDiscriminator Loss: 0.530351\tGenerator Loss: 1.373524\n",
      "Train Epoch: 87 [57600/60000 (96%)]\tDiscriminator Loss: 0.498524\tGenerator Loss: 1.412045\n",
      "Train Epoch: 87 [58880/60000 (98%)]\tDiscriminator Loss: 0.503138\tGenerator Loss: 1.168626\n",
      "Train Epoch: 88 [0/60000 (0%)]\tDiscriminator Loss: 0.416803\tGenerator Loss: 1.541113\n",
      "Train Epoch: 88 [1280/60000 (2%)]\tDiscriminator Loss: 0.480743\tGenerator Loss: 1.173610\n",
      "Train Epoch: 88 [2560/60000 (4%)]\tDiscriminator Loss: 0.485472\tGenerator Loss: 1.223107\n",
      "Train Epoch: 88 [3840/60000 (6%)]\tDiscriminator Loss: 0.495207\tGenerator Loss: 1.162647\n",
      "Train Epoch: 88 [5120/60000 (9%)]\tDiscriminator Loss: 0.493115\tGenerator Loss: 1.268585\n",
      "Train Epoch: 88 [6400/60000 (11%)]\tDiscriminator Loss: 0.477656\tGenerator Loss: 1.374485\n",
      "Train Epoch: 88 [7680/60000 (13%)]\tDiscriminator Loss: 0.425004\tGenerator Loss: 1.368357\n",
      "Train Epoch: 88 [8960/60000 (15%)]\tDiscriminator Loss: 0.456494\tGenerator Loss: 1.226427\n",
      "Train Epoch: 88 [10240/60000 (17%)]\tDiscriminator Loss: 0.480623\tGenerator Loss: 1.688410\n",
      "Train Epoch: 88 [11520/60000 (19%)]\tDiscriminator Loss: 0.460708\tGenerator Loss: 1.225877\n",
      "Train Epoch: 88 [12800/60000 (21%)]\tDiscriminator Loss: 0.464804\tGenerator Loss: 1.573382\n",
      "Train Epoch: 88 [14080/60000 (23%)]\tDiscriminator Loss: 0.471947\tGenerator Loss: 1.377771\n",
      "Train Epoch: 88 [15360/60000 (26%)]\tDiscriminator Loss: 0.470535\tGenerator Loss: 1.608084\n",
      "Train Epoch: 88 [16640/60000 (28%)]\tDiscriminator Loss: 0.396900\tGenerator Loss: 1.805848\n",
      "Train Epoch: 88 [17920/60000 (30%)]\tDiscriminator Loss: 0.545642\tGenerator Loss: 1.576692\n",
      "Train Epoch: 88 [19200/60000 (32%)]\tDiscriminator Loss: 0.482510\tGenerator Loss: 1.310203\n",
      "Train Epoch: 88 [20480/60000 (34%)]\tDiscriminator Loss: 0.473631\tGenerator Loss: 1.424125\n",
      "Train Epoch: 88 [21760/60000 (36%)]\tDiscriminator Loss: 0.486234\tGenerator Loss: 1.312819\n",
      "Train Epoch: 88 [23040/60000 (38%)]\tDiscriminator Loss: 0.623827\tGenerator Loss: 2.157787\n",
      "Train Epoch: 88 [24320/60000 (41%)]\tDiscriminator Loss: 0.486927\tGenerator Loss: 1.519301\n",
      "Train Epoch: 88 [25600/60000 (43%)]\tDiscriminator Loss: 0.500465\tGenerator Loss: 1.222892\n",
      "Train Epoch: 88 [26880/60000 (45%)]\tDiscriminator Loss: 0.514737\tGenerator Loss: 1.160400\n",
      "Train Epoch: 88 [28160/60000 (47%)]\tDiscriminator Loss: 0.486607\tGenerator Loss: 1.314705\n",
      "Train Epoch: 88 [29440/60000 (49%)]\tDiscriminator Loss: 0.459136\tGenerator Loss: 1.460618\n",
      "Train Epoch: 88 [30720/60000 (51%)]\tDiscriminator Loss: 0.482551\tGenerator Loss: 1.334634\n",
      "Train Epoch: 88 [32000/60000 (53%)]\tDiscriminator Loss: 0.464868\tGenerator Loss: 1.154227\n",
      "Train Epoch: 88 [33280/60000 (55%)]\tDiscriminator Loss: 0.434112\tGenerator Loss: 1.763223\n",
      "Train Epoch: 88 [34560/60000 (58%)]\tDiscriminator Loss: 0.436140\tGenerator Loss: 1.486454\n",
      "Train Epoch: 88 [35840/60000 (60%)]\tDiscriminator Loss: 0.482199\tGenerator Loss: 1.162683\n",
      "Train Epoch: 88 [37120/60000 (62%)]\tDiscriminator Loss: 0.443340\tGenerator Loss: 1.451451\n",
      "Train Epoch: 88 [38400/60000 (64%)]\tDiscriminator Loss: 0.525646\tGenerator Loss: 1.510158\n",
      "Train Epoch: 88 [39680/60000 (66%)]\tDiscriminator Loss: 0.463832\tGenerator Loss: 1.373554\n",
      "Train Epoch: 88 [40960/60000 (68%)]\tDiscriminator Loss: 0.414044\tGenerator Loss: 1.688206\n",
      "Train Epoch: 88 [42240/60000 (70%)]\tDiscriminator Loss: 0.450158\tGenerator Loss: 1.325515\n",
      "Train Epoch: 88 [43520/60000 (72%)]\tDiscriminator Loss: 0.445971\tGenerator Loss: 1.302865\n",
      "Train Epoch: 88 [44800/60000 (75%)]\tDiscriminator Loss: 0.507084\tGenerator Loss: 1.067490\n",
      "Train Epoch: 88 [46080/60000 (77%)]\tDiscriminator Loss: 0.493171\tGenerator Loss: 1.434209\n",
      "Train Epoch: 88 [47360/60000 (79%)]\tDiscriminator Loss: 0.496066\tGenerator Loss: 1.546757\n",
      "Train Epoch: 88 [48640/60000 (81%)]\tDiscriminator Loss: 0.516301\tGenerator Loss: 1.325757\n",
      "Train Epoch: 88 [49920/60000 (83%)]\tDiscriminator Loss: 0.426556\tGenerator Loss: 1.649710\n",
      "Train Epoch: 88 [51200/60000 (85%)]\tDiscriminator Loss: 0.477849\tGenerator Loss: 1.403448\n",
      "Train Epoch: 88 [52480/60000 (87%)]\tDiscriminator Loss: 0.527063\tGenerator Loss: 1.581208\n",
      "Train Epoch: 88 [53760/60000 (90%)]\tDiscriminator Loss: 0.466998\tGenerator Loss: 1.507157\n",
      "Train Epoch: 88 [55040/60000 (92%)]\tDiscriminator Loss: 0.451142\tGenerator Loss: 1.491248\n",
      "Train Epoch: 88 [56320/60000 (94%)]\tDiscriminator Loss: 0.513977\tGenerator Loss: 1.177681\n",
      "Train Epoch: 88 [57600/60000 (96%)]\tDiscriminator Loss: 0.453468\tGenerator Loss: 1.399391\n",
      "Train Epoch: 88 [58880/60000 (98%)]\tDiscriminator Loss: 0.538006\tGenerator Loss: 1.209401\n",
      "Train Epoch: 89 [0/60000 (0%)]\tDiscriminator Loss: 0.499769\tGenerator Loss: 1.509245\n",
      "Train Epoch: 89 [1280/60000 (2%)]\tDiscriminator Loss: 0.368139\tGenerator Loss: 1.635351\n",
      "Train Epoch: 89 [2560/60000 (4%)]\tDiscriminator Loss: 0.447502\tGenerator Loss: 1.314934\n",
      "Train Epoch: 89 [3840/60000 (6%)]\tDiscriminator Loss: 0.428840\tGenerator Loss: 1.361546\n",
      "Train Epoch: 89 [5120/60000 (9%)]\tDiscriminator Loss: 0.468775\tGenerator Loss: 1.182064\n",
      "Train Epoch: 89 [6400/60000 (11%)]\tDiscriminator Loss: 0.470810\tGenerator Loss: 2.135951\n",
      "Train Epoch: 89 [7680/60000 (13%)]\tDiscriminator Loss: 0.445078\tGenerator Loss: 1.496861\n",
      "Train Epoch: 89 [8960/60000 (15%)]\tDiscriminator Loss: 0.474464\tGenerator Loss: 1.676371\n",
      "Train Epoch: 89 [10240/60000 (17%)]\tDiscriminator Loss: 0.483288\tGenerator Loss: 1.250015\n",
      "Train Epoch: 89 [11520/60000 (19%)]\tDiscriminator Loss: 0.442739\tGenerator Loss: 1.505033\n",
      "Train Epoch: 89 [12800/60000 (21%)]\tDiscriminator Loss: 0.474152\tGenerator Loss: 1.332098\n",
      "Train Epoch: 89 [14080/60000 (23%)]\tDiscriminator Loss: 0.460649\tGenerator Loss: 1.422985\n",
      "Train Epoch: 89 [15360/60000 (26%)]\tDiscriminator Loss: 0.438043\tGenerator Loss: 1.429075\n",
      "Train Epoch: 89 [16640/60000 (28%)]\tDiscriminator Loss: 0.445761\tGenerator Loss: 1.539247\n",
      "Train Epoch: 89 [17920/60000 (30%)]\tDiscriminator Loss: 0.506304\tGenerator Loss: 1.890618\n",
      "Train Epoch: 89 [19200/60000 (32%)]\tDiscriminator Loss: 0.486083\tGenerator Loss: 1.386698\n",
      "Train Epoch: 89 [20480/60000 (34%)]\tDiscriminator Loss: 0.511669\tGenerator Loss: 1.154602\n",
      "Train Epoch: 89 [21760/60000 (36%)]\tDiscriminator Loss: 0.423367\tGenerator Loss: 1.446505\n",
      "Train Epoch: 89 [23040/60000 (38%)]\tDiscriminator Loss: 0.463298\tGenerator Loss: 1.005263\n",
      "Train Epoch: 89 [24320/60000 (41%)]\tDiscriminator Loss: 0.473501\tGenerator Loss: 1.762208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 89 [25600/60000 (43%)]\tDiscriminator Loss: 0.451093\tGenerator Loss: 1.593490\n",
      "Train Epoch: 89 [26880/60000 (45%)]\tDiscriminator Loss: 0.439535\tGenerator Loss: 1.607917\n",
      "Train Epoch: 89 [28160/60000 (47%)]\tDiscriminator Loss: 0.501077\tGenerator Loss: 1.533305\n",
      "Train Epoch: 89 [29440/60000 (49%)]\tDiscriminator Loss: 0.456877\tGenerator Loss: 1.365448\n",
      "Train Epoch: 89 [30720/60000 (51%)]\tDiscriminator Loss: 0.519309\tGenerator Loss: 1.396572\n",
      "Train Epoch: 89 [32000/60000 (53%)]\tDiscriminator Loss: 0.460839\tGenerator Loss: 1.321685\n",
      "Train Epoch: 89 [33280/60000 (55%)]\tDiscriminator Loss: 0.499820\tGenerator Loss: 1.226456\n",
      "Train Epoch: 89 [34560/60000 (58%)]\tDiscriminator Loss: 0.474322\tGenerator Loss: 1.059532\n",
      "Train Epoch: 89 [35840/60000 (60%)]\tDiscriminator Loss: 0.471878\tGenerator Loss: 1.592540\n",
      "Train Epoch: 89 [37120/60000 (62%)]\tDiscriminator Loss: 0.429493\tGenerator Loss: 1.369504\n",
      "Train Epoch: 89 [38400/60000 (64%)]\tDiscriminator Loss: 0.483566\tGenerator Loss: 1.434374\n",
      "Train Epoch: 89 [39680/60000 (66%)]\tDiscriminator Loss: 0.474869\tGenerator Loss: 1.072826\n",
      "Train Epoch: 89 [40960/60000 (68%)]\tDiscriminator Loss: 0.485736\tGenerator Loss: 1.461099\n",
      "Train Epoch: 89 [42240/60000 (70%)]\tDiscriminator Loss: 0.516677\tGenerator Loss: 1.366959\n",
      "Train Epoch: 89 [43520/60000 (72%)]\tDiscriminator Loss: 0.454586\tGenerator Loss: 1.352544\n",
      "Train Epoch: 89 [44800/60000 (75%)]\tDiscriminator Loss: 0.464137\tGenerator Loss: 1.202800\n",
      "Train Epoch: 89 [46080/60000 (77%)]\tDiscriminator Loss: 0.490855\tGenerator Loss: 1.928684\n",
      "Train Epoch: 89 [47360/60000 (79%)]\tDiscriminator Loss: 0.492751\tGenerator Loss: 1.349401\n",
      "Train Epoch: 89 [48640/60000 (81%)]\tDiscriminator Loss: 0.492412\tGenerator Loss: 1.074061\n",
      "Train Epoch: 89 [49920/60000 (83%)]\tDiscriminator Loss: 0.540557\tGenerator Loss: 1.389236\n",
      "Train Epoch: 89 [51200/60000 (85%)]\tDiscriminator Loss: 0.495738\tGenerator Loss: 1.298729\n",
      "Train Epoch: 89 [52480/60000 (87%)]\tDiscriminator Loss: 0.418073\tGenerator Loss: 1.542395\n",
      "Train Epoch: 89 [53760/60000 (90%)]\tDiscriminator Loss: 0.499601\tGenerator Loss: 1.504916\n",
      "Train Epoch: 89 [55040/60000 (92%)]\tDiscriminator Loss: 0.477102\tGenerator Loss: 1.269542\n",
      "Train Epoch: 89 [56320/60000 (94%)]\tDiscriminator Loss: 0.537971\tGenerator Loss: 1.211255\n",
      "Train Epoch: 89 [57600/60000 (96%)]\tDiscriminator Loss: 0.508784\tGenerator Loss: 1.668128\n",
      "Train Epoch: 89 [58880/60000 (98%)]\tDiscriminator Loss: 0.479168\tGenerator Loss: 1.264406\n",
      "Train Epoch: 90 [0/60000 (0%)]\tDiscriminator Loss: 0.459518\tGenerator Loss: 1.505932\n",
      "Train Epoch: 90 [1280/60000 (2%)]\tDiscriminator Loss: 0.430651\tGenerator Loss: 1.749296\n",
      "Train Epoch: 90 [2560/60000 (4%)]\tDiscriminator Loss: 0.455582\tGenerator Loss: 1.541342\n",
      "Train Epoch: 90 [3840/60000 (6%)]\tDiscriminator Loss: 0.484975\tGenerator Loss: 1.640253\n",
      "Train Epoch: 90 [5120/60000 (9%)]\tDiscriminator Loss: 0.525838\tGenerator Loss: 1.719080\n",
      "Train Epoch: 90 [6400/60000 (11%)]\tDiscriminator Loss: 0.475607\tGenerator Loss: 1.427265\n",
      "Train Epoch: 90 [7680/60000 (13%)]\tDiscriminator Loss: 0.463275\tGenerator Loss: 1.398244\n",
      "Train Epoch: 90 [8960/60000 (15%)]\tDiscriminator Loss: 0.486128\tGenerator Loss: 1.539632\n",
      "Train Epoch: 90 [10240/60000 (17%)]\tDiscriminator Loss: 0.517724\tGenerator Loss: 1.572902\n",
      "Train Epoch: 90 [11520/60000 (19%)]\tDiscriminator Loss: 0.492232\tGenerator Loss: 1.105491\n",
      "Train Epoch: 90 [12800/60000 (21%)]\tDiscriminator Loss: 0.453012\tGenerator Loss: 1.444045\n",
      "Train Epoch: 90 [14080/60000 (23%)]\tDiscriminator Loss: 0.416180\tGenerator Loss: 1.825827\n",
      "Train Epoch: 90 [15360/60000 (26%)]\tDiscriminator Loss: 0.532581\tGenerator Loss: 2.015228\n",
      "Train Epoch: 90 [16640/60000 (28%)]\tDiscriminator Loss: 0.433493\tGenerator Loss: 1.589061\n",
      "Train Epoch: 90 [17920/60000 (30%)]\tDiscriminator Loss: 0.479953\tGenerator Loss: 1.123308\n",
      "Train Epoch: 90 [19200/60000 (32%)]\tDiscriminator Loss: 0.472817\tGenerator Loss: 1.165727\n",
      "Train Epoch: 90 [20480/60000 (34%)]\tDiscriminator Loss: 0.521717\tGenerator Loss: 1.348449\n",
      "Train Epoch: 90 [21760/60000 (36%)]\tDiscriminator Loss: 0.469097\tGenerator Loss: 1.875664\n",
      "Train Epoch: 90 [23040/60000 (38%)]\tDiscriminator Loss: 0.451720\tGenerator Loss: 1.220716\n",
      "Train Epoch: 90 [24320/60000 (41%)]\tDiscriminator Loss: 0.494213\tGenerator Loss: 1.153905\n",
      "Train Epoch: 90 [25600/60000 (43%)]\tDiscriminator Loss: 0.467594\tGenerator Loss: 1.549494\n",
      "Train Epoch: 90 [26880/60000 (45%)]\tDiscriminator Loss: 0.516094\tGenerator Loss: 1.382685\n",
      "Train Epoch: 90 [28160/60000 (47%)]\tDiscriminator Loss: 0.502095\tGenerator Loss: 1.107656\n",
      "Train Epoch: 90 [29440/60000 (49%)]\tDiscriminator Loss: 0.513689\tGenerator Loss: 1.318319\n",
      "Train Epoch: 90 [30720/60000 (51%)]\tDiscriminator Loss: 0.444137\tGenerator Loss: 1.321477\n",
      "Train Epoch: 90 [32000/60000 (53%)]\tDiscriminator Loss: 0.469237\tGenerator Loss: 1.415896\n",
      "Train Epoch: 90 [33280/60000 (55%)]\tDiscriminator Loss: 0.527706\tGenerator Loss: 1.495076\n",
      "Train Epoch: 90 [34560/60000 (58%)]\tDiscriminator Loss: 0.500770\tGenerator Loss: 1.268872\n",
      "Train Epoch: 90 [35840/60000 (60%)]\tDiscriminator Loss: 0.486002\tGenerator Loss: 1.245599\n",
      "Train Epoch: 90 [37120/60000 (62%)]\tDiscriminator Loss: 0.478893\tGenerator Loss: 1.414059\n",
      "Train Epoch: 90 [38400/60000 (64%)]\tDiscriminator Loss: 0.436314\tGenerator Loss: 1.254749\n",
      "Train Epoch: 90 [39680/60000 (66%)]\tDiscriminator Loss: 0.459175\tGenerator Loss: 1.817754\n",
      "Train Epoch: 90 [40960/60000 (68%)]\tDiscriminator Loss: 0.469934\tGenerator Loss: 1.306117\n",
      "Train Epoch: 90 [42240/60000 (70%)]\tDiscriminator Loss: 0.473065\tGenerator Loss: 1.072601\n",
      "Train Epoch: 90 [43520/60000 (72%)]\tDiscriminator Loss: 0.490499\tGenerator Loss: 1.740663\n",
      "Train Epoch: 90 [44800/60000 (75%)]\tDiscriminator Loss: 0.439743\tGenerator Loss: 1.537875\n",
      "Train Epoch: 90 [46080/60000 (77%)]\tDiscriminator Loss: 0.451181\tGenerator Loss: 1.538762\n",
      "Train Epoch: 90 [47360/60000 (79%)]\tDiscriminator Loss: 0.592295\tGenerator Loss: 1.998729\n",
      "Train Epoch: 90 [48640/60000 (81%)]\tDiscriminator Loss: 0.431335\tGenerator Loss: 1.204434\n",
      "Train Epoch: 90 [49920/60000 (83%)]\tDiscriminator Loss: 0.467975\tGenerator Loss: 1.364223\n",
      "Train Epoch: 90 [51200/60000 (85%)]\tDiscriminator Loss: 0.432174\tGenerator Loss: 1.349380\n",
      "Train Epoch: 90 [52480/60000 (87%)]\tDiscriminator Loss: 0.476233\tGenerator Loss: 1.315542\n",
      "Train Epoch: 90 [53760/60000 (90%)]\tDiscriminator Loss: 0.514589\tGenerator Loss: 1.315011\n",
      "Train Epoch: 90 [55040/60000 (92%)]\tDiscriminator Loss: 0.508087\tGenerator Loss: 1.736212\n",
      "Train Epoch: 90 [56320/60000 (94%)]\tDiscriminator Loss: 0.433409\tGenerator Loss: 1.462907\n",
      "Train Epoch: 90 [57600/60000 (96%)]\tDiscriminator Loss: 0.519812\tGenerator Loss: 1.774796\n",
      "Train Epoch: 90 [58880/60000 (98%)]\tDiscriminator Loss: 0.490944\tGenerator Loss: 1.336630\n",
      "Train Epoch: 91 [0/60000 (0%)]\tDiscriminator Loss: 0.457058\tGenerator Loss: 1.332767\n",
      "Train Epoch: 91 [1280/60000 (2%)]\tDiscriminator Loss: 0.419069\tGenerator Loss: 1.481290\n",
      "Train Epoch: 91 [2560/60000 (4%)]\tDiscriminator Loss: 0.477966\tGenerator Loss: 1.374869\n",
      "Train Epoch: 91 [3840/60000 (6%)]\tDiscriminator Loss: 0.492655\tGenerator Loss: 1.760395\n",
      "Train Epoch: 91 [5120/60000 (9%)]\tDiscriminator Loss: 0.428445\tGenerator Loss: 1.811219\n",
      "Train Epoch: 91 [6400/60000 (11%)]\tDiscriminator Loss: 0.473248\tGenerator Loss: 1.518330\n",
      "Train Epoch: 91 [7680/60000 (13%)]\tDiscriminator Loss: 0.521885\tGenerator Loss: 1.382944\n",
      "Train Epoch: 91 [8960/60000 (15%)]\tDiscriminator Loss: 0.481101\tGenerator Loss: 1.286154\n",
      "Train Epoch: 91 [10240/60000 (17%)]\tDiscriminator Loss: 0.459391\tGenerator Loss: 1.448549\n",
      "Train Epoch: 91 [11520/60000 (19%)]\tDiscriminator Loss: 0.473756\tGenerator Loss: 0.990171\n",
      "Train Epoch: 91 [12800/60000 (21%)]\tDiscriminator Loss: 0.450482\tGenerator Loss: 1.538393\n",
      "Train Epoch: 91 [14080/60000 (23%)]\tDiscriminator Loss: 0.591853\tGenerator Loss: 1.369498\n",
      "Train Epoch: 91 [15360/60000 (26%)]\tDiscriminator Loss: 0.441239\tGenerator Loss: 1.714456\n",
      "Train Epoch: 91 [16640/60000 (28%)]\tDiscriminator Loss: 0.482842\tGenerator Loss: 1.365396\n",
      "Train Epoch: 91 [17920/60000 (30%)]\tDiscriminator Loss: 0.451004\tGenerator Loss: 1.642490\n",
      "Train Epoch: 91 [19200/60000 (32%)]\tDiscriminator Loss: 0.535476\tGenerator Loss: 1.113311\n",
      "Train Epoch: 91 [20480/60000 (34%)]\tDiscriminator Loss: 0.467633\tGenerator Loss: 2.218554\n",
      "Train Epoch: 91 [21760/60000 (36%)]\tDiscriminator Loss: 0.457751\tGenerator Loss: 1.485982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 91 [23040/60000 (38%)]\tDiscriminator Loss: 0.545489\tGenerator Loss: 1.002796\n",
      "Train Epoch: 91 [24320/60000 (41%)]\tDiscriminator Loss: 0.520519\tGenerator Loss: 1.801974\n",
      "Train Epoch: 91 [25600/60000 (43%)]\tDiscriminator Loss: 0.617522\tGenerator Loss: 1.535082\n",
      "Train Epoch: 91 [26880/60000 (45%)]\tDiscriminator Loss: 0.430098\tGenerator Loss: 1.457161\n",
      "Train Epoch: 91 [28160/60000 (47%)]\tDiscriminator Loss: 0.452150\tGenerator Loss: 1.795238\n",
      "Train Epoch: 91 [29440/60000 (49%)]\tDiscriminator Loss: 0.499118\tGenerator Loss: 1.175824\n",
      "Train Epoch: 91 [30720/60000 (51%)]\tDiscriminator Loss: 0.535119\tGenerator Loss: 1.474549\n",
      "Train Epoch: 91 [32000/60000 (53%)]\tDiscriminator Loss: 0.473435\tGenerator Loss: 1.657811\n",
      "Train Epoch: 91 [33280/60000 (55%)]\tDiscriminator Loss: 0.478592\tGenerator Loss: 1.075420\n",
      "Train Epoch: 91 [34560/60000 (58%)]\tDiscriminator Loss: 0.514350\tGenerator Loss: 1.532362\n",
      "Train Epoch: 91 [35840/60000 (60%)]\tDiscriminator Loss: 0.492237\tGenerator Loss: 1.217498\n",
      "Train Epoch: 91 [37120/60000 (62%)]\tDiscriminator Loss: 0.515986\tGenerator Loss: 1.490009\n",
      "Train Epoch: 91 [38400/60000 (64%)]\tDiscriminator Loss: 0.453073\tGenerator Loss: 1.421398\n",
      "Train Epoch: 91 [39680/60000 (66%)]\tDiscriminator Loss: 0.519140\tGenerator Loss: 1.247963\n",
      "Train Epoch: 91 [40960/60000 (68%)]\tDiscriminator Loss: 0.495791\tGenerator Loss: 1.275124\n",
      "Train Epoch: 91 [42240/60000 (70%)]\tDiscriminator Loss: 0.521611\tGenerator Loss: 1.355614\n",
      "Train Epoch: 91 [43520/60000 (72%)]\tDiscriminator Loss: 0.468078\tGenerator Loss: 1.483855\n",
      "Train Epoch: 91 [44800/60000 (75%)]\tDiscriminator Loss: 0.464999\tGenerator Loss: 1.406245\n",
      "Train Epoch: 91 [46080/60000 (77%)]\tDiscriminator Loss: 0.478957\tGenerator Loss: 1.421779\n",
      "Train Epoch: 91 [47360/60000 (79%)]\tDiscriminator Loss: 0.520845\tGenerator Loss: 1.223808\n",
      "Train Epoch: 91 [48640/60000 (81%)]\tDiscriminator Loss: 0.540085\tGenerator Loss: 1.060444\n",
      "Train Epoch: 91 [49920/60000 (83%)]\tDiscriminator Loss: 0.393517\tGenerator Loss: 1.639363\n",
      "Train Epoch: 91 [51200/60000 (85%)]\tDiscriminator Loss: 0.454666\tGenerator Loss: 1.121391\n",
      "Train Epoch: 91 [52480/60000 (87%)]\tDiscriminator Loss: 0.474672\tGenerator Loss: 1.119674\n",
      "Train Epoch: 91 [53760/60000 (90%)]\tDiscriminator Loss: 0.463885\tGenerator Loss: 1.319816\n",
      "Train Epoch: 91 [55040/60000 (92%)]\tDiscriminator Loss: 0.531616\tGenerator Loss: 1.325152\n",
      "Train Epoch: 91 [56320/60000 (94%)]\tDiscriminator Loss: 0.477921\tGenerator Loss: 1.195051\n",
      "Train Epoch: 91 [57600/60000 (96%)]\tDiscriminator Loss: 0.417286\tGenerator Loss: 1.633980\n",
      "Train Epoch: 91 [58880/60000 (98%)]\tDiscriminator Loss: 0.454067\tGenerator Loss: 1.190373\n",
      "Train Epoch: 92 [0/60000 (0%)]\tDiscriminator Loss: 0.476632\tGenerator Loss: 1.732704\n",
      "Train Epoch: 92 [1280/60000 (2%)]\tDiscriminator Loss: 0.445219\tGenerator Loss: 1.278644\n",
      "Train Epoch: 92 [2560/60000 (4%)]\tDiscriminator Loss: 0.511625\tGenerator Loss: 0.970530\n",
      "Train Epoch: 92 [3840/60000 (6%)]\tDiscriminator Loss: 0.544554\tGenerator Loss: 1.696672\n",
      "Train Epoch: 92 [5120/60000 (9%)]\tDiscriminator Loss: 0.523663\tGenerator Loss: 1.112640\n",
      "Train Epoch: 92 [6400/60000 (11%)]\tDiscriminator Loss: 0.424132\tGenerator Loss: 1.311798\n",
      "Train Epoch: 92 [7680/60000 (13%)]\tDiscriminator Loss: 0.443682\tGenerator Loss: 1.722400\n",
      "Train Epoch: 92 [8960/60000 (15%)]\tDiscriminator Loss: 0.456103\tGenerator Loss: 1.261823\n",
      "Train Epoch: 92 [10240/60000 (17%)]\tDiscriminator Loss: 0.435549\tGenerator Loss: 1.284323\n",
      "Train Epoch: 92 [11520/60000 (19%)]\tDiscriminator Loss: 0.546792\tGenerator Loss: 1.809458\n",
      "Train Epoch: 92 [12800/60000 (21%)]\tDiscriminator Loss: 0.457824\tGenerator Loss: 1.516147\n",
      "Train Epoch: 92 [14080/60000 (23%)]\tDiscriminator Loss: 0.442979\tGenerator Loss: 1.408831\n",
      "Train Epoch: 92 [15360/60000 (26%)]\tDiscriminator Loss: 0.516041\tGenerator Loss: 1.372493\n",
      "Train Epoch: 92 [16640/60000 (28%)]\tDiscriminator Loss: 0.417052\tGenerator Loss: 1.635039\n",
      "Train Epoch: 92 [17920/60000 (30%)]\tDiscriminator Loss: 0.452285\tGenerator Loss: 1.391152\n",
      "Train Epoch: 92 [19200/60000 (32%)]\tDiscriminator Loss: 0.490820\tGenerator Loss: 1.389769\n",
      "Train Epoch: 92 [20480/60000 (34%)]\tDiscriminator Loss: 0.457188\tGenerator Loss: 1.713106\n",
      "Train Epoch: 92 [21760/60000 (36%)]\tDiscriminator Loss: 0.526138\tGenerator Loss: 1.184467\n",
      "Train Epoch: 92 [23040/60000 (38%)]\tDiscriminator Loss: 0.484914\tGenerator Loss: 1.608003\n",
      "Train Epoch: 92 [24320/60000 (41%)]\tDiscriminator Loss: 0.495031\tGenerator Loss: 1.620510\n",
      "Train Epoch: 92 [25600/60000 (43%)]\tDiscriminator Loss: 0.506980\tGenerator Loss: 1.150128\n",
      "Train Epoch: 92 [26880/60000 (45%)]\tDiscriminator Loss: 0.505632\tGenerator Loss: 1.038149\n",
      "Train Epoch: 92 [28160/60000 (47%)]\tDiscriminator Loss: 0.538140\tGenerator Loss: 1.574995\n",
      "Train Epoch: 92 [29440/60000 (49%)]\tDiscriminator Loss: 0.464362\tGenerator Loss: 1.583702\n",
      "Train Epoch: 92 [30720/60000 (51%)]\tDiscriminator Loss: 0.452755\tGenerator Loss: 1.251211\n",
      "Train Epoch: 92 [32000/60000 (53%)]\tDiscriminator Loss: 0.477580\tGenerator Loss: 1.388008\n",
      "Train Epoch: 92 [33280/60000 (55%)]\tDiscriminator Loss: 0.473974\tGenerator Loss: 1.402120\n",
      "Train Epoch: 92 [34560/60000 (58%)]\tDiscriminator Loss: 0.408581\tGenerator Loss: 1.767146\n",
      "Train Epoch: 92 [35840/60000 (60%)]\tDiscriminator Loss: 0.502967\tGenerator Loss: 1.400013\n",
      "Train Epoch: 92 [37120/60000 (62%)]\tDiscriminator Loss: 0.449348\tGenerator Loss: 1.449455\n",
      "Train Epoch: 92 [38400/60000 (64%)]\tDiscriminator Loss: 0.445572\tGenerator Loss: 1.150051\n",
      "Train Epoch: 92 [39680/60000 (66%)]\tDiscriminator Loss: 0.405955\tGenerator Loss: 1.477259\n",
      "Train Epoch: 92 [40960/60000 (68%)]\tDiscriminator Loss: 0.478689\tGenerator Loss: 1.645025\n",
      "Train Epoch: 92 [42240/60000 (70%)]\tDiscriminator Loss: 0.494433\tGenerator Loss: 1.114122\n",
      "Train Epoch: 92 [43520/60000 (72%)]\tDiscriminator Loss: 0.468706\tGenerator Loss: 1.465053\n",
      "Train Epoch: 92 [44800/60000 (75%)]\tDiscriminator Loss: 0.454469\tGenerator Loss: 1.275555\n",
      "Train Epoch: 92 [46080/60000 (77%)]\tDiscriminator Loss: 0.458193\tGenerator Loss: 1.474355\n",
      "Train Epoch: 92 [47360/60000 (79%)]\tDiscriminator Loss: 0.436370\tGenerator Loss: 1.355971\n",
      "Train Epoch: 92 [48640/60000 (81%)]\tDiscriminator Loss: 0.499115\tGenerator Loss: 1.772730\n",
      "Train Epoch: 92 [49920/60000 (83%)]\tDiscriminator Loss: 0.498008\tGenerator Loss: 1.405624\n",
      "Train Epoch: 92 [51200/60000 (85%)]\tDiscriminator Loss: 0.466509\tGenerator Loss: 1.496021\n",
      "Train Epoch: 92 [52480/60000 (87%)]\tDiscriminator Loss: 0.478754\tGenerator Loss: 1.560085\n",
      "Train Epoch: 92 [53760/60000 (90%)]\tDiscriminator Loss: 0.472259\tGenerator Loss: 1.237172\n",
      "Train Epoch: 92 [55040/60000 (92%)]\tDiscriminator Loss: 0.447375\tGenerator Loss: 1.451429\n",
      "Train Epoch: 92 [56320/60000 (94%)]\tDiscriminator Loss: 0.436345\tGenerator Loss: 1.541114\n",
      "Train Epoch: 92 [57600/60000 (96%)]\tDiscriminator Loss: 0.434370\tGenerator Loss: 1.792903\n",
      "Train Epoch: 92 [58880/60000 (98%)]\tDiscriminator Loss: 0.457837\tGenerator Loss: 1.707024\n",
      "Train Epoch: 93 [0/60000 (0%)]\tDiscriminator Loss: 0.440017\tGenerator Loss: 1.323291\n",
      "Train Epoch: 93 [1280/60000 (2%)]\tDiscriminator Loss: 0.517200\tGenerator Loss: 1.585795\n",
      "Train Epoch: 93 [2560/60000 (4%)]\tDiscriminator Loss: 0.484199\tGenerator Loss: 1.222581\n",
      "Train Epoch: 93 [3840/60000 (6%)]\tDiscriminator Loss: 0.461723\tGenerator Loss: 1.496688\n",
      "Train Epoch: 93 [5120/60000 (9%)]\tDiscriminator Loss: 0.455349\tGenerator Loss: 1.625376\n",
      "Train Epoch: 93 [6400/60000 (11%)]\tDiscriminator Loss: 0.477543\tGenerator Loss: 1.431839\n",
      "Train Epoch: 93 [7680/60000 (13%)]\tDiscriminator Loss: 0.465286\tGenerator Loss: 1.712927\n",
      "Train Epoch: 93 [8960/60000 (15%)]\tDiscriminator Loss: 0.493581\tGenerator Loss: 0.925321\n",
      "Train Epoch: 93 [10240/60000 (17%)]\tDiscriminator Loss: 0.478468\tGenerator Loss: 1.421060\n",
      "Train Epoch: 93 [11520/60000 (19%)]\tDiscriminator Loss: 0.488695\tGenerator Loss: 1.754714\n",
      "Train Epoch: 93 [12800/60000 (21%)]\tDiscriminator Loss: 0.452912\tGenerator Loss: 1.641143\n",
      "Train Epoch: 93 [14080/60000 (23%)]\tDiscriminator Loss: 0.499866\tGenerator Loss: 1.323215\n",
      "Train Epoch: 93 [15360/60000 (26%)]\tDiscriminator Loss: 0.486583\tGenerator Loss: 1.377059\n",
      "Train Epoch: 93 [16640/60000 (28%)]\tDiscriminator Loss: 0.495616\tGenerator Loss: 1.316571\n",
      "Train Epoch: 93 [17920/60000 (30%)]\tDiscriminator Loss: 0.452644\tGenerator Loss: 1.211946\n",
      "Train Epoch: 93 [19200/60000 (32%)]\tDiscriminator Loss: 0.473700\tGenerator Loss: 1.823410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 93 [20480/60000 (34%)]\tDiscriminator Loss: 0.421013\tGenerator Loss: 1.536371\n",
      "Train Epoch: 93 [21760/60000 (36%)]\tDiscriminator Loss: 0.451631\tGenerator Loss: 1.470450\n",
      "Train Epoch: 93 [23040/60000 (38%)]\tDiscriminator Loss: 0.529785\tGenerator Loss: 1.446685\n",
      "Train Epoch: 93 [24320/60000 (41%)]\tDiscriminator Loss: 0.529178\tGenerator Loss: 1.009386\n",
      "Train Epoch: 93 [25600/60000 (43%)]\tDiscriminator Loss: 0.489490\tGenerator Loss: 1.452364\n",
      "Train Epoch: 93 [26880/60000 (45%)]\tDiscriminator Loss: 0.520966\tGenerator Loss: 1.258149\n",
      "Train Epoch: 93 [28160/60000 (47%)]\tDiscriminator Loss: 0.417938\tGenerator Loss: 1.675033\n",
      "Train Epoch: 93 [29440/60000 (49%)]\tDiscriminator Loss: 0.444050\tGenerator Loss: 1.330121\n",
      "Train Epoch: 93 [30720/60000 (51%)]\tDiscriminator Loss: 0.452644\tGenerator Loss: 1.699210\n",
      "Train Epoch: 93 [32000/60000 (53%)]\tDiscriminator Loss: 0.466892\tGenerator Loss: 1.289161\n",
      "Train Epoch: 93 [33280/60000 (55%)]\tDiscriminator Loss: 0.430750\tGenerator Loss: 1.498166\n",
      "Train Epoch: 93 [34560/60000 (58%)]\tDiscriminator Loss: 0.479466\tGenerator Loss: 1.468462\n",
      "Train Epoch: 93 [35840/60000 (60%)]\tDiscriminator Loss: 0.548348\tGenerator Loss: 1.219612\n",
      "Train Epoch: 93 [37120/60000 (62%)]\tDiscriminator Loss: 0.466876\tGenerator Loss: 1.277507\n",
      "Train Epoch: 93 [38400/60000 (64%)]\tDiscriminator Loss: 0.480524\tGenerator Loss: 1.236649\n",
      "Train Epoch: 93 [39680/60000 (66%)]\tDiscriminator Loss: 0.467338\tGenerator Loss: 1.598571\n",
      "Train Epoch: 93 [40960/60000 (68%)]\tDiscriminator Loss: 0.596313\tGenerator Loss: 1.183059\n",
      "Train Epoch: 93 [42240/60000 (70%)]\tDiscriminator Loss: 0.418347\tGenerator Loss: 1.260375\n",
      "Train Epoch: 93 [43520/60000 (72%)]\tDiscriminator Loss: 0.441759\tGenerator Loss: 1.332273\n",
      "Train Epoch: 93 [44800/60000 (75%)]\tDiscriminator Loss: 0.483127\tGenerator Loss: 1.345130\n",
      "Train Epoch: 93 [46080/60000 (77%)]\tDiscriminator Loss: 0.538719\tGenerator Loss: 1.191341\n",
      "Train Epoch: 93 [47360/60000 (79%)]\tDiscriminator Loss: 0.524229\tGenerator Loss: 1.570950\n",
      "Train Epoch: 93 [48640/60000 (81%)]\tDiscriminator Loss: 0.483123\tGenerator Loss: 1.606507\n",
      "Train Epoch: 93 [49920/60000 (83%)]\tDiscriminator Loss: 0.496254\tGenerator Loss: 1.216305\n",
      "Train Epoch: 93 [51200/60000 (85%)]\tDiscriminator Loss: 0.450891\tGenerator Loss: 1.315988\n",
      "Train Epoch: 93 [52480/60000 (87%)]\tDiscriminator Loss: 0.515605\tGenerator Loss: 1.754407\n",
      "Train Epoch: 93 [53760/60000 (90%)]\tDiscriminator Loss: 0.512728\tGenerator Loss: 0.992818\n",
      "Train Epoch: 93 [55040/60000 (92%)]\tDiscriminator Loss: 0.460423\tGenerator Loss: 1.631213\n",
      "Train Epoch: 93 [56320/60000 (94%)]\tDiscriminator Loss: 0.430559\tGenerator Loss: 1.463329\n",
      "Train Epoch: 93 [57600/60000 (96%)]\tDiscriminator Loss: 0.451487\tGenerator Loss: 1.528018\n",
      "Train Epoch: 93 [58880/60000 (98%)]\tDiscriminator Loss: 0.503341\tGenerator Loss: 1.290507\n",
      "Train Epoch: 94 [0/60000 (0%)]\tDiscriminator Loss: 0.496332\tGenerator Loss: 1.321587\n",
      "Train Epoch: 94 [1280/60000 (2%)]\tDiscriminator Loss: 0.550623\tGenerator Loss: 1.833276\n",
      "Train Epoch: 94 [2560/60000 (4%)]\tDiscriminator Loss: 0.462970\tGenerator Loss: 1.528097\n",
      "Train Epoch: 94 [3840/60000 (6%)]\tDiscriminator Loss: 0.472657\tGenerator Loss: 1.249216\n",
      "Train Epoch: 94 [5120/60000 (9%)]\tDiscriminator Loss: 0.503563\tGenerator Loss: 1.271333\n",
      "Train Epoch: 94 [6400/60000 (11%)]\tDiscriminator Loss: 0.505583\tGenerator Loss: 1.252975\n",
      "Train Epoch: 94 [7680/60000 (13%)]\tDiscriminator Loss: 0.441459\tGenerator Loss: 1.593417\n",
      "Train Epoch: 94 [8960/60000 (15%)]\tDiscriminator Loss: 0.436682\tGenerator Loss: 1.451898\n",
      "Train Epoch: 94 [10240/60000 (17%)]\tDiscriminator Loss: 0.404378\tGenerator Loss: 1.550674\n",
      "Train Epoch: 94 [11520/60000 (19%)]\tDiscriminator Loss: 0.432802\tGenerator Loss: 1.572589\n",
      "Train Epoch: 94 [12800/60000 (21%)]\tDiscriminator Loss: 0.511676\tGenerator Loss: 2.020999\n",
      "Train Epoch: 94 [14080/60000 (23%)]\tDiscriminator Loss: 0.493309\tGenerator Loss: 1.074059\n",
      "Train Epoch: 94 [15360/60000 (26%)]\tDiscriminator Loss: 0.444638\tGenerator Loss: 1.333995\n",
      "Train Epoch: 94 [16640/60000 (28%)]\tDiscriminator Loss: 0.449568\tGenerator Loss: 1.544583\n",
      "Train Epoch: 94 [17920/60000 (30%)]\tDiscriminator Loss: 0.450931\tGenerator Loss: 1.665880\n",
      "Train Epoch: 94 [19200/60000 (32%)]\tDiscriminator Loss: 0.455751\tGenerator Loss: 1.593127\n",
      "Train Epoch: 94 [20480/60000 (34%)]\tDiscriminator Loss: 0.424785\tGenerator Loss: 1.273754\n",
      "Train Epoch: 94 [21760/60000 (36%)]\tDiscriminator Loss: 0.444345\tGenerator Loss: 1.280616\n",
      "Train Epoch: 94 [23040/60000 (38%)]\tDiscriminator Loss: 0.461787\tGenerator Loss: 1.554271\n",
      "Train Epoch: 94 [24320/60000 (41%)]\tDiscriminator Loss: 0.456391\tGenerator Loss: 1.606245\n",
      "Train Epoch: 94 [25600/60000 (43%)]\tDiscriminator Loss: 0.537984\tGenerator Loss: 1.218961\n",
      "Train Epoch: 94 [26880/60000 (45%)]\tDiscriminator Loss: 0.456969\tGenerator Loss: 1.359598\n",
      "Train Epoch: 94 [28160/60000 (47%)]\tDiscriminator Loss: 0.511135\tGenerator Loss: 1.733747\n",
      "Train Epoch: 94 [29440/60000 (49%)]\tDiscriminator Loss: 0.482082\tGenerator Loss: 1.348843\n",
      "Train Epoch: 94 [30720/60000 (51%)]\tDiscriminator Loss: 0.502435\tGenerator Loss: 1.516284\n",
      "Train Epoch: 94 [32000/60000 (53%)]\tDiscriminator Loss: 0.489675\tGenerator Loss: 1.153341\n",
      "Train Epoch: 94 [33280/60000 (55%)]\tDiscriminator Loss: 0.467728\tGenerator Loss: 1.606826\n",
      "Train Epoch: 94 [34560/60000 (58%)]\tDiscriminator Loss: 0.432941\tGenerator Loss: 1.665643\n",
      "Train Epoch: 94 [35840/60000 (60%)]\tDiscriminator Loss: 0.552850\tGenerator Loss: 0.938107\n",
      "Train Epoch: 94 [37120/60000 (62%)]\tDiscriminator Loss: 0.515854\tGenerator Loss: 1.723192\n",
      "Train Epoch: 94 [38400/60000 (64%)]\tDiscriminator Loss: 0.495022\tGenerator Loss: 1.184965\n",
      "Train Epoch: 94 [39680/60000 (66%)]\tDiscriminator Loss: 0.450164\tGenerator Loss: 1.766810\n",
      "Train Epoch: 94 [40960/60000 (68%)]\tDiscriminator Loss: 0.459811\tGenerator Loss: 1.901825\n",
      "Train Epoch: 94 [42240/60000 (70%)]\tDiscriminator Loss: 0.436215\tGenerator Loss: 1.446354\n",
      "Train Epoch: 94 [43520/60000 (72%)]\tDiscriminator Loss: 0.445891\tGenerator Loss: 1.564882\n",
      "Train Epoch: 94 [44800/60000 (75%)]\tDiscriminator Loss: 0.469477\tGenerator Loss: 1.181743\n",
      "Train Epoch: 94 [46080/60000 (77%)]\tDiscriminator Loss: 0.495069\tGenerator Loss: 1.538261\n",
      "Train Epoch: 94 [47360/60000 (79%)]\tDiscriminator Loss: 0.481381\tGenerator Loss: 1.100764\n",
      "Train Epoch: 94 [48640/60000 (81%)]\tDiscriminator Loss: 0.461343\tGenerator Loss: 1.406659\n",
      "Train Epoch: 94 [49920/60000 (83%)]\tDiscriminator Loss: 0.502255\tGenerator Loss: 1.727336\n",
      "Train Epoch: 94 [51200/60000 (85%)]\tDiscriminator Loss: 0.555692\tGenerator Loss: 0.845677\n",
      "Train Epoch: 94 [52480/60000 (87%)]\tDiscriminator Loss: 0.446570\tGenerator Loss: 1.210752\n",
      "Train Epoch: 94 [53760/60000 (90%)]\tDiscriminator Loss: 0.436114\tGenerator Loss: 1.456832\n",
      "Train Epoch: 94 [55040/60000 (92%)]\tDiscriminator Loss: 0.451238\tGenerator Loss: 1.474302\n",
      "Train Epoch: 94 [56320/60000 (94%)]\tDiscriminator Loss: 0.461121\tGenerator Loss: 1.508046\n",
      "Train Epoch: 94 [57600/60000 (96%)]\tDiscriminator Loss: 0.458648\tGenerator Loss: 1.436239\n",
      "Train Epoch: 94 [58880/60000 (98%)]\tDiscriminator Loss: 0.480022\tGenerator Loss: 1.841745\n",
      "Train Epoch: 95 [0/60000 (0%)]\tDiscriminator Loss: 0.536513\tGenerator Loss: 0.977924\n",
      "Train Epoch: 95 [1280/60000 (2%)]\tDiscriminator Loss: 0.441998\tGenerator Loss: 1.427663\n",
      "Train Epoch: 95 [2560/60000 (4%)]\tDiscriminator Loss: 0.437088\tGenerator Loss: 1.360171\n",
      "Train Epoch: 95 [3840/60000 (6%)]\tDiscriminator Loss: 0.479616\tGenerator Loss: 1.682433\n",
      "Train Epoch: 95 [5120/60000 (9%)]\tDiscriminator Loss: 0.405111\tGenerator Loss: 1.779896\n",
      "Train Epoch: 95 [6400/60000 (11%)]\tDiscriminator Loss: 0.557905\tGenerator Loss: 0.901824\n",
      "Train Epoch: 95 [7680/60000 (13%)]\tDiscriminator Loss: 0.413022\tGenerator Loss: 1.694829\n",
      "Train Epoch: 95 [8960/60000 (15%)]\tDiscriminator Loss: 0.511567\tGenerator Loss: 1.398136\n",
      "Train Epoch: 95 [10240/60000 (17%)]\tDiscriminator Loss: 0.466744\tGenerator Loss: 1.418180\n",
      "Train Epoch: 95 [11520/60000 (19%)]\tDiscriminator Loss: 0.511999\tGenerator Loss: 1.200385\n",
      "Train Epoch: 95 [12800/60000 (21%)]\tDiscriminator Loss: 0.413820\tGenerator Loss: 1.613410\n",
      "Train Epoch: 95 [14080/60000 (23%)]\tDiscriminator Loss: 0.500497\tGenerator Loss: 1.354310\n",
      "Train Epoch: 95 [15360/60000 (26%)]\tDiscriminator Loss: 0.528979\tGenerator Loss: 0.863994\n",
      "Train Epoch: 95 [16640/60000 (28%)]\tDiscriminator Loss: 0.463084\tGenerator Loss: 1.512805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 95 [17920/60000 (30%)]\tDiscriminator Loss: 0.468051\tGenerator Loss: 1.488560\n",
      "Train Epoch: 95 [19200/60000 (32%)]\tDiscriminator Loss: 0.497011\tGenerator Loss: 1.556756\n",
      "Train Epoch: 95 [20480/60000 (34%)]\tDiscriminator Loss: 0.438239\tGenerator Loss: 1.267560\n",
      "Train Epoch: 95 [21760/60000 (36%)]\tDiscriminator Loss: 0.438942\tGenerator Loss: 1.305651\n",
      "Train Epoch: 95 [23040/60000 (38%)]\tDiscriminator Loss: 0.502771\tGenerator Loss: 1.447619\n",
      "Train Epoch: 95 [24320/60000 (41%)]\tDiscriminator Loss: 0.448542\tGenerator Loss: 1.365831\n",
      "Train Epoch: 95 [25600/60000 (43%)]\tDiscriminator Loss: 0.483170\tGenerator Loss: 1.431427\n",
      "Train Epoch: 95 [26880/60000 (45%)]\tDiscriminator Loss: 0.427949\tGenerator Loss: 1.430919\n",
      "Train Epoch: 95 [28160/60000 (47%)]\tDiscriminator Loss: 0.480498\tGenerator Loss: 1.136993\n",
      "Train Epoch: 95 [29440/60000 (49%)]\tDiscriminator Loss: 0.462082\tGenerator Loss: 1.438333\n",
      "Train Epoch: 95 [30720/60000 (51%)]\tDiscriminator Loss: 0.473150\tGenerator Loss: 1.267324\n",
      "Train Epoch: 95 [32000/60000 (53%)]\tDiscriminator Loss: 0.418846\tGenerator Loss: 1.614616\n",
      "Train Epoch: 95 [33280/60000 (55%)]\tDiscriminator Loss: 0.421182\tGenerator Loss: 1.628667\n",
      "Train Epoch: 95 [34560/60000 (58%)]\tDiscriminator Loss: 0.452219\tGenerator Loss: 1.190551\n",
      "Train Epoch: 95 [35840/60000 (60%)]\tDiscriminator Loss: 0.445997\tGenerator Loss: 1.406477\n",
      "Train Epoch: 95 [37120/60000 (62%)]\tDiscriminator Loss: 0.492582\tGenerator Loss: 1.836907\n",
      "Train Epoch: 95 [38400/60000 (64%)]\tDiscriminator Loss: 0.464560\tGenerator Loss: 1.604403\n",
      "Train Epoch: 95 [39680/60000 (66%)]\tDiscriminator Loss: 0.498555\tGenerator Loss: 1.091643\n",
      "Train Epoch: 95 [40960/60000 (68%)]\tDiscriminator Loss: 0.510713\tGenerator Loss: 1.738292\n",
      "Train Epoch: 95 [42240/60000 (70%)]\tDiscriminator Loss: 0.444620\tGenerator Loss: 1.366149\n",
      "Train Epoch: 95 [43520/60000 (72%)]\tDiscriminator Loss: 0.468103\tGenerator Loss: 1.410085\n",
      "Train Epoch: 95 [44800/60000 (75%)]\tDiscriminator Loss: 0.421000\tGenerator Loss: 1.639466\n",
      "Train Epoch: 95 [46080/60000 (77%)]\tDiscriminator Loss: 0.471183\tGenerator Loss: 1.548388\n",
      "Train Epoch: 95 [47360/60000 (79%)]\tDiscriminator Loss: 0.478782\tGenerator Loss: 1.483484\n",
      "Train Epoch: 95 [48640/60000 (81%)]\tDiscriminator Loss: 0.461331\tGenerator Loss: 1.228894\n",
      "Train Epoch: 95 [49920/60000 (83%)]\tDiscriminator Loss: 0.474516\tGenerator Loss: 1.560313\n",
      "Train Epoch: 95 [51200/60000 (85%)]\tDiscriminator Loss: 0.474851\tGenerator Loss: 1.483374\n",
      "Train Epoch: 95 [52480/60000 (87%)]\tDiscriminator Loss: 0.520749\tGenerator Loss: 1.365586\n",
      "Train Epoch: 95 [53760/60000 (90%)]\tDiscriminator Loss: 0.439581\tGenerator Loss: 1.312822\n",
      "Train Epoch: 95 [55040/60000 (92%)]\tDiscriminator Loss: 0.521377\tGenerator Loss: 1.634162\n",
      "Train Epoch: 95 [56320/60000 (94%)]\tDiscriminator Loss: 0.468751\tGenerator Loss: 1.468672\n",
      "Train Epoch: 95 [57600/60000 (96%)]\tDiscriminator Loss: 0.494154\tGenerator Loss: 1.330666\n",
      "Train Epoch: 95 [58880/60000 (98%)]\tDiscriminator Loss: 0.458618\tGenerator Loss: 1.703525\n",
      "Train Epoch: 96 [0/60000 (0%)]\tDiscriminator Loss: 0.401781\tGenerator Loss: 1.671939\n",
      "Train Epoch: 96 [1280/60000 (2%)]\tDiscriminator Loss: 0.495737\tGenerator Loss: 1.135583\n",
      "Train Epoch: 96 [2560/60000 (4%)]\tDiscriminator Loss: 0.461463\tGenerator Loss: 1.743766\n",
      "Train Epoch: 96 [3840/60000 (6%)]\tDiscriminator Loss: 0.472075\tGenerator Loss: 1.644125\n",
      "Train Epoch: 96 [5120/60000 (9%)]\tDiscriminator Loss: 0.465877\tGenerator Loss: 1.703252\n",
      "Train Epoch: 96 [6400/60000 (11%)]\tDiscriminator Loss: 0.446179\tGenerator Loss: 1.527382\n",
      "Train Epoch: 96 [7680/60000 (13%)]\tDiscriminator Loss: 0.458885\tGenerator Loss: 1.700302\n",
      "Train Epoch: 96 [8960/60000 (15%)]\tDiscriminator Loss: 0.470377\tGenerator Loss: 1.492170\n",
      "Train Epoch: 96 [10240/60000 (17%)]\tDiscriminator Loss: 0.426089\tGenerator Loss: 1.425961\n",
      "Train Epoch: 96 [11520/60000 (19%)]\tDiscriminator Loss: 0.464024\tGenerator Loss: 1.361255\n",
      "Train Epoch: 96 [12800/60000 (21%)]\tDiscriminator Loss: 0.486817\tGenerator Loss: 1.157259\n",
      "Train Epoch: 96 [14080/60000 (23%)]\tDiscriminator Loss: 0.455862\tGenerator Loss: 1.102296\n",
      "Train Epoch: 96 [15360/60000 (26%)]\tDiscriminator Loss: 0.469641\tGenerator Loss: 1.697739\n",
      "Train Epoch: 96 [16640/60000 (28%)]\tDiscriminator Loss: 0.452704\tGenerator Loss: 1.641883\n",
      "Train Epoch: 96 [17920/60000 (30%)]\tDiscriminator Loss: 0.520444\tGenerator Loss: 1.924164\n",
      "Train Epoch: 96 [19200/60000 (32%)]\tDiscriminator Loss: 0.432606\tGenerator Loss: 1.497046\n",
      "Train Epoch: 96 [20480/60000 (34%)]\tDiscriminator Loss: 0.462938\tGenerator Loss: 1.491395\n",
      "Train Epoch: 96 [21760/60000 (36%)]\tDiscriminator Loss: 0.454847\tGenerator Loss: 1.682959\n",
      "Train Epoch: 96 [23040/60000 (38%)]\tDiscriminator Loss: 0.511875\tGenerator Loss: 1.198175\n",
      "Train Epoch: 96 [24320/60000 (41%)]\tDiscriminator Loss: 0.450125\tGenerator Loss: 1.581629\n",
      "Train Epoch: 96 [25600/60000 (43%)]\tDiscriminator Loss: 0.488567\tGenerator Loss: 1.626358\n",
      "Train Epoch: 96 [26880/60000 (45%)]\tDiscriminator Loss: 0.469136\tGenerator Loss: 1.352074\n",
      "Train Epoch: 96 [28160/60000 (47%)]\tDiscriminator Loss: 0.393492\tGenerator Loss: 1.585712\n",
      "Train Epoch: 96 [29440/60000 (49%)]\tDiscriminator Loss: 0.567684\tGenerator Loss: 1.165874\n",
      "Train Epoch: 96 [30720/60000 (51%)]\tDiscriminator Loss: 0.467201\tGenerator Loss: 1.316884\n",
      "Train Epoch: 96 [32000/60000 (53%)]\tDiscriminator Loss: 0.495213\tGenerator Loss: 1.243271\n",
      "Train Epoch: 96 [33280/60000 (55%)]\tDiscriminator Loss: 0.455642\tGenerator Loss: 1.483894\n",
      "Train Epoch: 96 [34560/60000 (58%)]\tDiscriminator Loss: 0.550006\tGenerator Loss: 0.987400\n",
      "Train Epoch: 96 [35840/60000 (60%)]\tDiscriminator Loss: 0.519556\tGenerator Loss: 1.295620\n",
      "Train Epoch: 96 [37120/60000 (62%)]\tDiscriminator Loss: 0.507845\tGenerator Loss: 1.194816\n",
      "Train Epoch: 96 [38400/60000 (64%)]\tDiscriminator Loss: 0.455779\tGenerator Loss: 1.780563\n",
      "Train Epoch: 96 [39680/60000 (66%)]\tDiscriminator Loss: 0.466757\tGenerator Loss: 1.622369\n",
      "Train Epoch: 96 [40960/60000 (68%)]\tDiscriminator Loss: 0.494827\tGenerator Loss: 1.660384\n",
      "Train Epoch: 96 [42240/60000 (70%)]\tDiscriminator Loss: 0.464008\tGenerator Loss: 1.257106\n",
      "Train Epoch: 96 [43520/60000 (72%)]\tDiscriminator Loss: 0.485689\tGenerator Loss: 1.703664\n",
      "Train Epoch: 96 [44800/60000 (75%)]\tDiscriminator Loss: 0.502076\tGenerator Loss: 1.287377\n",
      "Train Epoch: 96 [46080/60000 (77%)]\tDiscriminator Loss: 0.503453\tGenerator Loss: 1.816686\n",
      "Train Epoch: 96 [47360/60000 (79%)]\tDiscriminator Loss: 0.503412\tGenerator Loss: 1.655598\n",
      "Train Epoch: 96 [48640/60000 (81%)]\tDiscriminator Loss: 0.469170\tGenerator Loss: 1.515310\n",
      "Train Epoch: 96 [49920/60000 (83%)]\tDiscriminator Loss: 0.456365\tGenerator Loss: 1.458667\n",
      "Train Epoch: 96 [51200/60000 (85%)]\tDiscriminator Loss: 0.502126\tGenerator Loss: 1.711314\n",
      "Train Epoch: 96 [52480/60000 (87%)]\tDiscriminator Loss: 0.456887\tGenerator Loss: 1.417896\n",
      "Train Epoch: 96 [53760/60000 (90%)]\tDiscriminator Loss: 0.490625\tGenerator Loss: 1.342772\n",
      "Train Epoch: 96 [55040/60000 (92%)]\tDiscriminator Loss: 0.399495\tGenerator Loss: 1.388331\n",
      "Train Epoch: 96 [56320/60000 (94%)]\tDiscriminator Loss: 0.446856\tGenerator Loss: 1.391812\n",
      "Train Epoch: 96 [57600/60000 (96%)]\tDiscriminator Loss: 0.439155\tGenerator Loss: 1.686226\n",
      "Train Epoch: 96 [58880/60000 (98%)]\tDiscriminator Loss: 0.504068\tGenerator Loss: 1.582013\n",
      "Train Epoch: 97 [0/60000 (0%)]\tDiscriminator Loss: 0.623479\tGenerator Loss: 0.696110\n",
      "Train Epoch: 97 [1280/60000 (2%)]\tDiscriminator Loss: 0.470816\tGenerator Loss: 1.549609\n",
      "Train Epoch: 97 [2560/60000 (4%)]\tDiscriminator Loss: 0.432534\tGenerator Loss: 1.368998\n",
      "Train Epoch: 97 [3840/60000 (6%)]\tDiscriminator Loss: 0.473091\tGenerator Loss: 1.510919\n",
      "Train Epoch: 97 [5120/60000 (9%)]\tDiscriminator Loss: 0.568010\tGenerator Loss: 0.897797\n",
      "Train Epoch: 97 [6400/60000 (11%)]\tDiscriminator Loss: 0.442716\tGenerator Loss: 1.929036\n",
      "Train Epoch: 97 [7680/60000 (13%)]\tDiscriminator Loss: 0.449658\tGenerator Loss: 1.402828\n",
      "Train Epoch: 97 [8960/60000 (15%)]\tDiscriminator Loss: 0.453964\tGenerator Loss: 1.935808\n",
      "Train Epoch: 97 [10240/60000 (17%)]\tDiscriminator Loss: 0.528664\tGenerator Loss: 0.960886\n",
      "Train Epoch: 97 [11520/60000 (19%)]\tDiscriminator Loss: 0.443573\tGenerator Loss: 1.478291\n",
      "Train Epoch: 97 [12800/60000 (21%)]\tDiscriminator Loss: 0.457314\tGenerator Loss: 1.222727\n",
      "Train Epoch: 97 [14080/60000 (23%)]\tDiscriminator Loss: 0.504615\tGenerator Loss: 1.435194\n",
      "Train Epoch: 97 [15360/60000 (26%)]\tDiscriminator Loss: 0.516979\tGenerator Loss: 1.730278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 97 [16640/60000 (28%)]\tDiscriminator Loss: 0.474248\tGenerator Loss: 1.565450\n",
      "Train Epoch: 97 [17920/60000 (30%)]\tDiscriminator Loss: 0.463922\tGenerator Loss: 1.169676\n",
      "Train Epoch: 97 [19200/60000 (32%)]\tDiscriminator Loss: 0.463748\tGenerator Loss: 1.372529\n",
      "Train Epoch: 97 [20480/60000 (34%)]\tDiscriminator Loss: 0.428053\tGenerator Loss: 1.288376\n",
      "Train Epoch: 97 [21760/60000 (36%)]\tDiscriminator Loss: 0.474051\tGenerator Loss: 1.514064\n",
      "Train Epoch: 97 [23040/60000 (38%)]\tDiscriminator Loss: 0.471837\tGenerator Loss: 1.395544\n",
      "Train Epoch: 97 [24320/60000 (41%)]\tDiscriminator Loss: 0.424830\tGenerator Loss: 1.526950\n",
      "Train Epoch: 97 [25600/60000 (43%)]\tDiscriminator Loss: 0.537334\tGenerator Loss: 1.638917\n",
      "Train Epoch: 97 [26880/60000 (45%)]\tDiscriminator Loss: 0.473164\tGenerator Loss: 1.512348\n",
      "Train Epoch: 97 [28160/60000 (47%)]\tDiscriminator Loss: 0.465605\tGenerator Loss: 1.758945\n",
      "Train Epoch: 97 [29440/60000 (49%)]\tDiscriminator Loss: 0.404770\tGenerator Loss: 1.537889\n",
      "Train Epoch: 97 [30720/60000 (51%)]\tDiscriminator Loss: 0.448502\tGenerator Loss: 1.673522\n",
      "Train Epoch: 97 [32000/60000 (53%)]\tDiscriminator Loss: 0.433576\tGenerator Loss: 1.248676\n",
      "Train Epoch: 97 [33280/60000 (55%)]\tDiscriminator Loss: 0.452559\tGenerator Loss: 1.532040\n",
      "Train Epoch: 97 [34560/60000 (58%)]\tDiscriminator Loss: 0.506665\tGenerator Loss: 1.183443\n",
      "Train Epoch: 97 [35840/60000 (60%)]\tDiscriminator Loss: 0.504272\tGenerator Loss: 1.399801\n",
      "Train Epoch: 97 [37120/60000 (62%)]\tDiscriminator Loss: 0.486238\tGenerator Loss: 1.438632\n",
      "Train Epoch: 97 [38400/60000 (64%)]\tDiscriminator Loss: 0.451523\tGenerator Loss: 1.351612\n",
      "Train Epoch: 97 [39680/60000 (66%)]\tDiscriminator Loss: 0.487833\tGenerator Loss: 1.404768\n",
      "Train Epoch: 97 [40960/60000 (68%)]\tDiscriminator Loss: 0.494447\tGenerator Loss: 1.819337\n",
      "Train Epoch: 97 [42240/60000 (70%)]\tDiscriminator Loss: 0.520164\tGenerator Loss: 1.216817\n",
      "Train Epoch: 97 [43520/60000 (72%)]\tDiscriminator Loss: 0.518469\tGenerator Loss: 1.151934\n",
      "Train Epoch: 97 [44800/60000 (75%)]\tDiscriminator Loss: 0.500513\tGenerator Loss: 1.085855\n",
      "Train Epoch: 97 [46080/60000 (77%)]\tDiscriminator Loss: 0.522654\tGenerator Loss: 1.019932\n",
      "Train Epoch: 97 [47360/60000 (79%)]\tDiscriminator Loss: 0.462796\tGenerator Loss: 1.739491\n",
      "Train Epoch: 97 [48640/60000 (81%)]\tDiscriminator Loss: 0.456848\tGenerator Loss: 1.816766\n",
      "Train Epoch: 97 [49920/60000 (83%)]\tDiscriminator Loss: 0.455533\tGenerator Loss: 1.783771\n",
      "Train Epoch: 97 [51200/60000 (85%)]\tDiscriminator Loss: 0.506394\tGenerator Loss: 1.599316\n",
      "Train Epoch: 97 [52480/60000 (87%)]\tDiscriminator Loss: 0.421150\tGenerator Loss: 1.329939\n",
      "Train Epoch: 97 [53760/60000 (90%)]\tDiscriminator Loss: 0.454824\tGenerator Loss: 1.262610\n",
      "Train Epoch: 97 [55040/60000 (92%)]\tDiscriminator Loss: 0.462976\tGenerator Loss: 1.401545\n",
      "Train Epoch: 97 [56320/60000 (94%)]\tDiscriminator Loss: 0.420520\tGenerator Loss: 1.439242\n",
      "Train Epoch: 97 [57600/60000 (96%)]\tDiscriminator Loss: 0.532359\tGenerator Loss: 1.178660\n",
      "Train Epoch: 97 [58880/60000 (98%)]\tDiscriminator Loss: 0.479269\tGenerator Loss: 1.456863\n",
      "Train Epoch: 98 [0/60000 (0%)]\tDiscriminator Loss: 0.476786\tGenerator Loss: 1.013205\n",
      "Train Epoch: 98 [1280/60000 (2%)]\tDiscriminator Loss: 0.521395\tGenerator Loss: 1.701845\n",
      "Train Epoch: 98 [2560/60000 (4%)]\tDiscriminator Loss: 0.505549\tGenerator Loss: 1.155999\n",
      "Train Epoch: 98 [3840/60000 (6%)]\tDiscriminator Loss: 0.506088\tGenerator Loss: 1.171721\n",
      "Train Epoch: 98 [5120/60000 (9%)]\tDiscriminator Loss: 0.459639\tGenerator Loss: 2.086145\n",
      "Train Epoch: 98 [6400/60000 (11%)]\tDiscriminator Loss: 0.429809\tGenerator Loss: 1.182915\n",
      "Train Epoch: 98 [7680/60000 (13%)]\tDiscriminator Loss: 0.484219\tGenerator Loss: 1.552947\n",
      "Train Epoch: 98 [8960/60000 (15%)]\tDiscriminator Loss: 0.441332\tGenerator Loss: 1.309472\n",
      "Train Epoch: 98 [10240/60000 (17%)]\tDiscriminator Loss: 0.471894\tGenerator Loss: 1.477587\n",
      "Train Epoch: 98 [11520/60000 (19%)]\tDiscriminator Loss: 0.478752\tGenerator Loss: 1.441013\n",
      "Train Epoch: 98 [12800/60000 (21%)]\tDiscriminator Loss: 0.428143\tGenerator Loss: 1.511905\n",
      "Train Epoch: 98 [14080/60000 (23%)]\tDiscriminator Loss: 0.458685\tGenerator Loss: 1.435255\n",
      "Train Epoch: 98 [15360/60000 (26%)]\tDiscriminator Loss: 0.477571\tGenerator Loss: 1.475012\n",
      "Train Epoch: 98 [16640/60000 (28%)]\tDiscriminator Loss: 0.476370\tGenerator Loss: 1.184261\n",
      "Train Epoch: 98 [17920/60000 (30%)]\tDiscriminator Loss: 0.421480\tGenerator Loss: 1.619131\n",
      "Train Epoch: 98 [19200/60000 (32%)]\tDiscriminator Loss: 0.459797\tGenerator Loss: 1.478169\n",
      "Train Epoch: 98 [20480/60000 (34%)]\tDiscriminator Loss: 0.502132\tGenerator Loss: 1.470474\n",
      "Train Epoch: 98 [21760/60000 (36%)]\tDiscriminator Loss: 0.511357\tGenerator Loss: 1.499766\n",
      "Train Epoch: 98 [23040/60000 (38%)]\tDiscriminator Loss: 0.444939\tGenerator Loss: 1.656193\n",
      "Train Epoch: 98 [24320/60000 (41%)]\tDiscriminator Loss: 0.474359\tGenerator Loss: 1.432202\n",
      "Train Epoch: 98 [25600/60000 (43%)]\tDiscriminator Loss: 0.517544\tGenerator Loss: 1.039384\n",
      "Train Epoch: 98 [26880/60000 (45%)]\tDiscriminator Loss: 0.465791\tGenerator Loss: 1.882409\n",
      "Train Epoch: 98 [28160/60000 (47%)]\tDiscriminator Loss: 0.482066\tGenerator Loss: 1.153282\n",
      "Train Epoch: 98 [29440/60000 (49%)]\tDiscriminator Loss: 0.458533\tGenerator Loss: 1.452205\n",
      "Train Epoch: 98 [30720/60000 (51%)]\tDiscriminator Loss: 0.435740\tGenerator Loss: 1.624244\n",
      "Train Epoch: 98 [32000/60000 (53%)]\tDiscriminator Loss: 0.428989\tGenerator Loss: 1.466669\n",
      "Train Epoch: 98 [33280/60000 (55%)]\tDiscriminator Loss: 0.528813\tGenerator Loss: 1.438522\n",
      "Train Epoch: 98 [34560/60000 (58%)]\tDiscriminator Loss: 0.654467\tGenerator Loss: 2.158800\n",
      "Train Epoch: 98 [35840/60000 (60%)]\tDiscriminator Loss: 0.467773\tGenerator Loss: 1.307891\n",
      "Train Epoch: 98 [37120/60000 (62%)]\tDiscriminator Loss: 0.465395\tGenerator Loss: 1.402582\n",
      "Train Epoch: 98 [38400/60000 (64%)]\tDiscriminator Loss: 0.459967\tGenerator Loss: 1.224339\n",
      "Train Epoch: 98 [39680/60000 (66%)]\tDiscriminator Loss: 0.455787\tGenerator Loss: 1.564264\n",
      "Train Epoch: 98 [40960/60000 (68%)]\tDiscriminator Loss: 0.426657\tGenerator Loss: 1.452346\n",
      "Train Epoch: 98 [42240/60000 (70%)]\tDiscriminator Loss: 0.528149\tGenerator Loss: 1.589049\n",
      "Train Epoch: 98 [43520/60000 (72%)]\tDiscriminator Loss: 0.493401\tGenerator Loss: 1.371263\n",
      "Train Epoch: 98 [44800/60000 (75%)]\tDiscriminator Loss: 0.488257\tGenerator Loss: 1.463310\n",
      "Train Epoch: 98 [46080/60000 (77%)]\tDiscriminator Loss: 0.477396\tGenerator Loss: 1.794448\n",
      "Train Epoch: 98 [47360/60000 (79%)]\tDiscriminator Loss: 0.474972\tGenerator Loss: 1.289944\n",
      "Train Epoch: 98 [48640/60000 (81%)]\tDiscriminator Loss: 0.421883\tGenerator Loss: 1.465382\n",
      "Train Epoch: 98 [49920/60000 (83%)]\tDiscriminator Loss: 0.450538\tGenerator Loss: 1.385861\n",
      "Train Epoch: 98 [51200/60000 (85%)]\tDiscriminator Loss: 0.561051\tGenerator Loss: 1.398832\n",
      "Train Epoch: 98 [52480/60000 (87%)]\tDiscriminator Loss: 0.492585\tGenerator Loss: 1.170985\n",
      "Train Epoch: 98 [53760/60000 (90%)]\tDiscriminator Loss: 0.413924\tGenerator Loss: 1.792968\n",
      "Train Epoch: 98 [55040/60000 (92%)]\tDiscriminator Loss: 0.459336\tGenerator Loss: 1.472221\n",
      "Train Epoch: 98 [56320/60000 (94%)]\tDiscriminator Loss: 0.458915\tGenerator Loss: 1.372213\n",
      "Train Epoch: 98 [57600/60000 (96%)]\tDiscriminator Loss: 0.514870\tGenerator Loss: 1.607101\n",
      "Train Epoch: 98 [58880/60000 (98%)]\tDiscriminator Loss: 0.486760\tGenerator Loss: 1.318779\n",
      "Train Epoch: 99 [0/60000 (0%)]\tDiscriminator Loss: 0.476233\tGenerator Loss: 1.192051\n",
      "Train Epoch: 99 [1280/60000 (2%)]\tDiscriminator Loss: 0.395416\tGenerator Loss: 1.507109\n",
      "Train Epoch: 99 [2560/60000 (4%)]\tDiscriminator Loss: 0.438818\tGenerator Loss: 1.337764\n",
      "Train Epoch: 99 [3840/60000 (6%)]\tDiscriminator Loss: 0.536178\tGenerator Loss: 1.632503\n",
      "Train Epoch: 99 [5120/60000 (9%)]\tDiscriminator Loss: 0.486631\tGenerator Loss: 1.838904\n",
      "Train Epoch: 99 [6400/60000 (11%)]\tDiscriminator Loss: 0.467718\tGenerator Loss: 1.670019\n",
      "Train Epoch: 99 [7680/60000 (13%)]\tDiscriminator Loss: 0.517760\tGenerator Loss: 1.665419\n",
      "Train Epoch: 99 [8960/60000 (15%)]\tDiscriminator Loss: 0.438489\tGenerator Loss: 1.318677\n",
      "Train Epoch: 99 [10240/60000 (17%)]\tDiscriminator Loss: 0.466907\tGenerator Loss: 1.200983\n",
      "Train Epoch: 99 [11520/60000 (19%)]\tDiscriminator Loss: 0.434011\tGenerator Loss: 1.859330\n",
      "Train Epoch: 99 [12800/60000 (21%)]\tDiscriminator Loss: 0.495019\tGenerator Loss: 1.311465\n",
      "Train Epoch: 99 [14080/60000 (23%)]\tDiscriminator Loss: 0.463141\tGenerator Loss: 1.370704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 99 [15360/60000 (26%)]\tDiscriminator Loss: 0.492202\tGenerator Loss: 1.227243\n",
      "Train Epoch: 99 [16640/60000 (28%)]\tDiscriminator Loss: 0.438143\tGenerator Loss: 1.636431\n",
      "Train Epoch: 99 [17920/60000 (30%)]\tDiscriminator Loss: 0.474608\tGenerator Loss: 1.215804\n",
      "Train Epoch: 99 [19200/60000 (32%)]\tDiscriminator Loss: 0.393594\tGenerator Loss: 1.656152\n",
      "Train Epoch: 99 [20480/60000 (34%)]\tDiscriminator Loss: 0.478604\tGenerator Loss: 1.334631\n",
      "Train Epoch: 99 [21760/60000 (36%)]\tDiscriminator Loss: 0.494502\tGenerator Loss: 1.397555\n",
      "Train Epoch: 99 [23040/60000 (38%)]\tDiscriminator Loss: 0.515284\tGenerator Loss: 1.376151\n",
      "Train Epoch: 99 [24320/60000 (41%)]\tDiscriminator Loss: 0.512322\tGenerator Loss: 1.613918\n",
      "Train Epoch: 99 [25600/60000 (43%)]\tDiscriminator Loss: 0.440930\tGenerator Loss: 1.584580\n",
      "Train Epoch: 99 [26880/60000 (45%)]\tDiscriminator Loss: 0.457527\tGenerator Loss: 1.889975\n",
      "Train Epoch: 99 [28160/60000 (47%)]\tDiscriminator Loss: 0.513540\tGenerator Loss: 1.426976\n",
      "Train Epoch: 99 [29440/60000 (49%)]\tDiscriminator Loss: 0.497116\tGenerator Loss: 1.615928\n",
      "Train Epoch: 99 [30720/60000 (51%)]\tDiscriminator Loss: 0.465776\tGenerator Loss: 1.832873\n",
      "Train Epoch: 99 [32000/60000 (53%)]\tDiscriminator Loss: 0.442339\tGenerator Loss: 1.801809\n",
      "Train Epoch: 99 [33280/60000 (55%)]\tDiscriminator Loss: 0.509939\tGenerator Loss: 1.656572\n",
      "Train Epoch: 99 [34560/60000 (58%)]\tDiscriminator Loss: 0.488682\tGenerator Loss: 1.320289\n",
      "Train Epoch: 99 [35840/60000 (60%)]\tDiscriminator Loss: 0.409718\tGenerator Loss: 1.283890\n",
      "Train Epoch: 99 [37120/60000 (62%)]\tDiscriminator Loss: 0.471251\tGenerator Loss: 1.436753\n",
      "Train Epoch: 99 [38400/60000 (64%)]\tDiscriminator Loss: 0.520960\tGenerator Loss: 1.813522\n",
      "Train Epoch: 99 [39680/60000 (66%)]\tDiscriminator Loss: 0.490341\tGenerator Loss: 0.957361\n",
      "Train Epoch: 99 [40960/60000 (68%)]\tDiscriminator Loss: 0.514444\tGenerator Loss: 1.852592\n",
      "Train Epoch: 99 [42240/60000 (70%)]\tDiscriminator Loss: 0.425124\tGenerator Loss: 1.957082\n",
      "Train Epoch: 99 [43520/60000 (72%)]\tDiscriminator Loss: 0.497602\tGenerator Loss: 1.761436\n",
      "Train Epoch: 99 [44800/60000 (75%)]\tDiscriminator Loss: 0.442392\tGenerator Loss: 1.401027\n",
      "Train Epoch: 99 [46080/60000 (77%)]\tDiscriminator Loss: 0.492796\tGenerator Loss: 1.236597\n",
      "Train Epoch: 99 [47360/60000 (79%)]\tDiscriminator Loss: 0.478710\tGenerator Loss: 1.119274\n",
      "Train Epoch: 99 [48640/60000 (81%)]\tDiscriminator Loss: 0.504202\tGenerator Loss: 1.115369\n",
      "Train Epoch: 99 [49920/60000 (83%)]\tDiscriminator Loss: 0.468975\tGenerator Loss: 1.257059\n",
      "Train Epoch: 99 [51200/60000 (85%)]\tDiscriminator Loss: 0.493434\tGenerator Loss: 1.698230\n",
      "Train Epoch: 99 [52480/60000 (87%)]\tDiscriminator Loss: 0.421522\tGenerator Loss: 1.331598\n",
      "Train Epoch: 99 [53760/60000 (90%)]\tDiscriminator Loss: 0.473251\tGenerator Loss: 1.497318\n",
      "Train Epoch: 99 [55040/60000 (92%)]\tDiscriminator Loss: 0.465272\tGenerator Loss: 1.122222\n",
      "Train Epoch: 99 [56320/60000 (94%)]\tDiscriminator Loss: 0.530384\tGenerator Loss: 1.829998\n",
      "Train Epoch: 99 [57600/60000 (96%)]\tDiscriminator Loss: 0.459002\tGenerator Loss: 1.531257\n",
      "Train Epoch: 99 [58880/60000 (98%)]\tDiscriminator Loss: 0.461796\tGenerator Loss: 1.430765\n",
      "Train Epoch: 100 [0/60000 (0%)]\tDiscriminator Loss: 0.412082\tGenerator Loss: 1.407993\n",
      "Train Epoch: 100 [1280/60000 (2%)]\tDiscriminator Loss: 0.540190\tGenerator Loss: 0.974752\n",
      "Train Epoch: 100 [2560/60000 (4%)]\tDiscriminator Loss: 0.410003\tGenerator Loss: 1.499293\n",
      "Train Epoch: 100 [3840/60000 (6%)]\tDiscriminator Loss: 0.434898\tGenerator Loss: 1.560180\n",
      "Train Epoch: 100 [5120/60000 (9%)]\tDiscriminator Loss: 0.440408\tGenerator Loss: 1.591969\n",
      "Train Epoch: 100 [6400/60000 (11%)]\tDiscriminator Loss: 0.465470\tGenerator Loss: 1.366974\n",
      "Train Epoch: 100 [7680/60000 (13%)]\tDiscriminator Loss: 0.535787\tGenerator Loss: 1.011051\n",
      "Train Epoch: 100 [8960/60000 (15%)]\tDiscriminator Loss: 0.464624\tGenerator Loss: 1.378711\n",
      "Train Epoch: 100 [10240/60000 (17%)]\tDiscriminator Loss: 0.490872\tGenerator Loss: 1.726527\n",
      "Train Epoch: 100 [11520/60000 (19%)]\tDiscriminator Loss: 0.383458\tGenerator Loss: 1.688738\n",
      "Train Epoch: 100 [12800/60000 (21%)]\tDiscriminator Loss: 0.450600\tGenerator Loss: 1.353793\n",
      "Train Epoch: 100 [14080/60000 (23%)]\tDiscriminator Loss: 0.453607\tGenerator Loss: 1.557179\n",
      "Train Epoch: 100 [15360/60000 (26%)]\tDiscriminator Loss: 0.451925\tGenerator Loss: 1.275145\n",
      "Train Epoch: 100 [16640/60000 (28%)]\tDiscriminator Loss: 0.402961\tGenerator Loss: 1.473682\n",
      "Train Epoch: 100 [17920/60000 (30%)]\tDiscriminator Loss: 0.425762\tGenerator Loss: 1.778362\n",
      "Train Epoch: 100 [19200/60000 (32%)]\tDiscriminator Loss: 0.440095\tGenerator Loss: 1.244509\n",
      "Train Epoch: 100 [20480/60000 (34%)]\tDiscriminator Loss: 0.463259\tGenerator Loss: 1.459534\n",
      "Train Epoch: 100 [21760/60000 (36%)]\tDiscriminator Loss: 0.547521\tGenerator Loss: 1.040897\n",
      "Train Epoch: 100 [23040/60000 (38%)]\tDiscriminator Loss: 0.464826\tGenerator Loss: 1.391064\n",
      "Train Epoch: 100 [24320/60000 (41%)]\tDiscriminator Loss: 0.453949\tGenerator Loss: 1.729198\n",
      "Train Epoch: 100 [25600/60000 (43%)]\tDiscriminator Loss: 0.441494\tGenerator Loss: 1.369652\n",
      "Train Epoch: 100 [26880/60000 (45%)]\tDiscriminator Loss: 0.460838\tGenerator Loss: 1.301914\n",
      "Train Epoch: 100 [28160/60000 (47%)]\tDiscriminator Loss: 0.507385\tGenerator Loss: 1.058874\n",
      "Train Epoch: 100 [29440/60000 (49%)]\tDiscriminator Loss: 0.449446\tGenerator Loss: 1.622902\n",
      "Train Epoch: 100 [30720/60000 (51%)]\tDiscriminator Loss: 0.526823\tGenerator Loss: 1.337071\n",
      "Train Epoch: 100 [32000/60000 (53%)]\tDiscriminator Loss: 0.480931\tGenerator Loss: 1.367447\n",
      "Train Epoch: 100 [33280/60000 (55%)]\tDiscriminator Loss: 0.435800\tGenerator Loss: 1.645212\n",
      "Train Epoch: 100 [34560/60000 (58%)]\tDiscriminator Loss: 0.467492\tGenerator Loss: 1.842292\n",
      "Train Epoch: 100 [35840/60000 (60%)]\tDiscriminator Loss: 0.558818\tGenerator Loss: 1.775550\n",
      "Train Epoch: 100 [37120/60000 (62%)]\tDiscriminator Loss: 0.454886\tGenerator Loss: 1.559209\n",
      "Train Epoch: 100 [38400/60000 (64%)]\tDiscriminator Loss: 0.527244\tGenerator Loss: 1.202604\n",
      "Train Epoch: 100 [39680/60000 (66%)]\tDiscriminator Loss: 0.438417\tGenerator Loss: 1.242589\n",
      "Train Epoch: 100 [40960/60000 (68%)]\tDiscriminator Loss: 0.414210\tGenerator Loss: 1.391598\n",
      "Train Epoch: 100 [42240/60000 (70%)]\tDiscriminator Loss: 0.509723\tGenerator Loss: 1.219326\n",
      "Train Epoch: 100 [43520/60000 (72%)]\tDiscriminator Loss: 0.429583\tGenerator Loss: 1.423979\n",
      "Train Epoch: 100 [44800/60000 (75%)]\tDiscriminator Loss: 0.468063\tGenerator Loss: 1.663794\n",
      "Train Epoch: 100 [46080/60000 (77%)]\tDiscriminator Loss: 0.545249\tGenerator Loss: 1.603424\n",
      "Train Epoch: 100 [47360/60000 (79%)]\tDiscriminator Loss: 0.405653\tGenerator Loss: 1.659016\n",
      "Train Epoch: 100 [48640/60000 (81%)]\tDiscriminator Loss: 0.456669\tGenerator Loss: 1.455186\n",
      "Train Epoch: 100 [49920/60000 (83%)]\tDiscriminator Loss: 0.501784\tGenerator Loss: 1.521248\n",
      "Train Epoch: 100 [51200/60000 (85%)]\tDiscriminator Loss: 0.438149\tGenerator Loss: 1.437345\n",
      "Train Epoch: 100 [52480/60000 (87%)]\tDiscriminator Loss: 0.446903\tGenerator Loss: 1.504341\n",
      "Train Epoch: 100 [53760/60000 (90%)]\tDiscriminator Loss: 0.495073\tGenerator Loss: 1.356868\n",
      "Train Epoch: 100 [55040/60000 (92%)]\tDiscriminator Loss: 0.518603\tGenerator Loss: 1.433880\n",
      "Train Epoch: 100 [56320/60000 (94%)]\tDiscriminator Loss: 0.455016\tGenerator Loss: 1.532751\n",
      "Train Epoch: 100 [57600/60000 (96%)]\tDiscriminator Loss: 0.464765\tGenerator Loss: 1.493662\n",
      "Train Epoch: 100 [58880/60000 (98%)]\tDiscriminator Loss: 0.571883\tGenerator Loss: 0.937775\n"
     ]
    }
   ],
   "source": [
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "gan_losses = []\n",
    "for epoch in range(1, epochs+1):\n",
    "    for i, (real_imgs, _) in enumerate(dataloader):\n",
    "        real_imgs = real_imgs.to(device)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = torch.ones([real_imgs.size(0), 1], device=device)\n",
    "        fake = torch.zeros([real_imgs.size(0), 1], device=device)\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_generator.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = torch.randn(real_imgs.shape[0], nz, device=device)\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "\n",
    "        # classify images using discriminator\n",
    "        classifications = discriminator(gen_imgs)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(classifications, valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_generator.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_discriminator.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "        # detaching since the backprop does not need to run on the generator\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        # take the average of loss against generated images and loss against real images\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_discriminator.step()\n",
    "\n",
    "        if i % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tDiscriminator Loss: {:.6f}\\tGenerator Loss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    i * len(real_imgs),\n",
    "                    len(dataloader.dataset),\n",
    "                    100.0 * i / len(dataloader),\n",
    "                    d_loss.item(),\n",
    "                    g_loss.item()\n",
    "                )\n",
    "            )\n",
    "    gan_losses.append((g_loss, real_loss, fake_loss, d_loss))\n",
    "    with torch.no_grad():\n",
    "        n_images = 64\n",
    "        sample = torch.randn(n_images, nz).to(device)\n",
    "        sample = generator(sample).cpu()\n",
    "        save_image(\n",
    "            sample.view(n_images, *img_shape),\n",
    "            gan_results_directory + \"/sample_\" + str(epoch) + \".png\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02600236",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_model_path = os.path.join(curr_dirname, \"gan_generator.pt\")\n",
    "discriminator_model_path = os.path.join(curr_dirname, \"gan_discriminator.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef5ffa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(generator.state_dict(), generator_model_path)\n",
    "torch.save(discriminator.state_dict(), discriminator_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f53e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.load_state_dict(torch.load(generator_model_path))\n",
    "generator.eval()\n",
    "sum([param.nelement() for param in generator.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f760b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "discriminator.load_state_dict(torch.load(discriminator_model_path))\n",
    "discriminator.eval()\n",
    "sum([param.nelement() for param in discriminator.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7328e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_results_directory_name = \"results/vae\"\n",
    "os.makedirs(vae_results_directory_name, exist_ok=True)\n",
    "vae_results_directory = os.path.join(curr_dirname, vae_results_directory_name)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(data_directory, train=True, download=False, transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(data_directory, train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35bf165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc2 = nn.Linear(400, 200)\n",
    "        self.fc31 = nn.Linear(200, nz)\n",
    "        self.fc32 = nn.Linear(200, nz)\n",
    "        self.fc4 = nn.Linear(nz, 200)\n",
    "        self.fc5 = nn.Linear(200, 400)\n",
    "        self.fc6 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        return self.fc31(h2), self.fc32(h2)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc4(z))\n",
    "        h4 = F.relu(self.fc5(h3))\n",
    "        return torch.sigmoid(self.fc6(h4))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb0107e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE().to(device)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=lr, betas=(b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9e5eeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_train_losses = []\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def vae_loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction=\"sum\")\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD\n",
    "\n",
    "\n",
    "def train_vae(epoch):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = vae(data)\n",
    "        loss = vae_loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item() / len(data),\n",
    "                )\n",
    "            )\n",
    "    vae_train_losses.append(train_loss)\n",
    "    print(\n",
    "        \"====> Epoch: {} Average loss: {:.4f}\".format(\n",
    "            epoch, train_loss / len(train_loader.dataset)\n",
    "        )\n",
    "    )\n",
    "\n",
    "vae_test_losses = []\n",
    "def test_vae(epoch):\n",
    "    vae.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = vae(data)\n",
    "            test_loss += vae_loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat(\n",
    "                    [data[:n], recon_batch.view(batch_size, 1, 28, 28)[:n]]\n",
    "                )\n",
    "                save_image(\n",
    "                    comparison.cpu(),\n",
    "                    vae_results_directory + \"/reconstruction_\" + str(epoch) + \".png\",\n",
    "                    nrow=n,\n",
    "                )\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    vae_test_losses.append(test_loss)\n",
    "    print(\"====> Test set loss: {:.4f}\".format(test_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b1b42b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 95.248001\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 96.876022\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 99.917221\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 98.185410\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 100.431976\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 94.715401\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 97.479179\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 99.614693\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 97.418388\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 99.099380\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 101.109901\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 95.110466\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 93.961792\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 98.167831\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 97.271393\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 98.369759\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 94.996254\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 98.337578\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 100.460243\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 98.747559\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 96.681534\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 98.148331\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 98.150314\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 97.004700\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 95.722595\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 100.842789\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 94.863312\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 100.601791\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 97.327087\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 99.927963\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 96.074821\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 94.496353\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 95.877792\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 94.909317\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 97.735306\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 97.733521\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 100.299492\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 99.823044\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 92.058731\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 99.008926\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 100.686668\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 98.188080\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 93.517563\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 100.637161\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 100.250877\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 99.595161\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 101.471130\n",
      "====> Epoch: 1 Average loss: 98.3533\n",
      "====> Test set loss: 99.9285\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 93.741562\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 99.653244\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 101.415169\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 101.818176\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 97.778931\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 98.846085\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 97.849022\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 97.661568\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 97.712769\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 100.263580\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 95.973083\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 99.177132\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 96.946518\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 99.932297\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 101.750679\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 95.097855\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 99.055428\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 98.984848\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 99.536011\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 98.384338\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 97.102074\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 100.716476\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 99.229561\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 99.981667\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 97.152237\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 98.050842\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 98.984528\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 99.215591\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 101.887878\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 96.698288\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 97.130836\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 95.211349\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 95.370857\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 95.213058\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 100.462463\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 98.900513\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 96.712540\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 97.714180\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 96.357658\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 101.411621\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 98.736893\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 98.947807\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 93.129990\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 103.058365\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 99.239197\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 101.144775\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 99.057175\n",
      "====> Epoch: 2 Average loss: 98.2703\n",
      "====> Test set loss: 99.6376\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 97.398972\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 98.909348\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 96.229546\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 94.035950\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 100.440346\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 101.835968\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 100.343384\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 95.318268\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 97.051270\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 98.965485\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 100.185036\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 95.411697\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 99.260933\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 99.416016\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 98.606995\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 98.683441\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 98.761688\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 95.551712\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 97.819870\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 97.433136\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 96.725021\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 101.151352\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 97.262718\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 96.876480\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 97.740707\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 96.625839\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 97.724472\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 99.902283\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 96.474731\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 102.308258\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 100.494827\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 97.123764\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 99.816956\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 99.205017\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 101.660263\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 98.513145\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 94.792061\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 97.994873\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 97.824585\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 100.209396\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 98.149445\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 95.915337\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 102.480667\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 95.459663\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 100.196297\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 95.811188\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 97.838715\n",
      "====> Epoch: 3 Average loss: 98.2018\n",
      "====> Test set loss: 99.4603\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 100.775467\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 94.842003\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 98.678299\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 95.587051\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 99.560524\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 98.123161\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 97.989357\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 97.303169\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 99.248596\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 101.502785\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 95.756805\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 99.557251\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 99.366386\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 98.621994\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 97.957718\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 99.643280\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 98.017441\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 92.193687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 96.430519\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 99.009460\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 95.933533\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 98.428146\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 93.905167\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 97.412140\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 98.713654\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 96.035957\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 98.332581\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 95.575073\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 99.959000\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 98.904633\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 99.337944\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 99.610939\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 100.931267\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 98.891281\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 97.438499\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 99.348572\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 95.905807\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 97.358810\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 99.357216\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 102.917168\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 100.213875\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 100.950996\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 96.574692\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 96.124649\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 103.731720\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 99.275658\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 97.694290\n",
      "====> Epoch: 4 Average loss: 98.1394\n",
      "====> Test set loss: 99.8687\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 95.981117\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 99.246033\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 98.265457\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 96.737679\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 99.714783\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 95.514313\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 94.470177\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 99.577187\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 99.901085\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 96.950546\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 99.734024\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 97.445580\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 98.044815\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 101.576096\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 97.816330\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 99.331337\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 96.876190\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 101.192787\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 101.270920\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 95.656830\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 96.937134\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 99.214905\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 94.822250\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 96.936783\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 97.831543\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 101.712448\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 97.955299\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 97.232391\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 100.681725\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 101.366623\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 100.314041\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 100.231705\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 93.663834\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 94.201385\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 94.898209\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 98.453316\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 97.215561\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 96.390793\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 96.945251\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 98.758484\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 102.816879\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 101.133591\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 96.168266\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 101.850067\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 96.343079\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 98.729774\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 99.373932\n",
      "====> Epoch: 5 Average loss: 98.1103\n",
      "====> Test set loss: 99.7798\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 97.591881\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 95.896576\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 97.826691\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 101.708786\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 97.409531\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 97.587524\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 98.751251\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 101.337814\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 97.226349\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 96.231796\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 97.883102\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 99.345047\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 100.831177\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 100.488914\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 97.135048\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 95.734970\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 99.691116\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 96.485023\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 101.696404\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 98.217010\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 96.318901\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 95.050812\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 98.592064\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 101.197014\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 96.678947\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 97.816437\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 101.667297\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 101.245705\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 97.922234\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 99.027466\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 99.471199\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 98.007240\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 96.209251\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 98.108704\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 101.617523\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 100.450340\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 97.082489\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 96.376625\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 97.782883\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 98.904572\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 98.806717\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 93.113434\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 98.752380\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 100.174080\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 92.636688\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 96.999763\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 98.236107\n",
      "====> Epoch: 6 Average loss: 98.0889\n",
      "====> Test set loss: 99.6072\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 100.890511\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 97.639664\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 99.739441\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 94.838760\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 98.813530\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 99.711212\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 99.029335\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 99.925201\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 97.727646\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 98.815010\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 100.897354\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 97.666924\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 96.447113\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 94.852997\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 96.207169\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 102.408577\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 98.969391\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 97.415695\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 101.142365\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 97.415077\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 99.064316\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 99.221397\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 96.115883\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 95.427994\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 97.371201\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 95.480286\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 96.749985\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 94.507263\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 94.123016\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 98.703278\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 94.016357\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 101.593170\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 95.956764\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 102.136566\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 100.563095\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 96.139099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 94.949631\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 98.203674\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 99.388794\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 96.735199\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 96.902916\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 101.772003\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 98.857971\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 96.543243\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 99.587914\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 95.791428\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 98.307343\n",
      "====> Epoch: 7 Average loss: 98.1073\n",
      "====> Test set loss: 99.5612\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 98.832932\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 94.826515\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 97.453972\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 98.090065\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 101.053871\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 93.830414\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 98.821991\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 100.339966\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 96.478119\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 102.578262\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 99.613045\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 97.250984\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 99.878639\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 93.565605\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 96.025398\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 99.612518\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 98.933662\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 96.817101\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 93.867874\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 96.818893\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 98.703812\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 98.252663\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 97.912193\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 95.453453\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 99.360992\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 99.706833\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 98.589813\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 99.427185\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 96.363647\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 95.875565\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 98.681915\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 97.465820\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 101.695702\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 96.785843\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 100.560707\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 103.345184\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 99.536240\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 95.032410\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 97.975197\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 96.723763\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 97.225868\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 101.913254\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 99.420044\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 94.289673\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 101.049896\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 97.668793\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 92.772552\n",
      "====> Epoch: 8 Average loss: 98.0149\n",
      "====> Test set loss: 99.4915\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 95.030441\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 95.463135\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 95.595001\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 99.410805\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 98.628174\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 92.518394\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 100.667633\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 98.080948\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 93.427704\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 102.153976\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 95.711449\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 101.869049\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 99.891106\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 95.399315\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 101.409775\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 97.723679\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 98.300812\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 95.477112\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 101.107071\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 94.225388\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 95.867920\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 98.086868\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 100.350388\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 98.606995\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 97.875854\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 97.763084\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 98.360527\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 99.773209\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 97.295074\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 99.138794\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 100.021088\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 95.204147\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 94.326019\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 99.178558\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 98.058487\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 96.773331\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 98.165466\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 97.937561\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 101.044930\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 105.520248\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 96.718155\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 98.938232\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 97.132774\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 96.332336\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 99.081551\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 97.728027\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 93.700500\n",
      "====> Epoch: 9 Average loss: 97.9978\n",
      "====> Test set loss: 99.3934\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 94.662430\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 101.146065\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 96.062485\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 98.991074\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 93.935631\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 98.865341\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 98.412849\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 100.699738\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 97.404907\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 97.158585\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 101.012253\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 98.434006\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 96.721848\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 98.629692\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 101.100815\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 99.432480\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 97.587387\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 101.510193\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 97.853394\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 96.201035\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 99.966385\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 92.453049\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 98.879997\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 96.154617\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 95.954323\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 95.594589\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 98.827499\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 101.006363\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 96.900909\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 97.614708\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 101.844849\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 95.731956\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 99.244827\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 95.941132\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 93.618896\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 101.909302\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 99.078217\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 101.221542\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 100.248688\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 95.693619\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 98.074539\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 97.554718\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 98.793617\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 98.825867\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 99.166985\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 97.237892\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 100.682625\n",
      "====> Epoch: 10 Average loss: 97.9139\n",
      "====> Test set loss: 99.3740\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 93.572998\n",
      "Train Epoch: 11 [1280/60000 (2%)]\tLoss: 96.972137\n",
      "Train Epoch: 11 [2560/60000 (4%)]\tLoss: 98.816452\n",
      "Train Epoch: 11 [3840/60000 (6%)]\tLoss: 97.512070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 11 [5120/60000 (9%)]\tLoss: 97.631073\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 101.518303\n",
      "Train Epoch: 11 [7680/60000 (13%)]\tLoss: 97.180206\n",
      "Train Epoch: 11 [8960/60000 (15%)]\tLoss: 100.902321\n",
      "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 94.640686\n",
      "Train Epoch: 11 [11520/60000 (19%)]\tLoss: 94.820480\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 97.779015\n",
      "Train Epoch: 11 [14080/60000 (23%)]\tLoss: 98.569809\n",
      "Train Epoch: 11 [15360/60000 (26%)]\tLoss: 98.732376\n",
      "Train Epoch: 11 [16640/60000 (28%)]\tLoss: 95.078323\n",
      "Train Epoch: 11 [17920/60000 (30%)]\tLoss: 97.913765\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 96.699249\n",
      "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 95.745850\n",
      "Train Epoch: 11 [21760/60000 (36%)]\tLoss: 100.259254\n",
      "Train Epoch: 11 [23040/60000 (38%)]\tLoss: 100.928925\n",
      "Train Epoch: 11 [24320/60000 (41%)]\tLoss: 97.188751\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 95.545853\n",
      "Train Epoch: 11 [26880/60000 (45%)]\tLoss: 92.966354\n",
      "Train Epoch: 11 [28160/60000 (47%)]\tLoss: 99.159714\n",
      "Train Epoch: 11 [29440/60000 (49%)]\tLoss: 99.005226\n",
      "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 100.486710\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 96.581238\n",
      "Train Epoch: 11 [33280/60000 (55%)]\tLoss: 97.343956\n",
      "Train Epoch: 11 [34560/60000 (58%)]\tLoss: 94.204620\n",
      "Train Epoch: 11 [35840/60000 (60%)]\tLoss: 93.976440\n",
      "Train Epoch: 11 [37120/60000 (62%)]\tLoss: 99.162895\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 98.620667\n",
      "Train Epoch: 11 [39680/60000 (66%)]\tLoss: 93.571899\n",
      "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 101.564461\n",
      "Train Epoch: 11 [42240/60000 (70%)]\tLoss: 99.321426\n",
      "Train Epoch: 11 [43520/60000 (72%)]\tLoss: 102.482285\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 97.706139\n",
      "Train Epoch: 11 [46080/60000 (77%)]\tLoss: 98.175522\n",
      "Train Epoch: 11 [47360/60000 (79%)]\tLoss: 94.404388\n",
      "Train Epoch: 11 [48640/60000 (81%)]\tLoss: 99.251228\n",
      "Train Epoch: 11 [49920/60000 (83%)]\tLoss: 94.630859\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 97.292450\n",
      "Train Epoch: 11 [52480/60000 (87%)]\tLoss: 102.772583\n",
      "Train Epoch: 11 [53760/60000 (90%)]\tLoss: 95.701309\n",
      "Train Epoch: 11 [55040/60000 (92%)]\tLoss: 100.859421\n",
      "Train Epoch: 11 [56320/60000 (94%)]\tLoss: 95.603195\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 98.817215\n",
      "Train Epoch: 11 [58880/60000 (98%)]\tLoss: 99.521385\n",
      "====> Epoch: 11 Average loss: 97.8788\n",
      "====> Test set loss: 99.4664\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 96.910606\n",
      "Train Epoch: 12 [1280/60000 (2%)]\tLoss: 96.491867\n",
      "Train Epoch: 12 [2560/60000 (4%)]\tLoss: 100.278030\n",
      "Train Epoch: 12 [3840/60000 (6%)]\tLoss: 97.568085\n",
      "Train Epoch: 12 [5120/60000 (9%)]\tLoss: 101.179115\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 95.789398\n",
      "Train Epoch: 12 [7680/60000 (13%)]\tLoss: 98.142639\n",
      "Train Epoch: 12 [8960/60000 (15%)]\tLoss: 92.342972\n",
      "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 94.422394\n",
      "Train Epoch: 12 [11520/60000 (19%)]\tLoss: 94.429359\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 96.845787\n",
      "Train Epoch: 12 [14080/60000 (23%)]\tLoss: 93.894226\n",
      "Train Epoch: 12 [15360/60000 (26%)]\tLoss: 98.107521\n",
      "Train Epoch: 12 [16640/60000 (28%)]\tLoss: 100.410072\n",
      "Train Epoch: 12 [17920/60000 (30%)]\tLoss: 99.883446\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 98.917221\n",
      "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 100.754028\n",
      "Train Epoch: 12 [21760/60000 (36%)]\tLoss: 96.746712\n",
      "Train Epoch: 12 [23040/60000 (38%)]\tLoss: 98.322128\n",
      "Train Epoch: 12 [24320/60000 (41%)]\tLoss: 99.703491\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 98.097336\n",
      "Train Epoch: 12 [26880/60000 (45%)]\tLoss: 97.947815\n",
      "Train Epoch: 12 [28160/60000 (47%)]\tLoss: 102.118301\n",
      "Train Epoch: 12 [29440/60000 (49%)]\tLoss: 94.990929\n",
      "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 101.629959\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 99.382172\n",
      "Train Epoch: 12 [33280/60000 (55%)]\tLoss: 95.976830\n",
      "Train Epoch: 12 [34560/60000 (58%)]\tLoss: 96.886078\n",
      "Train Epoch: 12 [35840/60000 (60%)]\tLoss: 96.323967\n",
      "Train Epoch: 12 [37120/60000 (62%)]\tLoss: 96.488075\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 96.588608\n",
      "Train Epoch: 12 [39680/60000 (66%)]\tLoss: 98.420982\n",
      "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 94.765778\n",
      "Train Epoch: 12 [42240/60000 (70%)]\tLoss: 98.314758\n",
      "Train Epoch: 12 [43520/60000 (72%)]\tLoss: 96.774452\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 95.185448\n",
      "Train Epoch: 12 [46080/60000 (77%)]\tLoss: 98.476723\n",
      "Train Epoch: 12 [47360/60000 (79%)]\tLoss: 97.374130\n",
      "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 100.418747\n",
      "Train Epoch: 12 [49920/60000 (83%)]\tLoss: 101.567932\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 99.193626\n",
      "Train Epoch: 12 [52480/60000 (87%)]\tLoss: 99.613205\n",
      "Train Epoch: 12 [53760/60000 (90%)]\tLoss: 96.408836\n",
      "Train Epoch: 12 [55040/60000 (92%)]\tLoss: 98.050446\n",
      "Train Epoch: 12 [56320/60000 (94%)]\tLoss: 96.672653\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 96.056900\n",
      "Train Epoch: 12 [58880/60000 (98%)]\tLoss: 94.177292\n",
      "====> Epoch: 12 Average loss: 97.8305\n",
      "====> Test set loss: 99.3261\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 98.695045\n",
      "Train Epoch: 13 [1280/60000 (2%)]\tLoss: 97.235184\n",
      "Train Epoch: 13 [2560/60000 (4%)]\tLoss: 97.381195\n",
      "Train Epoch: 13 [3840/60000 (6%)]\tLoss: 100.836853\n",
      "Train Epoch: 13 [5120/60000 (9%)]\tLoss: 97.669006\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 98.178993\n",
      "Train Epoch: 13 [7680/60000 (13%)]\tLoss: 97.210960\n",
      "Train Epoch: 13 [8960/60000 (15%)]\tLoss: 93.701431\n",
      "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 95.642410\n",
      "Train Epoch: 13 [11520/60000 (19%)]\tLoss: 94.110535\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 98.775925\n",
      "Train Epoch: 13 [14080/60000 (23%)]\tLoss: 95.323624\n",
      "Train Epoch: 13 [15360/60000 (26%)]\tLoss: 98.305138\n",
      "Train Epoch: 13 [16640/60000 (28%)]\tLoss: 92.165497\n",
      "Train Epoch: 13 [17920/60000 (30%)]\tLoss: 96.622391\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 94.886620\n",
      "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 95.408134\n",
      "Train Epoch: 13 [21760/60000 (36%)]\tLoss: 98.018143\n",
      "Train Epoch: 13 [23040/60000 (38%)]\tLoss: 97.471771\n",
      "Train Epoch: 13 [24320/60000 (41%)]\tLoss: 98.249214\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 97.288086\n",
      "Train Epoch: 13 [26880/60000 (45%)]\tLoss: 97.862572\n",
      "Train Epoch: 13 [28160/60000 (47%)]\tLoss: 97.273766\n",
      "Train Epoch: 13 [29440/60000 (49%)]\tLoss: 97.621941\n",
      "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 96.440941\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 95.665939\n",
      "Train Epoch: 13 [33280/60000 (55%)]\tLoss: 96.554779\n",
      "Train Epoch: 13 [34560/60000 (58%)]\tLoss: 97.016411\n",
      "Train Epoch: 13 [35840/60000 (60%)]\tLoss: 96.813889\n",
      "Train Epoch: 13 [37120/60000 (62%)]\tLoss: 99.998215\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 101.433136\n",
      "Train Epoch: 13 [39680/60000 (66%)]\tLoss: 95.704544\n",
      "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 103.932114\n",
      "Train Epoch: 13 [42240/60000 (70%)]\tLoss: 94.343796\n",
      "Train Epoch: 13 [43520/60000 (72%)]\tLoss: 97.594162\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 97.790329\n",
      "Train Epoch: 13 [46080/60000 (77%)]\tLoss: 100.053299\n",
      "Train Epoch: 13 [47360/60000 (79%)]\tLoss: 99.037308\n",
      "Train Epoch: 13 [48640/60000 (81%)]\tLoss: 98.756866\n",
      "Train Epoch: 13 [49920/60000 (83%)]\tLoss: 94.446533\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 101.887314\n",
      "Train Epoch: 13 [52480/60000 (87%)]\tLoss: 95.014160\n",
      "Train Epoch: 13 [53760/60000 (90%)]\tLoss: 97.699303\n",
      "Train Epoch: 13 [55040/60000 (92%)]\tLoss: 96.791168\n",
      "Train Epoch: 13 [56320/60000 (94%)]\tLoss: 100.623390\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 96.401093\n",
      "Train Epoch: 13 [58880/60000 (98%)]\tLoss: 94.266983\n",
      "====> Epoch: 13 Average loss: 97.8454\n",
      "====> Test set loss: 99.4950\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 98.476768\n",
      "Train Epoch: 14 [1280/60000 (2%)]\tLoss: 100.080399\n",
      "Train Epoch: 14 [2560/60000 (4%)]\tLoss: 94.873894\n",
      "Train Epoch: 14 [3840/60000 (6%)]\tLoss: 95.419121\n",
      "Train Epoch: 14 [5120/60000 (9%)]\tLoss: 98.248077\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 97.059639\n",
      "Train Epoch: 14 [7680/60000 (13%)]\tLoss: 97.003746\n",
      "Train Epoch: 14 [8960/60000 (15%)]\tLoss: 98.093437\n",
      "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 99.708298\n",
      "Train Epoch: 14 [11520/60000 (19%)]\tLoss: 97.786613\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 99.079491\n",
      "Train Epoch: 14 [14080/60000 (23%)]\tLoss: 100.016998\n",
      "Train Epoch: 14 [15360/60000 (26%)]\tLoss: 98.662827\n",
      "Train Epoch: 14 [16640/60000 (28%)]\tLoss: 95.843811\n",
      "Train Epoch: 14 [17920/60000 (30%)]\tLoss: 97.888298\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 98.679108\n",
      "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 101.799416\n",
      "Train Epoch: 14 [21760/60000 (36%)]\tLoss: 96.717445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 14 [23040/60000 (38%)]\tLoss: 97.664124\n",
      "Train Epoch: 14 [24320/60000 (41%)]\tLoss: 98.642357\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 98.114929\n",
      "Train Epoch: 14 [26880/60000 (45%)]\tLoss: 94.038750\n",
      "Train Epoch: 14 [28160/60000 (47%)]\tLoss: 97.472122\n",
      "Train Epoch: 14 [29440/60000 (49%)]\tLoss: 100.379623\n",
      "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 97.497375\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 93.623932\n",
      "Train Epoch: 14 [33280/60000 (55%)]\tLoss: 96.908493\n",
      "Train Epoch: 14 [34560/60000 (58%)]\tLoss: 98.272217\n",
      "Train Epoch: 14 [35840/60000 (60%)]\tLoss: 91.680862\n",
      "Train Epoch: 14 [37120/60000 (62%)]\tLoss: 94.576828\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 98.479393\n",
      "Train Epoch: 14 [39680/60000 (66%)]\tLoss: 99.276215\n",
      "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 97.613144\n",
      "Train Epoch: 14 [42240/60000 (70%)]\tLoss: 100.873154\n",
      "Train Epoch: 14 [43520/60000 (72%)]\tLoss: 99.534622\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 93.221542\n",
      "Train Epoch: 14 [46080/60000 (77%)]\tLoss: 93.454620\n",
      "Train Epoch: 14 [47360/60000 (79%)]\tLoss: 99.740860\n",
      "Train Epoch: 14 [48640/60000 (81%)]\tLoss: 102.907745\n",
      "Train Epoch: 14 [49920/60000 (83%)]\tLoss: 99.458809\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 100.476318\n",
      "Train Epoch: 14 [52480/60000 (87%)]\tLoss: 97.448647\n",
      "Train Epoch: 14 [53760/60000 (90%)]\tLoss: 100.222229\n",
      "Train Epoch: 14 [55040/60000 (92%)]\tLoss: 99.707565\n",
      "Train Epoch: 14 [56320/60000 (94%)]\tLoss: 98.624832\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 95.260468\n",
      "Train Epoch: 14 [58880/60000 (98%)]\tLoss: 99.907257\n",
      "====> Epoch: 14 Average loss: 97.7734\n",
      "====> Test set loss: 100.3459\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 98.070694\n",
      "Train Epoch: 15 [1280/60000 (2%)]\tLoss: 99.302200\n",
      "Train Epoch: 15 [2560/60000 (4%)]\tLoss: 94.699089\n",
      "Train Epoch: 15 [3840/60000 (6%)]\tLoss: 96.339081\n",
      "Train Epoch: 15 [5120/60000 (9%)]\tLoss: 98.530396\n",
      "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 97.045166\n",
      "Train Epoch: 15 [7680/60000 (13%)]\tLoss: 96.053665\n",
      "Train Epoch: 15 [8960/60000 (15%)]\tLoss: 99.639412\n",
      "Train Epoch: 15 [10240/60000 (17%)]\tLoss: 94.271706\n",
      "Train Epoch: 15 [11520/60000 (19%)]\tLoss: 100.595108\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 99.898224\n",
      "Train Epoch: 15 [14080/60000 (23%)]\tLoss: 93.294159\n",
      "Train Epoch: 15 [15360/60000 (26%)]\tLoss: 100.338165\n",
      "Train Epoch: 15 [16640/60000 (28%)]\tLoss: 99.999275\n",
      "Train Epoch: 15 [17920/60000 (30%)]\tLoss: 95.040184\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 97.042831\n",
      "Train Epoch: 15 [20480/60000 (34%)]\tLoss: 96.289680\n",
      "Train Epoch: 15 [21760/60000 (36%)]\tLoss: 99.050156\n",
      "Train Epoch: 15 [23040/60000 (38%)]\tLoss: 100.667969\n",
      "Train Epoch: 15 [24320/60000 (41%)]\tLoss: 98.556992\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 96.870209\n",
      "Train Epoch: 15 [26880/60000 (45%)]\tLoss: 95.360275\n",
      "Train Epoch: 15 [28160/60000 (47%)]\tLoss: 101.296921\n",
      "Train Epoch: 15 [29440/60000 (49%)]\tLoss: 99.373138\n",
      "Train Epoch: 15 [30720/60000 (51%)]\tLoss: 97.738693\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 94.989426\n",
      "Train Epoch: 15 [33280/60000 (55%)]\tLoss: 97.018478\n",
      "Train Epoch: 15 [34560/60000 (58%)]\tLoss: 96.076279\n",
      "Train Epoch: 15 [35840/60000 (60%)]\tLoss: 96.893631\n",
      "Train Epoch: 15 [37120/60000 (62%)]\tLoss: 100.262573\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 99.499039\n",
      "Train Epoch: 15 [39680/60000 (66%)]\tLoss: 98.375656\n",
      "Train Epoch: 15 [40960/60000 (68%)]\tLoss: 97.649200\n",
      "Train Epoch: 15 [42240/60000 (70%)]\tLoss: 101.320045\n",
      "Train Epoch: 15 [43520/60000 (72%)]\tLoss: 96.956985\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 96.674232\n",
      "Train Epoch: 15 [46080/60000 (77%)]\tLoss: 100.240707\n",
      "Train Epoch: 15 [47360/60000 (79%)]\tLoss: 100.151062\n",
      "Train Epoch: 15 [48640/60000 (81%)]\tLoss: 98.809830\n",
      "Train Epoch: 15 [49920/60000 (83%)]\tLoss: 98.462929\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 96.816101\n",
      "Train Epoch: 15 [52480/60000 (87%)]\tLoss: 97.225349\n",
      "Train Epoch: 15 [53760/60000 (90%)]\tLoss: 95.490837\n",
      "Train Epoch: 15 [55040/60000 (92%)]\tLoss: 98.698547\n",
      "Train Epoch: 15 [56320/60000 (94%)]\tLoss: 96.956108\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 98.071808\n",
      "Train Epoch: 15 [58880/60000 (98%)]\tLoss: 96.316391\n",
      "====> Epoch: 15 Average loss: 97.7526\n",
      "====> Test set loss: 99.3821\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 100.531387\n",
      "Train Epoch: 16 [1280/60000 (2%)]\tLoss: 96.401535\n",
      "Train Epoch: 16 [2560/60000 (4%)]\tLoss: 93.596138\n",
      "Train Epoch: 16 [3840/60000 (6%)]\tLoss: 97.749207\n",
      "Train Epoch: 16 [5120/60000 (9%)]\tLoss: 100.227112\n",
      "Train Epoch: 16 [6400/60000 (11%)]\tLoss: 96.400360\n",
      "Train Epoch: 16 [7680/60000 (13%)]\tLoss: 97.471039\n",
      "Train Epoch: 16 [8960/60000 (15%)]\tLoss: 95.980042\n",
      "Train Epoch: 16 [10240/60000 (17%)]\tLoss: 97.952377\n",
      "Train Epoch: 16 [11520/60000 (19%)]\tLoss: 100.105583\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 95.278534\n",
      "Train Epoch: 16 [14080/60000 (23%)]\tLoss: 98.630997\n",
      "Train Epoch: 16 [15360/60000 (26%)]\tLoss: 98.481140\n",
      "Train Epoch: 16 [16640/60000 (28%)]\tLoss: 100.231422\n",
      "Train Epoch: 16 [17920/60000 (30%)]\tLoss: 99.993286\n",
      "Train Epoch: 16 [19200/60000 (32%)]\tLoss: 95.083664\n",
      "Train Epoch: 16 [20480/60000 (34%)]\tLoss: 100.694359\n",
      "Train Epoch: 16 [21760/60000 (36%)]\tLoss: 98.307327\n",
      "Train Epoch: 16 [23040/60000 (38%)]\tLoss: 98.025154\n",
      "Train Epoch: 16 [24320/60000 (41%)]\tLoss: 95.760368\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 99.145958\n",
      "Train Epoch: 16 [26880/60000 (45%)]\tLoss: 95.705544\n",
      "Train Epoch: 16 [28160/60000 (47%)]\tLoss: 96.381691\n",
      "Train Epoch: 16 [29440/60000 (49%)]\tLoss: 99.643806\n",
      "Train Epoch: 16 [30720/60000 (51%)]\tLoss: 96.855148\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 99.145660\n",
      "Train Epoch: 16 [33280/60000 (55%)]\tLoss: 102.304596\n",
      "Train Epoch: 16 [34560/60000 (58%)]\tLoss: 100.077347\n",
      "Train Epoch: 16 [35840/60000 (60%)]\tLoss: 93.381744\n",
      "Train Epoch: 16 [37120/60000 (62%)]\tLoss: 99.859322\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 96.133514\n",
      "Train Epoch: 16 [39680/60000 (66%)]\tLoss: 91.762192\n",
      "Train Epoch: 16 [40960/60000 (68%)]\tLoss: 101.217789\n",
      "Train Epoch: 16 [42240/60000 (70%)]\tLoss: 97.487213\n",
      "Train Epoch: 16 [43520/60000 (72%)]\tLoss: 102.602798\n",
      "Train Epoch: 16 [44800/60000 (75%)]\tLoss: 94.388947\n",
      "Train Epoch: 16 [46080/60000 (77%)]\tLoss: 99.337830\n",
      "Train Epoch: 16 [47360/60000 (79%)]\tLoss: 96.122742\n",
      "Train Epoch: 16 [48640/60000 (81%)]\tLoss: 100.997818\n",
      "Train Epoch: 16 [49920/60000 (83%)]\tLoss: 97.253204\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 98.642830\n",
      "Train Epoch: 16 [52480/60000 (87%)]\tLoss: 97.873985\n",
      "Train Epoch: 16 [53760/60000 (90%)]\tLoss: 97.759766\n",
      "Train Epoch: 16 [55040/60000 (92%)]\tLoss: 95.210701\n",
      "Train Epoch: 16 [56320/60000 (94%)]\tLoss: 104.187515\n",
      "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 96.414230\n",
      "Train Epoch: 16 [58880/60000 (98%)]\tLoss: 100.471382\n",
      "====> Epoch: 16 Average loss: 97.7014\n",
      "====> Test set loss: 99.6686\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 96.364304\n",
      "Train Epoch: 17 [1280/60000 (2%)]\tLoss: 96.369148\n",
      "Train Epoch: 17 [2560/60000 (4%)]\tLoss: 98.111679\n",
      "Train Epoch: 17 [3840/60000 (6%)]\tLoss: 96.629929\n",
      "Train Epoch: 17 [5120/60000 (9%)]\tLoss: 97.333153\n",
      "Train Epoch: 17 [6400/60000 (11%)]\tLoss: 98.459389\n",
      "Train Epoch: 17 [7680/60000 (13%)]\tLoss: 93.235817\n",
      "Train Epoch: 17 [8960/60000 (15%)]\tLoss: 96.193810\n",
      "Train Epoch: 17 [10240/60000 (17%)]\tLoss: 94.898895\n",
      "Train Epoch: 17 [11520/60000 (19%)]\tLoss: 97.454620\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 98.113647\n",
      "Train Epoch: 17 [14080/60000 (23%)]\tLoss: 98.292282\n",
      "Train Epoch: 17 [15360/60000 (26%)]\tLoss: 99.831787\n",
      "Train Epoch: 17 [16640/60000 (28%)]\tLoss: 97.765617\n",
      "Train Epoch: 17 [17920/60000 (30%)]\tLoss: 100.127594\n",
      "Train Epoch: 17 [19200/60000 (32%)]\tLoss: 93.637314\n",
      "Train Epoch: 17 [20480/60000 (34%)]\tLoss: 97.158867\n",
      "Train Epoch: 17 [21760/60000 (36%)]\tLoss: 102.689316\n",
      "Train Epoch: 17 [23040/60000 (38%)]\tLoss: 95.639679\n",
      "Train Epoch: 17 [24320/60000 (41%)]\tLoss: 99.095215\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 95.069595\n",
      "Train Epoch: 17 [26880/60000 (45%)]\tLoss: 99.680779\n",
      "Train Epoch: 17 [28160/60000 (47%)]\tLoss: 99.757088\n",
      "Train Epoch: 17 [29440/60000 (49%)]\tLoss: 97.025589\n",
      "Train Epoch: 17 [30720/60000 (51%)]\tLoss: 97.419662\n",
      "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 94.613342\n",
      "Train Epoch: 17 [33280/60000 (55%)]\tLoss: 100.067429\n",
      "Train Epoch: 17 [34560/60000 (58%)]\tLoss: 97.207306\n",
      "Train Epoch: 17 [35840/60000 (60%)]\tLoss: 97.636696\n",
      "Train Epoch: 17 [37120/60000 (62%)]\tLoss: 99.127686\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 94.964172\n",
      "Train Epoch: 17 [39680/60000 (66%)]\tLoss: 100.284790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 17 [40960/60000 (68%)]\tLoss: 95.713425\n",
      "Train Epoch: 17 [42240/60000 (70%)]\tLoss: 98.311569\n",
      "Train Epoch: 17 [43520/60000 (72%)]\tLoss: 99.005875\n",
      "Train Epoch: 17 [44800/60000 (75%)]\tLoss: 95.605721\n",
      "Train Epoch: 17 [46080/60000 (77%)]\tLoss: 95.864853\n",
      "Train Epoch: 17 [47360/60000 (79%)]\tLoss: 96.982468\n",
      "Train Epoch: 17 [48640/60000 (81%)]\tLoss: 101.751511\n",
      "Train Epoch: 17 [49920/60000 (83%)]\tLoss: 99.506294\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 101.066666\n",
      "Train Epoch: 17 [52480/60000 (87%)]\tLoss: 97.122665\n",
      "Train Epoch: 17 [53760/60000 (90%)]\tLoss: 97.387756\n",
      "Train Epoch: 17 [55040/60000 (92%)]\tLoss: 99.254837\n",
      "Train Epoch: 17 [56320/60000 (94%)]\tLoss: 102.060555\n",
      "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 99.145859\n",
      "Train Epoch: 17 [58880/60000 (98%)]\tLoss: 96.611023\n",
      "====> Epoch: 17 Average loss: 97.6930\n",
      "====> Test set loss: 99.4443\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 94.508026\n",
      "Train Epoch: 18 [1280/60000 (2%)]\tLoss: 94.974075\n",
      "Train Epoch: 18 [2560/60000 (4%)]\tLoss: 98.374161\n",
      "Train Epoch: 18 [3840/60000 (6%)]\tLoss: 96.719200\n",
      "Train Epoch: 18 [5120/60000 (9%)]\tLoss: 91.943130\n",
      "Train Epoch: 18 [6400/60000 (11%)]\tLoss: 100.591751\n",
      "Train Epoch: 18 [7680/60000 (13%)]\tLoss: 94.900299\n",
      "Train Epoch: 18 [8960/60000 (15%)]\tLoss: 98.281693\n",
      "Train Epoch: 18 [10240/60000 (17%)]\tLoss: 98.955109\n",
      "Train Epoch: 18 [11520/60000 (19%)]\tLoss: 98.459305\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 100.504532\n",
      "Train Epoch: 18 [14080/60000 (23%)]\tLoss: 99.895653\n",
      "Train Epoch: 18 [15360/60000 (26%)]\tLoss: 94.432884\n",
      "Train Epoch: 18 [16640/60000 (28%)]\tLoss: 99.589600\n",
      "Train Epoch: 18 [17920/60000 (30%)]\tLoss: 94.645065\n",
      "Train Epoch: 18 [19200/60000 (32%)]\tLoss: 97.260864\n",
      "Train Epoch: 18 [20480/60000 (34%)]\tLoss: 95.073471\n",
      "Train Epoch: 18 [21760/60000 (36%)]\tLoss: 91.861931\n",
      "Train Epoch: 18 [23040/60000 (38%)]\tLoss: 97.859184\n",
      "Train Epoch: 18 [24320/60000 (41%)]\tLoss: 99.133316\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 99.912994\n",
      "Train Epoch: 18 [26880/60000 (45%)]\tLoss: 98.946709\n",
      "Train Epoch: 18 [28160/60000 (47%)]\tLoss: 99.372116\n",
      "Train Epoch: 18 [29440/60000 (49%)]\tLoss: 95.598274\n",
      "Train Epoch: 18 [30720/60000 (51%)]\tLoss: 95.253494\n",
      "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 94.282860\n",
      "Train Epoch: 18 [33280/60000 (55%)]\tLoss: 94.794724\n",
      "Train Epoch: 18 [34560/60000 (58%)]\tLoss: 98.227516\n",
      "Train Epoch: 18 [35840/60000 (60%)]\tLoss: 96.673279\n",
      "Train Epoch: 18 [37120/60000 (62%)]\tLoss: 101.307098\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 99.059532\n",
      "Train Epoch: 18 [39680/60000 (66%)]\tLoss: 100.088211\n",
      "Train Epoch: 18 [40960/60000 (68%)]\tLoss: 99.124023\n",
      "Train Epoch: 18 [42240/60000 (70%)]\tLoss: 97.666191\n",
      "Train Epoch: 18 [43520/60000 (72%)]\tLoss: 97.007980\n",
      "Train Epoch: 18 [44800/60000 (75%)]\tLoss: 95.904526\n",
      "Train Epoch: 18 [46080/60000 (77%)]\tLoss: 100.460983\n",
      "Train Epoch: 18 [47360/60000 (79%)]\tLoss: 96.946274\n",
      "Train Epoch: 18 [48640/60000 (81%)]\tLoss: 97.471527\n",
      "Train Epoch: 18 [49920/60000 (83%)]\tLoss: 99.215889\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 97.881233\n",
      "Train Epoch: 18 [52480/60000 (87%)]\tLoss: 97.163773\n",
      "Train Epoch: 18 [53760/60000 (90%)]\tLoss: 99.205887\n",
      "Train Epoch: 18 [55040/60000 (92%)]\tLoss: 99.019455\n",
      "Train Epoch: 18 [56320/60000 (94%)]\tLoss: 100.468536\n",
      "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 95.117584\n",
      "Train Epoch: 18 [58880/60000 (98%)]\tLoss: 97.819809\n",
      "====> Epoch: 18 Average loss: 97.6544\n",
      "====> Test set loss: 99.3251\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 96.858551\n",
      "Train Epoch: 19 [1280/60000 (2%)]\tLoss: 97.197754\n",
      "Train Epoch: 19 [2560/60000 (4%)]\tLoss: 96.587280\n",
      "Train Epoch: 19 [3840/60000 (6%)]\tLoss: 98.413895\n",
      "Train Epoch: 19 [5120/60000 (9%)]\tLoss: 98.068985\n",
      "Train Epoch: 19 [6400/60000 (11%)]\tLoss: 96.537979\n",
      "Train Epoch: 19 [7680/60000 (13%)]\tLoss: 95.836845\n",
      "Train Epoch: 19 [8960/60000 (15%)]\tLoss: 97.478210\n",
      "Train Epoch: 19 [10240/60000 (17%)]\tLoss: 95.423676\n",
      "Train Epoch: 19 [11520/60000 (19%)]\tLoss: 95.445908\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 99.067383\n",
      "Train Epoch: 19 [14080/60000 (23%)]\tLoss: 95.337982\n",
      "Train Epoch: 19 [15360/60000 (26%)]\tLoss: 95.650948\n",
      "Train Epoch: 19 [16640/60000 (28%)]\tLoss: 94.837555\n",
      "Train Epoch: 19 [17920/60000 (30%)]\tLoss: 97.790154\n",
      "Train Epoch: 19 [19200/60000 (32%)]\tLoss: 99.263870\n",
      "Train Epoch: 19 [20480/60000 (34%)]\tLoss: 101.338028\n",
      "Train Epoch: 19 [21760/60000 (36%)]\tLoss: 94.635757\n",
      "Train Epoch: 19 [23040/60000 (38%)]\tLoss: 97.836449\n",
      "Train Epoch: 19 [24320/60000 (41%)]\tLoss: 98.173523\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 99.690170\n",
      "Train Epoch: 19 [26880/60000 (45%)]\tLoss: 101.159622\n",
      "Train Epoch: 19 [28160/60000 (47%)]\tLoss: 96.467056\n",
      "Train Epoch: 19 [29440/60000 (49%)]\tLoss: 97.399147\n",
      "Train Epoch: 19 [30720/60000 (51%)]\tLoss: 98.610001\n",
      "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 95.252563\n",
      "Train Epoch: 19 [33280/60000 (55%)]\tLoss: 99.843674\n",
      "Train Epoch: 19 [34560/60000 (58%)]\tLoss: 95.021538\n",
      "Train Epoch: 19 [35840/60000 (60%)]\tLoss: 100.079704\n",
      "Train Epoch: 19 [37120/60000 (62%)]\tLoss: 94.586380\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 99.505600\n",
      "Train Epoch: 19 [39680/60000 (66%)]\tLoss: 91.357498\n",
      "Train Epoch: 19 [40960/60000 (68%)]\tLoss: 95.681099\n",
      "Train Epoch: 19 [42240/60000 (70%)]\tLoss: 100.847801\n",
      "Train Epoch: 19 [43520/60000 (72%)]\tLoss: 93.555374\n",
      "Train Epoch: 19 [44800/60000 (75%)]\tLoss: 99.292580\n",
      "Train Epoch: 19 [46080/60000 (77%)]\tLoss: 96.469269\n",
      "Train Epoch: 19 [47360/60000 (79%)]\tLoss: 96.265793\n",
      "Train Epoch: 19 [48640/60000 (81%)]\tLoss: 97.290329\n",
      "Train Epoch: 19 [49920/60000 (83%)]\tLoss: 100.960541\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 98.537888\n",
      "Train Epoch: 19 [52480/60000 (87%)]\tLoss: 101.430588\n",
      "Train Epoch: 19 [53760/60000 (90%)]\tLoss: 95.914238\n",
      "Train Epoch: 19 [55040/60000 (92%)]\tLoss: 96.355331\n",
      "Train Epoch: 19 [56320/60000 (94%)]\tLoss: 98.801361\n",
      "Train Epoch: 19 [57600/60000 (96%)]\tLoss: 100.447289\n",
      "Train Epoch: 19 [58880/60000 (98%)]\tLoss: 100.703491\n",
      "====> Epoch: 19 Average loss: 97.6487\n",
      "====> Test set loss: 99.1944\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 96.913689\n",
      "Train Epoch: 20 [1280/60000 (2%)]\tLoss: 96.511887\n",
      "Train Epoch: 20 [2560/60000 (4%)]\tLoss: 96.607109\n",
      "Train Epoch: 20 [3840/60000 (6%)]\tLoss: 94.126892\n",
      "Train Epoch: 20 [5120/60000 (9%)]\tLoss: 94.500816\n",
      "Train Epoch: 20 [6400/60000 (11%)]\tLoss: 99.409866\n",
      "Train Epoch: 20 [7680/60000 (13%)]\tLoss: 98.214424\n",
      "Train Epoch: 20 [8960/60000 (15%)]\tLoss: 96.445862\n",
      "Train Epoch: 20 [10240/60000 (17%)]\tLoss: 98.105087\n",
      "Train Epoch: 20 [11520/60000 (19%)]\tLoss: 99.419464\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 97.492844\n",
      "Train Epoch: 20 [14080/60000 (23%)]\tLoss: 98.704483\n",
      "Train Epoch: 20 [15360/60000 (26%)]\tLoss: 95.866867\n",
      "Train Epoch: 20 [16640/60000 (28%)]\tLoss: 96.938843\n",
      "Train Epoch: 20 [17920/60000 (30%)]\tLoss: 99.269012\n",
      "Train Epoch: 20 [19200/60000 (32%)]\tLoss: 99.970154\n",
      "Train Epoch: 20 [20480/60000 (34%)]\tLoss: 93.673126\n",
      "Train Epoch: 20 [21760/60000 (36%)]\tLoss: 96.198250\n",
      "Train Epoch: 20 [23040/60000 (38%)]\tLoss: 97.661263\n",
      "Train Epoch: 20 [24320/60000 (41%)]\tLoss: 95.392380\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 93.861343\n",
      "Train Epoch: 20 [26880/60000 (45%)]\tLoss: 94.439713\n",
      "Train Epoch: 20 [28160/60000 (47%)]\tLoss: 93.534271\n",
      "Train Epoch: 20 [29440/60000 (49%)]\tLoss: 97.505066\n",
      "Train Epoch: 20 [30720/60000 (51%)]\tLoss: 95.644707\n",
      "Train Epoch: 20 [32000/60000 (53%)]\tLoss: 95.092560\n",
      "Train Epoch: 20 [33280/60000 (55%)]\tLoss: 97.641357\n",
      "Train Epoch: 20 [34560/60000 (58%)]\tLoss: 98.193268\n",
      "Train Epoch: 20 [35840/60000 (60%)]\tLoss: 100.322021\n",
      "Train Epoch: 20 [37120/60000 (62%)]\tLoss: 97.760147\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 98.178482\n",
      "Train Epoch: 20 [39680/60000 (66%)]\tLoss: 95.663528\n",
      "Train Epoch: 20 [40960/60000 (68%)]\tLoss: 96.206543\n",
      "Train Epoch: 20 [42240/60000 (70%)]\tLoss: 93.436317\n",
      "Train Epoch: 20 [43520/60000 (72%)]\tLoss: 98.724289\n",
      "Train Epoch: 20 [44800/60000 (75%)]\tLoss: 98.612289\n",
      "Train Epoch: 20 [46080/60000 (77%)]\tLoss: 98.715652\n",
      "Train Epoch: 20 [47360/60000 (79%)]\tLoss: 97.025909\n",
      "Train Epoch: 20 [48640/60000 (81%)]\tLoss: 95.524948\n",
      "Train Epoch: 20 [49920/60000 (83%)]\tLoss: 99.220734\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 98.825409\n",
      "Train Epoch: 20 [52480/60000 (87%)]\tLoss: 94.120430\n",
      "Train Epoch: 20 [53760/60000 (90%)]\tLoss: 96.849220\n",
      "Train Epoch: 20 [55040/60000 (92%)]\tLoss: 99.848152\n",
      "Train Epoch: 20 [56320/60000 (94%)]\tLoss: 96.008163\n",
      "Train Epoch: 20 [57600/60000 (96%)]\tLoss: 96.543274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 20 [58880/60000 (98%)]\tLoss: 95.127403\n",
      "====> Epoch: 20 Average loss: 97.6187\n",
      "====> Test set loss: 99.3499\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 96.977280\n",
      "Train Epoch: 21 [1280/60000 (2%)]\tLoss: 99.484314\n",
      "Train Epoch: 21 [2560/60000 (4%)]\tLoss: 97.955490\n",
      "Train Epoch: 21 [3840/60000 (6%)]\tLoss: 91.257973\n",
      "Train Epoch: 21 [5120/60000 (9%)]\tLoss: 102.969307\n",
      "Train Epoch: 21 [6400/60000 (11%)]\tLoss: 94.719818\n",
      "Train Epoch: 21 [7680/60000 (13%)]\tLoss: 97.080566\n",
      "Train Epoch: 21 [8960/60000 (15%)]\tLoss: 94.105690\n",
      "Train Epoch: 21 [10240/60000 (17%)]\tLoss: 93.978027\n",
      "Train Epoch: 21 [11520/60000 (19%)]\tLoss: 97.330322\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 97.192726\n",
      "Train Epoch: 21 [14080/60000 (23%)]\tLoss: 100.161819\n",
      "Train Epoch: 21 [15360/60000 (26%)]\tLoss: 95.762047\n",
      "Train Epoch: 21 [16640/60000 (28%)]\tLoss: 97.784882\n",
      "Train Epoch: 21 [17920/60000 (30%)]\tLoss: 96.603645\n",
      "Train Epoch: 21 [19200/60000 (32%)]\tLoss: 98.695045\n",
      "Train Epoch: 21 [20480/60000 (34%)]\tLoss: 99.065689\n",
      "Train Epoch: 21 [21760/60000 (36%)]\tLoss: 98.578537\n",
      "Train Epoch: 21 [23040/60000 (38%)]\tLoss: 99.682434\n",
      "Train Epoch: 21 [24320/60000 (41%)]\tLoss: 98.151993\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 99.478394\n",
      "Train Epoch: 21 [26880/60000 (45%)]\tLoss: 100.845627\n",
      "Train Epoch: 21 [28160/60000 (47%)]\tLoss: 92.493134\n",
      "Train Epoch: 21 [29440/60000 (49%)]\tLoss: 97.516762\n",
      "Train Epoch: 21 [30720/60000 (51%)]\tLoss: 97.428230\n",
      "Train Epoch: 21 [32000/60000 (53%)]\tLoss: 96.259735\n",
      "Train Epoch: 21 [33280/60000 (55%)]\tLoss: 97.149399\n",
      "Train Epoch: 21 [34560/60000 (58%)]\tLoss: 98.681519\n",
      "Train Epoch: 21 [35840/60000 (60%)]\tLoss: 94.927727\n",
      "Train Epoch: 21 [37120/60000 (62%)]\tLoss: 99.450203\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 97.854385\n",
      "Train Epoch: 21 [39680/60000 (66%)]\tLoss: 99.107719\n",
      "Train Epoch: 21 [40960/60000 (68%)]\tLoss: 101.237808\n",
      "Train Epoch: 21 [42240/60000 (70%)]\tLoss: 100.054016\n",
      "Train Epoch: 21 [43520/60000 (72%)]\tLoss: 98.897491\n",
      "Train Epoch: 21 [44800/60000 (75%)]\tLoss: 101.401947\n",
      "Train Epoch: 21 [46080/60000 (77%)]\tLoss: 96.804581\n",
      "Train Epoch: 21 [47360/60000 (79%)]\tLoss: 100.689926\n",
      "Train Epoch: 21 [48640/60000 (81%)]\tLoss: 99.935425\n",
      "Train Epoch: 21 [49920/60000 (83%)]\tLoss: 95.943954\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 93.886368\n",
      "Train Epoch: 21 [52480/60000 (87%)]\tLoss: 96.194901\n",
      "Train Epoch: 21 [53760/60000 (90%)]\tLoss: 97.487640\n",
      "Train Epoch: 21 [55040/60000 (92%)]\tLoss: 96.073151\n",
      "Train Epoch: 21 [56320/60000 (94%)]\tLoss: 97.524780\n",
      "Train Epoch: 21 [57600/60000 (96%)]\tLoss: 93.254013\n",
      "Train Epoch: 21 [58880/60000 (98%)]\tLoss: 98.648087\n",
      "====> Epoch: 21 Average loss: 97.5811\n",
      "====> Test set loss: 99.2487\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 95.029320\n",
      "Train Epoch: 22 [1280/60000 (2%)]\tLoss: 99.981445\n",
      "Train Epoch: 22 [2560/60000 (4%)]\tLoss: 94.131706\n",
      "Train Epoch: 22 [3840/60000 (6%)]\tLoss: 96.237999\n",
      "Train Epoch: 22 [5120/60000 (9%)]\tLoss: 99.352684\n",
      "Train Epoch: 22 [6400/60000 (11%)]\tLoss: 101.603561\n",
      "Train Epoch: 22 [7680/60000 (13%)]\tLoss: 96.579788\n",
      "Train Epoch: 22 [8960/60000 (15%)]\tLoss: 96.369011\n",
      "Train Epoch: 22 [10240/60000 (17%)]\tLoss: 96.619476\n",
      "Train Epoch: 22 [11520/60000 (19%)]\tLoss: 99.900375\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 92.586937\n",
      "Train Epoch: 22 [14080/60000 (23%)]\tLoss: 99.795280\n",
      "Train Epoch: 22 [15360/60000 (26%)]\tLoss: 98.338249\n",
      "Train Epoch: 22 [16640/60000 (28%)]\tLoss: 97.472626\n",
      "Train Epoch: 22 [17920/60000 (30%)]\tLoss: 91.239288\n",
      "Train Epoch: 22 [19200/60000 (32%)]\tLoss: 98.974754\n",
      "Train Epoch: 22 [20480/60000 (34%)]\tLoss: 100.186897\n",
      "Train Epoch: 22 [21760/60000 (36%)]\tLoss: 93.072021\n",
      "Train Epoch: 22 [23040/60000 (38%)]\tLoss: 97.154877\n",
      "Train Epoch: 22 [24320/60000 (41%)]\tLoss: 95.782181\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 95.446121\n",
      "Train Epoch: 22 [26880/60000 (45%)]\tLoss: 99.860062\n",
      "Train Epoch: 22 [28160/60000 (47%)]\tLoss: 99.085556\n",
      "Train Epoch: 22 [29440/60000 (49%)]\tLoss: 99.558945\n",
      "Train Epoch: 22 [30720/60000 (51%)]\tLoss: 96.962364\n",
      "Train Epoch: 22 [32000/60000 (53%)]\tLoss: 97.775650\n",
      "Train Epoch: 22 [33280/60000 (55%)]\tLoss: 97.919159\n",
      "Train Epoch: 22 [34560/60000 (58%)]\tLoss: 97.641342\n",
      "Train Epoch: 22 [35840/60000 (60%)]\tLoss: 98.673340\n",
      "Train Epoch: 22 [37120/60000 (62%)]\tLoss: 95.640938\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 95.151413\n",
      "Train Epoch: 22 [39680/60000 (66%)]\tLoss: 94.227684\n",
      "Train Epoch: 22 [40960/60000 (68%)]\tLoss: 100.659065\n",
      "Train Epoch: 22 [42240/60000 (70%)]\tLoss: 91.971329\n",
      "Train Epoch: 22 [43520/60000 (72%)]\tLoss: 94.533195\n",
      "Train Epoch: 22 [44800/60000 (75%)]\tLoss: 100.849335\n",
      "Train Epoch: 22 [46080/60000 (77%)]\tLoss: 96.697617\n",
      "Train Epoch: 22 [47360/60000 (79%)]\tLoss: 96.158066\n",
      "Train Epoch: 22 [48640/60000 (81%)]\tLoss: 96.917015\n",
      "Train Epoch: 22 [49920/60000 (83%)]\tLoss: 95.492989\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 95.887695\n",
      "Train Epoch: 22 [52480/60000 (87%)]\tLoss: 96.451324\n",
      "Train Epoch: 22 [53760/60000 (90%)]\tLoss: 98.953522\n",
      "Train Epoch: 22 [55040/60000 (92%)]\tLoss: 96.309967\n",
      "Train Epoch: 22 [56320/60000 (94%)]\tLoss: 99.238937\n",
      "Train Epoch: 22 [57600/60000 (96%)]\tLoss: 99.372116\n",
      "Train Epoch: 22 [58880/60000 (98%)]\tLoss: 97.363800\n",
      "====> Epoch: 22 Average loss: 97.5192\n",
      "====> Test set loss: 99.1717\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 98.783859\n",
      "Train Epoch: 23 [1280/60000 (2%)]\tLoss: 96.437462\n",
      "Train Epoch: 23 [2560/60000 (4%)]\tLoss: 98.783203\n",
      "Train Epoch: 23 [3840/60000 (6%)]\tLoss: 96.780525\n",
      "Train Epoch: 23 [5120/60000 (9%)]\tLoss: 96.951439\n",
      "Train Epoch: 23 [6400/60000 (11%)]\tLoss: 97.216125\n",
      "Train Epoch: 23 [7680/60000 (13%)]\tLoss: 99.522995\n",
      "Train Epoch: 23 [8960/60000 (15%)]\tLoss: 96.963455\n",
      "Train Epoch: 23 [10240/60000 (17%)]\tLoss: 94.810326\n",
      "Train Epoch: 23 [11520/60000 (19%)]\tLoss: 94.723289\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 101.579834\n",
      "Train Epoch: 23 [14080/60000 (23%)]\tLoss: 95.774635\n",
      "Train Epoch: 23 [15360/60000 (26%)]\tLoss: 95.734276\n",
      "Train Epoch: 23 [16640/60000 (28%)]\tLoss: 99.027100\n",
      "Train Epoch: 23 [17920/60000 (30%)]\tLoss: 97.591362\n",
      "Train Epoch: 23 [19200/60000 (32%)]\tLoss: 95.659058\n",
      "Train Epoch: 23 [20480/60000 (34%)]\tLoss: 100.725464\n",
      "Train Epoch: 23 [21760/60000 (36%)]\tLoss: 95.878014\n",
      "Train Epoch: 23 [23040/60000 (38%)]\tLoss: 98.747826\n",
      "Train Epoch: 23 [24320/60000 (41%)]\tLoss: 96.974800\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 93.388504\n",
      "Train Epoch: 23 [26880/60000 (45%)]\tLoss: 100.321259\n",
      "Train Epoch: 23 [28160/60000 (47%)]\tLoss: 98.915970\n",
      "Train Epoch: 23 [29440/60000 (49%)]\tLoss: 97.372086\n",
      "Train Epoch: 23 [30720/60000 (51%)]\tLoss: 97.473244\n",
      "Train Epoch: 23 [32000/60000 (53%)]\tLoss: 95.576859\n",
      "Train Epoch: 23 [33280/60000 (55%)]\tLoss: 94.640961\n",
      "Train Epoch: 23 [34560/60000 (58%)]\tLoss: 96.439804\n",
      "Train Epoch: 23 [35840/60000 (60%)]\tLoss: 99.143089\n",
      "Train Epoch: 23 [37120/60000 (62%)]\tLoss: 95.594322\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 96.210587\n",
      "Train Epoch: 23 [39680/60000 (66%)]\tLoss: 96.431282\n",
      "Train Epoch: 23 [40960/60000 (68%)]\tLoss: 96.172546\n",
      "Train Epoch: 23 [42240/60000 (70%)]\tLoss: 95.348236\n",
      "Train Epoch: 23 [43520/60000 (72%)]\tLoss: 92.229187\n",
      "Train Epoch: 23 [44800/60000 (75%)]\tLoss: 100.735931\n",
      "Train Epoch: 23 [46080/60000 (77%)]\tLoss: 96.162994\n",
      "Train Epoch: 23 [47360/60000 (79%)]\tLoss: 97.951424\n",
      "Train Epoch: 23 [48640/60000 (81%)]\tLoss: 97.607819\n",
      "Train Epoch: 23 [49920/60000 (83%)]\tLoss: 99.412239\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 94.607574\n",
      "Train Epoch: 23 [52480/60000 (87%)]\tLoss: 98.723892\n",
      "Train Epoch: 23 [53760/60000 (90%)]\tLoss: 102.315956\n",
      "Train Epoch: 23 [55040/60000 (92%)]\tLoss: 95.649460\n",
      "Train Epoch: 23 [56320/60000 (94%)]\tLoss: 96.361816\n",
      "Train Epoch: 23 [57600/60000 (96%)]\tLoss: 94.419800\n",
      "Train Epoch: 23 [58880/60000 (98%)]\tLoss: 95.911209\n",
      "====> Epoch: 23 Average loss: 97.5288\n",
      "====> Test set loss: 99.3440\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 96.024902\n",
      "Train Epoch: 24 [1280/60000 (2%)]\tLoss: 92.413895\n",
      "Train Epoch: 24 [2560/60000 (4%)]\tLoss: 99.520958\n",
      "Train Epoch: 24 [3840/60000 (6%)]\tLoss: 92.928101\n",
      "Train Epoch: 24 [5120/60000 (9%)]\tLoss: 97.000290\n",
      "Train Epoch: 24 [6400/60000 (11%)]\tLoss: 97.070267\n",
      "Train Epoch: 24 [7680/60000 (13%)]\tLoss: 98.193459\n",
      "Train Epoch: 24 [8960/60000 (15%)]\tLoss: 94.162735\n",
      "Train Epoch: 24 [10240/60000 (17%)]\tLoss: 96.915314\n",
      "Train Epoch: 24 [11520/60000 (19%)]\tLoss: 100.905655\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 99.704750\n",
      "Train Epoch: 24 [14080/60000 (23%)]\tLoss: 95.047241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 24 [15360/60000 (26%)]\tLoss: 97.299118\n",
      "Train Epoch: 24 [16640/60000 (28%)]\tLoss: 95.900208\n",
      "Train Epoch: 24 [17920/60000 (30%)]\tLoss: 100.492584\n",
      "Train Epoch: 24 [19200/60000 (32%)]\tLoss: 99.759949\n",
      "Train Epoch: 24 [20480/60000 (34%)]\tLoss: 95.314529\n",
      "Train Epoch: 24 [21760/60000 (36%)]\tLoss: 99.341782\n",
      "Train Epoch: 24 [23040/60000 (38%)]\tLoss: 97.844650\n",
      "Train Epoch: 24 [24320/60000 (41%)]\tLoss: 96.199081\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 96.868896\n",
      "Train Epoch: 24 [26880/60000 (45%)]\tLoss: 98.420288\n",
      "Train Epoch: 24 [28160/60000 (47%)]\tLoss: 100.163048\n",
      "Train Epoch: 24 [29440/60000 (49%)]\tLoss: 94.807266\n",
      "Train Epoch: 24 [30720/60000 (51%)]\tLoss: 99.417786\n",
      "Train Epoch: 24 [32000/60000 (53%)]\tLoss: 96.878571\n",
      "Train Epoch: 24 [33280/60000 (55%)]\tLoss: 94.595802\n",
      "Train Epoch: 24 [34560/60000 (58%)]\tLoss: 102.676949\n",
      "Train Epoch: 24 [35840/60000 (60%)]\tLoss: 94.741302\n",
      "Train Epoch: 24 [37120/60000 (62%)]\tLoss: 93.985672\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 96.781433\n",
      "Train Epoch: 24 [39680/60000 (66%)]\tLoss: 97.457581\n",
      "Train Epoch: 24 [40960/60000 (68%)]\tLoss: 97.299866\n",
      "Train Epoch: 24 [42240/60000 (70%)]\tLoss: 97.225418\n",
      "Train Epoch: 24 [43520/60000 (72%)]\tLoss: 96.764992\n",
      "Train Epoch: 24 [44800/60000 (75%)]\tLoss: 99.162476\n",
      "Train Epoch: 24 [46080/60000 (77%)]\tLoss: 95.455482\n",
      "Train Epoch: 24 [47360/60000 (79%)]\tLoss: 96.188522\n",
      "Train Epoch: 24 [48640/60000 (81%)]\tLoss: 95.304611\n",
      "Train Epoch: 24 [49920/60000 (83%)]\tLoss: 94.239548\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 93.799004\n",
      "Train Epoch: 24 [52480/60000 (87%)]\tLoss: 94.713066\n",
      "Train Epoch: 24 [53760/60000 (90%)]\tLoss: 96.179550\n",
      "Train Epoch: 24 [55040/60000 (92%)]\tLoss: 96.837494\n",
      "Train Epoch: 24 [56320/60000 (94%)]\tLoss: 100.415909\n",
      "Train Epoch: 24 [57600/60000 (96%)]\tLoss: 94.735031\n",
      "Train Epoch: 24 [58880/60000 (98%)]\tLoss: 99.589775\n",
      "====> Epoch: 24 Average loss: 97.4880\n",
      "====> Test set loss: 99.2632\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 98.207703\n",
      "Train Epoch: 25 [1280/60000 (2%)]\tLoss: 92.523628\n",
      "Train Epoch: 25 [2560/60000 (4%)]\tLoss: 92.129440\n",
      "Train Epoch: 25 [3840/60000 (6%)]\tLoss: 98.716904\n",
      "Train Epoch: 25 [5120/60000 (9%)]\tLoss: 98.254814\n",
      "Train Epoch: 25 [6400/60000 (11%)]\tLoss: 98.956146\n",
      "Train Epoch: 25 [7680/60000 (13%)]\tLoss: 95.727806\n",
      "Train Epoch: 25 [8960/60000 (15%)]\tLoss: 103.350830\n",
      "Train Epoch: 25 [10240/60000 (17%)]\tLoss: 93.128448\n",
      "Train Epoch: 25 [11520/60000 (19%)]\tLoss: 96.558510\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 98.124100\n",
      "Train Epoch: 25 [14080/60000 (23%)]\tLoss: 94.822105\n",
      "Train Epoch: 25 [15360/60000 (26%)]\tLoss: 97.189880\n",
      "Train Epoch: 25 [16640/60000 (28%)]\tLoss: 93.133911\n",
      "Train Epoch: 25 [17920/60000 (30%)]\tLoss: 95.926216\n",
      "Train Epoch: 25 [19200/60000 (32%)]\tLoss: 95.169724\n",
      "Train Epoch: 25 [20480/60000 (34%)]\tLoss: 97.717522\n",
      "Train Epoch: 25 [21760/60000 (36%)]\tLoss: 97.272156\n",
      "Train Epoch: 25 [23040/60000 (38%)]\tLoss: 96.355682\n",
      "Train Epoch: 25 [24320/60000 (41%)]\tLoss: 95.204460\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 97.987198\n",
      "Train Epoch: 25 [26880/60000 (45%)]\tLoss: 100.883301\n",
      "Train Epoch: 25 [28160/60000 (47%)]\tLoss: 98.655685\n",
      "Train Epoch: 25 [29440/60000 (49%)]\tLoss: 93.483459\n",
      "Train Epoch: 25 [30720/60000 (51%)]\tLoss: 100.878769\n",
      "Train Epoch: 25 [32000/60000 (53%)]\tLoss: 96.117027\n",
      "Train Epoch: 25 [33280/60000 (55%)]\tLoss: 98.028191\n",
      "Train Epoch: 25 [34560/60000 (58%)]\tLoss: 98.134109\n",
      "Train Epoch: 25 [35840/60000 (60%)]\tLoss: 96.970604\n",
      "Train Epoch: 25 [37120/60000 (62%)]\tLoss: 99.249908\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 95.591492\n",
      "Train Epoch: 25 [39680/60000 (66%)]\tLoss: 96.760269\n",
      "Train Epoch: 25 [40960/60000 (68%)]\tLoss: 101.583504\n",
      "Train Epoch: 25 [42240/60000 (70%)]\tLoss: 98.254211\n",
      "Train Epoch: 25 [43520/60000 (72%)]\tLoss: 97.168289\n",
      "Train Epoch: 25 [44800/60000 (75%)]\tLoss: 99.745941\n",
      "Train Epoch: 25 [46080/60000 (77%)]\tLoss: 94.610443\n",
      "Train Epoch: 25 [47360/60000 (79%)]\tLoss: 100.582863\n",
      "Train Epoch: 25 [48640/60000 (81%)]\tLoss: 96.236816\n",
      "Train Epoch: 25 [49920/60000 (83%)]\tLoss: 101.929565\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 98.144272\n",
      "Train Epoch: 25 [52480/60000 (87%)]\tLoss: 95.512993\n",
      "Train Epoch: 25 [53760/60000 (90%)]\tLoss: 98.978561\n",
      "Train Epoch: 25 [55040/60000 (92%)]\tLoss: 95.149162\n",
      "Train Epoch: 25 [56320/60000 (94%)]\tLoss: 98.694260\n",
      "Train Epoch: 25 [57600/60000 (96%)]\tLoss: 98.875641\n",
      "Train Epoch: 25 [58880/60000 (98%)]\tLoss: 97.174026\n",
      "====> Epoch: 25 Average loss: 97.4183\n",
      "====> Test set loss: 99.2179\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 95.909264\n",
      "Train Epoch: 26 [1280/60000 (2%)]\tLoss: 97.936752\n",
      "Train Epoch: 26 [2560/60000 (4%)]\tLoss: 97.067841\n",
      "Train Epoch: 26 [3840/60000 (6%)]\tLoss: 100.509300\n",
      "Train Epoch: 26 [5120/60000 (9%)]\tLoss: 95.334229\n",
      "Train Epoch: 26 [6400/60000 (11%)]\tLoss: 94.634338\n",
      "Train Epoch: 26 [7680/60000 (13%)]\tLoss: 97.135666\n",
      "Train Epoch: 26 [8960/60000 (15%)]\tLoss: 96.288605\n",
      "Train Epoch: 26 [10240/60000 (17%)]\tLoss: 98.093788\n",
      "Train Epoch: 26 [11520/60000 (19%)]\tLoss: 93.568344\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 101.077484\n",
      "Train Epoch: 26 [14080/60000 (23%)]\tLoss: 97.727028\n",
      "Train Epoch: 26 [15360/60000 (26%)]\tLoss: 93.613831\n",
      "Train Epoch: 26 [16640/60000 (28%)]\tLoss: 96.721001\n",
      "Train Epoch: 26 [17920/60000 (30%)]\tLoss: 102.244637\n",
      "Train Epoch: 26 [19200/60000 (32%)]\tLoss: 92.041977\n",
      "Train Epoch: 26 [20480/60000 (34%)]\tLoss: 100.399353\n",
      "Train Epoch: 26 [21760/60000 (36%)]\tLoss: 96.658226\n",
      "Train Epoch: 26 [23040/60000 (38%)]\tLoss: 99.674332\n",
      "Train Epoch: 26 [24320/60000 (41%)]\tLoss: 97.987488\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 97.769768\n",
      "Train Epoch: 26 [26880/60000 (45%)]\tLoss: 92.728310\n",
      "Train Epoch: 26 [28160/60000 (47%)]\tLoss: 97.943481\n",
      "Train Epoch: 26 [29440/60000 (49%)]\tLoss: 99.158669\n",
      "Train Epoch: 26 [30720/60000 (51%)]\tLoss: 96.253960\n",
      "Train Epoch: 26 [32000/60000 (53%)]\tLoss: 99.350937\n",
      "Train Epoch: 26 [33280/60000 (55%)]\tLoss: 96.924896\n",
      "Train Epoch: 26 [34560/60000 (58%)]\tLoss: 98.266083\n",
      "Train Epoch: 26 [35840/60000 (60%)]\tLoss: 94.877731\n",
      "Train Epoch: 26 [37120/60000 (62%)]\tLoss: 99.630989\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 96.553726\n",
      "Train Epoch: 26 [39680/60000 (66%)]\tLoss: 98.431389\n",
      "Train Epoch: 26 [40960/60000 (68%)]\tLoss: 95.127525\n",
      "Train Epoch: 26 [42240/60000 (70%)]\tLoss: 99.110580\n",
      "Train Epoch: 26 [43520/60000 (72%)]\tLoss: 97.105392\n",
      "Train Epoch: 26 [44800/60000 (75%)]\tLoss: 99.306511\n",
      "Train Epoch: 26 [46080/60000 (77%)]\tLoss: 96.765633\n",
      "Train Epoch: 26 [47360/60000 (79%)]\tLoss: 94.702538\n",
      "Train Epoch: 26 [48640/60000 (81%)]\tLoss: 94.359238\n",
      "Train Epoch: 26 [49920/60000 (83%)]\tLoss: 94.123947\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 95.654434\n",
      "Train Epoch: 26 [52480/60000 (87%)]\tLoss: 101.286194\n",
      "Train Epoch: 26 [53760/60000 (90%)]\tLoss: 99.306244\n",
      "Train Epoch: 26 [55040/60000 (92%)]\tLoss: 97.056213\n",
      "Train Epoch: 26 [56320/60000 (94%)]\tLoss: 101.462692\n",
      "Train Epoch: 26 [57600/60000 (96%)]\tLoss: 96.190399\n",
      "Train Epoch: 26 [58880/60000 (98%)]\tLoss: 95.876968\n",
      "====> Epoch: 26 Average loss: 97.4016\n",
      "====> Test set loss: 99.3459\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 98.210388\n",
      "Train Epoch: 27 [1280/60000 (2%)]\tLoss: 99.269302\n",
      "Train Epoch: 27 [2560/60000 (4%)]\tLoss: 98.711670\n",
      "Train Epoch: 27 [3840/60000 (6%)]\tLoss: 95.264046\n",
      "Train Epoch: 27 [5120/60000 (9%)]\tLoss: 99.458786\n",
      "Train Epoch: 27 [6400/60000 (11%)]\tLoss: 95.577530\n",
      "Train Epoch: 27 [7680/60000 (13%)]\tLoss: 98.226257\n",
      "Train Epoch: 27 [8960/60000 (15%)]\tLoss: 93.803612\n",
      "Train Epoch: 27 [10240/60000 (17%)]\tLoss: 99.112488\n",
      "Train Epoch: 27 [11520/60000 (19%)]\tLoss: 98.724335\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 94.612213\n",
      "Train Epoch: 27 [14080/60000 (23%)]\tLoss: 92.126251\n",
      "Train Epoch: 27 [15360/60000 (26%)]\tLoss: 99.306534\n",
      "Train Epoch: 27 [16640/60000 (28%)]\tLoss: 99.005058\n",
      "Train Epoch: 27 [17920/60000 (30%)]\tLoss: 97.910492\n",
      "Train Epoch: 27 [19200/60000 (32%)]\tLoss: 96.316566\n",
      "Train Epoch: 27 [20480/60000 (34%)]\tLoss: 95.957306\n",
      "Train Epoch: 27 [21760/60000 (36%)]\tLoss: 98.424362\n",
      "Train Epoch: 27 [23040/60000 (38%)]\tLoss: 94.599777\n",
      "Train Epoch: 27 [24320/60000 (41%)]\tLoss: 98.811874\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 94.896835\n",
      "Train Epoch: 27 [26880/60000 (45%)]\tLoss: 100.392365\n",
      "Train Epoch: 27 [28160/60000 (47%)]\tLoss: 94.939072\n",
      "Train Epoch: 27 [29440/60000 (49%)]\tLoss: 98.604614\n",
      "Train Epoch: 27 [30720/60000 (51%)]\tLoss: 101.024254\n",
      "Train Epoch: 27 [32000/60000 (53%)]\tLoss: 101.005226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 27 [33280/60000 (55%)]\tLoss: 92.973206\n",
      "Train Epoch: 27 [34560/60000 (58%)]\tLoss: 94.867340\n",
      "Train Epoch: 27 [35840/60000 (60%)]\tLoss: 99.823242\n",
      "Train Epoch: 27 [37120/60000 (62%)]\tLoss: 98.585869\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 101.121132\n",
      "Train Epoch: 27 [39680/60000 (66%)]\tLoss: 98.299797\n",
      "Train Epoch: 27 [40960/60000 (68%)]\tLoss: 96.629166\n",
      "Train Epoch: 27 [42240/60000 (70%)]\tLoss: 95.781525\n",
      "Train Epoch: 27 [43520/60000 (72%)]\tLoss: 99.272949\n",
      "Train Epoch: 27 [44800/60000 (75%)]\tLoss: 95.515411\n",
      "Train Epoch: 27 [46080/60000 (77%)]\tLoss: 99.542282\n",
      "Train Epoch: 27 [47360/60000 (79%)]\tLoss: 99.142258\n",
      "Train Epoch: 27 [48640/60000 (81%)]\tLoss: 101.870895\n",
      "Train Epoch: 27 [49920/60000 (83%)]\tLoss: 97.527802\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 98.733765\n",
      "Train Epoch: 27 [52480/60000 (87%)]\tLoss: 99.582504\n",
      "Train Epoch: 27 [53760/60000 (90%)]\tLoss: 100.863998\n",
      "Train Epoch: 27 [55040/60000 (92%)]\tLoss: 97.544304\n",
      "Train Epoch: 27 [56320/60000 (94%)]\tLoss: 96.072678\n",
      "Train Epoch: 27 [57600/60000 (96%)]\tLoss: 97.070641\n",
      "Train Epoch: 27 [58880/60000 (98%)]\tLoss: 98.362663\n",
      "====> Epoch: 27 Average loss: 97.4033\n",
      "====> Test set loss: 99.3243\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 94.964729\n",
      "Train Epoch: 28 [1280/60000 (2%)]\tLoss: 98.207977\n",
      "Train Epoch: 28 [2560/60000 (4%)]\tLoss: 96.734985\n",
      "Train Epoch: 28 [3840/60000 (6%)]\tLoss: 97.861885\n",
      "Train Epoch: 28 [5120/60000 (9%)]\tLoss: 100.564369\n",
      "Train Epoch: 28 [6400/60000 (11%)]\tLoss: 97.711914\n",
      "Train Epoch: 28 [7680/60000 (13%)]\tLoss: 93.488693\n",
      "Train Epoch: 28 [8960/60000 (15%)]\tLoss: 94.192131\n",
      "Train Epoch: 28 [10240/60000 (17%)]\tLoss: 92.196236\n",
      "Train Epoch: 28 [11520/60000 (19%)]\tLoss: 98.101013\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 94.379158\n",
      "Train Epoch: 28 [14080/60000 (23%)]\tLoss: 98.216110\n",
      "Train Epoch: 28 [15360/60000 (26%)]\tLoss: 94.659073\n",
      "Train Epoch: 28 [16640/60000 (28%)]\tLoss: 95.520554\n",
      "Train Epoch: 28 [17920/60000 (30%)]\tLoss: 100.501114\n",
      "Train Epoch: 28 [19200/60000 (32%)]\tLoss: 95.205589\n",
      "Train Epoch: 28 [20480/60000 (34%)]\tLoss: 94.521660\n",
      "Train Epoch: 28 [21760/60000 (36%)]\tLoss: 96.086990\n",
      "Train Epoch: 28 [23040/60000 (38%)]\tLoss: 101.670670\n",
      "Train Epoch: 28 [24320/60000 (41%)]\tLoss: 93.372993\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 99.303360\n",
      "Train Epoch: 28 [26880/60000 (45%)]\tLoss: 96.691856\n",
      "Train Epoch: 28 [28160/60000 (47%)]\tLoss: 98.252106\n",
      "Train Epoch: 28 [29440/60000 (49%)]\tLoss: 99.801041\n",
      "Train Epoch: 28 [30720/60000 (51%)]\tLoss: 98.040833\n",
      "Train Epoch: 28 [32000/60000 (53%)]\tLoss: 97.343658\n",
      "Train Epoch: 28 [33280/60000 (55%)]\tLoss: 96.731232\n",
      "Train Epoch: 28 [34560/60000 (58%)]\tLoss: 93.335350\n",
      "Train Epoch: 28 [35840/60000 (60%)]\tLoss: 98.007065\n",
      "Train Epoch: 28 [37120/60000 (62%)]\tLoss: 95.946098\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 98.904198\n",
      "Train Epoch: 28 [39680/60000 (66%)]\tLoss: 94.856720\n",
      "Train Epoch: 28 [40960/60000 (68%)]\tLoss: 97.259132\n",
      "Train Epoch: 28 [42240/60000 (70%)]\tLoss: 94.530251\n",
      "Train Epoch: 28 [43520/60000 (72%)]\tLoss: 93.485100\n",
      "Train Epoch: 28 [44800/60000 (75%)]\tLoss: 96.222290\n",
      "Train Epoch: 28 [46080/60000 (77%)]\tLoss: 101.989578\n",
      "Train Epoch: 28 [47360/60000 (79%)]\tLoss: 95.889160\n",
      "Train Epoch: 28 [48640/60000 (81%)]\tLoss: 97.022209\n",
      "Train Epoch: 28 [49920/60000 (83%)]\tLoss: 96.575272\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 96.204468\n",
      "Train Epoch: 28 [52480/60000 (87%)]\tLoss: 95.895813\n",
      "Train Epoch: 28 [53760/60000 (90%)]\tLoss: 96.779030\n",
      "Train Epoch: 28 [55040/60000 (92%)]\tLoss: 95.322113\n",
      "Train Epoch: 28 [56320/60000 (94%)]\tLoss: 96.218605\n",
      "Train Epoch: 28 [57600/60000 (96%)]\tLoss: 94.975838\n",
      "Train Epoch: 28 [58880/60000 (98%)]\tLoss: 98.429382\n",
      "====> Epoch: 28 Average loss: 97.3440\n",
      "====> Test set loss: 99.0278\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 93.126038\n",
      "Train Epoch: 29 [1280/60000 (2%)]\tLoss: 96.483200\n",
      "Train Epoch: 29 [2560/60000 (4%)]\tLoss: 99.954636\n",
      "Train Epoch: 29 [3840/60000 (6%)]\tLoss: 94.687950\n",
      "Train Epoch: 29 [5120/60000 (9%)]\tLoss: 96.518173\n",
      "Train Epoch: 29 [6400/60000 (11%)]\tLoss: 94.448120\n",
      "Train Epoch: 29 [7680/60000 (13%)]\tLoss: 93.034698\n",
      "Train Epoch: 29 [8960/60000 (15%)]\tLoss: 97.906090\n",
      "Train Epoch: 29 [10240/60000 (17%)]\tLoss: 98.564178\n",
      "Train Epoch: 29 [11520/60000 (19%)]\tLoss: 97.874557\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 93.598351\n",
      "Train Epoch: 29 [14080/60000 (23%)]\tLoss: 95.158768\n",
      "Train Epoch: 29 [15360/60000 (26%)]\tLoss: 96.678787\n",
      "Train Epoch: 29 [16640/60000 (28%)]\tLoss: 96.077698\n",
      "Train Epoch: 29 [17920/60000 (30%)]\tLoss: 99.679474\n",
      "Train Epoch: 29 [19200/60000 (32%)]\tLoss: 101.555237\n",
      "Train Epoch: 29 [20480/60000 (34%)]\tLoss: 93.277847\n",
      "Train Epoch: 29 [21760/60000 (36%)]\tLoss: 96.982216\n",
      "Train Epoch: 29 [23040/60000 (38%)]\tLoss: 96.374947\n",
      "Train Epoch: 29 [24320/60000 (41%)]\tLoss: 100.267525\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 94.583588\n",
      "Train Epoch: 29 [26880/60000 (45%)]\tLoss: 99.146561\n",
      "Train Epoch: 29 [28160/60000 (47%)]\tLoss: 99.843536\n",
      "Train Epoch: 29 [29440/60000 (49%)]\tLoss: 97.180008\n",
      "Train Epoch: 29 [30720/60000 (51%)]\tLoss: 97.284256\n",
      "Train Epoch: 29 [32000/60000 (53%)]\tLoss: 96.844429\n",
      "Train Epoch: 29 [33280/60000 (55%)]\tLoss: 95.359695\n",
      "Train Epoch: 29 [34560/60000 (58%)]\tLoss: 95.771774\n",
      "Train Epoch: 29 [35840/60000 (60%)]\tLoss: 96.522812\n",
      "Train Epoch: 29 [37120/60000 (62%)]\tLoss: 100.821037\n",
      "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 98.085541\n",
      "Train Epoch: 29 [39680/60000 (66%)]\tLoss: 97.129074\n",
      "Train Epoch: 29 [40960/60000 (68%)]\tLoss: 98.062271\n",
      "Train Epoch: 29 [42240/60000 (70%)]\tLoss: 101.289841\n",
      "Train Epoch: 29 [43520/60000 (72%)]\tLoss: 100.636673\n",
      "Train Epoch: 29 [44800/60000 (75%)]\tLoss: 93.419632\n",
      "Train Epoch: 29 [46080/60000 (77%)]\tLoss: 97.737991\n",
      "Train Epoch: 29 [47360/60000 (79%)]\tLoss: 102.728577\n",
      "Train Epoch: 29 [48640/60000 (81%)]\tLoss: 98.154877\n",
      "Train Epoch: 29 [49920/60000 (83%)]\tLoss: 98.928558\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 95.737946\n",
      "Train Epoch: 29 [52480/60000 (87%)]\tLoss: 95.751953\n",
      "Train Epoch: 29 [53760/60000 (90%)]\tLoss: 101.232559\n",
      "Train Epoch: 29 [55040/60000 (92%)]\tLoss: 98.160110\n",
      "Train Epoch: 29 [56320/60000 (94%)]\tLoss: 97.366577\n",
      "Train Epoch: 29 [57600/60000 (96%)]\tLoss: 96.931839\n",
      "Train Epoch: 29 [58880/60000 (98%)]\tLoss: 96.767479\n",
      "====> Epoch: 29 Average loss: 97.3467\n",
      "====> Test set loss: 99.2416\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 95.443871\n",
      "Train Epoch: 30 [1280/60000 (2%)]\tLoss: 93.620636\n",
      "Train Epoch: 30 [2560/60000 (4%)]\tLoss: 96.789627\n",
      "Train Epoch: 30 [3840/60000 (6%)]\tLoss: 96.370476\n",
      "Train Epoch: 30 [5120/60000 (9%)]\tLoss: 97.846077\n",
      "Train Epoch: 30 [6400/60000 (11%)]\tLoss: 94.373245\n",
      "Train Epoch: 30 [7680/60000 (13%)]\tLoss: 97.786232\n",
      "Train Epoch: 30 [8960/60000 (15%)]\tLoss: 95.540901\n",
      "Train Epoch: 30 [10240/60000 (17%)]\tLoss: 106.042854\n",
      "Train Epoch: 30 [11520/60000 (19%)]\tLoss: 96.090111\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 99.715187\n",
      "Train Epoch: 30 [14080/60000 (23%)]\tLoss: 97.551056\n",
      "Train Epoch: 30 [15360/60000 (26%)]\tLoss: 97.929596\n",
      "Train Epoch: 30 [16640/60000 (28%)]\tLoss: 96.572968\n",
      "Train Epoch: 30 [17920/60000 (30%)]\tLoss: 100.628166\n",
      "Train Epoch: 30 [19200/60000 (32%)]\tLoss: 97.219727\n",
      "Train Epoch: 30 [20480/60000 (34%)]\tLoss: 94.564026\n",
      "Train Epoch: 30 [21760/60000 (36%)]\tLoss: 98.987823\n",
      "Train Epoch: 30 [23040/60000 (38%)]\tLoss: 98.567291\n",
      "Train Epoch: 30 [24320/60000 (41%)]\tLoss: 99.020195\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 98.361908\n",
      "Train Epoch: 30 [26880/60000 (45%)]\tLoss: 99.712410\n",
      "Train Epoch: 30 [28160/60000 (47%)]\tLoss: 96.638412\n",
      "Train Epoch: 30 [29440/60000 (49%)]\tLoss: 101.574448\n",
      "Train Epoch: 30 [30720/60000 (51%)]\tLoss: 99.711334\n",
      "Train Epoch: 30 [32000/60000 (53%)]\tLoss: 98.446609\n",
      "Train Epoch: 30 [33280/60000 (55%)]\tLoss: 94.167709\n",
      "Train Epoch: 30 [34560/60000 (58%)]\tLoss: 94.793114\n",
      "Train Epoch: 30 [35840/60000 (60%)]\tLoss: 95.196640\n",
      "Train Epoch: 30 [37120/60000 (62%)]\tLoss: 98.205536\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 99.705429\n",
      "Train Epoch: 30 [39680/60000 (66%)]\tLoss: 95.268318\n",
      "Train Epoch: 30 [40960/60000 (68%)]\tLoss: 102.513542\n",
      "Train Epoch: 30 [42240/60000 (70%)]\tLoss: 97.685120\n",
      "Train Epoch: 30 [43520/60000 (72%)]\tLoss: 97.121201\n",
      "Train Epoch: 30 [44800/60000 (75%)]\tLoss: 94.134842\n",
      "Train Epoch: 30 [46080/60000 (77%)]\tLoss: 93.183495\n",
      "Train Epoch: 30 [47360/60000 (79%)]\tLoss: 99.393158\n",
      "Train Epoch: 30 [48640/60000 (81%)]\tLoss: 95.187569\n",
      "Train Epoch: 30 [49920/60000 (83%)]\tLoss: 98.759697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 96.724106\n",
      "Train Epoch: 30 [52480/60000 (87%)]\tLoss: 96.376030\n",
      "Train Epoch: 30 [53760/60000 (90%)]\tLoss: 97.917091\n",
      "Train Epoch: 30 [55040/60000 (92%)]\tLoss: 94.400681\n",
      "Train Epoch: 30 [56320/60000 (94%)]\tLoss: 97.423172\n",
      "Train Epoch: 30 [57600/60000 (96%)]\tLoss: 97.051643\n",
      "Train Epoch: 30 [58880/60000 (98%)]\tLoss: 95.031448\n",
      "====> Epoch: 30 Average loss: 97.3053\n",
      "====> Test set loss: 99.2484\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 98.143044\n",
      "Train Epoch: 31 [1280/60000 (2%)]\tLoss: 96.230080\n",
      "Train Epoch: 31 [2560/60000 (4%)]\tLoss: 97.334305\n",
      "Train Epoch: 31 [3840/60000 (6%)]\tLoss: 96.585480\n",
      "Train Epoch: 31 [5120/60000 (9%)]\tLoss: 96.756622\n",
      "Train Epoch: 31 [6400/60000 (11%)]\tLoss: 96.411667\n",
      "Train Epoch: 31 [7680/60000 (13%)]\tLoss: 94.665619\n",
      "Train Epoch: 31 [8960/60000 (15%)]\tLoss: 96.228203\n",
      "Train Epoch: 31 [10240/60000 (17%)]\tLoss: 95.238480\n",
      "Train Epoch: 31 [11520/60000 (19%)]\tLoss: 95.476761\n",
      "Train Epoch: 31 [12800/60000 (21%)]\tLoss: 91.693054\n",
      "Train Epoch: 31 [14080/60000 (23%)]\tLoss: 98.669189\n",
      "Train Epoch: 31 [15360/60000 (26%)]\tLoss: 97.815285\n",
      "Train Epoch: 31 [16640/60000 (28%)]\tLoss: 93.921333\n",
      "Train Epoch: 31 [17920/60000 (30%)]\tLoss: 98.550003\n",
      "Train Epoch: 31 [19200/60000 (32%)]\tLoss: 98.769196\n",
      "Train Epoch: 31 [20480/60000 (34%)]\tLoss: 102.061035\n",
      "Train Epoch: 31 [21760/60000 (36%)]\tLoss: 100.355751\n",
      "Train Epoch: 31 [23040/60000 (38%)]\tLoss: 98.693100\n",
      "Train Epoch: 31 [24320/60000 (41%)]\tLoss: 101.079208\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 92.845169\n",
      "Train Epoch: 31 [26880/60000 (45%)]\tLoss: 99.781837\n",
      "Train Epoch: 31 [28160/60000 (47%)]\tLoss: 95.810684\n",
      "Train Epoch: 31 [29440/60000 (49%)]\tLoss: 97.936935\n",
      "Train Epoch: 31 [30720/60000 (51%)]\tLoss: 98.542618\n",
      "Train Epoch: 31 [32000/60000 (53%)]\tLoss: 99.170647\n",
      "Train Epoch: 31 [33280/60000 (55%)]\tLoss: 97.090157\n",
      "Train Epoch: 31 [34560/60000 (58%)]\tLoss: 99.008636\n",
      "Train Epoch: 31 [35840/60000 (60%)]\tLoss: 94.824539\n",
      "Train Epoch: 31 [37120/60000 (62%)]\tLoss: 98.836060\n",
      "Train Epoch: 31 [38400/60000 (64%)]\tLoss: 98.839432\n",
      "Train Epoch: 31 [39680/60000 (66%)]\tLoss: 96.901764\n",
      "Train Epoch: 31 [40960/60000 (68%)]\tLoss: 97.469940\n",
      "Train Epoch: 31 [42240/60000 (70%)]\tLoss: 98.728745\n",
      "Train Epoch: 31 [43520/60000 (72%)]\tLoss: 97.867157\n",
      "Train Epoch: 31 [44800/60000 (75%)]\tLoss: 99.465889\n",
      "Train Epoch: 31 [46080/60000 (77%)]\tLoss: 97.654892\n",
      "Train Epoch: 31 [47360/60000 (79%)]\tLoss: 99.361771\n",
      "Train Epoch: 31 [48640/60000 (81%)]\tLoss: 95.518402\n",
      "Train Epoch: 31 [49920/60000 (83%)]\tLoss: 98.552399\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 94.443306\n",
      "Train Epoch: 31 [52480/60000 (87%)]\tLoss: 96.510429\n",
      "Train Epoch: 31 [53760/60000 (90%)]\tLoss: 96.606186\n",
      "Train Epoch: 31 [55040/60000 (92%)]\tLoss: 100.206291\n",
      "Train Epoch: 31 [56320/60000 (94%)]\tLoss: 99.927429\n",
      "Train Epoch: 31 [57600/60000 (96%)]\tLoss: 94.464188\n",
      "Train Epoch: 31 [58880/60000 (98%)]\tLoss: 97.927910\n",
      "====> Epoch: 31 Average loss: 97.2980\n",
      "====> Test set loss: 99.0464\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 95.352646\n",
      "Train Epoch: 32 [1280/60000 (2%)]\tLoss: 97.230255\n",
      "Train Epoch: 32 [2560/60000 (4%)]\tLoss: 92.693985\n",
      "Train Epoch: 32 [3840/60000 (6%)]\tLoss: 96.466286\n",
      "Train Epoch: 32 [5120/60000 (9%)]\tLoss: 91.230034\n",
      "Train Epoch: 32 [6400/60000 (11%)]\tLoss: 99.543961\n",
      "Train Epoch: 32 [7680/60000 (13%)]\tLoss: 95.972618\n",
      "Train Epoch: 32 [8960/60000 (15%)]\tLoss: 97.549713\n",
      "Train Epoch: 32 [10240/60000 (17%)]\tLoss: 94.354767\n",
      "Train Epoch: 32 [11520/60000 (19%)]\tLoss: 95.456055\n",
      "Train Epoch: 32 [12800/60000 (21%)]\tLoss: 94.625305\n",
      "Train Epoch: 32 [14080/60000 (23%)]\tLoss: 96.104630\n",
      "Train Epoch: 32 [15360/60000 (26%)]\tLoss: 97.714264\n",
      "Train Epoch: 32 [16640/60000 (28%)]\tLoss: 98.850555\n",
      "Train Epoch: 32 [17920/60000 (30%)]\tLoss: 94.098434\n",
      "Train Epoch: 32 [19200/60000 (32%)]\tLoss: 100.836296\n",
      "Train Epoch: 32 [20480/60000 (34%)]\tLoss: 97.413139\n",
      "Train Epoch: 32 [21760/60000 (36%)]\tLoss: 93.568893\n",
      "Train Epoch: 32 [23040/60000 (38%)]\tLoss: 97.988327\n",
      "Train Epoch: 32 [24320/60000 (41%)]\tLoss: 97.186157\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 101.238365\n",
      "Train Epoch: 32 [26880/60000 (45%)]\tLoss: 96.687851\n",
      "Train Epoch: 32 [28160/60000 (47%)]\tLoss: 96.758469\n",
      "Train Epoch: 32 [29440/60000 (49%)]\tLoss: 99.878510\n",
      "Train Epoch: 32 [30720/60000 (51%)]\tLoss: 94.135437\n",
      "Train Epoch: 32 [32000/60000 (53%)]\tLoss: 90.694809\n",
      "Train Epoch: 32 [33280/60000 (55%)]\tLoss: 95.819382\n",
      "Train Epoch: 32 [34560/60000 (58%)]\tLoss: 98.411118\n",
      "Train Epoch: 32 [35840/60000 (60%)]\tLoss: 100.997169\n",
      "Train Epoch: 32 [37120/60000 (62%)]\tLoss: 100.849670\n",
      "Train Epoch: 32 [38400/60000 (64%)]\tLoss: 97.222839\n",
      "Train Epoch: 32 [39680/60000 (66%)]\tLoss: 94.663834\n",
      "Train Epoch: 32 [40960/60000 (68%)]\tLoss: 97.457314\n",
      "Train Epoch: 32 [42240/60000 (70%)]\tLoss: 95.749741\n",
      "Train Epoch: 32 [43520/60000 (72%)]\tLoss: 100.349823\n",
      "Train Epoch: 32 [44800/60000 (75%)]\tLoss: 99.316757\n",
      "Train Epoch: 32 [46080/60000 (77%)]\tLoss: 96.268219\n",
      "Train Epoch: 32 [47360/60000 (79%)]\tLoss: 96.655724\n",
      "Train Epoch: 32 [48640/60000 (81%)]\tLoss: 96.113419\n",
      "Train Epoch: 32 [49920/60000 (83%)]\tLoss: 98.392502\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 98.385239\n",
      "Train Epoch: 32 [52480/60000 (87%)]\tLoss: 99.812721\n",
      "Train Epoch: 32 [53760/60000 (90%)]\tLoss: 94.413116\n",
      "Train Epoch: 32 [55040/60000 (92%)]\tLoss: 101.235718\n",
      "Train Epoch: 32 [56320/60000 (94%)]\tLoss: 98.302727\n",
      "Train Epoch: 32 [57600/60000 (96%)]\tLoss: 96.955086\n",
      "Train Epoch: 32 [58880/60000 (98%)]\tLoss: 97.260178\n",
      "====> Epoch: 32 Average loss: 97.2414\n",
      "====> Test set loss: 99.2836\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 94.863846\n",
      "Train Epoch: 33 [1280/60000 (2%)]\tLoss: 93.220406\n",
      "Train Epoch: 33 [2560/60000 (4%)]\tLoss: 93.240463\n",
      "Train Epoch: 33 [3840/60000 (6%)]\tLoss: 98.922646\n",
      "Train Epoch: 33 [5120/60000 (9%)]\tLoss: 97.997841\n",
      "Train Epoch: 33 [6400/60000 (11%)]\tLoss: 99.181007\n",
      "Train Epoch: 33 [7680/60000 (13%)]\tLoss: 96.061157\n",
      "Train Epoch: 33 [8960/60000 (15%)]\tLoss: 97.106468\n",
      "Train Epoch: 33 [10240/60000 (17%)]\tLoss: 96.369064\n",
      "Train Epoch: 33 [11520/60000 (19%)]\tLoss: 98.763840\n",
      "Train Epoch: 33 [12800/60000 (21%)]\tLoss: 98.251312\n",
      "Train Epoch: 33 [14080/60000 (23%)]\tLoss: 95.689667\n",
      "Train Epoch: 33 [15360/60000 (26%)]\tLoss: 100.156372\n",
      "Train Epoch: 33 [16640/60000 (28%)]\tLoss: 97.039436\n",
      "Train Epoch: 33 [17920/60000 (30%)]\tLoss: 94.398239\n",
      "Train Epoch: 33 [19200/60000 (32%)]\tLoss: 98.352188\n",
      "Train Epoch: 33 [20480/60000 (34%)]\tLoss: 95.274155\n",
      "Train Epoch: 33 [21760/60000 (36%)]\tLoss: 97.437347\n",
      "Train Epoch: 33 [23040/60000 (38%)]\tLoss: 95.368317\n",
      "Train Epoch: 33 [24320/60000 (41%)]\tLoss: 97.476463\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 92.355209\n",
      "Train Epoch: 33 [26880/60000 (45%)]\tLoss: 95.705673\n",
      "Train Epoch: 33 [28160/60000 (47%)]\tLoss: 100.363358\n",
      "Train Epoch: 33 [29440/60000 (49%)]\tLoss: 97.826195\n",
      "Train Epoch: 33 [30720/60000 (51%)]\tLoss: 97.489632\n",
      "Train Epoch: 33 [32000/60000 (53%)]\tLoss: 94.756577\n",
      "Train Epoch: 33 [33280/60000 (55%)]\tLoss: 95.757179\n",
      "Train Epoch: 33 [34560/60000 (58%)]\tLoss: 99.173584\n",
      "Train Epoch: 33 [35840/60000 (60%)]\tLoss: 98.085655\n",
      "Train Epoch: 33 [37120/60000 (62%)]\tLoss: 99.627151\n",
      "Train Epoch: 33 [38400/60000 (64%)]\tLoss: 95.569275\n",
      "Train Epoch: 33 [39680/60000 (66%)]\tLoss: 92.361259\n",
      "Train Epoch: 33 [40960/60000 (68%)]\tLoss: 96.248230\n",
      "Train Epoch: 33 [42240/60000 (70%)]\tLoss: 99.631241\n",
      "Train Epoch: 33 [43520/60000 (72%)]\tLoss: 94.447548\n",
      "Train Epoch: 33 [44800/60000 (75%)]\tLoss: 95.163177\n",
      "Train Epoch: 33 [46080/60000 (77%)]\tLoss: 94.047737\n",
      "Train Epoch: 33 [47360/60000 (79%)]\tLoss: 92.403603\n",
      "Train Epoch: 33 [48640/60000 (81%)]\tLoss: 93.335999\n",
      "Train Epoch: 33 [49920/60000 (83%)]\tLoss: 93.905472\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 94.331383\n",
      "Train Epoch: 33 [52480/60000 (87%)]\tLoss: 97.007996\n",
      "Train Epoch: 33 [53760/60000 (90%)]\tLoss: 100.659943\n",
      "Train Epoch: 33 [55040/60000 (92%)]\tLoss: 99.216026\n",
      "Train Epoch: 33 [56320/60000 (94%)]\tLoss: 97.142006\n",
      "Train Epoch: 33 [57600/60000 (96%)]\tLoss: 95.965866\n",
      "Train Epoch: 33 [58880/60000 (98%)]\tLoss: 99.899055\n",
      "====> Epoch: 33 Average loss: 97.2191\n",
      "====> Test set loss: 99.2893\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 96.366577\n",
      "Train Epoch: 34 [1280/60000 (2%)]\tLoss: 97.741493\n",
      "Train Epoch: 34 [2560/60000 (4%)]\tLoss: 94.671997\n",
      "Train Epoch: 34 [3840/60000 (6%)]\tLoss: 96.223839\n",
      "Train Epoch: 34 [5120/60000 (9%)]\tLoss: 96.440262\n",
      "Train Epoch: 34 [6400/60000 (11%)]\tLoss: 96.495193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 34 [7680/60000 (13%)]\tLoss: 96.281128\n",
      "Train Epoch: 34 [8960/60000 (15%)]\tLoss: 100.102280\n",
      "Train Epoch: 34 [10240/60000 (17%)]\tLoss: 97.543671\n",
      "Train Epoch: 34 [11520/60000 (19%)]\tLoss: 93.670242\n",
      "Train Epoch: 34 [12800/60000 (21%)]\tLoss: 98.741913\n",
      "Train Epoch: 34 [14080/60000 (23%)]\tLoss: 96.824707\n",
      "Train Epoch: 34 [15360/60000 (26%)]\tLoss: 98.220726\n",
      "Train Epoch: 34 [16640/60000 (28%)]\tLoss: 97.964973\n",
      "Train Epoch: 34 [17920/60000 (30%)]\tLoss: 93.529922\n",
      "Train Epoch: 34 [19200/60000 (32%)]\tLoss: 100.133163\n",
      "Train Epoch: 34 [20480/60000 (34%)]\tLoss: 95.868568\n",
      "Train Epoch: 34 [21760/60000 (36%)]\tLoss: 97.847458\n",
      "Train Epoch: 34 [23040/60000 (38%)]\tLoss: 95.359207\n",
      "Train Epoch: 34 [24320/60000 (41%)]\tLoss: 97.258759\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 95.366264\n",
      "Train Epoch: 34 [26880/60000 (45%)]\tLoss: 96.903854\n",
      "Train Epoch: 34 [28160/60000 (47%)]\tLoss: 95.379959\n",
      "Train Epoch: 34 [29440/60000 (49%)]\tLoss: 95.176636\n",
      "Train Epoch: 34 [30720/60000 (51%)]\tLoss: 96.717659\n",
      "Train Epoch: 34 [32000/60000 (53%)]\tLoss: 97.675812\n",
      "Train Epoch: 34 [33280/60000 (55%)]\tLoss: 97.948097\n",
      "Train Epoch: 34 [34560/60000 (58%)]\tLoss: 95.400681\n",
      "Train Epoch: 34 [35840/60000 (60%)]\tLoss: 93.105370\n",
      "Train Epoch: 34 [37120/60000 (62%)]\tLoss: 96.564804\n",
      "Train Epoch: 34 [38400/60000 (64%)]\tLoss: 94.838623\n",
      "Train Epoch: 34 [39680/60000 (66%)]\tLoss: 99.982063\n",
      "Train Epoch: 34 [40960/60000 (68%)]\tLoss: 103.745041\n",
      "Train Epoch: 34 [42240/60000 (70%)]\tLoss: 98.466789\n",
      "Train Epoch: 34 [43520/60000 (72%)]\tLoss: 95.404404\n",
      "Train Epoch: 34 [44800/60000 (75%)]\tLoss: 97.003067\n",
      "Train Epoch: 34 [46080/60000 (77%)]\tLoss: 100.472397\n",
      "Train Epoch: 34 [47360/60000 (79%)]\tLoss: 99.337463\n",
      "Train Epoch: 34 [48640/60000 (81%)]\tLoss: 100.848106\n",
      "Train Epoch: 34 [49920/60000 (83%)]\tLoss: 102.331917\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 99.393356\n",
      "Train Epoch: 34 [52480/60000 (87%)]\tLoss: 97.509949\n",
      "Train Epoch: 34 [53760/60000 (90%)]\tLoss: 96.229385\n",
      "Train Epoch: 34 [55040/60000 (92%)]\tLoss: 97.639397\n",
      "Train Epoch: 34 [56320/60000 (94%)]\tLoss: 100.724968\n",
      "Train Epoch: 34 [57600/60000 (96%)]\tLoss: 99.388214\n",
      "Train Epoch: 34 [58880/60000 (98%)]\tLoss: 100.406662\n",
      "====> Epoch: 34 Average loss: 97.2050\n",
      "====> Test set loss: 99.3892\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 94.909286\n",
      "Train Epoch: 35 [1280/60000 (2%)]\tLoss: 92.970650\n",
      "Train Epoch: 35 [2560/60000 (4%)]\tLoss: 98.700226\n",
      "Train Epoch: 35 [3840/60000 (6%)]\tLoss: 99.182785\n",
      "Train Epoch: 35 [5120/60000 (9%)]\tLoss: 96.682556\n",
      "Train Epoch: 35 [6400/60000 (11%)]\tLoss: 96.961578\n",
      "Train Epoch: 35 [7680/60000 (13%)]\tLoss: 98.463654\n",
      "Train Epoch: 35 [8960/60000 (15%)]\tLoss: 99.382042\n",
      "Train Epoch: 35 [10240/60000 (17%)]\tLoss: 100.205345\n",
      "Train Epoch: 35 [11520/60000 (19%)]\tLoss: 96.575211\n",
      "Train Epoch: 35 [12800/60000 (21%)]\tLoss: 98.154327\n",
      "Train Epoch: 35 [14080/60000 (23%)]\tLoss: 97.869179\n",
      "Train Epoch: 35 [15360/60000 (26%)]\tLoss: 96.757385\n",
      "Train Epoch: 35 [16640/60000 (28%)]\tLoss: 98.157867\n",
      "Train Epoch: 35 [17920/60000 (30%)]\tLoss: 96.045258\n",
      "Train Epoch: 35 [19200/60000 (32%)]\tLoss: 92.875961\n",
      "Train Epoch: 35 [20480/60000 (34%)]\tLoss: 96.981049\n",
      "Train Epoch: 35 [21760/60000 (36%)]\tLoss: 99.377289\n",
      "Train Epoch: 35 [23040/60000 (38%)]\tLoss: 97.983154\n",
      "Train Epoch: 35 [24320/60000 (41%)]\tLoss: 96.188179\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 97.539871\n",
      "Train Epoch: 35 [26880/60000 (45%)]\tLoss: 97.818733\n",
      "Train Epoch: 35 [28160/60000 (47%)]\tLoss: 96.882690\n",
      "Train Epoch: 35 [29440/60000 (49%)]\tLoss: 96.013664\n",
      "Train Epoch: 35 [30720/60000 (51%)]\tLoss: 98.349022\n",
      "Train Epoch: 35 [32000/60000 (53%)]\tLoss: 95.392136\n",
      "Train Epoch: 35 [33280/60000 (55%)]\tLoss: 99.028214\n",
      "Train Epoch: 35 [34560/60000 (58%)]\tLoss: 94.069855\n",
      "Train Epoch: 35 [35840/60000 (60%)]\tLoss: 97.193436\n",
      "Train Epoch: 35 [37120/60000 (62%)]\tLoss: 97.513916\n",
      "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 101.776627\n",
      "Train Epoch: 35 [39680/60000 (66%)]\tLoss: 98.532211\n",
      "Train Epoch: 35 [40960/60000 (68%)]\tLoss: 95.098724\n",
      "Train Epoch: 35 [42240/60000 (70%)]\tLoss: 94.282761\n",
      "Train Epoch: 35 [43520/60000 (72%)]\tLoss: 95.310966\n",
      "Train Epoch: 35 [44800/60000 (75%)]\tLoss: 98.585358\n",
      "Train Epoch: 35 [46080/60000 (77%)]\tLoss: 99.884964\n",
      "Train Epoch: 35 [47360/60000 (79%)]\tLoss: 102.333496\n",
      "Train Epoch: 35 [48640/60000 (81%)]\tLoss: 96.390381\n",
      "Train Epoch: 35 [49920/60000 (83%)]\tLoss: 94.873566\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 97.208199\n",
      "Train Epoch: 35 [52480/60000 (87%)]\tLoss: 98.711594\n",
      "Train Epoch: 35 [53760/60000 (90%)]\tLoss: 97.485657\n",
      "Train Epoch: 35 [55040/60000 (92%)]\tLoss: 96.516891\n",
      "Train Epoch: 35 [56320/60000 (94%)]\tLoss: 92.899612\n",
      "Train Epoch: 35 [57600/60000 (96%)]\tLoss: 96.445007\n",
      "Train Epoch: 35 [58880/60000 (98%)]\tLoss: 97.759789\n",
      "====> Epoch: 35 Average loss: 97.1976\n",
      "====> Test set loss: 98.9658\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 101.123306\n",
      "Train Epoch: 36 [1280/60000 (2%)]\tLoss: 100.645966\n",
      "Train Epoch: 36 [2560/60000 (4%)]\tLoss: 96.773201\n",
      "Train Epoch: 36 [3840/60000 (6%)]\tLoss: 97.333092\n",
      "Train Epoch: 36 [5120/60000 (9%)]\tLoss: 95.665688\n",
      "Train Epoch: 36 [6400/60000 (11%)]\tLoss: 95.772682\n",
      "Train Epoch: 36 [7680/60000 (13%)]\tLoss: 95.106766\n",
      "Train Epoch: 36 [8960/60000 (15%)]\tLoss: 97.504486\n",
      "Train Epoch: 36 [10240/60000 (17%)]\tLoss: 93.259201\n",
      "Train Epoch: 36 [11520/60000 (19%)]\tLoss: 98.376755\n",
      "Train Epoch: 36 [12800/60000 (21%)]\tLoss: 97.061234\n",
      "Train Epoch: 36 [14080/60000 (23%)]\tLoss: 96.321983\n",
      "Train Epoch: 36 [15360/60000 (26%)]\tLoss: 97.669189\n",
      "Train Epoch: 36 [16640/60000 (28%)]\tLoss: 98.435738\n",
      "Train Epoch: 36 [17920/60000 (30%)]\tLoss: 97.878983\n",
      "Train Epoch: 36 [19200/60000 (32%)]\tLoss: 93.876953\n",
      "Train Epoch: 36 [20480/60000 (34%)]\tLoss: 102.004845\n",
      "Train Epoch: 36 [21760/60000 (36%)]\tLoss: 93.455269\n",
      "Train Epoch: 36 [23040/60000 (38%)]\tLoss: 99.253769\n",
      "Train Epoch: 36 [24320/60000 (41%)]\tLoss: 92.723541\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 99.982521\n",
      "Train Epoch: 36 [26880/60000 (45%)]\tLoss: 96.471207\n",
      "Train Epoch: 36 [28160/60000 (47%)]\tLoss: 97.887955\n",
      "Train Epoch: 36 [29440/60000 (49%)]\tLoss: 97.613663\n",
      "Train Epoch: 36 [30720/60000 (51%)]\tLoss: 99.222260\n",
      "Train Epoch: 36 [32000/60000 (53%)]\tLoss: 96.841248\n",
      "Train Epoch: 36 [33280/60000 (55%)]\tLoss: 98.727371\n",
      "Train Epoch: 36 [34560/60000 (58%)]\tLoss: 99.722092\n",
      "Train Epoch: 36 [35840/60000 (60%)]\tLoss: 96.583862\n",
      "Train Epoch: 36 [37120/60000 (62%)]\tLoss: 96.945465\n",
      "Train Epoch: 36 [38400/60000 (64%)]\tLoss: 96.980896\n",
      "Train Epoch: 36 [39680/60000 (66%)]\tLoss: 96.658463\n",
      "Train Epoch: 36 [40960/60000 (68%)]\tLoss: 99.418457\n",
      "Train Epoch: 36 [42240/60000 (70%)]\tLoss: 98.415176\n",
      "Train Epoch: 36 [43520/60000 (72%)]\tLoss: 95.431107\n",
      "Train Epoch: 36 [44800/60000 (75%)]\tLoss: 100.937874\n",
      "Train Epoch: 36 [46080/60000 (77%)]\tLoss: 97.380898\n",
      "Train Epoch: 36 [47360/60000 (79%)]\tLoss: 94.035660\n",
      "Train Epoch: 36 [48640/60000 (81%)]\tLoss: 95.612045\n",
      "Train Epoch: 36 [49920/60000 (83%)]\tLoss: 96.326355\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 95.943192\n",
      "Train Epoch: 36 [52480/60000 (87%)]\tLoss: 93.287354\n",
      "Train Epoch: 36 [53760/60000 (90%)]\tLoss: 99.756371\n",
      "Train Epoch: 36 [55040/60000 (92%)]\tLoss: 96.387619\n",
      "Train Epoch: 36 [56320/60000 (94%)]\tLoss: 94.130371\n",
      "Train Epoch: 36 [57600/60000 (96%)]\tLoss: 98.010666\n",
      "Train Epoch: 36 [58880/60000 (98%)]\tLoss: 95.169792\n",
      "====> Epoch: 36 Average loss: 97.1487\n",
      "====> Test set loss: 99.0920\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 97.549156\n",
      "Train Epoch: 37 [1280/60000 (2%)]\tLoss: 95.818214\n",
      "Train Epoch: 37 [2560/60000 (4%)]\tLoss: 97.626122\n",
      "Train Epoch: 37 [3840/60000 (6%)]\tLoss: 95.832870\n",
      "Train Epoch: 37 [5120/60000 (9%)]\tLoss: 95.546616\n",
      "Train Epoch: 37 [6400/60000 (11%)]\tLoss: 95.813080\n",
      "Train Epoch: 37 [7680/60000 (13%)]\tLoss: 94.800797\n",
      "Train Epoch: 37 [8960/60000 (15%)]\tLoss: 96.470886\n",
      "Train Epoch: 37 [10240/60000 (17%)]\tLoss: 97.817841\n",
      "Train Epoch: 37 [11520/60000 (19%)]\tLoss: 96.170891\n",
      "Train Epoch: 37 [12800/60000 (21%)]\tLoss: 97.518974\n",
      "Train Epoch: 37 [14080/60000 (23%)]\tLoss: 96.976959\n",
      "Train Epoch: 37 [15360/60000 (26%)]\tLoss: 98.810104\n",
      "Train Epoch: 37 [16640/60000 (28%)]\tLoss: 95.470078\n",
      "Train Epoch: 37 [17920/60000 (30%)]\tLoss: 98.763565\n",
      "Train Epoch: 37 [19200/60000 (32%)]\tLoss: 96.535141\n",
      "Train Epoch: 37 [20480/60000 (34%)]\tLoss: 97.668304\n",
      "Train Epoch: 37 [21760/60000 (36%)]\tLoss: 93.505219\n",
      "Train Epoch: 37 [23040/60000 (38%)]\tLoss: 98.032089\n",
      "Train Epoch: 37 [24320/60000 (41%)]\tLoss: 97.168839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 94.913605\n",
      "Train Epoch: 37 [26880/60000 (45%)]\tLoss: 93.843224\n",
      "Train Epoch: 37 [28160/60000 (47%)]\tLoss: 97.138947\n",
      "Train Epoch: 37 [29440/60000 (49%)]\tLoss: 96.140060\n",
      "Train Epoch: 37 [30720/60000 (51%)]\tLoss: 97.323853\n",
      "Train Epoch: 37 [32000/60000 (53%)]\tLoss: 98.084625\n",
      "Train Epoch: 37 [33280/60000 (55%)]\tLoss: 98.823296\n",
      "Train Epoch: 37 [34560/60000 (58%)]\tLoss: 100.510788\n",
      "Train Epoch: 37 [35840/60000 (60%)]\tLoss: 96.001389\n",
      "Train Epoch: 37 [37120/60000 (62%)]\tLoss: 95.955345\n",
      "Train Epoch: 37 [38400/60000 (64%)]\tLoss: 99.945496\n",
      "Train Epoch: 37 [39680/60000 (66%)]\tLoss: 98.137436\n",
      "Train Epoch: 37 [40960/60000 (68%)]\tLoss: 95.642746\n",
      "Train Epoch: 37 [42240/60000 (70%)]\tLoss: 98.723885\n",
      "Train Epoch: 37 [43520/60000 (72%)]\tLoss: 96.125015\n",
      "Train Epoch: 37 [44800/60000 (75%)]\tLoss: 97.407265\n",
      "Train Epoch: 37 [46080/60000 (77%)]\tLoss: 102.338974\n",
      "Train Epoch: 37 [47360/60000 (79%)]\tLoss: 98.816948\n",
      "Train Epoch: 37 [48640/60000 (81%)]\tLoss: 100.805283\n",
      "Train Epoch: 37 [49920/60000 (83%)]\tLoss: 99.205032\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 94.101089\n",
      "Train Epoch: 37 [52480/60000 (87%)]\tLoss: 96.729156\n",
      "Train Epoch: 37 [53760/60000 (90%)]\tLoss: 95.986984\n",
      "Train Epoch: 37 [55040/60000 (92%)]\tLoss: 94.992569\n",
      "Train Epoch: 37 [56320/60000 (94%)]\tLoss: 94.731163\n",
      "Train Epoch: 37 [57600/60000 (96%)]\tLoss: 97.273315\n",
      "Train Epoch: 37 [58880/60000 (98%)]\tLoss: 99.763275\n",
      "====> Epoch: 37 Average loss: 97.1419\n",
      "====> Test set loss: 99.2790\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 95.990417\n",
      "Train Epoch: 38 [1280/60000 (2%)]\tLoss: 97.894150\n",
      "Train Epoch: 38 [2560/60000 (4%)]\tLoss: 100.044464\n",
      "Train Epoch: 38 [3840/60000 (6%)]\tLoss: 95.981857\n",
      "Train Epoch: 38 [5120/60000 (9%)]\tLoss: 96.537407\n",
      "Train Epoch: 38 [6400/60000 (11%)]\tLoss: 96.125443\n",
      "Train Epoch: 38 [7680/60000 (13%)]\tLoss: 100.914871\n",
      "Train Epoch: 38 [8960/60000 (15%)]\tLoss: 96.347763\n",
      "Train Epoch: 38 [10240/60000 (17%)]\tLoss: 98.725471\n",
      "Train Epoch: 38 [11520/60000 (19%)]\tLoss: 95.690781\n",
      "Train Epoch: 38 [12800/60000 (21%)]\tLoss: 98.496674\n",
      "Train Epoch: 38 [14080/60000 (23%)]\tLoss: 97.326492\n",
      "Train Epoch: 38 [15360/60000 (26%)]\tLoss: 95.551270\n",
      "Train Epoch: 38 [16640/60000 (28%)]\tLoss: 95.661514\n",
      "Train Epoch: 38 [17920/60000 (30%)]\tLoss: 93.037247\n",
      "Train Epoch: 38 [19200/60000 (32%)]\tLoss: 94.714081\n",
      "Train Epoch: 38 [20480/60000 (34%)]\tLoss: 97.365570\n",
      "Train Epoch: 38 [21760/60000 (36%)]\tLoss: 98.807632\n",
      "Train Epoch: 38 [23040/60000 (38%)]\tLoss: 99.918922\n",
      "Train Epoch: 38 [24320/60000 (41%)]\tLoss: 98.745743\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 95.556564\n",
      "Train Epoch: 38 [26880/60000 (45%)]\tLoss: 94.411697\n",
      "Train Epoch: 38 [28160/60000 (47%)]\tLoss: 95.543854\n",
      "Train Epoch: 38 [29440/60000 (49%)]\tLoss: 92.502167\n",
      "Train Epoch: 38 [30720/60000 (51%)]\tLoss: 97.046776\n",
      "Train Epoch: 38 [32000/60000 (53%)]\tLoss: 97.089264\n",
      "Train Epoch: 38 [33280/60000 (55%)]\tLoss: 99.993851\n",
      "Train Epoch: 38 [34560/60000 (58%)]\tLoss: 101.239258\n",
      "Train Epoch: 38 [35840/60000 (60%)]\tLoss: 96.653717\n",
      "Train Epoch: 38 [37120/60000 (62%)]\tLoss: 99.022614\n",
      "Train Epoch: 38 [38400/60000 (64%)]\tLoss: 97.962059\n",
      "Train Epoch: 38 [39680/60000 (66%)]\tLoss: 97.754753\n",
      "Train Epoch: 38 [40960/60000 (68%)]\tLoss: 97.253571\n",
      "Train Epoch: 38 [42240/60000 (70%)]\tLoss: 91.679634\n",
      "Train Epoch: 38 [43520/60000 (72%)]\tLoss: 94.594971\n",
      "Train Epoch: 38 [44800/60000 (75%)]\tLoss: 94.924835\n",
      "Train Epoch: 38 [46080/60000 (77%)]\tLoss: 95.254242\n",
      "Train Epoch: 38 [47360/60000 (79%)]\tLoss: 95.992905\n",
      "Train Epoch: 38 [48640/60000 (81%)]\tLoss: 94.516357\n",
      "Train Epoch: 38 [49920/60000 (83%)]\tLoss: 97.916382\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 96.606628\n",
      "Train Epoch: 38 [52480/60000 (87%)]\tLoss: 96.283867\n",
      "Train Epoch: 38 [53760/60000 (90%)]\tLoss: 99.828278\n",
      "Train Epoch: 38 [55040/60000 (92%)]\tLoss: 95.700104\n",
      "Train Epoch: 38 [56320/60000 (94%)]\tLoss: 92.684189\n",
      "Train Epoch: 38 [57600/60000 (96%)]\tLoss: 97.569458\n",
      "Train Epoch: 38 [58880/60000 (98%)]\tLoss: 98.831429\n",
      "====> Epoch: 38 Average loss: 97.1577\n",
      "====> Test set loss: 99.1861\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 94.988510\n",
      "Train Epoch: 39 [1280/60000 (2%)]\tLoss: 96.556046\n",
      "Train Epoch: 39 [2560/60000 (4%)]\tLoss: 95.090057\n",
      "Train Epoch: 39 [3840/60000 (6%)]\tLoss: 98.439468\n",
      "Train Epoch: 39 [5120/60000 (9%)]\tLoss: 96.937798\n",
      "Train Epoch: 39 [6400/60000 (11%)]\tLoss: 96.096359\n",
      "Train Epoch: 39 [7680/60000 (13%)]\tLoss: 99.001564\n",
      "Train Epoch: 39 [8960/60000 (15%)]\tLoss: 99.743759\n",
      "Train Epoch: 39 [10240/60000 (17%)]\tLoss: 98.144020\n",
      "Train Epoch: 39 [11520/60000 (19%)]\tLoss: 94.081200\n",
      "Train Epoch: 39 [12800/60000 (21%)]\tLoss: 96.369904\n",
      "Train Epoch: 39 [14080/60000 (23%)]\tLoss: 94.623299\n",
      "Train Epoch: 39 [15360/60000 (26%)]\tLoss: 96.022293\n",
      "Train Epoch: 39 [16640/60000 (28%)]\tLoss: 99.819214\n",
      "Train Epoch: 39 [17920/60000 (30%)]\tLoss: 94.070602\n",
      "Train Epoch: 39 [19200/60000 (32%)]\tLoss: 95.624893\n",
      "Train Epoch: 39 [20480/60000 (34%)]\tLoss: 96.820236\n",
      "Train Epoch: 39 [21760/60000 (36%)]\tLoss: 96.917786\n",
      "Train Epoch: 39 [23040/60000 (38%)]\tLoss: 98.599548\n",
      "Train Epoch: 39 [24320/60000 (41%)]\tLoss: 99.731750\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 94.017128\n",
      "Train Epoch: 39 [26880/60000 (45%)]\tLoss: 95.767273\n",
      "Train Epoch: 39 [28160/60000 (47%)]\tLoss: 97.888901\n",
      "Train Epoch: 39 [29440/60000 (49%)]\tLoss: 100.046371\n",
      "Train Epoch: 39 [30720/60000 (51%)]\tLoss: 97.380302\n",
      "Train Epoch: 39 [32000/60000 (53%)]\tLoss: 98.909805\n",
      "Train Epoch: 39 [33280/60000 (55%)]\tLoss: 98.708939\n",
      "Train Epoch: 39 [34560/60000 (58%)]\tLoss: 93.146271\n",
      "Train Epoch: 39 [35840/60000 (60%)]\tLoss: 98.011971\n",
      "Train Epoch: 39 [37120/60000 (62%)]\tLoss: 97.893875\n",
      "Train Epoch: 39 [38400/60000 (64%)]\tLoss: 92.972900\n",
      "Train Epoch: 39 [39680/60000 (66%)]\tLoss: 93.930199\n",
      "Train Epoch: 39 [40960/60000 (68%)]\tLoss: 95.567871\n",
      "Train Epoch: 39 [42240/60000 (70%)]\tLoss: 96.494064\n",
      "Train Epoch: 39 [43520/60000 (72%)]\tLoss: 101.882774\n",
      "Train Epoch: 39 [44800/60000 (75%)]\tLoss: 99.883743\n",
      "Train Epoch: 39 [46080/60000 (77%)]\tLoss: 95.843170\n",
      "Train Epoch: 39 [47360/60000 (79%)]\tLoss: 94.155731\n",
      "Train Epoch: 39 [48640/60000 (81%)]\tLoss: 97.452431\n",
      "Train Epoch: 39 [49920/60000 (83%)]\tLoss: 99.884521\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 97.038574\n",
      "Train Epoch: 39 [52480/60000 (87%)]\tLoss: 98.061203\n",
      "Train Epoch: 39 [53760/60000 (90%)]\tLoss: 95.768127\n",
      "Train Epoch: 39 [55040/60000 (92%)]\tLoss: 99.524399\n",
      "Train Epoch: 39 [56320/60000 (94%)]\tLoss: 97.870270\n",
      "Train Epoch: 39 [57600/60000 (96%)]\tLoss: 97.244446\n",
      "Train Epoch: 39 [58880/60000 (98%)]\tLoss: 98.644470\n",
      "====> Epoch: 39 Average loss: 97.0774\n",
      "====> Test set loss: 99.1735\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 100.851334\n",
      "Train Epoch: 40 [1280/60000 (2%)]\tLoss: 95.266121\n",
      "Train Epoch: 40 [2560/60000 (4%)]\tLoss: 96.623718\n",
      "Train Epoch: 40 [3840/60000 (6%)]\tLoss: 97.677132\n",
      "Train Epoch: 40 [5120/60000 (9%)]\tLoss: 91.930611\n",
      "Train Epoch: 40 [6400/60000 (11%)]\tLoss: 98.008453\n",
      "Train Epoch: 40 [7680/60000 (13%)]\tLoss: 96.045532\n",
      "Train Epoch: 40 [8960/60000 (15%)]\tLoss: 96.137497\n",
      "Train Epoch: 40 [10240/60000 (17%)]\tLoss: 98.166763\n",
      "Train Epoch: 40 [11520/60000 (19%)]\tLoss: 93.999374\n",
      "Train Epoch: 40 [12800/60000 (21%)]\tLoss: 99.043976\n",
      "Train Epoch: 40 [14080/60000 (23%)]\tLoss: 100.271729\n",
      "Train Epoch: 40 [15360/60000 (26%)]\tLoss: 97.254944\n",
      "Train Epoch: 40 [16640/60000 (28%)]\tLoss: 96.156265\n",
      "Train Epoch: 40 [17920/60000 (30%)]\tLoss: 101.893356\n",
      "Train Epoch: 40 [19200/60000 (32%)]\tLoss: 97.980347\n",
      "Train Epoch: 40 [20480/60000 (34%)]\tLoss: 99.207718\n",
      "Train Epoch: 40 [21760/60000 (36%)]\tLoss: 98.720917\n",
      "Train Epoch: 40 [23040/60000 (38%)]\tLoss: 98.235062\n",
      "Train Epoch: 40 [24320/60000 (41%)]\tLoss: 95.391312\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 96.573135\n",
      "Train Epoch: 40 [26880/60000 (45%)]\tLoss: 95.380028\n",
      "Train Epoch: 40 [28160/60000 (47%)]\tLoss: 95.870537\n",
      "Train Epoch: 40 [29440/60000 (49%)]\tLoss: 99.196304\n",
      "Train Epoch: 40 [30720/60000 (51%)]\tLoss: 95.923248\n",
      "Train Epoch: 40 [32000/60000 (53%)]\tLoss: 96.234283\n",
      "Train Epoch: 40 [33280/60000 (55%)]\tLoss: 96.781052\n",
      "Train Epoch: 40 [34560/60000 (58%)]\tLoss: 97.615311\n",
      "Train Epoch: 40 [35840/60000 (60%)]\tLoss: 97.148689\n",
      "Train Epoch: 40 [37120/60000 (62%)]\tLoss: 96.084778\n",
      "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 95.134354\n",
      "Train Epoch: 40 [39680/60000 (66%)]\tLoss: 96.677628\n",
      "Train Epoch: 40 [40960/60000 (68%)]\tLoss: 94.653252\n",
      "Train Epoch: 40 [42240/60000 (70%)]\tLoss: 96.431213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 40 [43520/60000 (72%)]\tLoss: 100.233612\n",
      "Train Epoch: 40 [44800/60000 (75%)]\tLoss: 92.789330\n",
      "Train Epoch: 40 [46080/60000 (77%)]\tLoss: 93.431305\n",
      "Train Epoch: 40 [47360/60000 (79%)]\tLoss: 94.618866\n",
      "Train Epoch: 40 [48640/60000 (81%)]\tLoss: 95.533127\n",
      "Train Epoch: 40 [49920/60000 (83%)]\tLoss: 97.621788\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 101.260078\n",
      "Train Epoch: 40 [52480/60000 (87%)]\tLoss: 99.143860\n",
      "Train Epoch: 40 [53760/60000 (90%)]\tLoss: 92.574127\n",
      "Train Epoch: 40 [55040/60000 (92%)]\tLoss: 97.917496\n",
      "Train Epoch: 40 [56320/60000 (94%)]\tLoss: 101.183609\n",
      "Train Epoch: 40 [57600/60000 (96%)]\tLoss: 94.213455\n",
      "Train Epoch: 40 [58880/60000 (98%)]\tLoss: 97.371994\n",
      "====> Epoch: 40 Average loss: 97.1112\n",
      "====> Test set loss: 99.0630\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 94.505020\n",
      "Train Epoch: 41 [1280/60000 (2%)]\tLoss: 96.750504\n",
      "Train Epoch: 41 [2560/60000 (4%)]\tLoss: 100.102982\n",
      "Train Epoch: 41 [3840/60000 (6%)]\tLoss: 93.149582\n",
      "Train Epoch: 41 [5120/60000 (9%)]\tLoss: 96.088531\n",
      "Train Epoch: 41 [6400/60000 (11%)]\tLoss: 98.757507\n",
      "Train Epoch: 41 [7680/60000 (13%)]\tLoss: 96.504379\n",
      "Train Epoch: 41 [8960/60000 (15%)]\tLoss: 94.339165\n",
      "Train Epoch: 41 [10240/60000 (17%)]\tLoss: 96.857018\n",
      "Train Epoch: 41 [11520/60000 (19%)]\tLoss: 100.045456\n",
      "Train Epoch: 41 [12800/60000 (21%)]\tLoss: 98.746849\n",
      "Train Epoch: 41 [14080/60000 (23%)]\tLoss: 98.263458\n",
      "Train Epoch: 41 [15360/60000 (26%)]\tLoss: 97.197914\n",
      "Train Epoch: 41 [16640/60000 (28%)]\tLoss: 93.400490\n",
      "Train Epoch: 41 [17920/60000 (30%)]\tLoss: 98.931030\n",
      "Train Epoch: 41 [19200/60000 (32%)]\tLoss: 96.284485\n",
      "Train Epoch: 41 [20480/60000 (34%)]\tLoss: 94.272438\n",
      "Train Epoch: 41 [21760/60000 (36%)]\tLoss: 98.550827\n",
      "Train Epoch: 41 [23040/60000 (38%)]\tLoss: 98.033997\n",
      "Train Epoch: 41 [24320/60000 (41%)]\tLoss: 96.107285\n",
      "Train Epoch: 41 [25600/60000 (43%)]\tLoss: 94.347748\n",
      "Train Epoch: 41 [26880/60000 (45%)]\tLoss: 96.016022\n",
      "Train Epoch: 41 [28160/60000 (47%)]\tLoss: 96.397385\n",
      "Train Epoch: 41 [29440/60000 (49%)]\tLoss: 96.529251\n",
      "Train Epoch: 41 [30720/60000 (51%)]\tLoss: 94.462906\n",
      "Train Epoch: 41 [32000/60000 (53%)]\tLoss: 96.822403\n",
      "Train Epoch: 41 [33280/60000 (55%)]\tLoss: 102.899704\n",
      "Train Epoch: 41 [34560/60000 (58%)]\tLoss: 97.095383\n",
      "Train Epoch: 41 [35840/60000 (60%)]\tLoss: 100.895538\n",
      "Train Epoch: 41 [37120/60000 (62%)]\tLoss: 97.344017\n",
      "Train Epoch: 41 [38400/60000 (64%)]\tLoss: 93.693329\n",
      "Train Epoch: 41 [39680/60000 (66%)]\tLoss: 99.152084\n",
      "Train Epoch: 41 [40960/60000 (68%)]\tLoss: 97.875870\n",
      "Train Epoch: 41 [42240/60000 (70%)]\tLoss: 96.850540\n",
      "Train Epoch: 41 [43520/60000 (72%)]\tLoss: 98.277489\n",
      "Train Epoch: 41 [44800/60000 (75%)]\tLoss: 99.566582\n",
      "Train Epoch: 41 [46080/60000 (77%)]\tLoss: 93.235893\n",
      "Train Epoch: 41 [47360/60000 (79%)]\tLoss: 96.236481\n",
      "Train Epoch: 41 [48640/60000 (81%)]\tLoss: 94.816391\n",
      "Train Epoch: 41 [49920/60000 (83%)]\tLoss: 95.056458\n",
      "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 98.050896\n",
      "Train Epoch: 41 [52480/60000 (87%)]\tLoss: 101.467712\n",
      "Train Epoch: 41 [53760/60000 (90%)]\tLoss: 99.655731\n",
      "Train Epoch: 41 [55040/60000 (92%)]\tLoss: 99.514511\n",
      "Train Epoch: 41 [56320/60000 (94%)]\tLoss: 99.678764\n",
      "Train Epoch: 41 [57600/60000 (96%)]\tLoss: 96.979836\n",
      "Train Epoch: 41 [58880/60000 (98%)]\tLoss: 96.516373\n",
      "====> Epoch: 41 Average loss: 97.0267\n",
      "====> Test set loss: 99.1081\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 96.520721\n",
      "Train Epoch: 42 [1280/60000 (2%)]\tLoss: 96.781982\n",
      "Train Epoch: 42 [2560/60000 (4%)]\tLoss: 96.820450\n",
      "Train Epoch: 42 [3840/60000 (6%)]\tLoss: 95.799332\n",
      "Train Epoch: 42 [5120/60000 (9%)]\tLoss: 92.828720\n",
      "Train Epoch: 42 [6400/60000 (11%)]\tLoss: 96.101463\n",
      "Train Epoch: 42 [7680/60000 (13%)]\tLoss: 95.539833\n",
      "Train Epoch: 42 [8960/60000 (15%)]\tLoss: 97.002007\n",
      "Train Epoch: 42 [10240/60000 (17%)]\tLoss: 98.406845\n",
      "Train Epoch: 42 [11520/60000 (19%)]\tLoss: 97.548058\n",
      "Train Epoch: 42 [12800/60000 (21%)]\tLoss: 95.249603\n",
      "Train Epoch: 42 [14080/60000 (23%)]\tLoss: 99.416519\n",
      "Train Epoch: 42 [15360/60000 (26%)]\tLoss: 92.634705\n",
      "Train Epoch: 42 [16640/60000 (28%)]\tLoss: 99.071655\n",
      "Train Epoch: 42 [17920/60000 (30%)]\tLoss: 96.165718\n",
      "Train Epoch: 42 [19200/60000 (32%)]\tLoss: 98.468445\n",
      "Train Epoch: 42 [20480/60000 (34%)]\tLoss: 97.561172\n",
      "Train Epoch: 42 [21760/60000 (36%)]\tLoss: 96.002434\n",
      "Train Epoch: 42 [23040/60000 (38%)]\tLoss: 96.968613\n",
      "Train Epoch: 42 [24320/60000 (41%)]\tLoss: 97.175674\n",
      "Train Epoch: 42 [25600/60000 (43%)]\tLoss: 96.789406\n",
      "Train Epoch: 42 [26880/60000 (45%)]\tLoss: 99.014221\n",
      "Train Epoch: 42 [28160/60000 (47%)]\tLoss: 95.957916\n",
      "Train Epoch: 42 [29440/60000 (49%)]\tLoss: 92.003082\n",
      "Train Epoch: 42 [30720/60000 (51%)]\tLoss: 95.693298\n",
      "Train Epoch: 42 [32000/60000 (53%)]\tLoss: 99.504013\n",
      "Train Epoch: 42 [33280/60000 (55%)]\tLoss: 98.318848\n",
      "Train Epoch: 42 [34560/60000 (58%)]\tLoss: 99.296158\n",
      "Train Epoch: 42 [35840/60000 (60%)]\tLoss: 98.927170\n",
      "Train Epoch: 42 [37120/60000 (62%)]\tLoss: 98.588303\n",
      "Train Epoch: 42 [38400/60000 (64%)]\tLoss: 94.420143\n",
      "Train Epoch: 42 [39680/60000 (66%)]\tLoss: 94.277039\n",
      "Train Epoch: 42 [40960/60000 (68%)]\tLoss: 97.500084\n",
      "Train Epoch: 42 [42240/60000 (70%)]\tLoss: 91.654236\n",
      "Train Epoch: 42 [43520/60000 (72%)]\tLoss: 96.830788\n",
      "Train Epoch: 42 [44800/60000 (75%)]\tLoss: 98.318115\n",
      "Train Epoch: 42 [46080/60000 (77%)]\tLoss: 93.549850\n",
      "Train Epoch: 42 [47360/60000 (79%)]\tLoss: 94.800110\n",
      "Train Epoch: 42 [48640/60000 (81%)]\tLoss: 99.376404\n",
      "Train Epoch: 42 [49920/60000 (83%)]\tLoss: 96.514946\n",
      "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 102.175537\n",
      "Train Epoch: 42 [52480/60000 (87%)]\tLoss: 95.116028\n",
      "Train Epoch: 42 [53760/60000 (90%)]\tLoss: 97.682846\n",
      "Train Epoch: 42 [55040/60000 (92%)]\tLoss: 103.909523\n",
      "Train Epoch: 42 [56320/60000 (94%)]\tLoss: 97.382278\n",
      "Train Epoch: 42 [57600/60000 (96%)]\tLoss: 94.965897\n",
      "Train Epoch: 42 [58880/60000 (98%)]\tLoss: 96.567818\n",
      "====> Epoch: 42 Average loss: 97.0120\n",
      "====> Test set loss: 99.0367\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 97.226120\n",
      "Train Epoch: 43 [1280/60000 (2%)]\tLoss: 97.030128\n",
      "Train Epoch: 43 [2560/60000 (4%)]\tLoss: 100.201538\n",
      "Train Epoch: 43 [3840/60000 (6%)]\tLoss: 95.478981\n",
      "Train Epoch: 43 [5120/60000 (9%)]\tLoss: 95.489845\n",
      "Train Epoch: 43 [6400/60000 (11%)]\tLoss: 98.103851\n",
      "Train Epoch: 43 [7680/60000 (13%)]\tLoss: 96.309563\n",
      "Train Epoch: 43 [8960/60000 (15%)]\tLoss: 100.907227\n",
      "Train Epoch: 43 [10240/60000 (17%)]\tLoss: 96.537216\n",
      "Train Epoch: 43 [11520/60000 (19%)]\tLoss: 97.307831\n",
      "Train Epoch: 43 [12800/60000 (21%)]\tLoss: 97.257889\n",
      "Train Epoch: 43 [14080/60000 (23%)]\tLoss: 94.323715\n",
      "Train Epoch: 43 [15360/60000 (26%)]\tLoss: 96.003586\n",
      "Train Epoch: 43 [16640/60000 (28%)]\tLoss: 96.636971\n",
      "Train Epoch: 43 [17920/60000 (30%)]\tLoss: 97.001587\n",
      "Train Epoch: 43 [19200/60000 (32%)]\tLoss: 95.743637\n",
      "Train Epoch: 43 [20480/60000 (34%)]\tLoss: 99.323792\n",
      "Train Epoch: 43 [21760/60000 (36%)]\tLoss: 96.067261\n",
      "Train Epoch: 43 [23040/60000 (38%)]\tLoss: 95.995949\n",
      "Train Epoch: 43 [24320/60000 (41%)]\tLoss: 96.309189\n",
      "Train Epoch: 43 [25600/60000 (43%)]\tLoss: 96.946411\n",
      "Train Epoch: 43 [26880/60000 (45%)]\tLoss: 94.735229\n",
      "Train Epoch: 43 [28160/60000 (47%)]\tLoss: 99.610870\n",
      "Train Epoch: 43 [29440/60000 (49%)]\tLoss: 93.897179\n",
      "Train Epoch: 43 [30720/60000 (51%)]\tLoss: 98.602364\n",
      "Train Epoch: 43 [32000/60000 (53%)]\tLoss: 98.318100\n",
      "Train Epoch: 43 [33280/60000 (55%)]\tLoss: 94.182175\n",
      "Train Epoch: 43 [34560/60000 (58%)]\tLoss: 97.432892\n",
      "Train Epoch: 43 [35840/60000 (60%)]\tLoss: 97.535370\n",
      "Train Epoch: 43 [37120/60000 (62%)]\tLoss: 97.856659\n",
      "Train Epoch: 43 [38400/60000 (64%)]\tLoss: 97.988098\n",
      "Train Epoch: 43 [39680/60000 (66%)]\tLoss: 95.793907\n",
      "Train Epoch: 43 [40960/60000 (68%)]\tLoss: 100.473061\n",
      "Train Epoch: 43 [42240/60000 (70%)]\tLoss: 96.203934\n",
      "Train Epoch: 43 [43520/60000 (72%)]\tLoss: 98.150520\n",
      "Train Epoch: 43 [44800/60000 (75%)]\tLoss: 96.355965\n",
      "Train Epoch: 43 [46080/60000 (77%)]\tLoss: 98.022202\n",
      "Train Epoch: 43 [47360/60000 (79%)]\tLoss: 96.070679\n",
      "Train Epoch: 43 [48640/60000 (81%)]\tLoss: 98.862244\n",
      "Train Epoch: 43 [49920/60000 (83%)]\tLoss: 96.024445\n",
      "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 98.944611\n",
      "Train Epoch: 43 [52480/60000 (87%)]\tLoss: 97.277435\n",
      "Train Epoch: 43 [53760/60000 (90%)]\tLoss: 98.935020\n",
      "Train Epoch: 43 [55040/60000 (92%)]\tLoss: 98.881630\n",
      "Train Epoch: 43 [56320/60000 (94%)]\tLoss: 99.428680\n",
      "Train Epoch: 43 [57600/60000 (96%)]\tLoss: 96.080055\n",
      "Train Epoch: 43 [58880/60000 (98%)]\tLoss: 95.815079\n",
      "====> Epoch: 43 Average loss: 96.9965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 99.0917\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 96.874077\n",
      "Train Epoch: 44 [1280/60000 (2%)]\tLoss: 100.137108\n",
      "Train Epoch: 44 [2560/60000 (4%)]\tLoss: 91.629051\n",
      "Train Epoch: 44 [3840/60000 (6%)]\tLoss: 92.437149\n",
      "Train Epoch: 44 [5120/60000 (9%)]\tLoss: 97.047821\n",
      "Train Epoch: 44 [6400/60000 (11%)]\tLoss: 94.750595\n",
      "Train Epoch: 44 [7680/60000 (13%)]\tLoss: 98.292053\n",
      "Train Epoch: 44 [8960/60000 (15%)]\tLoss: 96.806160\n",
      "Train Epoch: 44 [10240/60000 (17%)]\tLoss: 94.010834\n",
      "Train Epoch: 44 [11520/60000 (19%)]\tLoss: 92.691284\n",
      "Train Epoch: 44 [12800/60000 (21%)]\tLoss: 96.177017\n",
      "Train Epoch: 44 [14080/60000 (23%)]\tLoss: 97.728241\n",
      "Train Epoch: 44 [15360/60000 (26%)]\tLoss: 98.817764\n",
      "Train Epoch: 44 [16640/60000 (28%)]\tLoss: 100.604919\n",
      "Train Epoch: 44 [17920/60000 (30%)]\tLoss: 97.427460\n",
      "Train Epoch: 44 [19200/60000 (32%)]\tLoss: 96.665749\n",
      "Train Epoch: 44 [20480/60000 (34%)]\tLoss: 99.692978\n",
      "Train Epoch: 44 [21760/60000 (36%)]\tLoss: 98.105469\n",
      "Train Epoch: 44 [23040/60000 (38%)]\tLoss: 97.986519\n",
      "Train Epoch: 44 [24320/60000 (41%)]\tLoss: 99.480972\n",
      "Train Epoch: 44 [25600/60000 (43%)]\tLoss: 96.001572\n",
      "Train Epoch: 44 [26880/60000 (45%)]\tLoss: 98.310455\n",
      "Train Epoch: 44 [28160/60000 (47%)]\tLoss: 95.601761\n",
      "Train Epoch: 44 [29440/60000 (49%)]\tLoss: 95.266472\n",
      "Train Epoch: 44 [30720/60000 (51%)]\tLoss: 94.807053\n",
      "Train Epoch: 44 [32000/60000 (53%)]\tLoss: 96.426590\n",
      "Train Epoch: 44 [33280/60000 (55%)]\tLoss: 96.104706\n",
      "Train Epoch: 44 [34560/60000 (58%)]\tLoss: 99.130547\n",
      "Train Epoch: 44 [35840/60000 (60%)]\tLoss: 94.904816\n",
      "Train Epoch: 44 [37120/60000 (62%)]\tLoss: 95.790703\n",
      "Train Epoch: 44 [38400/60000 (64%)]\tLoss: 97.404678\n",
      "Train Epoch: 44 [39680/60000 (66%)]\tLoss: 95.552139\n",
      "Train Epoch: 44 [40960/60000 (68%)]\tLoss: 97.416473\n",
      "Train Epoch: 44 [42240/60000 (70%)]\tLoss: 99.622314\n",
      "Train Epoch: 44 [43520/60000 (72%)]\tLoss: 95.181625\n",
      "Train Epoch: 44 [44800/60000 (75%)]\tLoss: 96.463036\n",
      "Train Epoch: 44 [46080/60000 (77%)]\tLoss: 103.858322\n",
      "Train Epoch: 44 [47360/60000 (79%)]\tLoss: 93.143982\n",
      "Train Epoch: 44 [48640/60000 (81%)]\tLoss: 94.951004\n",
      "Train Epoch: 44 [49920/60000 (83%)]\tLoss: 95.274475\n",
      "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 98.289917\n",
      "Train Epoch: 44 [52480/60000 (87%)]\tLoss: 92.854225\n",
      "Train Epoch: 44 [53760/60000 (90%)]\tLoss: 95.351547\n",
      "Train Epoch: 44 [55040/60000 (92%)]\tLoss: 94.560684\n",
      "Train Epoch: 44 [56320/60000 (94%)]\tLoss: 101.677322\n",
      "Train Epoch: 44 [57600/60000 (96%)]\tLoss: 94.433456\n",
      "Train Epoch: 44 [58880/60000 (98%)]\tLoss: 99.556900\n",
      "====> Epoch: 44 Average loss: 97.0012\n",
      "====> Test set loss: 99.2816\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 96.143478\n",
      "Train Epoch: 45 [1280/60000 (2%)]\tLoss: 93.702362\n",
      "Train Epoch: 45 [2560/60000 (4%)]\tLoss: 94.744682\n",
      "Train Epoch: 45 [3840/60000 (6%)]\tLoss: 93.979034\n",
      "Train Epoch: 45 [5120/60000 (9%)]\tLoss: 98.702927\n",
      "Train Epoch: 45 [6400/60000 (11%)]\tLoss: 94.363434\n",
      "Train Epoch: 45 [7680/60000 (13%)]\tLoss: 95.358124\n",
      "Train Epoch: 45 [8960/60000 (15%)]\tLoss: 99.366867\n",
      "Train Epoch: 45 [10240/60000 (17%)]\tLoss: 99.461967\n",
      "Train Epoch: 45 [11520/60000 (19%)]\tLoss: 98.565353\n",
      "Train Epoch: 45 [12800/60000 (21%)]\tLoss: 101.713715\n",
      "Train Epoch: 45 [14080/60000 (23%)]\tLoss: 99.197456\n",
      "Train Epoch: 45 [15360/60000 (26%)]\tLoss: 96.344482\n",
      "Train Epoch: 45 [16640/60000 (28%)]\tLoss: 98.403534\n",
      "Train Epoch: 45 [17920/60000 (30%)]\tLoss: 99.742798\n",
      "Train Epoch: 45 [19200/60000 (32%)]\tLoss: 93.961906\n",
      "Train Epoch: 45 [20480/60000 (34%)]\tLoss: 99.508362\n",
      "Train Epoch: 45 [21760/60000 (36%)]\tLoss: 96.393021\n",
      "Train Epoch: 45 [23040/60000 (38%)]\tLoss: 99.989059\n",
      "Train Epoch: 45 [24320/60000 (41%)]\tLoss: 98.699326\n",
      "Train Epoch: 45 [25600/60000 (43%)]\tLoss: 96.196724\n",
      "Train Epoch: 45 [26880/60000 (45%)]\tLoss: 96.862892\n",
      "Train Epoch: 45 [28160/60000 (47%)]\tLoss: 99.984566\n",
      "Train Epoch: 45 [29440/60000 (49%)]\tLoss: 92.894104\n",
      "Train Epoch: 45 [30720/60000 (51%)]\tLoss: 99.995491\n",
      "Train Epoch: 45 [32000/60000 (53%)]\tLoss: 95.864052\n",
      "Train Epoch: 45 [33280/60000 (55%)]\tLoss: 102.146301\n",
      "Train Epoch: 45 [34560/60000 (58%)]\tLoss: 99.746704\n",
      "Train Epoch: 45 [35840/60000 (60%)]\tLoss: 94.022209\n",
      "Train Epoch: 45 [37120/60000 (62%)]\tLoss: 94.619034\n",
      "Train Epoch: 45 [38400/60000 (64%)]\tLoss: 99.936470\n",
      "Train Epoch: 45 [39680/60000 (66%)]\tLoss: 94.042252\n",
      "Train Epoch: 45 [40960/60000 (68%)]\tLoss: 100.104187\n",
      "Train Epoch: 45 [42240/60000 (70%)]\tLoss: 97.144295\n",
      "Train Epoch: 45 [43520/60000 (72%)]\tLoss: 94.824829\n",
      "Train Epoch: 45 [44800/60000 (75%)]\tLoss: 95.502838\n",
      "Train Epoch: 45 [46080/60000 (77%)]\tLoss: 94.895401\n",
      "Train Epoch: 45 [47360/60000 (79%)]\tLoss: 93.675461\n",
      "Train Epoch: 45 [48640/60000 (81%)]\tLoss: 95.549599\n",
      "Train Epoch: 45 [49920/60000 (83%)]\tLoss: 97.067238\n",
      "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 96.506485\n",
      "Train Epoch: 45 [52480/60000 (87%)]\tLoss: 99.428131\n",
      "Train Epoch: 45 [53760/60000 (90%)]\tLoss: 101.876999\n",
      "Train Epoch: 45 [55040/60000 (92%)]\tLoss: 100.178818\n",
      "Train Epoch: 45 [56320/60000 (94%)]\tLoss: 95.213287\n",
      "Train Epoch: 45 [57600/60000 (96%)]\tLoss: 99.766632\n",
      "Train Epoch: 45 [58880/60000 (98%)]\tLoss: 98.058472\n",
      "====> Epoch: 45 Average loss: 96.9670\n",
      "====> Test set loss: 99.0707\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 99.159622\n",
      "Train Epoch: 46 [1280/60000 (2%)]\tLoss: 97.658127\n",
      "Train Epoch: 46 [2560/60000 (4%)]\tLoss: 96.204132\n",
      "Train Epoch: 46 [3840/60000 (6%)]\tLoss: 96.639961\n",
      "Train Epoch: 46 [5120/60000 (9%)]\tLoss: 97.774811\n",
      "Train Epoch: 46 [6400/60000 (11%)]\tLoss: 96.177948\n",
      "Train Epoch: 46 [7680/60000 (13%)]\tLoss: 96.387764\n",
      "Train Epoch: 46 [8960/60000 (15%)]\tLoss: 98.176720\n",
      "Train Epoch: 46 [10240/60000 (17%)]\tLoss: 97.960571\n",
      "Train Epoch: 46 [11520/60000 (19%)]\tLoss: 99.874313\n",
      "Train Epoch: 46 [12800/60000 (21%)]\tLoss: 96.207718\n",
      "Train Epoch: 46 [14080/60000 (23%)]\tLoss: 93.992149\n",
      "Train Epoch: 46 [15360/60000 (26%)]\tLoss: 96.755219\n",
      "Train Epoch: 46 [16640/60000 (28%)]\tLoss: 96.576859\n",
      "Train Epoch: 46 [17920/60000 (30%)]\tLoss: 101.163277\n",
      "Train Epoch: 46 [19200/60000 (32%)]\tLoss: 95.862289\n",
      "Train Epoch: 46 [20480/60000 (34%)]\tLoss: 96.840637\n",
      "Train Epoch: 46 [21760/60000 (36%)]\tLoss: 96.792931\n",
      "Train Epoch: 46 [23040/60000 (38%)]\tLoss: 96.311066\n",
      "Train Epoch: 46 [24320/60000 (41%)]\tLoss: 95.183128\n",
      "Train Epoch: 46 [25600/60000 (43%)]\tLoss: 97.547455\n",
      "Train Epoch: 46 [26880/60000 (45%)]\tLoss: 96.110016\n",
      "Train Epoch: 46 [28160/60000 (47%)]\tLoss: 99.255753\n",
      "Train Epoch: 46 [29440/60000 (49%)]\tLoss: 97.097725\n",
      "Train Epoch: 46 [30720/60000 (51%)]\tLoss: 99.246826\n",
      "Train Epoch: 46 [32000/60000 (53%)]\tLoss: 95.655182\n",
      "Train Epoch: 46 [33280/60000 (55%)]\tLoss: 95.062576\n",
      "Train Epoch: 46 [34560/60000 (58%)]\tLoss: 94.644470\n",
      "Train Epoch: 46 [35840/60000 (60%)]\tLoss: 95.710014\n",
      "Train Epoch: 46 [37120/60000 (62%)]\tLoss: 96.971268\n",
      "Train Epoch: 46 [38400/60000 (64%)]\tLoss: 98.549820\n",
      "Train Epoch: 46 [39680/60000 (66%)]\tLoss: 96.640991\n",
      "Train Epoch: 46 [40960/60000 (68%)]\tLoss: 96.328400\n",
      "Train Epoch: 46 [42240/60000 (70%)]\tLoss: 96.673286\n",
      "Train Epoch: 46 [43520/60000 (72%)]\tLoss: 96.540451\n",
      "Train Epoch: 46 [44800/60000 (75%)]\tLoss: 98.858032\n",
      "Train Epoch: 46 [46080/60000 (77%)]\tLoss: 97.724243\n",
      "Train Epoch: 46 [47360/60000 (79%)]\tLoss: 96.234970\n",
      "Train Epoch: 46 [48640/60000 (81%)]\tLoss: 94.852402\n",
      "Train Epoch: 46 [49920/60000 (83%)]\tLoss: 97.091934\n",
      "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 100.687057\n",
      "Train Epoch: 46 [52480/60000 (87%)]\tLoss: 96.490829\n",
      "Train Epoch: 46 [53760/60000 (90%)]\tLoss: 97.967331\n",
      "Train Epoch: 46 [55040/60000 (92%)]\tLoss: 97.580780\n",
      "Train Epoch: 46 [56320/60000 (94%)]\tLoss: 98.419350\n",
      "Train Epoch: 46 [57600/60000 (96%)]\tLoss: 94.928833\n",
      "Train Epoch: 46 [58880/60000 (98%)]\tLoss: 95.938599\n",
      "====> Epoch: 46 Average loss: 96.9406\n",
      "====> Test set loss: 99.3824\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 92.533249\n",
      "Train Epoch: 47 [1280/60000 (2%)]\tLoss: 95.969849\n",
      "Train Epoch: 47 [2560/60000 (4%)]\tLoss: 92.928528\n",
      "Train Epoch: 47 [3840/60000 (6%)]\tLoss: 94.344070\n",
      "Train Epoch: 47 [5120/60000 (9%)]\tLoss: 98.805656\n",
      "Train Epoch: 47 [6400/60000 (11%)]\tLoss: 100.658478\n",
      "Train Epoch: 47 [7680/60000 (13%)]\tLoss: 97.687874\n",
      "Train Epoch: 47 [8960/60000 (15%)]\tLoss: 95.286957\n",
      "Train Epoch: 47 [10240/60000 (17%)]\tLoss: 98.197525\n",
      "Train Epoch: 47 [11520/60000 (19%)]\tLoss: 100.823822\n",
      "Train Epoch: 47 [12800/60000 (21%)]\tLoss: 97.468918\n",
      "Train Epoch: 47 [14080/60000 (23%)]\tLoss: 94.713470\n",
      "Train Epoch: 47 [15360/60000 (26%)]\tLoss: 97.980377\n",
      "Train Epoch: 47 [16640/60000 (28%)]\tLoss: 97.867592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 47 [17920/60000 (30%)]\tLoss: 98.716362\n",
      "Train Epoch: 47 [19200/60000 (32%)]\tLoss: 95.147827\n",
      "Train Epoch: 47 [20480/60000 (34%)]\tLoss: 93.286331\n",
      "Train Epoch: 47 [21760/60000 (36%)]\tLoss: 97.603760\n",
      "Train Epoch: 47 [23040/60000 (38%)]\tLoss: 92.440781\n",
      "Train Epoch: 47 [24320/60000 (41%)]\tLoss: 96.067673\n",
      "Train Epoch: 47 [25600/60000 (43%)]\tLoss: 97.475174\n",
      "Train Epoch: 47 [26880/60000 (45%)]\tLoss: 96.636154\n",
      "Train Epoch: 47 [28160/60000 (47%)]\tLoss: 97.512009\n",
      "Train Epoch: 47 [29440/60000 (49%)]\tLoss: 99.041069\n",
      "Train Epoch: 47 [30720/60000 (51%)]\tLoss: 90.760620\n",
      "Train Epoch: 47 [32000/60000 (53%)]\tLoss: 94.066879\n",
      "Train Epoch: 47 [33280/60000 (55%)]\tLoss: 97.489105\n",
      "Train Epoch: 47 [34560/60000 (58%)]\tLoss: 97.046585\n",
      "Train Epoch: 47 [35840/60000 (60%)]\tLoss: 95.823700\n",
      "Train Epoch: 47 [37120/60000 (62%)]\tLoss: 96.384521\n",
      "Train Epoch: 47 [38400/60000 (64%)]\tLoss: 98.126747\n",
      "Train Epoch: 47 [39680/60000 (66%)]\tLoss: 95.411194\n",
      "Train Epoch: 47 [40960/60000 (68%)]\tLoss: 98.107697\n",
      "Train Epoch: 47 [42240/60000 (70%)]\tLoss: 98.673431\n",
      "Train Epoch: 47 [43520/60000 (72%)]\tLoss: 92.853561\n",
      "Train Epoch: 47 [44800/60000 (75%)]\tLoss: 95.556046\n",
      "Train Epoch: 47 [46080/60000 (77%)]\tLoss: 95.902321\n",
      "Train Epoch: 47 [47360/60000 (79%)]\tLoss: 93.740608\n",
      "Train Epoch: 47 [48640/60000 (81%)]\tLoss: 94.831764\n",
      "Train Epoch: 47 [49920/60000 (83%)]\tLoss: 92.844933\n",
      "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 96.978584\n",
      "Train Epoch: 47 [52480/60000 (87%)]\tLoss: 95.630882\n",
      "Train Epoch: 47 [53760/60000 (90%)]\tLoss: 95.280556\n",
      "Train Epoch: 47 [55040/60000 (92%)]\tLoss: 95.277237\n",
      "Train Epoch: 47 [56320/60000 (94%)]\tLoss: 99.925201\n",
      "Train Epoch: 47 [57600/60000 (96%)]\tLoss: 97.215668\n",
      "Train Epoch: 47 [58880/60000 (98%)]\tLoss: 97.530212\n",
      "====> Epoch: 47 Average loss: 96.9221\n",
      "====> Test set loss: 99.3374\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 97.166985\n",
      "Train Epoch: 48 [1280/60000 (2%)]\tLoss: 98.200905\n",
      "Train Epoch: 48 [2560/60000 (4%)]\tLoss: 102.062195\n",
      "Train Epoch: 48 [3840/60000 (6%)]\tLoss: 95.940872\n",
      "Train Epoch: 48 [5120/60000 (9%)]\tLoss: 97.145920\n",
      "Train Epoch: 48 [6400/60000 (11%)]\tLoss: 100.205269\n",
      "Train Epoch: 48 [7680/60000 (13%)]\tLoss: 97.886284\n",
      "Train Epoch: 48 [8960/60000 (15%)]\tLoss: 94.344498\n",
      "Train Epoch: 48 [10240/60000 (17%)]\tLoss: 98.994110\n",
      "Train Epoch: 48 [11520/60000 (19%)]\tLoss: 95.583405\n",
      "Train Epoch: 48 [12800/60000 (21%)]\tLoss: 97.996567\n",
      "Train Epoch: 48 [14080/60000 (23%)]\tLoss: 99.795502\n",
      "Train Epoch: 48 [15360/60000 (26%)]\tLoss: 96.847977\n",
      "Train Epoch: 48 [16640/60000 (28%)]\tLoss: 101.589958\n",
      "Train Epoch: 48 [17920/60000 (30%)]\tLoss: 95.713890\n",
      "Train Epoch: 48 [19200/60000 (32%)]\tLoss: 98.633896\n",
      "Train Epoch: 48 [20480/60000 (34%)]\tLoss: 98.380760\n",
      "Train Epoch: 48 [21760/60000 (36%)]\tLoss: 95.535126\n",
      "Train Epoch: 48 [23040/60000 (38%)]\tLoss: 101.297668\n",
      "Train Epoch: 48 [24320/60000 (41%)]\tLoss: 92.566849\n",
      "Train Epoch: 48 [25600/60000 (43%)]\tLoss: 95.068886\n",
      "Train Epoch: 48 [26880/60000 (45%)]\tLoss: 96.298141\n",
      "Train Epoch: 48 [28160/60000 (47%)]\tLoss: 95.187317\n",
      "Train Epoch: 48 [29440/60000 (49%)]\tLoss: 100.367500\n",
      "Train Epoch: 48 [30720/60000 (51%)]\tLoss: 93.982918\n",
      "Train Epoch: 48 [32000/60000 (53%)]\tLoss: 96.743973\n",
      "Train Epoch: 48 [33280/60000 (55%)]\tLoss: 94.797882\n",
      "Train Epoch: 48 [34560/60000 (58%)]\tLoss: 98.825912\n",
      "Train Epoch: 48 [35840/60000 (60%)]\tLoss: 96.667053\n",
      "Train Epoch: 48 [37120/60000 (62%)]\tLoss: 95.687187\n",
      "Train Epoch: 48 [38400/60000 (64%)]\tLoss: 100.136375\n",
      "Train Epoch: 48 [39680/60000 (66%)]\tLoss: 100.293472\n",
      "Train Epoch: 48 [40960/60000 (68%)]\tLoss: 97.582268\n",
      "Train Epoch: 48 [42240/60000 (70%)]\tLoss: 95.164337\n",
      "Train Epoch: 48 [43520/60000 (72%)]\tLoss: 97.926888\n",
      "Train Epoch: 48 [44800/60000 (75%)]\tLoss: 102.147964\n",
      "Train Epoch: 48 [46080/60000 (77%)]\tLoss: 97.930405\n",
      "Train Epoch: 48 [47360/60000 (79%)]\tLoss: 98.321686\n",
      "Train Epoch: 48 [48640/60000 (81%)]\tLoss: 98.496109\n",
      "Train Epoch: 48 [49920/60000 (83%)]\tLoss: 95.234116\n",
      "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 98.895462\n",
      "Train Epoch: 48 [52480/60000 (87%)]\tLoss: 97.597687\n",
      "Train Epoch: 48 [53760/60000 (90%)]\tLoss: 96.398544\n",
      "Train Epoch: 48 [55040/60000 (92%)]\tLoss: 96.832680\n",
      "Train Epoch: 48 [56320/60000 (94%)]\tLoss: 97.281944\n",
      "Train Epoch: 48 [57600/60000 (96%)]\tLoss: 99.636841\n",
      "Train Epoch: 48 [58880/60000 (98%)]\tLoss: 100.142464\n",
      "====> Epoch: 48 Average loss: 96.9210\n",
      "====> Test set loss: 99.0272\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 96.056259\n",
      "Train Epoch: 49 [1280/60000 (2%)]\tLoss: 98.262398\n",
      "Train Epoch: 49 [2560/60000 (4%)]\tLoss: 95.601639\n",
      "Train Epoch: 49 [3840/60000 (6%)]\tLoss: 98.185822\n",
      "Train Epoch: 49 [5120/60000 (9%)]\tLoss: 91.692039\n",
      "Train Epoch: 49 [6400/60000 (11%)]\tLoss: 99.909782\n",
      "Train Epoch: 49 [7680/60000 (13%)]\tLoss: 96.460983\n",
      "Train Epoch: 49 [8960/60000 (15%)]\tLoss: 95.820206\n",
      "Train Epoch: 49 [10240/60000 (17%)]\tLoss: 99.311859\n",
      "Train Epoch: 49 [11520/60000 (19%)]\tLoss: 94.301605\n",
      "Train Epoch: 49 [12800/60000 (21%)]\tLoss: 94.863289\n",
      "Train Epoch: 49 [14080/60000 (23%)]\tLoss: 98.299408\n",
      "Train Epoch: 49 [15360/60000 (26%)]\tLoss: 97.226982\n",
      "Train Epoch: 49 [16640/60000 (28%)]\tLoss: 97.161987\n",
      "Train Epoch: 49 [17920/60000 (30%)]\tLoss: 101.197617\n",
      "Train Epoch: 49 [19200/60000 (32%)]\tLoss: 94.328873\n",
      "Train Epoch: 49 [20480/60000 (34%)]\tLoss: 93.153748\n",
      "Train Epoch: 49 [21760/60000 (36%)]\tLoss: 97.413017\n",
      "Train Epoch: 49 [23040/60000 (38%)]\tLoss: 98.983818\n",
      "Train Epoch: 49 [24320/60000 (41%)]\tLoss: 98.679718\n",
      "Train Epoch: 49 [25600/60000 (43%)]\tLoss: 98.058624\n",
      "Train Epoch: 49 [26880/60000 (45%)]\tLoss: 98.380249\n",
      "Train Epoch: 49 [28160/60000 (47%)]\tLoss: 95.919754\n",
      "Train Epoch: 49 [29440/60000 (49%)]\tLoss: 98.681129\n",
      "Train Epoch: 49 [30720/60000 (51%)]\tLoss: 98.961266\n",
      "Train Epoch: 49 [32000/60000 (53%)]\tLoss: 101.298126\n",
      "Train Epoch: 49 [33280/60000 (55%)]\tLoss: 95.931976\n",
      "Train Epoch: 49 [34560/60000 (58%)]\tLoss: 95.193092\n",
      "Train Epoch: 49 [35840/60000 (60%)]\tLoss: 99.391571\n",
      "Train Epoch: 49 [37120/60000 (62%)]\tLoss: 93.041573\n",
      "Train Epoch: 49 [38400/60000 (64%)]\tLoss: 98.096558\n",
      "Train Epoch: 49 [39680/60000 (66%)]\tLoss: 97.927910\n",
      "Train Epoch: 49 [40960/60000 (68%)]\tLoss: 94.430344\n",
      "Train Epoch: 49 [42240/60000 (70%)]\tLoss: 92.308640\n",
      "Train Epoch: 49 [43520/60000 (72%)]\tLoss: 94.955879\n",
      "Train Epoch: 49 [44800/60000 (75%)]\tLoss: 98.445526\n",
      "Train Epoch: 49 [46080/60000 (77%)]\tLoss: 97.103256\n",
      "Train Epoch: 49 [47360/60000 (79%)]\tLoss: 101.480270\n",
      "Train Epoch: 49 [48640/60000 (81%)]\tLoss: 97.753540\n",
      "Train Epoch: 49 [49920/60000 (83%)]\tLoss: 95.363724\n",
      "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 99.872940\n",
      "Train Epoch: 49 [52480/60000 (87%)]\tLoss: 94.861443\n",
      "Train Epoch: 49 [53760/60000 (90%)]\tLoss: 99.360321\n",
      "Train Epoch: 49 [55040/60000 (92%)]\tLoss: 97.392014\n",
      "Train Epoch: 49 [56320/60000 (94%)]\tLoss: 95.764786\n",
      "Train Epoch: 49 [57600/60000 (96%)]\tLoss: 92.579422\n",
      "Train Epoch: 49 [58880/60000 (98%)]\tLoss: 95.971466\n",
      "====> Epoch: 49 Average loss: 96.9005\n",
      "====> Test set loss: 99.0679\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 97.460815\n",
      "Train Epoch: 50 [1280/60000 (2%)]\tLoss: 98.245110\n",
      "Train Epoch: 50 [2560/60000 (4%)]\tLoss: 99.917908\n",
      "Train Epoch: 50 [3840/60000 (6%)]\tLoss: 97.974663\n",
      "Train Epoch: 50 [5120/60000 (9%)]\tLoss: 101.371582\n",
      "Train Epoch: 50 [6400/60000 (11%)]\tLoss: 99.162109\n",
      "Train Epoch: 50 [7680/60000 (13%)]\tLoss: 99.775818\n",
      "Train Epoch: 50 [8960/60000 (15%)]\tLoss: 96.852066\n",
      "Train Epoch: 50 [10240/60000 (17%)]\tLoss: 97.751915\n",
      "Train Epoch: 50 [11520/60000 (19%)]\tLoss: 94.793236\n",
      "Train Epoch: 50 [12800/60000 (21%)]\tLoss: 97.337357\n",
      "Train Epoch: 50 [14080/60000 (23%)]\tLoss: 95.395195\n",
      "Train Epoch: 50 [15360/60000 (26%)]\tLoss: 98.385635\n",
      "Train Epoch: 50 [16640/60000 (28%)]\tLoss: 94.927467\n",
      "Train Epoch: 50 [17920/60000 (30%)]\tLoss: 98.413666\n",
      "Train Epoch: 50 [19200/60000 (32%)]\tLoss: 98.678230\n",
      "Train Epoch: 50 [20480/60000 (34%)]\tLoss: 93.995163\n",
      "Train Epoch: 50 [21760/60000 (36%)]\tLoss: 93.950233\n",
      "Train Epoch: 50 [23040/60000 (38%)]\tLoss: 99.592529\n",
      "Train Epoch: 50 [24320/60000 (41%)]\tLoss: 96.562958\n",
      "Train Epoch: 50 [25600/60000 (43%)]\tLoss: 95.236122\n",
      "Train Epoch: 50 [26880/60000 (45%)]\tLoss: 99.164543\n",
      "Train Epoch: 50 [28160/60000 (47%)]\tLoss: 100.482925\n",
      "Train Epoch: 50 [29440/60000 (49%)]\tLoss: 99.073410\n",
      "Train Epoch: 50 [30720/60000 (51%)]\tLoss: 96.125267\n",
      "Train Epoch: 50 [32000/60000 (53%)]\tLoss: 94.157593\n",
      "Train Epoch: 50 [33280/60000 (55%)]\tLoss: 101.642944\n",
      "Train Epoch: 50 [34560/60000 (58%)]\tLoss: 96.968651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 50 [35840/60000 (60%)]\tLoss: 97.622910\n",
      "Train Epoch: 50 [37120/60000 (62%)]\tLoss: 96.216675\n",
      "Train Epoch: 50 [38400/60000 (64%)]\tLoss: 100.338844\n",
      "Train Epoch: 50 [39680/60000 (66%)]\tLoss: 104.263885\n",
      "Train Epoch: 50 [40960/60000 (68%)]\tLoss: 99.027481\n",
      "Train Epoch: 50 [42240/60000 (70%)]\tLoss: 97.276466\n",
      "Train Epoch: 50 [43520/60000 (72%)]\tLoss: 96.966743\n",
      "Train Epoch: 50 [44800/60000 (75%)]\tLoss: 97.939392\n",
      "Train Epoch: 50 [46080/60000 (77%)]\tLoss: 92.967285\n",
      "Train Epoch: 50 [47360/60000 (79%)]\tLoss: 96.335732\n",
      "Train Epoch: 50 [48640/60000 (81%)]\tLoss: 97.627274\n",
      "Train Epoch: 50 [49920/60000 (83%)]\tLoss: 94.832611\n",
      "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 96.584946\n",
      "Train Epoch: 50 [52480/60000 (87%)]\tLoss: 100.541801\n",
      "Train Epoch: 50 [53760/60000 (90%)]\tLoss: 98.365471\n",
      "Train Epoch: 50 [55040/60000 (92%)]\tLoss: 99.487732\n",
      "Train Epoch: 50 [56320/60000 (94%)]\tLoss: 96.357262\n",
      "Train Epoch: 50 [57600/60000 (96%)]\tLoss: 98.482941\n",
      "Train Epoch: 50 [58880/60000 (98%)]\tLoss: 100.797562\n",
      "====> Epoch: 50 Average loss: 96.8895\n",
      "====> Test set loss: 99.0115\n",
      "Train Epoch: 51 [0/60000 (0%)]\tLoss: 95.027908\n",
      "Train Epoch: 51 [1280/60000 (2%)]\tLoss: 98.483032\n",
      "Train Epoch: 51 [2560/60000 (4%)]\tLoss: 98.530045\n",
      "Train Epoch: 51 [3840/60000 (6%)]\tLoss: 95.179214\n",
      "Train Epoch: 51 [5120/60000 (9%)]\tLoss: 96.557274\n",
      "Train Epoch: 51 [6400/60000 (11%)]\tLoss: 94.412628\n",
      "Train Epoch: 51 [7680/60000 (13%)]\tLoss: 95.564896\n",
      "Train Epoch: 51 [8960/60000 (15%)]\tLoss: 97.672409\n",
      "Train Epoch: 51 [10240/60000 (17%)]\tLoss: 97.455597\n",
      "Train Epoch: 51 [11520/60000 (19%)]\tLoss: 97.026535\n",
      "Train Epoch: 51 [12800/60000 (21%)]\tLoss: 95.995850\n",
      "Train Epoch: 51 [14080/60000 (23%)]\tLoss: 97.265335\n",
      "Train Epoch: 51 [15360/60000 (26%)]\tLoss: 95.277603\n",
      "Train Epoch: 51 [16640/60000 (28%)]\tLoss: 96.781204\n",
      "Train Epoch: 51 [17920/60000 (30%)]\tLoss: 99.864182\n",
      "Train Epoch: 51 [19200/60000 (32%)]\tLoss: 100.725952\n",
      "Train Epoch: 51 [20480/60000 (34%)]\tLoss: 97.422394\n",
      "Train Epoch: 51 [21760/60000 (36%)]\tLoss: 97.078339\n",
      "Train Epoch: 51 [23040/60000 (38%)]\tLoss: 95.021904\n",
      "Train Epoch: 51 [24320/60000 (41%)]\tLoss: 95.953445\n",
      "Train Epoch: 51 [25600/60000 (43%)]\tLoss: 96.466164\n",
      "Train Epoch: 51 [26880/60000 (45%)]\tLoss: 99.711868\n",
      "Train Epoch: 51 [28160/60000 (47%)]\tLoss: 97.684647\n",
      "Train Epoch: 51 [29440/60000 (49%)]\tLoss: 99.457581\n",
      "Train Epoch: 51 [30720/60000 (51%)]\tLoss: 94.096985\n",
      "Train Epoch: 51 [32000/60000 (53%)]\tLoss: 93.150841\n",
      "Train Epoch: 51 [33280/60000 (55%)]\tLoss: 100.577438\n",
      "Train Epoch: 51 [34560/60000 (58%)]\tLoss: 96.774803\n",
      "Train Epoch: 51 [35840/60000 (60%)]\tLoss: 96.611008\n",
      "Train Epoch: 51 [37120/60000 (62%)]\tLoss: 94.724350\n",
      "Train Epoch: 51 [38400/60000 (64%)]\tLoss: 99.298775\n",
      "Train Epoch: 51 [39680/60000 (66%)]\tLoss: 99.541183\n",
      "Train Epoch: 51 [40960/60000 (68%)]\tLoss: 101.689392\n",
      "Train Epoch: 51 [42240/60000 (70%)]\tLoss: 94.068336\n",
      "Train Epoch: 51 [43520/60000 (72%)]\tLoss: 96.807297\n",
      "Train Epoch: 51 [44800/60000 (75%)]\tLoss: 94.748215\n",
      "Train Epoch: 51 [46080/60000 (77%)]\tLoss: 96.260010\n",
      "Train Epoch: 51 [47360/60000 (79%)]\tLoss: 98.248184\n",
      "Train Epoch: 51 [48640/60000 (81%)]\tLoss: 99.557053\n",
      "Train Epoch: 51 [49920/60000 (83%)]\tLoss: 94.617004\n",
      "Train Epoch: 51 [51200/60000 (85%)]\tLoss: 102.824661\n",
      "Train Epoch: 51 [52480/60000 (87%)]\tLoss: 95.889908\n",
      "Train Epoch: 51 [53760/60000 (90%)]\tLoss: 98.084068\n",
      "Train Epoch: 51 [55040/60000 (92%)]\tLoss: 96.428864\n",
      "Train Epoch: 51 [56320/60000 (94%)]\tLoss: 97.237244\n",
      "Train Epoch: 51 [57600/60000 (96%)]\tLoss: 93.101028\n",
      "Train Epoch: 51 [58880/60000 (98%)]\tLoss: 95.371353\n",
      "====> Epoch: 51 Average loss: 96.8527\n",
      "====> Test set loss: 99.0622\n",
      "Train Epoch: 52 [0/60000 (0%)]\tLoss: 94.219261\n",
      "Train Epoch: 52 [1280/60000 (2%)]\tLoss: 98.246048\n",
      "Train Epoch: 52 [2560/60000 (4%)]\tLoss: 96.889252\n",
      "Train Epoch: 52 [3840/60000 (6%)]\tLoss: 93.697693\n",
      "Train Epoch: 52 [5120/60000 (9%)]\tLoss: 90.827087\n",
      "Train Epoch: 52 [6400/60000 (11%)]\tLoss: 100.441666\n",
      "Train Epoch: 52 [7680/60000 (13%)]\tLoss: 98.423668\n",
      "Train Epoch: 52 [8960/60000 (15%)]\tLoss: 98.405502\n",
      "Train Epoch: 52 [10240/60000 (17%)]\tLoss: 96.147041\n",
      "Train Epoch: 52 [11520/60000 (19%)]\tLoss: 95.992416\n",
      "Train Epoch: 52 [12800/60000 (21%)]\tLoss: 93.690697\n",
      "Train Epoch: 52 [14080/60000 (23%)]\tLoss: 98.798607\n",
      "Train Epoch: 52 [15360/60000 (26%)]\tLoss: 102.720627\n",
      "Train Epoch: 52 [16640/60000 (28%)]\tLoss: 96.585808\n",
      "Train Epoch: 52 [17920/60000 (30%)]\tLoss: 96.008972\n",
      "Train Epoch: 52 [19200/60000 (32%)]\tLoss: 96.603775\n",
      "Train Epoch: 52 [20480/60000 (34%)]\tLoss: 92.214951\n",
      "Train Epoch: 52 [21760/60000 (36%)]\tLoss: 96.474854\n",
      "Train Epoch: 52 [23040/60000 (38%)]\tLoss: 92.444901\n",
      "Train Epoch: 52 [24320/60000 (41%)]\tLoss: 98.662155\n",
      "Train Epoch: 52 [25600/60000 (43%)]\tLoss: 95.031708\n",
      "Train Epoch: 52 [26880/60000 (45%)]\tLoss: 97.290100\n",
      "Train Epoch: 52 [28160/60000 (47%)]\tLoss: 95.512833\n",
      "Train Epoch: 52 [29440/60000 (49%)]\tLoss: 98.397270\n",
      "Train Epoch: 52 [30720/60000 (51%)]\tLoss: 96.214371\n",
      "Train Epoch: 52 [32000/60000 (53%)]\tLoss: 99.140511\n",
      "Train Epoch: 52 [33280/60000 (55%)]\tLoss: 102.459038\n",
      "Train Epoch: 52 [34560/60000 (58%)]\tLoss: 96.403236\n",
      "Train Epoch: 52 [35840/60000 (60%)]\tLoss: 98.393753\n",
      "Train Epoch: 52 [37120/60000 (62%)]\tLoss: 93.352844\n",
      "Train Epoch: 52 [38400/60000 (64%)]\tLoss: 93.775375\n",
      "Train Epoch: 52 [39680/60000 (66%)]\tLoss: 94.439980\n",
      "Train Epoch: 52 [40960/60000 (68%)]\tLoss: 93.625099\n",
      "Train Epoch: 52 [42240/60000 (70%)]\tLoss: 97.127495\n",
      "Train Epoch: 52 [43520/60000 (72%)]\tLoss: 98.168274\n",
      "Train Epoch: 52 [44800/60000 (75%)]\tLoss: 99.931602\n",
      "Train Epoch: 52 [46080/60000 (77%)]\tLoss: 93.626595\n",
      "Train Epoch: 52 [47360/60000 (79%)]\tLoss: 98.047318\n",
      "Train Epoch: 52 [48640/60000 (81%)]\tLoss: 96.197792\n",
      "Train Epoch: 52 [49920/60000 (83%)]\tLoss: 93.648613\n",
      "Train Epoch: 52 [51200/60000 (85%)]\tLoss: 94.282257\n",
      "Train Epoch: 52 [52480/60000 (87%)]\tLoss: 99.737335\n",
      "Train Epoch: 52 [53760/60000 (90%)]\tLoss: 95.395889\n",
      "Train Epoch: 52 [55040/60000 (92%)]\tLoss: 95.631210\n",
      "Train Epoch: 52 [56320/60000 (94%)]\tLoss: 99.322296\n",
      "Train Epoch: 52 [57600/60000 (96%)]\tLoss: 98.300636\n",
      "Train Epoch: 52 [58880/60000 (98%)]\tLoss: 95.575386\n",
      "====> Epoch: 52 Average loss: 96.8354\n",
      "====> Test set loss: 99.0000\n",
      "Train Epoch: 53 [0/60000 (0%)]\tLoss: 100.096512\n",
      "Train Epoch: 53 [1280/60000 (2%)]\tLoss: 99.092003\n",
      "Train Epoch: 53 [2560/60000 (4%)]\tLoss: 92.341354\n",
      "Train Epoch: 53 [3840/60000 (6%)]\tLoss: 96.682709\n",
      "Train Epoch: 53 [5120/60000 (9%)]\tLoss: 95.797592\n",
      "Train Epoch: 53 [6400/60000 (11%)]\tLoss: 97.390182\n",
      "Train Epoch: 53 [7680/60000 (13%)]\tLoss: 93.900940\n",
      "Train Epoch: 53 [8960/60000 (15%)]\tLoss: 95.675751\n",
      "Train Epoch: 53 [10240/60000 (17%)]\tLoss: 97.432732\n",
      "Train Epoch: 53 [11520/60000 (19%)]\tLoss: 95.794563\n",
      "Train Epoch: 53 [12800/60000 (21%)]\tLoss: 95.408539\n",
      "Train Epoch: 53 [14080/60000 (23%)]\tLoss: 96.756958\n",
      "Train Epoch: 53 [15360/60000 (26%)]\tLoss: 94.352074\n",
      "Train Epoch: 53 [16640/60000 (28%)]\tLoss: 97.059349\n",
      "Train Epoch: 53 [17920/60000 (30%)]\tLoss: 93.610626\n",
      "Train Epoch: 53 [19200/60000 (32%)]\tLoss: 98.712044\n",
      "Train Epoch: 53 [20480/60000 (34%)]\tLoss: 97.566582\n",
      "Train Epoch: 53 [21760/60000 (36%)]\tLoss: 95.535629\n",
      "Train Epoch: 53 [23040/60000 (38%)]\tLoss: 91.311951\n",
      "Train Epoch: 53 [24320/60000 (41%)]\tLoss: 97.941879\n",
      "Train Epoch: 53 [25600/60000 (43%)]\tLoss: 97.389755\n",
      "Train Epoch: 53 [26880/60000 (45%)]\tLoss: 92.875969\n",
      "Train Epoch: 53 [28160/60000 (47%)]\tLoss: 97.987350\n",
      "Train Epoch: 53 [29440/60000 (49%)]\tLoss: 93.704514\n",
      "Train Epoch: 53 [30720/60000 (51%)]\tLoss: 96.606186\n",
      "Train Epoch: 53 [32000/60000 (53%)]\tLoss: 98.850296\n",
      "Train Epoch: 53 [33280/60000 (55%)]\tLoss: 96.842316\n",
      "Train Epoch: 53 [34560/60000 (58%)]\tLoss: 101.505875\n",
      "Train Epoch: 53 [35840/60000 (60%)]\tLoss: 97.505112\n",
      "Train Epoch: 53 [37120/60000 (62%)]\tLoss: 98.875694\n",
      "Train Epoch: 53 [38400/60000 (64%)]\tLoss: 98.622902\n",
      "Train Epoch: 53 [39680/60000 (66%)]\tLoss: 95.325897\n",
      "Train Epoch: 53 [40960/60000 (68%)]\tLoss: 96.891060\n",
      "Train Epoch: 53 [42240/60000 (70%)]\tLoss: 93.673363\n",
      "Train Epoch: 53 [43520/60000 (72%)]\tLoss: 100.330399\n",
      "Train Epoch: 53 [44800/60000 (75%)]\tLoss: 98.268120\n",
      "Train Epoch: 53 [46080/60000 (77%)]\tLoss: 96.310165\n",
      "Train Epoch: 53 [47360/60000 (79%)]\tLoss: 94.552361\n",
      "Train Epoch: 53 [48640/60000 (81%)]\tLoss: 97.498711\n",
      "Train Epoch: 53 [49920/60000 (83%)]\tLoss: 96.709480\n",
      "Train Epoch: 53 [51200/60000 (85%)]\tLoss: 97.726128\n",
      "Train Epoch: 53 [52480/60000 (87%)]\tLoss: 101.439499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 53 [53760/60000 (90%)]\tLoss: 97.446075\n",
      "Train Epoch: 53 [55040/60000 (92%)]\tLoss: 96.415695\n",
      "Train Epoch: 53 [56320/60000 (94%)]\tLoss: 96.528130\n",
      "Train Epoch: 53 [57600/60000 (96%)]\tLoss: 98.513489\n",
      "Train Epoch: 53 [58880/60000 (98%)]\tLoss: 99.040459\n",
      "====> Epoch: 53 Average loss: 96.8462\n",
      "====> Test set loss: 99.1083\n",
      "Train Epoch: 54 [0/60000 (0%)]\tLoss: 98.028152\n",
      "Train Epoch: 54 [1280/60000 (2%)]\tLoss: 98.432709\n",
      "Train Epoch: 54 [2560/60000 (4%)]\tLoss: 93.003494\n",
      "Train Epoch: 54 [3840/60000 (6%)]\tLoss: 95.191437\n",
      "Train Epoch: 54 [5120/60000 (9%)]\tLoss: 97.832596\n",
      "Train Epoch: 54 [6400/60000 (11%)]\tLoss: 96.368301\n",
      "Train Epoch: 54 [7680/60000 (13%)]\tLoss: 93.959915\n",
      "Train Epoch: 54 [8960/60000 (15%)]\tLoss: 94.635910\n",
      "Train Epoch: 54 [10240/60000 (17%)]\tLoss: 98.157379\n",
      "Train Epoch: 54 [11520/60000 (19%)]\tLoss: 94.309608\n",
      "Train Epoch: 54 [12800/60000 (21%)]\tLoss: 95.709518\n",
      "Train Epoch: 54 [14080/60000 (23%)]\tLoss: 96.708519\n",
      "Train Epoch: 54 [15360/60000 (26%)]\tLoss: 97.005516\n",
      "Train Epoch: 54 [16640/60000 (28%)]\tLoss: 97.535919\n",
      "Train Epoch: 54 [17920/60000 (30%)]\tLoss: 97.603836\n",
      "Train Epoch: 54 [19200/60000 (32%)]\tLoss: 98.208450\n",
      "Train Epoch: 54 [20480/60000 (34%)]\tLoss: 98.414932\n",
      "Train Epoch: 54 [21760/60000 (36%)]\tLoss: 96.844315\n",
      "Train Epoch: 54 [23040/60000 (38%)]\tLoss: 94.608612\n",
      "Train Epoch: 54 [24320/60000 (41%)]\tLoss: 95.577438\n",
      "Train Epoch: 54 [25600/60000 (43%)]\tLoss: 96.234245\n",
      "Train Epoch: 54 [26880/60000 (45%)]\tLoss: 98.281296\n",
      "Train Epoch: 54 [28160/60000 (47%)]\tLoss: 99.726082\n",
      "Train Epoch: 54 [29440/60000 (49%)]\tLoss: 95.740082\n",
      "Train Epoch: 54 [30720/60000 (51%)]\tLoss: 95.532700\n",
      "Train Epoch: 54 [32000/60000 (53%)]\tLoss: 97.057205\n",
      "Train Epoch: 54 [33280/60000 (55%)]\tLoss: 93.406059\n",
      "Train Epoch: 54 [34560/60000 (58%)]\tLoss: 96.099777\n",
      "Train Epoch: 54 [35840/60000 (60%)]\tLoss: 96.911079\n",
      "Train Epoch: 54 [37120/60000 (62%)]\tLoss: 94.204590\n",
      "Train Epoch: 54 [38400/60000 (64%)]\tLoss: 90.437149\n",
      "Train Epoch: 54 [39680/60000 (66%)]\tLoss: 93.662277\n",
      "Train Epoch: 54 [40960/60000 (68%)]\tLoss: 98.198280\n",
      "Train Epoch: 54 [42240/60000 (70%)]\tLoss: 94.456192\n",
      "Train Epoch: 54 [43520/60000 (72%)]\tLoss: 96.703690\n",
      "Train Epoch: 54 [44800/60000 (75%)]\tLoss: 99.015121\n",
      "Train Epoch: 54 [46080/60000 (77%)]\tLoss: 97.131912\n",
      "Train Epoch: 54 [47360/60000 (79%)]\tLoss: 100.127396\n",
      "Train Epoch: 54 [48640/60000 (81%)]\tLoss: 96.333168\n",
      "Train Epoch: 54 [49920/60000 (83%)]\tLoss: 94.663361\n",
      "Train Epoch: 54 [51200/60000 (85%)]\tLoss: 95.696609\n",
      "Train Epoch: 54 [52480/60000 (87%)]\tLoss: 98.599007\n",
      "Train Epoch: 54 [53760/60000 (90%)]\tLoss: 97.706413\n",
      "Train Epoch: 54 [55040/60000 (92%)]\tLoss: 95.335533\n",
      "Train Epoch: 54 [56320/60000 (94%)]\tLoss: 99.294403\n",
      "Train Epoch: 54 [57600/60000 (96%)]\tLoss: 96.149750\n",
      "Train Epoch: 54 [58880/60000 (98%)]\tLoss: 96.832016\n",
      "====> Epoch: 54 Average loss: 96.7973\n",
      "====> Test set loss: 99.0358\n",
      "Train Epoch: 55 [0/60000 (0%)]\tLoss: 95.963974\n",
      "Train Epoch: 55 [1280/60000 (2%)]\tLoss: 96.011818\n",
      "Train Epoch: 55 [2560/60000 (4%)]\tLoss: 97.084373\n",
      "Train Epoch: 55 [3840/60000 (6%)]\tLoss: 97.894012\n",
      "Train Epoch: 55 [5120/60000 (9%)]\tLoss: 93.906830\n",
      "Train Epoch: 55 [6400/60000 (11%)]\tLoss: 94.769035\n",
      "Train Epoch: 55 [7680/60000 (13%)]\tLoss: 95.180573\n",
      "Train Epoch: 55 [8960/60000 (15%)]\tLoss: 98.216118\n",
      "Train Epoch: 55 [10240/60000 (17%)]\tLoss: 97.294212\n",
      "Train Epoch: 55 [11520/60000 (19%)]\tLoss: 96.503433\n",
      "Train Epoch: 55 [12800/60000 (21%)]\tLoss: 97.055626\n",
      "Train Epoch: 55 [14080/60000 (23%)]\tLoss: 97.586220\n",
      "Train Epoch: 55 [15360/60000 (26%)]\tLoss: 92.918976\n",
      "Train Epoch: 55 [16640/60000 (28%)]\tLoss: 95.440269\n",
      "Train Epoch: 55 [17920/60000 (30%)]\tLoss: 96.663857\n",
      "Train Epoch: 55 [19200/60000 (32%)]\tLoss: 95.810242\n",
      "Train Epoch: 55 [20480/60000 (34%)]\tLoss: 99.451546\n",
      "Train Epoch: 55 [21760/60000 (36%)]\tLoss: 94.055428\n",
      "Train Epoch: 55 [23040/60000 (38%)]\tLoss: 96.823975\n",
      "Train Epoch: 55 [24320/60000 (41%)]\tLoss: 96.670265\n",
      "Train Epoch: 55 [25600/60000 (43%)]\tLoss: 96.333931\n",
      "Train Epoch: 55 [26880/60000 (45%)]\tLoss: 97.299759\n",
      "Train Epoch: 55 [28160/60000 (47%)]\tLoss: 100.126671\n",
      "Train Epoch: 55 [29440/60000 (49%)]\tLoss: 95.481125\n",
      "Train Epoch: 55 [30720/60000 (51%)]\tLoss: 96.792671\n",
      "Train Epoch: 55 [32000/60000 (53%)]\tLoss: 95.914001\n",
      "Train Epoch: 55 [33280/60000 (55%)]\tLoss: 96.449821\n",
      "Train Epoch: 55 [34560/60000 (58%)]\tLoss: 94.401016\n",
      "Train Epoch: 55 [35840/60000 (60%)]\tLoss: 100.921913\n",
      "Train Epoch: 55 [37120/60000 (62%)]\tLoss: 95.243591\n",
      "Train Epoch: 55 [38400/60000 (64%)]\tLoss: 97.589981\n",
      "Train Epoch: 55 [39680/60000 (66%)]\tLoss: 101.281181\n",
      "Train Epoch: 55 [40960/60000 (68%)]\tLoss: 100.057213\n",
      "Train Epoch: 55 [42240/60000 (70%)]\tLoss: 98.626694\n",
      "Train Epoch: 55 [43520/60000 (72%)]\tLoss: 95.864761\n",
      "Train Epoch: 55 [44800/60000 (75%)]\tLoss: 98.016037\n",
      "Train Epoch: 55 [46080/60000 (77%)]\tLoss: 97.865219\n",
      "Train Epoch: 55 [47360/60000 (79%)]\tLoss: 98.745544\n",
      "Train Epoch: 55 [48640/60000 (81%)]\tLoss: 94.408318\n",
      "Train Epoch: 55 [49920/60000 (83%)]\tLoss: 94.386055\n",
      "Train Epoch: 55 [51200/60000 (85%)]\tLoss: 99.608749\n",
      "Train Epoch: 55 [52480/60000 (87%)]\tLoss: 93.805038\n",
      "Train Epoch: 55 [53760/60000 (90%)]\tLoss: 98.296158\n",
      "Train Epoch: 55 [55040/60000 (92%)]\tLoss: 95.372665\n",
      "Train Epoch: 55 [56320/60000 (94%)]\tLoss: 94.683189\n",
      "Train Epoch: 55 [57600/60000 (96%)]\tLoss: 93.195938\n",
      "Train Epoch: 55 [58880/60000 (98%)]\tLoss: 97.083366\n",
      "====> Epoch: 55 Average loss: 96.7772\n",
      "====> Test set loss: 98.8925\n",
      "Train Epoch: 56 [0/60000 (0%)]\tLoss: 96.380867\n",
      "Train Epoch: 56 [1280/60000 (2%)]\tLoss: 95.654388\n",
      "Train Epoch: 56 [2560/60000 (4%)]\tLoss: 97.689354\n",
      "Train Epoch: 56 [3840/60000 (6%)]\tLoss: 93.786537\n",
      "Train Epoch: 56 [5120/60000 (9%)]\tLoss: 100.382278\n",
      "Train Epoch: 56 [6400/60000 (11%)]\tLoss: 99.097229\n",
      "Train Epoch: 56 [7680/60000 (13%)]\tLoss: 97.546371\n",
      "Train Epoch: 56 [8960/60000 (15%)]\tLoss: 97.265991\n",
      "Train Epoch: 56 [10240/60000 (17%)]\tLoss: 96.375084\n",
      "Train Epoch: 56 [11520/60000 (19%)]\tLoss: 97.988693\n",
      "Train Epoch: 56 [12800/60000 (21%)]\tLoss: 92.011192\n",
      "Train Epoch: 56 [14080/60000 (23%)]\tLoss: 93.973351\n",
      "Train Epoch: 56 [15360/60000 (26%)]\tLoss: 94.058975\n",
      "Train Epoch: 56 [16640/60000 (28%)]\tLoss: 98.745987\n",
      "Train Epoch: 56 [17920/60000 (30%)]\tLoss: 99.980309\n",
      "Train Epoch: 56 [19200/60000 (32%)]\tLoss: 96.344299\n",
      "Train Epoch: 56 [20480/60000 (34%)]\tLoss: 98.085754\n",
      "Train Epoch: 56 [21760/60000 (36%)]\tLoss: 97.395172\n",
      "Train Epoch: 56 [23040/60000 (38%)]\tLoss: 96.731636\n",
      "Train Epoch: 56 [24320/60000 (41%)]\tLoss: 100.648346\n",
      "Train Epoch: 56 [25600/60000 (43%)]\tLoss: 98.349892\n",
      "Train Epoch: 56 [26880/60000 (45%)]\tLoss: 99.325653\n",
      "Train Epoch: 56 [28160/60000 (47%)]\tLoss: 98.877754\n",
      "Train Epoch: 56 [29440/60000 (49%)]\tLoss: 99.045944\n",
      "Train Epoch: 56 [30720/60000 (51%)]\tLoss: 100.248795\n",
      "Train Epoch: 56 [32000/60000 (53%)]\tLoss: 100.560669\n",
      "Train Epoch: 56 [33280/60000 (55%)]\tLoss: 98.546265\n",
      "Train Epoch: 56 [34560/60000 (58%)]\tLoss: 88.972580\n",
      "Train Epoch: 56 [35840/60000 (60%)]\tLoss: 94.068474\n",
      "Train Epoch: 56 [37120/60000 (62%)]\tLoss: 98.432556\n",
      "Train Epoch: 56 [38400/60000 (64%)]\tLoss: 95.222244\n",
      "Train Epoch: 56 [39680/60000 (66%)]\tLoss: 94.442032\n",
      "Train Epoch: 56 [40960/60000 (68%)]\tLoss: 98.102905\n",
      "Train Epoch: 56 [42240/60000 (70%)]\tLoss: 99.881378\n",
      "Train Epoch: 56 [43520/60000 (72%)]\tLoss: 98.854828\n",
      "Train Epoch: 56 [44800/60000 (75%)]\tLoss: 96.445534\n",
      "Train Epoch: 56 [46080/60000 (77%)]\tLoss: 94.469055\n",
      "Train Epoch: 56 [47360/60000 (79%)]\tLoss: 95.501213\n",
      "Train Epoch: 56 [48640/60000 (81%)]\tLoss: 97.054932\n",
      "Train Epoch: 56 [49920/60000 (83%)]\tLoss: 97.432846\n",
      "Train Epoch: 56 [51200/60000 (85%)]\tLoss: 96.082977\n",
      "Train Epoch: 56 [52480/60000 (87%)]\tLoss: 98.491402\n",
      "Train Epoch: 56 [53760/60000 (90%)]\tLoss: 98.020370\n",
      "Train Epoch: 56 [55040/60000 (92%)]\tLoss: 97.968613\n",
      "Train Epoch: 56 [56320/60000 (94%)]\tLoss: 98.678505\n",
      "Train Epoch: 56 [57600/60000 (96%)]\tLoss: 95.792496\n",
      "Train Epoch: 56 [58880/60000 (98%)]\tLoss: 95.477371\n",
      "====> Epoch: 56 Average loss: 96.7648\n",
      "====> Test set loss: 98.6974\n",
      "Train Epoch: 57 [0/60000 (0%)]\tLoss: 95.024124\n",
      "Train Epoch: 57 [1280/60000 (2%)]\tLoss: 95.420242\n",
      "Train Epoch: 57 [2560/60000 (4%)]\tLoss: 97.996330\n",
      "Train Epoch: 57 [3840/60000 (6%)]\tLoss: 96.228806\n",
      "Train Epoch: 57 [5120/60000 (9%)]\tLoss: 97.829300\n",
      "Train Epoch: 57 [6400/60000 (11%)]\tLoss: 95.875648\n",
      "Train Epoch: 57 [7680/60000 (13%)]\tLoss: 101.729935\n",
      "Train Epoch: 57 [8960/60000 (15%)]\tLoss: 97.299934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 57 [10240/60000 (17%)]\tLoss: 95.009445\n",
      "Train Epoch: 57 [11520/60000 (19%)]\tLoss: 94.808495\n",
      "Train Epoch: 57 [12800/60000 (21%)]\tLoss: 96.309891\n",
      "Train Epoch: 57 [14080/60000 (23%)]\tLoss: 97.011642\n",
      "Train Epoch: 57 [15360/60000 (26%)]\tLoss: 97.584808\n",
      "Train Epoch: 57 [16640/60000 (28%)]\tLoss: 97.169266\n",
      "Train Epoch: 57 [17920/60000 (30%)]\tLoss: 98.195755\n",
      "Train Epoch: 57 [19200/60000 (32%)]\tLoss: 94.587418\n",
      "Train Epoch: 57 [20480/60000 (34%)]\tLoss: 102.345535\n",
      "Train Epoch: 57 [21760/60000 (36%)]\tLoss: 96.871399\n",
      "Train Epoch: 57 [23040/60000 (38%)]\tLoss: 99.027771\n",
      "Train Epoch: 57 [24320/60000 (41%)]\tLoss: 101.005890\n",
      "Train Epoch: 57 [25600/60000 (43%)]\tLoss: 95.619667\n",
      "Train Epoch: 57 [26880/60000 (45%)]\tLoss: 96.143951\n",
      "Train Epoch: 57 [28160/60000 (47%)]\tLoss: 96.203735\n",
      "Train Epoch: 57 [29440/60000 (49%)]\tLoss: 98.951561\n",
      "Train Epoch: 57 [30720/60000 (51%)]\tLoss: 96.796417\n",
      "Train Epoch: 57 [32000/60000 (53%)]\tLoss: 97.759720\n",
      "Train Epoch: 57 [33280/60000 (55%)]\tLoss: 99.784706\n",
      "Train Epoch: 57 [34560/60000 (58%)]\tLoss: 98.767212\n",
      "Train Epoch: 57 [35840/60000 (60%)]\tLoss: 95.091614\n",
      "Train Epoch: 57 [37120/60000 (62%)]\tLoss: 96.948433\n",
      "Train Epoch: 57 [38400/60000 (64%)]\tLoss: 95.258537\n",
      "Train Epoch: 57 [39680/60000 (66%)]\tLoss: 95.473457\n",
      "Train Epoch: 57 [40960/60000 (68%)]\tLoss: 95.600151\n",
      "Train Epoch: 57 [42240/60000 (70%)]\tLoss: 98.741493\n",
      "Train Epoch: 57 [43520/60000 (72%)]\tLoss: 95.330925\n",
      "Train Epoch: 57 [44800/60000 (75%)]\tLoss: 96.967644\n",
      "Train Epoch: 57 [46080/60000 (77%)]\tLoss: 96.264786\n",
      "Train Epoch: 57 [47360/60000 (79%)]\tLoss: 95.284698\n",
      "Train Epoch: 57 [48640/60000 (81%)]\tLoss: 101.033478\n",
      "Train Epoch: 57 [49920/60000 (83%)]\tLoss: 92.211143\n",
      "Train Epoch: 57 [51200/60000 (85%)]\tLoss: 95.755447\n",
      "Train Epoch: 57 [52480/60000 (87%)]\tLoss: 95.871140\n",
      "Train Epoch: 57 [53760/60000 (90%)]\tLoss: 97.596703\n",
      "Train Epoch: 57 [55040/60000 (92%)]\tLoss: 97.630203\n",
      "Train Epoch: 57 [56320/60000 (94%)]\tLoss: 92.771469\n",
      "Train Epoch: 57 [57600/60000 (96%)]\tLoss: 96.587738\n",
      "Train Epoch: 57 [58880/60000 (98%)]\tLoss: 99.336220\n",
      "====> Epoch: 57 Average loss: 96.7445\n",
      "====> Test set loss: 98.9739\n",
      "Train Epoch: 58 [0/60000 (0%)]\tLoss: 94.016891\n",
      "Train Epoch: 58 [1280/60000 (2%)]\tLoss: 92.763329\n",
      "Train Epoch: 58 [2560/60000 (4%)]\tLoss: 93.778526\n",
      "Train Epoch: 58 [3840/60000 (6%)]\tLoss: 90.353294\n",
      "Train Epoch: 58 [5120/60000 (9%)]\tLoss: 98.341675\n",
      "Train Epoch: 58 [6400/60000 (11%)]\tLoss: 94.834000\n",
      "Train Epoch: 58 [7680/60000 (13%)]\tLoss: 97.679688\n",
      "Train Epoch: 58 [8960/60000 (15%)]\tLoss: 95.726395\n",
      "Train Epoch: 58 [10240/60000 (17%)]\tLoss: 95.248993\n",
      "Train Epoch: 58 [11520/60000 (19%)]\tLoss: 101.016060\n",
      "Train Epoch: 58 [12800/60000 (21%)]\tLoss: 96.441650\n",
      "Train Epoch: 58 [14080/60000 (23%)]\tLoss: 96.553139\n",
      "Train Epoch: 58 [15360/60000 (26%)]\tLoss: 100.136765\n",
      "Train Epoch: 58 [16640/60000 (28%)]\tLoss: 100.256088\n",
      "Train Epoch: 58 [17920/60000 (30%)]\tLoss: 99.017288\n",
      "Train Epoch: 58 [19200/60000 (32%)]\tLoss: 97.203835\n",
      "Train Epoch: 58 [20480/60000 (34%)]\tLoss: 95.626221\n",
      "Train Epoch: 58 [21760/60000 (36%)]\tLoss: 96.101067\n",
      "Train Epoch: 58 [23040/60000 (38%)]\tLoss: 95.655060\n",
      "Train Epoch: 58 [24320/60000 (41%)]\tLoss: 100.925919\n",
      "Train Epoch: 58 [25600/60000 (43%)]\tLoss: 95.651382\n",
      "Train Epoch: 58 [26880/60000 (45%)]\tLoss: 95.178146\n",
      "Train Epoch: 58 [28160/60000 (47%)]\tLoss: 93.755623\n",
      "Train Epoch: 58 [29440/60000 (49%)]\tLoss: 96.164566\n",
      "Train Epoch: 58 [30720/60000 (51%)]\tLoss: 95.800674\n",
      "Train Epoch: 58 [32000/60000 (53%)]\tLoss: 98.317291\n",
      "Train Epoch: 58 [33280/60000 (55%)]\tLoss: 97.587608\n",
      "Train Epoch: 58 [34560/60000 (58%)]\tLoss: 100.263931\n",
      "Train Epoch: 58 [35840/60000 (60%)]\tLoss: 95.019997\n",
      "Train Epoch: 58 [37120/60000 (62%)]\tLoss: 95.535568\n",
      "Train Epoch: 58 [38400/60000 (64%)]\tLoss: 99.702751\n",
      "Train Epoch: 58 [39680/60000 (66%)]\tLoss: 92.033066\n",
      "Train Epoch: 58 [40960/60000 (68%)]\tLoss: 96.181259\n",
      "Train Epoch: 58 [42240/60000 (70%)]\tLoss: 96.213463\n",
      "Train Epoch: 58 [43520/60000 (72%)]\tLoss: 95.753014\n",
      "Train Epoch: 58 [44800/60000 (75%)]\tLoss: 93.464981\n",
      "Train Epoch: 58 [46080/60000 (77%)]\tLoss: 95.693245\n",
      "Train Epoch: 58 [47360/60000 (79%)]\tLoss: 101.437515\n",
      "Train Epoch: 58 [48640/60000 (81%)]\tLoss: 100.350098\n",
      "Train Epoch: 58 [49920/60000 (83%)]\tLoss: 95.493126\n",
      "Train Epoch: 58 [51200/60000 (85%)]\tLoss: 95.605095\n",
      "Train Epoch: 58 [52480/60000 (87%)]\tLoss: 96.687263\n",
      "Train Epoch: 58 [53760/60000 (90%)]\tLoss: 98.310898\n",
      "Train Epoch: 58 [55040/60000 (92%)]\tLoss: 97.071014\n",
      "Train Epoch: 58 [56320/60000 (94%)]\tLoss: 97.647171\n",
      "Train Epoch: 58 [57600/60000 (96%)]\tLoss: 94.529236\n",
      "Train Epoch: 58 [58880/60000 (98%)]\tLoss: 99.447922\n",
      "====> Epoch: 58 Average loss: 96.7040\n",
      "====> Test set loss: 98.9830\n",
      "Train Epoch: 59 [0/60000 (0%)]\tLoss: 96.532684\n",
      "Train Epoch: 59 [1280/60000 (2%)]\tLoss: 97.876366\n",
      "Train Epoch: 59 [2560/60000 (4%)]\tLoss: 96.233734\n",
      "Train Epoch: 59 [3840/60000 (6%)]\tLoss: 93.237152\n",
      "Train Epoch: 59 [5120/60000 (9%)]\tLoss: 96.490921\n",
      "Train Epoch: 59 [6400/60000 (11%)]\tLoss: 98.278381\n",
      "Train Epoch: 59 [7680/60000 (13%)]\tLoss: 98.431847\n",
      "Train Epoch: 59 [8960/60000 (15%)]\tLoss: 97.152733\n",
      "Train Epoch: 59 [10240/60000 (17%)]\tLoss: 96.802841\n",
      "Train Epoch: 59 [11520/60000 (19%)]\tLoss: 97.143837\n",
      "Train Epoch: 59 [12800/60000 (21%)]\tLoss: 97.898972\n",
      "Train Epoch: 59 [14080/60000 (23%)]\tLoss: 92.817604\n",
      "Train Epoch: 59 [15360/60000 (26%)]\tLoss: 96.349251\n",
      "Train Epoch: 59 [16640/60000 (28%)]\tLoss: 96.613556\n",
      "Train Epoch: 59 [17920/60000 (30%)]\tLoss: 97.951721\n",
      "Train Epoch: 59 [19200/60000 (32%)]\tLoss: 96.500191\n",
      "Train Epoch: 59 [20480/60000 (34%)]\tLoss: 100.671730\n",
      "Train Epoch: 59 [21760/60000 (36%)]\tLoss: 100.896255\n",
      "Train Epoch: 59 [23040/60000 (38%)]\tLoss: 98.635132\n",
      "Train Epoch: 59 [24320/60000 (41%)]\tLoss: 96.843796\n",
      "Train Epoch: 59 [25600/60000 (43%)]\tLoss: 95.749535\n",
      "Train Epoch: 59 [26880/60000 (45%)]\tLoss: 97.414581\n",
      "Train Epoch: 59 [28160/60000 (47%)]\tLoss: 95.853333\n",
      "Train Epoch: 59 [29440/60000 (49%)]\tLoss: 98.720406\n",
      "Train Epoch: 59 [30720/60000 (51%)]\tLoss: 100.166801\n",
      "Train Epoch: 59 [32000/60000 (53%)]\tLoss: 98.050461\n",
      "Train Epoch: 59 [33280/60000 (55%)]\tLoss: 97.220932\n",
      "Train Epoch: 59 [34560/60000 (58%)]\tLoss: 97.574158\n",
      "Train Epoch: 59 [35840/60000 (60%)]\tLoss: 97.317001\n",
      "Train Epoch: 59 [37120/60000 (62%)]\tLoss: 97.698761\n",
      "Train Epoch: 59 [38400/60000 (64%)]\tLoss: 97.904015\n",
      "Train Epoch: 59 [39680/60000 (66%)]\tLoss: 96.886902\n",
      "Train Epoch: 59 [40960/60000 (68%)]\tLoss: 97.896484\n",
      "Train Epoch: 59 [42240/60000 (70%)]\tLoss: 98.362068\n",
      "Train Epoch: 59 [43520/60000 (72%)]\tLoss: 93.868790\n",
      "Train Epoch: 59 [44800/60000 (75%)]\tLoss: 95.328133\n",
      "Train Epoch: 59 [46080/60000 (77%)]\tLoss: 96.608192\n",
      "Train Epoch: 59 [47360/60000 (79%)]\tLoss: 97.807587\n",
      "Train Epoch: 59 [48640/60000 (81%)]\tLoss: 94.511703\n",
      "Train Epoch: 59 [49920/60000 (83%)]\tLoss: 99.969559\n",
      "Train Epoch: 59 [51200/60000 (85%)]\tLoss: 94.765442\n",
      "Train Epoch: 59 [52480/60000 (87%)]\tLoss: 96.255142\n",
      "Train Epoch: 59 [53760/60000 (90%)]\tLoss: 101.694878\n",
      "Train Epoch: 59 [55040/60000 (92%)]\tLoss: 95.165443\n",
      "Train Epoch: 59 [56320/60000 (94%)]\tLoss: 100.445992\n",
      "Train Epoch: 59 [57600/60000 (96%)]\tLoss: 95.985512\n",
      "Train Epoch: 59 [58880/60000 (98%)]\tLoss: 95.168503\n",
      "====> Epoch: 59 Average loss: 96.6983\n",
      "====> Test set loss: 98.8515\n",
      "Train Epoch: 60 [0/60000 (0%)]\tLoss: 97.633469\n",
      "Train Epoch: 60 [1280/60000 (2%)]\tLoss: 97.116913\n",
      "Train Epoch: 60 [2560/60000 (4%)]\tLoss: 97.476425\n",
      "Train Epoch: 60 [3840/60000 (6%)]\tLoss: 97.446449\n",
      "Train Epoch: 60 [5120/60000 (9%)]\tLoss: 96.401192\n",
      "Train Epoch: 60 [6400/60000 (11%)]\tLoss: 95.630363\n",
      "Train Epoch: 60 [7680/60000 (13%)]\tLoss: 94.556900\n",
      "Train Epoch: 60 [8960/60000 (15%)]\tLoss: 95.683311\n",
      "Train Epoch: 60 [10240/60000 (17%)]\tLoss: 93.151962\n",
      "Train Epoch: 60 [11520/60000 (19%)]\tLoss: 95.133560\n",
      "Train Epoch: 60 [12800/60000 (21%)]\tLoss: 94.635765\n",
      "Train Epoch: 60 [14080/60000 (23%)]\tLoss: 96.809319\n",
      "Train Epoch: 60 [15360/60000 (26%)]\tLoss: 98.766525\n",
      "Train Epoch: 60 [16640/60000 (28%)]\tLoss: 97.023689\n",
      "Train Epoch: 60 [17920/60000 (30%)]\tLoss: 96.951080\n",
      "Train Epoch: 60 [19200/60000 (32%)]\tLoss: 93.530540\n",
      "Train Epoch: 60 [20480/60000 (34%)]\tLoss: 95.781372\n",
      "Train Epoch: 60 [21760/60000 (36%)]\tLoss: 95.035210\n",
      "Train Epoch: 60 [23040/60000 (38%)]\tLoss: 96.988586\n",
      "Train Epoch: 60 [24320/60000 (41%)]\tLoss: 94.262657\n",
      "Train Epoch: 60 [25600/60000 (43%)]\tLoss: 97.467545\n",
      "Train Epoch: 60 [26880/60000 (45%)]\tLoss: 96.455536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 60 [28160/60000 (47%)]\tLoss: 100.560852\n",
      "Train Epoch: 60 [29440/60000 (49%)]\tLoss: 98.573257\n",
      "Train Epoch: 60 [30720/60000 (51%)]\tLoss: 94.735321\n",
      "Train Epoch: 60 [32000/60000 (53%)]\tLoss: 98.004440\n",
      "Train Epoch: 60 [33280/60000 (55%)]\tLoss: 94.587990\n",
      "Train Epoch: 60 [34560/60000 (58%)]\tLoss: 100.686806\n",
      "Train Epoch: 60 [35840/60000 (60%)]\tLoss: 97.765106\n",
      "Train Epoch: 60 [37120/60000 (62%)]\tLoss: 98.430481\n",
      "Train Epoch: 60 [38400/60000 (64%)]\tLoss: 95.161247\n",
      "Train Epoch: 60 [39680/60000 (66%)]\tLoss: 95.692131\n",
      "Train Epoch: 60 [40960/60000 (68%)]\tLoss: 93.141075\n",
      "Train Epoch: 60 [42240/60000 (70%)]\tLoss: 96.566948\n",
      "Train Epoch: 60 [43520/60000 (72%)]\tLoss: 95.874207\n",
      "Train Epoch: 60 [44800/60000 (75%)]\tLoss: 93.539841\n",
      "Train Epoch: 60 [46080/60000 (77%)]\tLoss: 96.986542\n",
      "Train Epoch: 60 [47360/60000 (79%)]\tLoss: 98.487854\n",
      "Train Epoch: 60 [48640/60000 (81%)]\tLoss: 95.818314\n",
      "Train Epoch: 60 [49920/60000 (83%)]\tLoss: 97.240860\n",
      "Train Epoch: 60 [51200/60000 (85%)]\tLoss: 97.819923\n",
      "Train Epoch: 60 [52480/60000 (87%)]\tLoss: 95.645004\n",
      "Train Epoch: 60 [53760/60000 (90%)]\tLoss: 97.292709\n",
      "Train Epoch: 60 [55040/60000 (92%)]\tLoss: 94.950401\n",
      "Train Epoch: 60 [56320/60000 (94%)]\tLoss: 97.598038\n",
      "Train Epoch: 60 [57600/60000 (96%)]\tLoss: 95.401611\n",
      "Train Epoch: 60 [58880/60000 (98%)]\tLoss: 97.305496\n",
      "====> Epoch: 60 Average loss: 96.7020\n",
      "====> Test set loss: 98.7475\n",
      "Train Epoch: 61 [0/60000 (0%)]\tLoss: 95.044197\n",
      "Train Epoch: 61 [1280/60000 (2%)]\tLoss: 99.401718\n",
      "Train Epoch: 61 [2560/60000 (4%)]\tLoss: 94.547997\n",
      "Train Epoch: 61 [3840/60000 (6%)]\tLoss: 99.177269\n",
      "Train Epoch: 61 [5120/60000 (9%)]\tLoss: 94.186111\n",
      "Train Epoch: 61 [6400/60000 (11%)]\tLoss: 96.227356\n",
      "Train Epoch: 61 [7680/60000 (13%)]\tLoss: 94.559105\n",
      "Train Epoch: 61 [8960/60000 (15%)]\tLoss: 94.753036\n",
      "Train Epoch: 61 [10240/60000 (17%)]\tLoss: 95.913589\n",
      "Train Epoch: 61 [11520/60000 (19%)]\tLoss: 93.295601\n",
      "Train Epoch: 61 [12800/60000 (21%)]\tLoss: 96.435394\n",
      "Train Epoch: 61 [14080/60000 (23%)]\tLoss: 100.071182\n",
      "Train Epoch: 61 [15360/60000 (26%)]\tLoss: 94.841515\n",
      "Train Epoch: 61 [16640/60000 (28%)]\tLoss: 101.453751\n",
      "Train Epoch: 61 [17920/60000 (30%)]\tLoss: 98.527481\n",
      "Train Epoch: 61 [19200/60000 (32%)]\tLoss: 98.826805\n",
      "Train Epoch: 61 [20480/60000 (34%)]\tLoss: 101.667366\n",
      "Train Epoch: 61 [21760/60000 (36%)]\tLoss: 97.658508\n",
      "Train Epoch: 61 [23040/60000 (38%)]\tLoss: 98.295410\n",
      "Train Epoch: 61 [24320/60000 (41%)]\tLoss: 97.026108\n",
      "Train Epoch: 61 [25600/60000 (43%)]\tLoss: 97.131187\n",
      "Train Epoch: 61 [26880/60000 (45%)]\tLoss: 96.228989\n",
      "Train Epoch: 61 [28160/60000 (47%)]\tLoss: 92.309875\n",
      "Train Epoch: 61 [29440/60000 (49%)]\tLoss: 96.323776\n",
      "Train Epoch: 61 [30720/60000 (51%)]\tLoss: 98.452591\n",
      "Train Epoch: 61 [32000/60000 (53%)]\tLoss: 97.889297\n",
      "Train Epoch: 61 [33280/60000 (55%)]\tLoss: 93.440063\n",
      "Train Epoch: 61 [34560/60000 (58%)]\tLoss: 98.485016\n",
      "Train Epoch: 61 [35840/60000 (60%)]\tLoss: 96.607262\n",
      "Train Epoch: 61 [37120/60000 (62%)]\tLoss: 98.722702\n",
      "Train Epoch: 61 [38400/60000 (64%)]\tLoss: 92.415298\n",
      "Train Epoch: 61 [39680/60000 (66%)]\tLoss: 93.739586\n",
      "Train Epoch: 61 [40960/60000 (68%)]\tLoss: 94.920868\n",
      "Train Epoch: 61 [42240/60000 (70%)]\tLoss: 94.583954\n",
      "Train Epoch: 61 [43520/60000 (72%)]\tLoss: 95.639114\n",
      "Train Epoch: 61 [44800/60000 (75%)]\tLoss: 96.313492\n",
      "Train Epoch: 61 [46080/60000 (77%)]\tLoss: 94.195320\n",
      "Train Epoch: 61 [47360/60000 (79%)]\tLoss: 95.532272\n",
      "Train Epoch: 61 [48640/60000 (81%)]\tLoss: 97.223663\n",
      "Train Epoch: 61 [49920/60000 (83%)]\tLoss: 99.273521\n",
      "Train Epoch: 61 [51200/60000 (85%)]\tLoss: 97.927437\n",
      "Train Epoch: 61 [52480/60000 (87%)]\tLoss: 96.135544\n",
      "Train Epoch: 61 [53760/60000 (90%)]\tLoss: 98.345184\n",
      "Train Epoch: 61 [55040/60000 (92%)]\tLoss: 94.448006\n",
      "Train Epoch: 61 [56320/60000 (94%)]\tLoss: 92.619431\n",
      "Train Epoch: 61 [57600/60000 (96%)]\tLoss: 94.573402\n",
      "Train Epoch: 61 [58880/60000 (98%)]\tLoss: 96.744568\n",
      "====> Epoch: 61 Average loss: 96.6537\n",
      "====> Test set loss: 99.4853\n",
      "Train Epoch: 62 [0/60000 (0%)]\tLoss: 93.509537\n",
      "Train Epoch: 62 [1280/60000 (2%)]\tLoss: 95.661263\n",
      "Train Epoch: 62 [2560/60000 (4%)]\tLoss: 94.078613\n",
      "Train Epoch: 62 [3840/60000 (6%)]\tLoss: 98.907898\n",
      "Train Epoch: 62 [5120/60000 (9%)]\tLoss: 99.136330\n",
      "Train Epoch: 62 [6400/60000 (11%)]\tLoss: 95.530365\n",
      "Train Epoch: 62 [7680/60000 (13%)]\tLoss: 100.641983\n",
      "Train Epoch: 62 [8960/60000 (15%)]\tLoss: 97.681953\n",
      "Train Epoch: 62 [10240/60000 (17%)]\tLoss: 98.211136\n",
      "Train Epoch: 62 [11520/60000 (19%)]\tLoss: 96.388367\n",
      "Train Epoch: 62 [12800/60000 (21%)]\tLoss: 96.216896\n",
      "Train Epoch: 62 [14080/60000 (23%)]\tLoss: 102.426750\n",
      "Train Epoch: 62 [15360/60000 (26%)]\tLoss: 95.483231\n",
      "Train Epoch: 62 [16640/60000 (28%)]\tLoss: 93.419235\n",
      "Train Epoch: 62 [17920/60000 (30%)]\tLoss: 99.396713\n",
      "Train Epoch: 62 [19200/60000 (32%)]\tLoss: 95.401543\n",
      "Train Epoch: 62 [20480/60000 (34%)]\tLoss: 96.476532\n",
      "Train Epoch: 62 [21760/60000 (36%)]\tLoss: 93.632179\n",
      "Train Epoch: 62 [23040/60000 (38%)]\tLoss: 94.815697\n",
      "Train Epoch: 62 [24320/60000 (41%)]\tLoss: 95.922096\n",
      "Train Epoch: 62 [25600/60000 (43%)]\tLoss: 103.705200\n",
      "Train Epoch: 62 [26880/60000 (45%)]\tLoss: 94.002480\n",
      "Train Epoch: 62 [28160/60000 (47%)]\tLoss: 97.568497\n",
      "Train Epoch: 62 [29440/60000 (49%)]\tLoss: 97.724365\n",
      "Train Epoch: 62 [30720/60000 (51%)]\tLoss: 97.169334\n",
      "Train Epoch: 62 [32000/60000 (53%)]\tLoss: 95.341812\n",
      "Train Epoch: 62 [33280/60000 (55%)]\tLoss: 96.922447\n",
      "Train Epoch: 62 [34560/60000 (58%)]\tLoss: 94.671867\n",
      "Train Epoch: 62 [35840/60000 (60%)]\tLoss: 94.566223\n",
      "Train Epoch: 62 [37120/60000 (62%)]\tLoss: 94.567467\n",
      "Train Epoch: 62 [38400/60000 (64%)]\tLoss: 98.506317\n",
      "Train Epoch: 62 [39680/60000 (66%)]\tLoss: 101.813332\n",
      "Train Epoch: 62 [40960/60000 (68%)]\tLoss: 92.227493\n",
      "Train Epoch: 62 [42240/60000 (70%)]\tLoss: 100.141968\n",
      "Train Epoch: 62 [43520/60000 (72%)]\tLoss: 93.069641\n",
      "Train Epoch: 62 [44800/60000 (75%)]\tLoss: 97.065765\n",
      "Train Epoch: 62 [46080/60000 (77%)]\tLoss: 96.059265\n",
      "Train Epoch: 62 [47360/60000 (79%)]\tLoss: 99.703583\n",
      "Train Epoch: 62 [48640/60000 (81%)]\tLoss: 96.927979\n",
      "Train Epoch: 62 [49920/60000 (83%)]\tLoss: 99.450439\n",
      "Train Epoch: 62 [51200/60000 (85%)]\tLoss: 94.998901\n",
      "Train Epoch: 62 [52480/60000 (87%)]\tLoss: 98.760185\n",
      "Train Epoch: 62 [53760/60000 (90%)]\tLoss: 93.756744\n",
      "Train Epoch: 62 [55040/60000 (92%)]\tLoss: 93.368774\n",
      "Train Epoch: 62 [56320/60000 (94%)]\tLoss: 100.212677\n",
      "Train Epoch: 62 [57600/60000 (96%)]\tLoss: 97.996979\n",
      "Train Epoch: 62 [58880/60000 (98%)]\tLoss: 93.480370\n",
      "====> Epoch: 62 Average loss: 96.6272\n",
      "====> Test set loss: 98.7833\n",
      "Train Epoch: 63 [0/60000 (0%)]\tLoss: 95.357727\n",
      "Train Epoch: 63 [1280/60000 (2%)]\tLoss: 95.951668\n",
      "Train Epoch: 63 [2560/60000 (4%)]\tLoss: 96.474747\n",
      "Train Epoch: 63 [3840/60000 (6%)]\tLoss: 102.476791\n",
      "Train Epoch: 63 [5120/60000 (9%)]\tLoss: 96.587700\n",
      "Train Epoch: 63 [6400/60000 (11%)]\tLoss: 98.701218\n",
      "Train Epoch: 63 [7680/60000 (13%)]\tLoss: 93.516472\n",
      "Train Epoch: 63 [8960/60000 (15%)]\tLoss: 98.097572\n",
      "Train Epoch: 63 [10240/60000 (17%)]\tLoss: 95.166321\n",
      "Train Epoch: 63 [11520/60000 (19%)]\tLoss: 94.546349\n",
      "Train Epoch: 63 [12800/60000 (21%)]\tLoss: 98.680984\n",
      "Train Epoch: 63 [14080/60000 (23%)]\tLoss: 97.325638\n",
      "Train Epoch: 63 [15360/60000 (26%)]\tLoss: 96.837936\n",
      "Train Epoch: 63 [16640/60000 (28%)]\tLoss: 96.234039\n",
      "Train Epoch: 63 [17920/60000 (30%)]\tLoss: 96.282928\n",
      "Train Epoch: 63 [19200/60000 (32%)]\tLoss: 97.588684\n",
      "Train Epoch: 63 [20480/60000 (34%)]\tLoss: 97.087326\n",
      "Train Epoch: 63 [21760/60000 (36%)]\tLoss: 101.525269\n",
      "Train Epoch: 63 [23040/60000 (38%)]\tLoss: 95.978943\n",
      "Train Epoch: 63 [24320/60000 (41%)]\tLoss: 97.524460\n",
      "Train Epoch: 63 [25600/60000 (43%)]\tLoss: 94.049782\n",
      "Train Epoch: 63 [26880/60000 (45%)]\tLoss: 95.917160\n",
      "Train Epoch: 63 [28160/60000 (47%)]\tLoss: 95.072411\n",
      "Train Epoch: 63 [29440/60000 (49%)]\tLoss: 97.908760\n",
      "Train Epoch: 63 [30720/60000 (51%)]\tLoss: 97.854248\n",
      "Train Epoch: 63 [32000/60000 (53%)]\tLoss: 95.055199\n",
      "Train Epoch: 63 [33280/60000 (55%)]\tLoss: 98.705795\n",
      "Train Epoch: 63 [34560/60000 (58%)]\tLoss: 96.026939\n",
      "Train Epoch: 63 [35840/60000 (60%)]\tLoss: 100.105705\n",
      "Train Epoch: 63 [37120/60000 (62%)]\tLoss: 97.046509\n",
      "Train Epoch: 63 [38400/60000 (64%)]\tLoss: 96.495575\n",
      "Train Epoch: 63 [39680/60000 (66%)]\tLoss: 96.870956\n",
      "Train Epoch: 63 [40960/60000 (68%)]\tLoss: 99.109665\n",
      "Train Epoch: 63 [42240/60000 (70%)]\tLoss: 96.057892\n",
      "Train Epoch: 63 [43520/60000 (72%)]\tLoss: 97.539719\n",
      "Train Epoch: 63 [44800/60000 (75%)]\tLoss: 99.696426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 63 [46080/60000 (77%)]\tLoss: 94.477196\n",
      "Train Epoch: 63 [47360/60000 (79%)]\tLoss: 97.244171\n",
      "Train Epoch: 63 [48640/60000 (81%)]\tLoss: 98.422028\n",
      "Train Epoch: 63 [49920/60000 (83%)]\tLoss: 97.350197\n",
      "Train Epoch: 63 [51200/60000 (85%)]\tLoss: 97.776970\n",
      "Train Epoch: 63 [52480/60000 (87%)]\tLoss: 98.585129\n",
      "Train Epoch: 63 [53760/60000 (90%)]\tLoss: 98.324730\n",
      "Train Epoch: 63 [55040/60000 (92%)]\tLoss: 96.282417\n",
      "Train Epoch: 63 [56320/60000 (94%)]\tLoss: 97.035355\n",
      "Train Epoch: 63 [57600/60000 (96%)]\tLoss: 95.778152\n",
      "Train Epoch: 63 [58880/60000 (98%)]\tLoss: 98.080170\n",
      "====> Epoch: 63 Average loss: 96.6265\n",
      "====> Test set loss: 98.8467\n",
      "Train Epoch: 64 [0/60000 (0%)]\tLoss: 93.903732\n",
      "Train Epoch: 64 [1280/60000 (2%)]\tLoss: 96.693161\n",
      "Train Epoch: 64 [2560/60000 (4%)]\tLoss: 93.883583\n",
      "Train Epoch: 64 [3840/60000 (6%)]\tLoss: 92.654739\n",
      "Train Epoch: 64 [5120/60000 (9%)]\tLoss: 98.784241\n",
      "Train Epoch: 64 [6400/60000 (11%)]\tLoss: 94.719643\n",
      "Train Epoch: 64 [7680/60000 (13%)]\tLoss: 98.603043\n",
      "Train Epoch: 64 [8960/60000 (15%)]\tLoss: 94.951691\n",
      "Train Epoch: 64 [10240/60000 (17%)]\tLoss: 93.175018\n",
      "Train Epoch: 64 [11520/60000 (19%)]\tLoss: 97.358322\n",
      "Train Epoch: 64 [12800/60000 (21%)]\tLoss: 100.678421\n",
      "Train Epoch: 64 [14080/60000 (23%)]\tLoss: 95.689697\n",
      "Train Epoch: 64 [15360/60000 (26%)]\tLoss: 96.039238\n",
      "Train Epoch: 64 [16640/60000 (28%)]\tLoss: 97.987167\n",
      "Train Epoch: 64 [17920/60000 (30%)]\tLoss: 99.428085\n",
      "Train Epoch: 64 [19200/60000 (32%)]\tLoss: 96.655586\n",
      "Train Epoch: 64 [20480/60000 (34%)]\tLoss: 98.496185\n",
      "Train Epoch: 64 [21760/60000 (36%)]\tLoss: 97.456482\n",
      "Train Epoch: 64 [23040/60000 (38%)]\tLoss: 95.774033\n",
      "Train Epoch: 64 [24320/60000 (41%)]\tLoss: 97.749649\n",
      "Train Epoch: 64 [25600/60000 (43%)]\tLoss: 97.283951\n",
      "Train Epoch: 64 [26880/60000 (45%)]\tLoss: 96.008377\n",
      "Train Epoch: 64 [28160/60000 (47%)]\tLoss: 98.863480\n",
      "Train Epoch: 64 [29440/60000 (49%)]\tLoss: 95.997513\n",
      "Train Epoch: 64 [30720/60000 (51%)]\tLoss: 94.892990\n",
      "Train Epoch: 64 [32000/60000 (53%)]\tLoss: 97.129005\n",
      "Train Epoch: 64 [33280/60000 (55%)]\tLoss: 97.763580\n",
      "Train Epoch: 64 [34560/60000 (58%)]\tLoss: 98.229881\n",
      "Train Epoch: 64 [35840/60000 (60%)]\tLoss: 97.671906\n",
      "Train Epoch: 64 [37120/60000 (62%)]\tLoss: 98.561806\n",
      "Train Epoch: 64 [38400/60000 (64%)]\tLoss: 94.832123\n",
      "Train Epoch: 64 [39680/60000 (66%)]\tLoss: 98.169006\n",
      "Train Epoch: 64 [40960/60000 (68%)]\tLoss: 98.200020\n",
      "Train Epoch: 64 [42240/60000 (70%)]\tLoss: 95.028946\n",
      "Train Epoch: 64 [43520/60000 (72%)]\tLoss: 96.596169\n",
      "Train Epoch: 64 [44800/60000 (75%)]\tLoss: 94.595078\n",
      "Train Epoch: 64 [46080/60000 (77%)]\tLoss: 97.107513\n",
      "Train Epoch: 64 [47360/60000 (79%)]\tLoss: 91.927429\n",
      "Train Epoch: 64 [48640/60000 (81%)]\tLoss: 96.243050\n",
      "Train Epoch: 64 [49920/60000 (83%)]\tLoss: 97.263832\n",
      "Train Epoch: 64 [51200/60000 (85%)]\tLoss: 97.575417\n",
      "Train Epoch: 64 [52480/60000 (87%)]\tLoss: 97.313011\n",
      "Train Epoch: 64 [53760/60000 (90%)]\tLoss: 95.822037\n",
      "Train Epoch: 64 [55040/60000 (92%)]\tLoss: 93.804855\n",
      "Train Epoch: 64 [56320/60000 (94%)]\tLoss: 96.013474\n",
      "Train Epoch: 64 [57600/60000 (96%)]\tLoss: 97.870331\n",
      "Train Epoch: 64 [58880/60000 (98%)]\tLoss: 92.606613\n",
      "====> Epoch: 64 Average loss: 96.6319\n",
      "====> Test set loss: 98.8005\n",
      "Train Epoch: 65 [0/60000 (0%)]\tLoss: 93.828445\n",
      "Train Epoch: 65 [1280/60000 (2%)]\tLoss: 99.724220\n",
      "Train Epoch: 65 [2560/60000 (4%)]\tLoss: 91.153564\n",
      "Train Epoch: 65 [3840/60000 (6%)]\tLoss: 95.647751\n",
      "Train Epoch: 65 [5120/60000 (9%)]\tLoss: 99.434532\n",
      "Train Epoch: 65 [6400/60000 (11%)]\tLoss: 96.835182\n",
      "Train Epoch: 65 [7680/60000 (13%)]\tLoss: 97.424530\n",
      "Train Epoch: 65 [8960/60000 (15%)]\tLoss: 95.994141\n",
      "Train Epoch: 65 [10240/60000 (17%)]\tLoss: 100.705704\n",
      "Train Epoch: 65 [11520/60000 (19%)]\tLoss: 94.725494\n",
      "Train Epoch: 65 [12800/60000 (21%)]\tLoss: 93.202621\n",
      "Train Epoch: 65 [14080/60000 (23%)]\tLoss: 99.290314\n",
      "Train Epoch: 65 [15360/60000 (26%)]\tLoss: 96.287865\n",
      "Train Epoch: 65 [16640/60000 (28%)]\tLoss: 94.355614\n",
      "Train Epoch: 65 [17920/60000 (30%)]\tLoss: 97.321953\n",
      "Train Epoch: 65 [19200/60000 (32%)]\tLoss: 93.477432\n",
      "Train Epoch: 65 [20480/60000 (34%)]\tLoss: 96.601128\n",
      "Train Epoch: 65 [21760/60000 (36%)]\tLoss: 96.800888\n",
      "Train Epoch: 65 [23040/60000 (38%)]\tLoss: 93.207047\n",
      "Train Epoch: 65 [24320/60000 (41%)]\tLoss: 95.079361\n",
      "Train Epoch: 65 [25600/60000 (43%)]\tLoss: 98.233704\n",
      "Train Epoch: 65 [26880/60000 (45%)]\tLoss: 94.552521\n",
      "Train Epoch: 65 [28160/60000 (47%)]\tLoss: 96.763855\n",
      "Train Epoch: 65 [29440/60000 (49%)]\tLoss: 96.851456\n",
      "Train Epoch: 65 [30720/60000 (51%)]\tLoss: 98.503151\n",
      "Train Epoch: 65 [32000/60000 (53%)]\tLoss: 98.330826\n",
      "Train Epoch: 65 [33280/60000 (55%)]\tLoss: 98.222122\n",
      "Train Epoch: 65 [34560/60000 (58%)]\tLoss: 97.121017\n",
      "Train Epoch: 65 [35840/60000 (60%)]\tLoss: 98.289215\n",
      "Train Epoch: 65 [37120/60000 (62%)]\tLoss: 97.753098\n",
      "Train Epoch: 65 [38400/60000 (64%)]\tLoss: 97.612534\n",
      "Train Epoch: 65 [39680/60000 (66%)]\tLoss: 95.399483\n",
      "Train Epoch: 65 [40960/60000 (68%)]\tLoss: 97.873871\n",
      "Train Epoch: 65 [42240/60000 (70%)]\tLoss: 94.551956\n",
      "Train Epoch: 65 [43520/60000 (72%)]\tLoss: 96.738785\n",
      "Train Epoch: 65 [44800/60000 (75%)]\tLoss: 92.902542\n",
      "Train Epoch: 65 [46080/60000 (77%)]\tLoss: 99.853058\n",
      "Train Epoch: 65 [47360/60000 (79%)]\tLoss: 97.919250\n",
      "Train Epoch: 65 [48640/60000 (81%)]\tLoss: 97.444870\n",
      "Train Epoch: 65 [49920/60000 (83%)]\tLoss: 94.638107\n",
      "Train Epoch: 65 [51200/60000 (85%)]\tLoss: 98.829330\n",
      "Train Epoch: 65 [52480/60000 (87%)]\tLoss: 97.464508\n",
      "Train Epoch: 65 [53760/60000 (90%)]\tLoss: 98.262802\n",
      "Train Epoch: 65 [55040/60000 (92%)]\tLoss: 95.397438\n",
      "Train Epoch: 65 [56320/60000 (94%)]\tLoss: 98.500793\n",
      "Train Epoch: 65 [57600/60000 (96%)]\tLoss: 97.834648\n",
      "Train Epoch: 65 [58880/60000 (98%)]\tLoss: 95.807739\n",
      "====> Epoch: 65 Average loss: 96.6468\n",
      "====> Test set loss: 99.2935\n",
      "Train Epoch: 66 [0/60000 (0%)]\tLoss: 92.458023\n",
      "Train Epoch: 66 [1280/60000 (2%)]\tLoss: 98.140266\n",
      "Train Epoch: 66 [2560/60000 (4%)]\tLoss: 97.234001\n",
      "Train Epoch: 66 [3840/60000 (6%)]\tLoss: 94.200462\n",
      "Train Epoch: 66 [5120/60000 (9%)]\tLoss: 97.934227\n",
      "Train Epoch: 66 [6400/60000 (11%)]\tLoss: 93.646385\n",
      "Train Epoch: 66 [7680/60000 (13%)]\tLoss: 97.084633\n",
      "Train Epoch: 66 [8960/60000 (15%)]\tLoss: 97.411713\n",
      "Train Epoch: 66 [10240/60000 (17%)]\tLoss: 92.662659\n",
      "Train Epoch: 66 [11520/60000 (19%)]\tLoss: 96.843338\n",
      "Train Epoch: 66 [12800/60000 (21%)]\tLoss: 97.076584\n",
      "Train Epoch: 66 [14080/60000 (23%)]\tLoss: 94.387650\n",
      "Train Epoch: 66 [15360/60000 (26%)]\tLoss: 93.699722\n",
      "Train Epoch: 66 [16640/60000 (28%)]\tLoss: 95.458862\n",
      "Train Epoch: 66 [17920/60000 (30%)]\tLoss: 95.867950\n",
      "Train Epoch: 66 [19200/60000 (32%)]\tLoss: 96.385735\n",
      "Train Epoch: 66 [20480/60000 (34%)]\tLoss: 92.994339\n",
      "Train Epoch: 66 [21760/60000 (36%)]\tLoss: 91.999863\n",
      "Train Epoch: 66 [23040/60000 (38%)]\tLoss: 95.406906\n",
      "Train Epoch: 66 [24320/60000 (41%)]\tLoss: 97.032646\n",
      "Train Epoch: 66 [25600/60000 (43%)]\tLoss: 98.752907\n",
      "Train Epoch: 66 [26880/60000 (45%)]\tLoss: 96.139633\n",
      "Train Epoch: 66 [28160/60000 (47%)]\tLoss: 97.440147\n",
      "Train Epoch: 66 [29440/60000 (49%)]\tLoss: 97.439240\n",
      "Train Epoch: 66 [30720/60000 (51%)]\tLoss: 96.355743\n",
      "Train Epoch: 66 [32000/60000 (53%)]\tLoss: 98.684296\n",
      "Train Epoch: 66 [33280/60000 (55%)]\tLoss: 97.147850\n",
      "Train Epoch: 66 [34560/60000 (58%)]\tLoss: 100.072685\n",
      "Train Epoch: 66 [35840/60000 (60%)]\tLoss: 96.370201\n",
      "Train Epoch: 66 [37120/60000 (62%)]\tLoss: 94.265427\n",
      "Train Epoch: 66 [38400/60000 (64%)]\tLoss: 96.277382\n",
      "Train Epoch: 66 [39680/60000 (66%)]\tLoss: 97.061203\n",
      "Train Epoch: 66 [40960/60000 (68%)]\tLoss: 99.347359\n",
      "Train Epoch: 66 [42240/60000 (70%)]\tLoss: 99.142883\n",
      "Train Epoch: 66 [43520/60000 (72%)]\tLoss: 95.866104\n",
      "Train Epoch: 66 [44800/60000 (75%)]\tLoss: 96.739395\n",
      "Train Epoch: 66 [46080/60000 (77%)]\tLoss: 95.008003\n",
      "Train Epoch: 66 [47360/60000 (79%)]\tLoss: 99.843109\n",
      "Train Epoch: 66 [48640/60000 (81%)]\tLoss: 95.522163\n",
      "Train Epoch: 66 [49920/60000 (83%)]\tLoss: 96.078911\n",
      "Train Epoch: 66 [51200/60000 (85%)]\tLoss: 94.183380\n",
      "Train Epoch: 66 [52480/60000 (87%)]\tLoss: 101.210243\n",
      "Train Epoch: 66 [53760/60000 (90%)]\tLoss: 95.802353\n",
      "Train Epoch: 66 [55040/60000 (92%)]\tLoss: 99.865784\n",
      "Train Epoch: 66 [56320/60000 (94%)]\tLoss: 98.615646\n",
      "Train Epoch: 66 [57600/60000 (96%)]\tLoss: 98.919212\n",
      "Train Epoch: 66 [58880/60000 (98%)]\tLoss: 98.503906\n",
      "====> Epoch: 66 Average loss: 96.6889\n",
      "====> Test set loss: 99.0867\n",
      "Train Epoch: 67 [0/60000 (0%)]\tLoss: 99.760155\n",
      "Train Epoch: 67 [1280/60000 (2%)]\tLoss: 95.117691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 67 [2560/60000 (4%)]\tLoss: 97.116455\n",
      "Train Epoch: 67 [3840/60000 (6%)]\tLoss: 97.317383\n",
      "Train Epoch: 67 [5120/60000 (9%)]\tLoss: 92.777695\n",
      "Train Epoch: 67 [6400/60000 (11%)]\tLoss: 93.576027\n",
      "Train Epoch: 67 [7680/60000 (13%)]\tLoss: 97.117111\n",
      "Train Epoch: 67 [8960/60000 (15%)]\tLoss: 95.640671\n",
      "Train Epoch: 67 [10240/60000 (17%)]\tLoss: 96.743156\n",
      "Train Epoch: 67 [11520/60000 (19%)]\tLoss: 97.378975\n",
      "Train Epoch: 67 [12800/60000 (21%)]\tLoss: 98.980026\n",
      "Train Epoch: 67 [14080/60000 (23%)]\tLoss: 96.587822\n",
      "Train Epoch: 67 [15360/60000 (26%)]\tLoss: 93.315697\n",
      "Train Epoch: 67 [16640/60000 (28%)]\tLoss: 96.251633\n",
      "Train Epoch: 67 [17920/60000 (30%)]\tLoss: 96.590469\n",
      "Train Epoch: 67 [19200/60000 (32%)]\tLoss: 99.098763\n",
      "Train Epoch: 67 [20480/60000 (34%)]\tLoss: 94.605209\n",
      "Train Epoch: 67 [21760/60000 (36%)]\tLoss: 95.064293\n",
      "Train Epoch: 67 [23040/60000 (38%)]\tLoss: 96.834343\n",
      "Train Epoch: 67 [24320/60000 (41%)]\tLoss: 95.301590\n",
      "Train Epoch: 67 [25600/60000 (43%)]\tLoss: 98.257942\n",
      "Train Epoch: 67 [26880/60000 (45%)]\tLoss: 96.718597\n",
      "Train Epoch: 67 [28160/60000 (47%)]\tLoss: 99.197098\n",
      "Train Epoch: 67 [29440/60000 (49%)]\tLoss: 92.676392\n",
      "Train Epoch: 67 [30720/60000 (51%)]\tLoss: 96.282242\n",
      "Train Epoch: 67 [32000/60000 (53%)]\tLoss: 91.349968\n",
      "Train Epoch: 67 [33280/60000 (55%)]\tLoss: 95.058990\n",
      "Train Epoch: 67 [34560/60000 (58%)]\tLoss: 97.758636\n",
      "Train Epoch: 67 [35840/60000 (60%)]\tLoss: 95.128967\n",
      "Train Epoch: 67 [37120/60000 (62%)]\tLoss: 96.167068\n",
      "Train Epoch: 67 [38400/60000 (64%)]\tLoss: 94.592148\n",
      "Train Epoch: 67 [39680/60000 (66%)]\tLoss: 97.677124\n",
      "Train Epoch: 67 [40960/60000 (68%)]\tLoss: 97.395660\n",
      "Train Epoch: 67 [42240/60000 (70%)]\tLoss: 98.042908\n",
      "Train Epoch: 67 [43520/60000 (72%)]\tLoss: 100.168732\n",
      "Train Epoch: 67 [44800/60000 (75%)]\tLoss: 97.866455\n",
      "Train Epoch: 67 [46080/60000 (77%)]\tLoss: 98.906693\n",
      "Train Epoch: 67 [47360/60000 (79%)]\tLoss: 96.835114\n",
      "Train Epoch: 67 [48640/60000 (81%)]\tLoss: 97.205544\n",
      "Train Epoch: 67 [49920/60000 (83%)]\tLoss: 98.946930\n",
      "Train Epoch: 67 [51200/60000 (85%)]\tLoss: 96.582191\n",
      "Train Epoch: 67 [52480/60000 (87%)]\tLoss: 96.693855\n",
      "Train Epoch: 67 [53760/60000 (90%)]\tLoss: 94.730934\n",
      "Train Epoch: 67 [55040/60000 (92%)]\tLoss: 101.533356\n",
      "Train Epoch: 67 [56320/60000 (94%)]\tLoss: 97.339920\n",
      "Train Epoch: 67 [57600/60000 (96%)]\tLoss: 93.333733\n",
      "Train Epoch: 67 [58880/60000 (98%)]\tLoss: 98.437332\n",
      "====> Epoch: 67 Average loss: 96.5793\n",
      "====> Test set loss: 98.8781\n",
      "Train Epoch: 68 [0/60000 (0%)]\tLoss: 93.681122\n",
      "Train Epoch: 68 [1280/60000 (2%)]\tLoss: 94.117378\n",
      "Train Epoch: 68 [2560/60000 (4%)]\tLoss: 96.270004\n",
      "Train Epoch: 68 [3840/60000 (6%)]\tLoss: 94.265572\n",
      "Train Epoch: 68 [5120/60000 (9%)]\tLoss: 96.932144\n",
      "Train Epoch: 68 [6400/60000 (11%)]\tLoss: 93.793518\n",
      "Train Epoch: 68 [7680/60000 (13%)]\tLoss: 102.660057\n",
      "Train Epoch: 68 [8960/60000 (15%)]\tLoss: 97.740936\n",
      "Train Epoch: 68 [10240/60000 (17%)]\tLoss: 100.342346\n",
      "Train Epoch: 68 [11520/60000 (19%)]\tLoss: 91.276047\n",
      "Train Epoch: 68 [12800/60000 (21%)]\tLoss: 96.785294\n",
      "Train Epoch: 68 [14080/60000 (23%)]\tLoss: 99.615814\n",
      "Train Epoch: 68 [15360/60000 (26%)]\tLoss: 94.903870\n",
      "Train Epoch: 68 [16640/60000 (28%)]\tLoss: 98.947563\n",
      "Train Epoch: 68 [17920/60000 (30%)]\tLoss: 93.665634\n",
      "Train Epoch: 68 [19200/60000 (32%)]\tLoss: 99.783478\n",
      "Train Epoch: 68 [20480/60000 (34%)]\tLoss: 95.287361\n",
      "Train Epoch: 68 [21760/60000 (36%)]\tLoss: 93.029495\n",
      "Train Epoch: 68 [23040/60000 (38%)]\tLoss: 98.417587\n",
      "Train Epoch: 68 [24320/60000 (41%)]\tLoss: 95.020203\n",
      "Train Epoch: 68 [25600/60000 (43%)]\tLoss: 97.803612\n",
      "Train Epoch: 68 [26880/60000 (45%)]\tLoss: 95.293602\n",
      "Train Epoch: 68 [28160/60000 (47%)]\tLoss: 92.674484\n",
      "Train Epoch: 68 [29440/60000 (49%)]\tLoss: 96.978188\n",
      "Train Epoch: 68 [30720/60000 (51%)]\tLoss: 95.593834\n",
      "Train Epoch: 68 [32000/60000 (53%)]\tLoss: 96.276337\n",
      "Train Epoch: 68 [33280/60000 (55%)]\tLoss: 94.525734\n",
      "Train Epoch: 68 [34560/60000 (58%)]\tLoss: 98.509926\n",
      "Train Epoch: 68 [35840/60000 (60%)]\tLoss: 93.806595\n",
      "Train Epoch: 68 [37120/60000 (62%)]\tLoss: 96.415771\n",
      "Train Epoch: 68 [38400/60000 (64%)]\tLoss: 98.709229\n",
      "Train Epoch: 68 [39680/60000 (66%)]\tLoss: 96.757874\n",
      "Train Epoch: 68 [40960/60000 (68%)]\tLoss: 98.003189\n",
      "Train Epoch: 68 [42240/60000 (70%)]\tLoss: 98.885223\n",
      "Train Epoch: 68 [43520/60000 (72%)]\tLoss: 93.266037\n",
      "Train Epoch: 68 [44800/60000 (75%)]\tLoss: 95.362289\n",
      "Train Epoch: 68 [46080/60000 (77%)]\tLoss: 99.046570\n",
      "Train Epoch: 68 [47360/60000 (79%)]\tLoss: 103.504974\n",
      "Train Epoch: 68 [48640/60000 (81%)]\tLoss: 93.162567\n",
      "Train Epoch: 68 [49920/60000 (83%)]\tLoss: 95.815063\n",
      "Train Epoch: 68 [51200/60000 (85%)]\tLoss: 100.258446\n",
      "Train Epoch: 68 [52480/60000 (87%)]\tLoss: 99.088730\n",
      "Train Epoch: 68 [53760/60000 (90%)]\tLoss: 102.557404\n",
      "Train Epoch: 68 [55040/60000 (92%)]\tLoss: 96.964035\n",
      "Train Epoch: 68 [56320/60000 (94%)]\tLoss: 96.680580\n",
      "Train Epoch: 68 [57600/60000 (96%)]\tLoss: 95.372574\n",
      "Train Epoch: 68 [58880/60000 (98%)]\tLoss: 92.290527\n",
      "====> Epoch: 68 Average loss: 96.5844\n",
      "====> Test set loss: 98.8352\n",
      "Train Epoch: 69 [0/60000 (0%)]\tLoss: 98.094833\n",
      "Train Epoch: 69 [1280/60000 (2%)]\tLoss: 99.807678\n",
      "Train Epoch: 69 [2560/60000 (4%)]\tLoss: 97.388123\n",
      "Train Epoch: 69 [3840/60000 (6%)]\tLoss: 100.213623\n",
      "Train Epoch: 69 [5120/60000 (9%)]\tLoss: 97.033173\n",
      "Train Epoch: 69 [6400/60000 (11%)]\tLoss: 100.277267\n",
      "Train Epoch: 69 [7680/60000 (13%)]\tLoss: 94.741493\n",
      "Train Epoch: 69 [8960/60000 (15%)]\tLoss: 98.648422\n",
      "Train Epoch: 69 [10240/60000 (17%)]\tLoss: 97.364807\n",
      "Train Epoch: 69 [11520/60000 (19%)]\tLoss: 98.411072\n",
      "Train Epoch: 69 [12800/60000 (21%)]\tLoss: 99.160957\n",
      "Train Epoch: 69 [14080/60000 (23%)]\tLoss: 95.476662\n",
      "Train Epoch: 69 [15360/60000 (26%)]\tLoss: 96.407272\n",
      "Train Epoch: 69 [16640/60000 (28%)]\tLoss: 97.891312\n",
      "Train Epoch: 69 [17920/60000 (30%)]\tLoss: 97.509499\n",
      "Train Epoch: 69 [19200/60000 (32%)]\tLoss: 97.079559\n",
      "Train Epoch: 69 [20480/60000 (34%)]\tLoss: 93.124847\n",
      "Train Epoch: 69 [21760/60000 (36%)]\tLoss: 97.832954\n",
      "Train Epoch: 69 [23040/60000 (38%)]\tLoss: 94.510132\n",
      "Train Epoch: 69 [24320/60000 (41%)]\tLoss: 98.300354\n",
      "Train Epoch: 69 [25600/60000 (43%)]\tLoss: 96.495819\n",
      "Train Epoch: 69 [26880/60000 (45%)]\tLoss: 97.119797\n",
      "Train Epoch: 69 [28160/60000 (47%)]\tLoss: 97.961113\n",
      "Train Epoch: 69 [29440/60000 (49%)]\tLoss: 96.808708\n",
      "Train Epoch: 69 [30720/60000 (51%)]\tLoss: 97.623154\n",
      "Train Epoch: 69 [32000/60000 (53%)]\tLoss: 99.444679\n",
      "Train Epoch: 69 [33280/60000 (55%)]\tLoss: 98.655258\n",
      "Train Epoch: 69 [34560/60000 (58%)]\tLoss: 96.998169\n",
      "Train Epoch: 69 [35840/60000 (60%)]\tLoss: 99.153755\n",
      "Train Epoch: 69 [37120/60000 (62%)]\tLoss: 97.542229\n",
      "Train Epoch: 69 [38400/60000 (64%)]\tLoss: 97.857452\n",
      "Train Epoch: 69 [39680/60000 (66%)]\tLoss: 95.023529\n",
      "Train Epoch: 69 [40960/60000 (68%)]\tLoss: 96.981300\n",
      "Train Epoch: 69 [42240/60000 (70%)]\tLoss: 97.185349\n",
      "Train Epoch: 69 [43520/60000 (72%)]\tLoss: 97.992416\n",
      "Train Epoch: 69 [44800/60000 (75%)]\tLoss: 96.241447\n",
      "Train Epoch: 69 [46080/60000 (77%)]\tLoss: 96.791649\n",
      "Train Epoch: 69 [47360/60000 (79%)]\tLoss: 94.273926\n",
      "Train Epoch: 69 [48640/60000 (81%)]\tLoss: 96.183533\n",
      "Train Epoch: 69 [49920/60000 (83%)]\tLoss: 99.543777\n",
      "Train Epoch: 69 [51200/60000 (85%)]\tLoss: 92.854065\n",
      "Train Epoch: 69 [52480/60000 (87%)]\tLoss: 94.889832\n",
      "Train Epoch: 69 [53760/60000 (90%)]\tLoss: 95.589516\n",
      "Train Epoch: 69 [55040/60000 (92%)]\tLoss: 94.239426\n",
      "Train Epoch: 69 [56320/60000 (94%)]\tLoss: 99.046425\n",
      "Train Epoch: 69 [57600/60000 (96%)]\tLoss: 96.933342\n",
      "Train Epoch: 69 [58880/60000 (98%)]\tLoss: 94.296379\n",
      "====> Epoch: 69 Average loss: 96.5178\n",
      "====> Test set loss: 98.8426\n",
      "Train Epoch: 70 [0/60000 (0%)]\tLoss: 93.305695\n",
      "Train Epoch: 70 [1280/60000 (2%)]\tLoss: 92.357727\n",
      "Train Epoch: 70 [2560/60000 (4%)]\tLoss: 96.863358\n",
      "Train Epoch: 70 [3840/60000 (6%)]\tLoss: 96.506248\n",
      "Train Epoch: 70 [5120/60000 (9%)]\tLoss: 97.326653\n",
      "Train Epoch: 70 [6400/60000 (11%)]\tLoss: 96.657677\n",
      "Train Epoch: 70 [7680/60000 (13%)]\tLoss: 93.351562\n",
      "Train Epoch: 70 [8960/60000 (15%)]\tLoss: 92.619141\n",
      "Train Epoch: 70 [10240/60000 (17%)]\tLoss: 98.927765\n",
      "Train Epoch: 70 [11520/60000 (19%)]\tLoss: 92.548073\n",
      "Train Epoch: 70 [12800/60000 (21%)]\tLoss: 95.087875\n",
      "Train Epoch: 70 [14080/60000 (23%)]\tLoss: 96.757614\n",
      "Train Epoch: 70 [15360/60000 (26%)]\tLoss: 94.497086\n",
      "Train Epoch: 70 [16640/60000 (28%)]\tLoss: 96.223618\n",
      "Train Epoch: 70 [17920/60000 (30%)]\tLoss: 93.655869\n",
      "Train Epoch: 70 [19200/60000 (32%)]\tLoss: 98.422791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 70 [20480/60000 (34%)]\tLoss: 97.085510\n",
      "Train Epoch: 70 [21760/60000 (36%)]\tLoss: 98.784370\n",
      "Train Epoch: 70 [23040/60000 (38%)]\tLoss: 97.234940\n",
      "Train Epoch: 70 [24320/60000 (41%)]\tLoss: 99.530655\n",
      "Train Epoch: 70 [25600/60000 (43%)]\tLoss: 96.067238\n",
      "Train Epoch: 70 [26880/60000 (45%)]\tLoss: 95.551537\n",
      "Train Epoch: 70 [28160/60000 (47%)]\tLoss: 95.648285\n",
      "Train Epoch: 70 [29440/60000 (49%)]\tLoss: 94.929291\n",
      "Train Epoch: 70 [30720/60000 (51%)]\tLoss: 95.084877\n",
      "Train Epoch: 70 [32000/60000 (53%)]\tLoss: 94.707680\n",
      "Train Epoch: 70 [33280/60000 (55%)]\tLoss: 94.838852\n",
      "Train Epoch: 70 [34560/60000 (58%)]\tLoss: 101.194565\n",
      "Train Epoch: 70 [35840/60000 (60%)]\tLoss: 95.118866\n",
      "Train Epoch: 70 [37120/60000 (62%)]\tLoss: 97.715614\n",
      "Train Epoch: 70 [38400/60000 (64%)]\tLoss: 96.772530\n",
      "Train Epoch: 70 [39680/60000 (66%)]\tLoss: 97.047539\n",
      "Train Epoch: 70 [40960/60000 (68%)]\tLoss: 96.275093\n",
      "Train Epoch: 70 [42240/60000 (70%)]\tLoss: 94.217239\n",
      "Train Epoch: 70 [43520/60000 (72%)]\tLoss: 95.163956\n",
      "Train Epoch: 70 [44800/60000 (75%)]\tLoss: 97.966736\n",
      "Train Epoch: 70 [46080/60000 (77%)]\tLoss: 101.771927\n",
      "Train Epoch: 70 [47360/60000 (79%)]\tLoss: 94.718071\n",
      "Train Epoch: 70 [48640/60000 (81%)]\tLoss: 98.695709\n",
      "Train Epoch: 70 [49920/60000 (83%)]\tLoss: 98.836052\n",
      "Train Epoch: 70 [51200/60000 (85%)]\tLoss: 94.675690\n",
      "Train Epoch: 70 [52480/60000 (87%)]\tLoss: 100.672470\n",
      "Train Epoch: 70 [53760/60000 (90%)]\tLoss: 93.438034\n",
      "Train Epoch: 70 [55040/60000 (92%)]\tLoss: 94.334961\n",
      "Train Epoch: 70 [56320/60000 (94%)]\tLoss: 97.306763\n",
      "Train Epoch: 70 [57600/60000 (96%)]\tLoss: 97.579506\n",
      "Train Epoch: 70 [58880/60000 (98%)]\tLoss: 97.888306\n",
      "====> Epoch: 70 Average loss: 96.5392\n",
      "====> Test set loss: 98.9539\n",
      "Train Epoch: 71 [0/60000 (0%)]\tLoss: 100.212982\n",
      "Train Epoch: 71 [1280/60000 (2%)]\tLoss: 94.333191\n",
      "Train Epoch: 71 [2560/60000 (4%)]\tLoss: 99.280640\n",
      "Train Epoch: 71 [3840/60000 (6%)]\tLoss: 92.891518\n",
      "Train Epoch: 71 [5120/60000 (9%)]\tLoss: 99.265327\n",
      "Train Epoch: 71 [6400/60000 (11%)]\tLoss: 96.552513\n",
      "Train Epoch: 71 [7680/60000 (13%)]\tLoss: 95.692055\n",
      "Train Epoch: 71 [8960/60000 (15%)]\tLoss: 91.466843\n",
      "Train Epoch: 71 [10240/60000 (17%)]\tLoss: 98.607910\n",
      "Train Epoch: 71 [11520/60000 (19%)]\tLoss: 98.186600\n",
      "Train Epoch: 71 [12800/60000 (21%)]\tLoss: 90.790169\n",
      "Train Epoch: 71 [14080/60000 (23%)]\tLoss: 96.105270\n",
      "Train Epoch: 71 [15360/60000 (26%)]\tLoss: 98.006699\n",
      "Train Epoch: 71 [16640/60000 (28%)]\tLoss: 96.098602\n",
      "Train Epoch: 71 [17920/60000 (30%)]\tLoss: 94.386566\n",
      "Train Epoch: 71 [19200/60000 (32%)]\tLoss: 98.194443\n",
      "Train Epoch: 71 [20480/60000 (34%)]\tLoss: 99.400208\n",
      "Train Epoch: 71 [21760/60000 (36%)]\tLoss: 94.884193\n",
      "Train Epoch: 71 [23040/60000 (38%)]\tLoss: 97.037491\n",
      "Train Epoch: 71 [24320/60000 (41%)]\tLoss: 94.814316\n",
      "Train Epoch: 71 [25600/60000 (43%)]\tLoss: 96.463730\n",
      "Train Epoch: 71 [26880/60000 (45%)]\tLoss: 93.822304\n",
      "Train Epoch: 71 [28160/60000 (47%)]\tLoss: 94.452011\n",
      "Train Epoch: 71 [29440/60000 (49%)]\tLoss: 98.301674\n",
      "Train Epoch: 71 [30720/60000 (51%)]\tLoss: 97.837601\n",
      "Train Epoch: 71 [32000/60000 (53%)]\tLoss: 95.529770\n",
      "Train Epoch: 71 [33280/60000 (55%)]\tLoss: 93.786545\n",
      "Train Epoch: 71 [34560/60000 (58%)]\tLoss: 97.950447\n",
      "Train Epoch: 71 [35840/60000 (60%)]\tLoss: 95.752243\n",
      "Train Epoch: 71 [37120/60000 (62%)]\tLoss: 96.408966\n",
      "Train Epoch: 71 [38400/60000 (64%)]\tLoss: 94.950851\n",
      "Train Epoch: 71 [39680/60000 (66%)]\tLoss: 97.991440\n",
      "Train Epoch: 71 [40960/60000 (68%)]\tLoss: 92.046562\n",
      "Train Epoch: 71 [42240/60000 (70%)]\tLoss: 100.861458\n",
      "Train Epoch: 71 [43520/60000 (72%)]\tLoss: 96.305321\n",
      "Train Epoch: 71 [44800/60000 (75%)]\tLoss: 97.121460\n",
      "Train Epoch: 71 [46080/60000 (77%)]\tLoss: 98.175972\n",
      "Train Epoch: 71 [47360/60000 (79%)]\tLoss: 92.045258\n",
      "Train Epoch: 71 [48640/60000 (81%)]\tLoss: 100.840553\n",
      "Train Epoch: 71 [49920/60000 (83%)]\tLoss: 96.413544\n",
      "Train Epoch: 71 [51200/60000 (85%)]\tLoss: 99.370094\n",
      "Train Epoch: 71 [52480/60000 (87%)]\tLoss: 95.093445\n",
      "Train Epoch: 71 [53760/60000 (90%)]\tLoss: 98.330101\n",
      "Train Epoch: 71 [55040/60000 (92%)]\tLoss: 94.796265\n",
      "Train Epoch: 71 [56320/60000 (94%)]\tLoss: 100.307968\n",
      "Train Epoch: 71 [57600/60000 (96%)]\tLoss: 96.416428\n",
      "Train Epoch: 71 [58880/60000 (98%)]\tLoss: 97.611221\n",
      "====> Epoch: 71 Average loss: 96.5416\n",
      "====> Test set loss: 98.8095\n",
      "Train Epoch: 72 [0/60000 (0%)]\tLoss: 93.952026\n",
      "Train Epoch: 72 [1280/60000 (2%)]\tLoss: 92.417435\n",
      "Train Epoch: 72 [2560/60000 (4%)]\tLoss: 98.310844\n",
      "Train Epoch: 72 [3840/60000 (6%)]\tLoss: 94.840744\n",
      "Train Epoch: 72 [5120/60000 (9%)]\tLoss: 96.937790\n",
      "Train Epoch: 72 [6400/60000 (11%)]\tLoss: 97.116241\n",
      "Train Epoch: 72 [7680/60000 (13%)]\tLoss: 96.487381\n",
      "Train Epoch: 72 [8960/60000 (15%)]\tLoss: 97.047745\n",
      "Train Epoch: 72 [10240/60000 (17%)]\tLoss: 97.115723\n",
      "Train Epoch: 72 [11520/60000 (19%)]\tLoss: 96.549255\n",
      "Train Epoch: 72 [12800/60000 (21%)]\tLoss: 95.424301\n",
      "Train Epoch: 72 [14080/60000 (23%)]\tLoss: 98.809799\n",
      "Train Epoch: 72 [15360/60000 (26%)]\tLoss: 95.940216\n",
      "Train Epoch: 72 [16640/60000 (28%)]\tLoss: 98.485764\n",
      "Train Epoch: 72 [17920/60000 (30%)]\tLoss: 98.050957\n",
      "Train Epoch: 72 [19200/60000 (32%)]\tLoss: 95.689919\n",
      "Train Epoch: 72 [20480/60000 (34%)]\tLoss: 95.822517\n",
      "Train Epoch: 72 [21760/60000 (36%)]\tLoss: 94.794731\n",
      "Train Epoch: 72 [23040/60000 (38%)]\tLoss: 94.493584\n",
      "Train Epoch: 72 [24320/60000 (41%)]\tLoss: 94.287781\n",
      "Train Epoch: 72 [25600/60000 (43%)]\tLoss: 93.978149\n",
      "Train Epoch: 72 [26880/60000 (45%)]\tLoss: 97.324585\n",
      "Train Epoch: 72 [28160/60000 (47%)]\tLoss: 98.463577\n",
      "Train Epoch: 72 [29440/60000 (49%)]\tLoss: 98.289093\n",
      "Train Epoch: 72 [30720/60000 (51%)]\tLoss: 93.138443\n",
      "Train Epoch: 72 [32000/60000 (53%)]\tLoss: 99.306778\n",
      "Train Epoch: 72 [33280/60000 (55%)]\tLoss: 98.675842\n",
      "Train Epoch: 72 [34560/60000 (58%)]\tLoss: 93.606628\n",
      "Train Epoch: 72 [35840/60000 (60%)]\tLoss: 97.117233\n",
      "Train Epoch: 72 [37120/60000 (62%)]\tLoss: 93.718643\n",
      "Train Epoch: 72 [38400/60000 (64%)]\tLoss: 95.673721\n",
      "Train Epoch: 72 [39680/60000 (66%)]\tLoss: 96.431694\n",
      "Train Epoch: 72 [40960/60000 (68%)]\tLoss: 95.590698\n",
      "Train Epoch: 72 [42240/60000 (70%)]\tLoss: 96.421516\n",
      "Train Epoch: 72 [43520/60000 (72%)]\tLoss: 95.636765\n",
      "Train Epoch: 72 [44800/60000 (75%)]\tLoss: 93.919189\n",
      "Train Epoch: 72 [46080/60000 (77%)]\tLoss: 98.418236\n",
      "Train Epoch: 72 [47360/60000 (79%)]\tLoss: 97.170006\n",
      "Train Epoch: 72 [48640/60000 (81%)]\tLoss: 94.859627\n",
      "Train Epoch: 72 [49920/60000 (83%)]\tLoss: 92.631386\n",
      "Train Epoch: 72 [51200/60000 (85%)]\tLoss: 95.188034\n",
      "Train Epoch: 72 [52480/60000 (87%)]\tLoss: 95.762283\n",
      "Train Epoch: 72 [53760/60000 (90%)]\tLoss: 96.882507\n",
      "Train Epoch: 72 [55040/60000 (92%)]\tLoss: 98.473190\n",
      "Train Epoch: 72 [56320/60000 (94%)]\tLoss: 95.376511\n",
      "Train Epoch: 72 [57600/60000 (96%)]\tLoss: 95.512314\n",
      "Train Epoch: 72 [58880/60000 (98%)]\tLoss: 97.069328\n",
      "====> Epoch: 72 Average loss: 96.5155\n",
      "====> Test set loss: 98.8208\n",
      "Train Epoch: 73 [0/60000 (0%)]\tLoss: 94.259964\n",
      "Train Epoch: 73 [1280/60000 (2%)]\tLoss: 97.542519\n",
      "Train Epoch: 73 [2560/60000 (4%)]\tLoss: 96.788795\n",
      "Train Epoch: 73 [3840/60000 (6%)]\tLoss: 95.905220\n",
      "Train Epoch: 73 [5120/60000 (9%)]\tLoss: 95.824173\n",
      "Train Epoch: 73 [6400/60000 (11%)]\tLoss: 98.562889\n",
      "Train Epoch: 73 [7680/60000 (13%)]\tLoss: 96.182083\n",
      "Train Epoch: 73 [8960/60000 (15%)]\tLoss: 96.797813\n",
      "Train Epoch: 73 [10240/60000 (17%)]\tLoss: 93.011986\n",
      "Train Epoch: 73 [11520/60000 (19%)]\tLoss: 95.603989\n",
      "Train Epoch: 73 [12800/60000 (21%)]\tLoss: 100.206772\n",
      "Train Epoch: 73 [14080/60000 (23%)]\tLoss: 96.911285\n",
      "Train Epoch: 73 [15360/60000 (26%)]\tLoss: 93.068787\n",
      "Train Epoch: 73 [16640/60000 (28%)]\tLoss: 95.316353\n",
      "Train Epoch: 73 [17920/60000 (30%)]\tLoss: 98.228607\n",
      "Train Epoch: 73 [19200/60000 (32%)]\tLoss: 98.828400\n",
      "Train Epoch: 73 [20480/60000 (34%)]\tLoss: 94.566025\n",
      "Train Epoch: 73 [21760/60000 (36%)]\tLoss: 95.178963\n",
      "Train Epoch: 73 [23040/60000 (38%)]\tLoss: 97.115234\n",
      "Train Epoch: 73 [24320/60000 (41%)]\tLoss: 96.067230\n",
      "Train Epoch: 73 [25600/60000 (43%)]\tLoss: 95.603424\n",
      "Train Epoch: 73 [26880/60000 (45%)]\tLoss: 98.764145\n",
      "Train Epoch: 73 [28160/60000 (47%)]\tLoss: 92.763016\n",
      "Train Epoch: 73 [29440/60000 (49%)]\tLoss: 96.498154\n",
      "Train Epoch: 73 [30720/60000 (51%)]\tLoss: 97.680984\n",
      "Train Epoch: 73 [32000/60000 (53%)]\tLoss: 92.672844\n",
      "Train Epoch: 73 [33280/60000 (55%)]\tLoss: 94.999786\n",
      "Train Epoch: 73 [34560/60000 (58%)]\tLoss: 93.956970\n",
      "Train Epoch: 73 [35840/60000 (60%)]\tLoss: 97.775101\n",
      "Train Epoch: 73 [37120/60000 (62%)]\tLoss: 96.254562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 73 [38400/60000 (64%)]\tLoss: 95.678986\n",
      "Train Epoch: 73 [39680/60000 (66%)]\tLoss: 97.488586\n",
      "Train Epoch: 73 [40960/60000 (68%)]\tLoss: 98.572693\n",
      "Train Epoch: 73 [42240/60000 (70%)]\tLoss: 98.483078\n",
      "Train Epoch: 73 [43520/60000 (72%)]\tLoss: 96.609406\n",
      "Train Epoch: 73 [44800/60000 (75%)]\tLoss: 96.474861\n",
      "Train Epoch: 73 [46080/60000 (77%)]\tLoss: 102.264168\n",
      "Train Epoch: 73 [47360/60000 (79%)]\tLoss: 95.752121\n",
      "Train Epoch: 73 [48640/60000 (81%)]\tLoss: 94.171432\n",
      "Train Epoch: 73 [49920/60000 (83%)]\tLoss: 94.810928\n",
      "Train Epoch: 73 [51200/60000 (85%)]\tLoss: 95.954987\n",
      "Train Epoch: 73 [52480/60000 (87%)]\tLoss: 98.146851\n",
      "Train Epoch: 73 [53760/60000 (90%)]\tLoss: 97.829033\n",
      "Train Epoch: 73 [55040/60000 (92%)]\tLoss: 96.186325\n",
      "Train Epoch: 73 [56320/60000 (94%)]\tLoss: 97.723824\n",
      "Train Epoch: 73 [57600/60000 (96%)]\tLoss: 97.157516\n",
      "Train Epoch: 73 [58880/60000 (98%)]\tLoss: 96.201614\n",
      "====> Epoch: 73 Average loss: 96.4908\n",
      "====> Test set loss: 98.8461\n",
      "Train Epoch: 74 [0/60000 (0%)]\tLoss: 96.600258\n",
      "Train Epoch: 74 [1280/60000 (2%)]\tLoss: 90.311066\n",
      "Train Epoch: 74 [2560/60000 (4%)]\tLoss: 97.071182\n",
      "Train Epoch: 74 [3840/60000 (6%)]\tLoss: 94.151390\n",
      "Train Epoch: 74 [5120/60000 (9%)]\tLoss: 96.787376\n",
      "Train Epoch: 74 [6400/60000 (11%)]\tLoss: 95.557793\n",
      "Train Epoch: 74 [7680/60000 (13%)]\tLoss: 91.170952\n",
      "Train Epoch: 74 [8960/60000 (15%)]\tLoss: 95.198303\n",
      "Train Epoch: 74 [10240/60000 (17%)]\tLoss: 96.106949\n",
      "Train Epoch: 74 [11520/60000 (19%)]\tLoss: 95.008347\n",
      "Train Epoch: 74 [12800/60000 (21%)]\tLoss: 97.362701\n",
      "Train Epoch: 74 [14080/60000 (23%)]\tLoss: 98.385971\n",
      "Train Epoch: 74 [15360/60000 (26%)]\tLoss: 92.280670\n",
      "Train Epoch: 74 [16640/60000 (28%)]\tLoss: 97.452408\n",
      "Train Epoch: 74 [17920/60000 (30%)]\tLoss: 94.301926\n",
      "Train Epoch: 74 [19200/60000 (32%)]\tLoss: 95.899796\n",
      "Train Epoch: 74 [20480/60000 (34%)]\tLoss: 95.839722\n",
      "Train Epoch: 74 [21760/60000 (36%)]\tLoss: 93.180550\n",
      "Train Epoch: 74 [23040/60000 (38%)]\tLoss: 94.115280\n",
      "Train Epoch: 74 [24320/60000 (41%)]\tLoss: 99.769119\n",
      "Train Epoch: 74 [25600/60000 (43%)]\tLoss: 97.697098\n",
      "Train Epoch: 74 [26880/60000 (45%)]\tLoss: 92.454010\n",
      "Train Epoch: 74 [28160/60000 (47%)]\tLoss: 91.424133\n",
      "Train Epoch: 74 [29440/60000 (49%)]\tLoss: 96.655594\n",
      "Train Epoch: 74 [30720/60000 (51%)]\tLoss: 96.813850\n",
      "Train Epoch: 74 [32000/60000 (53%)]\tLoss: 94.826920\n",
      "Train Epoch: 74 [33280/60000 (55%)]\tLoss: 96.821945\n",
      "Train Epoch: 74 [34560/60000 (58%)]\tLoss: 98.324196\n",
      "Train Epoch: 74 [35840/60000 (60%)]\tLoss: 100.971741\n",
      "Train Epoch: 74 [37120/60000 (62%)]\tLoss: 96.917038\n",
      "Train Epoch: 74 [38400/60000 (64%)]\tLoss: 96.730659\n",
      "Train Epoch: 74 [39680/60000 (66%)]\tLoss: 95.492760\n",
      "Train Epoch: 74 [40960/60000 (68%)]\tLoss: 95.899628\n",
      "Train Epoch: 74 [42240/60000 (70%)]\tLoss: 97.914124\n",
      "Train Epoch: 74 [43520/60000 (72%)]\tLoss: 95.527771\n",
      "Train Epoch: 74 [44800/60000 (75%)]\tLoss: 99.035484\n",
      "Train Epoch: 74 [46080/60000 (77%)]\tLoss: 96.648636\n",
      "Train Epoch: 74 [47360/60000 (79%)]\tLoss: 92.351006\n",
      "Train Epoch: 74 [48640/60000 (81%)]\tLoss: 95.889679\n",
      "Train Epoch: 74 [49920/60000 (83%)]\tLoss: 97.741516\n",
      "Train Epoch: 74 [51200/60000 (85%)]\tLoss: 93.180206\n",
      "Train Epoch: 74 [52480/60000 (87%)]\tLoss: 95.816704\n",
      "Train Epoch: 74 [53760/60000 (90%)]\tLoss: 97.192085\n",
      "Train Epoch: 74 [55040/60000 (92%)]\tLoss: 95.579704\n",
      "Train Epoch: 74 [56320/60000 (94%)]\tLoss: 98.368889\n",
      "Train Epoch: 74 [57600/60000 (96%)]\tLoss: 99.886208\n",
      "Train Epoch: 74 [58880/60000 (98%)]\tLoss: 97.966400\n",
      "====> Epoch: 74 Average loss: 96.4853\n",
      "====> Test set loss: 98.9189\n",
      "Train Epoch: 75 [0/60000 (0%)]\tLoss: 95.970924\n",
      "Train Epoch: 75 [1280/60000 (2%)]\tLoss: 94.200165\n",
      "Train Epoch: 75 [2560/60000 (4%)]\tLoss: 94.669083\n",
      "Train Epoch: 75 [3840/60000 (6%)]\tLoss: 96.780396\n",
      "Train Epoch: 75 [5120/60000 (9%)]\tLoss: 95.731621\n",
      "Train Epoch: 75 [6400/60000 (11%)]\tLoss: 93.272781\n",
      "Train Epoch: 75 [7680/60000 (13%)]\tLoss: 95.979912\n",
      "Train Epoch: 75 [8960/60000 (15%)]\tLoss: 94.510864\n",
      "Train Epoch: 75 [10240/60000 (17%)]\tLoss: 98.670410\n",
      "Train Epoch: 75 [11520/60000 (19%)]\tLoss: 93.240051\n",
      "Train Epoch: 75 [12800/60000 (21%)]\tLoss: 95.389290\n",
      "Train Epoch: 75 [14080/60000 (23%)]\tLoss: 98.184532\n",
      "Train Epoch: 75 [15360/60000 (26%)]\tLoss: 97.378723\n",
      "Train Epoch: 75 [16640/60000 (28%)]\tLoss: 98.248001\n",
      "Train Epoch: 75 [17920/60000 (30%)]\tLoss: 90.511673\n",
      "Train Epoch: 75 [19200/60000 (32%)]\tLoss: 91.434326\n",
      "Train Epoch: 75 [20480/60000 (34%)]\tLoss: 95.901367\n",
      "Train Epoch: 75 [21760/60000 (36%)]\tLoss: 93.092743\n",
      "Train Epoch: 75 [23040/60000 (38%)]\tLoss: 99.226097\n",
      "Train Epoch: 75 [24320/60000 (41%)]\tLoss: 100.383224\n",
      "Train Epoch: 75 [25600/60000 (43%)]\tLoss: 93.773819\n",
      "Train Epoch: 75 [26880/60000 (45%)]\tLoss: 93.757324\n",
      "Train Epoch: 75 [28160/60000 (47%)]\tLoss: 95.976013\n",
      "Train Epoch: 75 [29440/60000 (49%)]\tLoss: 95.084579\n",
      "Train Epoch: 75 [30720/60000 (51%)]\tLoss: 97.461456\n",
      "Train Epoch: 75 [32000/60000 (53%)]\tLoss: 97.150505\n",
      "Train Epoch: 75 [33280/60000 (55%)]\tLoss: 99.237640\n",
      "Train Epoch: 75 [34560/60000 (58%)]\tLoss: 95.247269\n",
      "Train Epoch: 75 [35840/60000 (60%)]\tLoss: 96.633789\n",
      "Train Epoch: 75 [37120/60000 (62%)]\tLoss: 92.746567\n",
      "Train Epoch: 75 [38400/60000 (64%)]\tLoss: 100.983109\n",
      "Train Epoch: 75 [39680/60000 (66%)]\tLoss: 94.959885\n",
      "Train Epoch: 75 [40960/60000 (68%)]\tLoss: 97.478050\n",
      "Train Epoch: 75 [42240/60000 (70%)]\tLoss: 96.799492\n",
      "Train Epoch: 75 [43520/60000 (72%)]\tLoss: 95.924492\n",
      "Train Epoch: 75 [44800/60000 (75%)]\tLoss: 98.381165\n",
      "Train Epoch: 75 [46080/60000 (77%)]\tLoss: 97.314156\n",
      "Train Epoch: 75 [47360/60000 (79%)]\tLoss: 95.452217\n",
      "Train Epoch: 75 [48640/60000 (81%)]\tLoss: 93.302872\n",
      "Train Epoch: 75 [49920/60000 (83%)]\tLoss: 103.778488\n",
      "Train Epoch: 75 [51200/60000 (85%)]\tLoss: 97.485756\n",
      "Train Epoch: 75 [52480/60000 (87%)]\tLoss: 98.283829\n",
      "Train Epoch: 75 [53760/60000 (90%)]\tLoss: 100.729355\n",
      "Train Epoch: 75 [55040/60000 (92%)]\tLoss: 96.082199\n",
      "Train Epoch: 75 [56320/60000 (94%)]\tLoss: 95.517639\n",
      "Train Epoch: 75 [57600/60000 (96%)]\tLoss: 95.676315\n",
      "Train Epoch: 75 [58880/60000 (98%)]\tLoss: 93.858269\n",
      "====> Epoch: 75 Average loss: 96.4895\n",
      "====> Test set loss: 98.8796\n",
      "Train Epoch: 76 [0/60000 (0%)]\tLoss: 94.931190\n",
      "Train Epoch: 76 [1280/60000 (2%)]\tLoss: 97.354744\n",
      "Train Epoch: 76 [2560/60000 (4%)]\tLoss: 96.182114\n",
      "Train Epoch: 76 [3840/60000 (6%)]\tLoss: 96.049232\n",
      "Train Epoch: 76 [5120/60000 (9%)]\tLoss: 97.978271\n",
      "Train Epoch: 76 [6400/60000 (11%)]\tLoss: 95.140442\n",
      "Train Epoch: 76 [7680/60000 (13%)]\tLoss: 96.645241\n",
      "Train Epoch: 76 [8960/60000 (15%)]\tLoss: 95.237610\n",
      "Train Epoch: 76 [10240/60000 (17%)]\tLoss: 96.492859\n",
      "Train Epoch: 76 [11520/60000 (19%)]\tLoss: 100.852516\n",
      "Train Epoch: 76 [12800/60000 (21%)]\tLoss: 95.001198\n",
      "Train Epoch: 76 [14080/60000 (23%)]\tLoss: 97.648308\n",
      "Train Epoch: 76 [15360/60000 (26%)]\tLoss: 98.534988\n",
      "Train Epoch: 76 [16640/60000 (28%)]\tLoss: 98.187721\n",
      "Train Epoch: 76 [17920/60000 (30%)]\tLoss: 96.647667\n",
      "Train Epoch: 76 [19200/60000 (32%)]\tLoss: 98.867279\n",
      "Train Epoch: 76 [20480/60000 (34%)]\tLoss: 96.876389\n",
      "Train Epoch: 76 [21760/60000 (36%)]\tLoss: 98.263855\n",
      "Train Epoch: 76 [23040/60000 (38%)]\tLoss: 93.763580\n",
      "Train Epoch: 76 [24320/60000 (41%)]\tLoss: 98.533295\n",
      "Train Epoch: 76 [25600/60000 (43%)]\tLoss: 93.043243\n",
      "Train Epoch: 76 [26880/60000 (45%)]\tLoss: 93.452843\n",
      "Train Epoch: 76 [28160/60000 (47%)]\tLoss: 96.096596\n",
      "Train Epoch: 76 [29440/60000 (49%)]\tLoss: 94.129646\n",
      "Train Epoch: 76 [30720/60000 (51%)]\tLoss: 97.134972\n",
      "Train Epoch: 76 [32000/60000 (53%)]\tLoss: 97.080475\n",
      "Train Epoch: 76 [33280/60000 (55%)]\tLoss: 99.851166\n",
      "Train Epoch: 76 [34560/60000 (58%)]\tLoss: 96.331375\n",
      "Train Epoch: 76 [35840/60000 (60%)]\tLoss: 95.118263\n",
      "Train Epoch: 76 [37120/60000 (62%)]\tLoss: 95.266045\n",
      "Train Epoch: 76 [38400/60000 (64%)]\tLoss: 101.676964\n",
      "Train Epoch: 76 [39680/60000 (66%)]\tLoss: 94.740952\n",
      "Train Epoch: 76 [40960/60000 (68%)]\tLoss: 96.325714\n",
      "Train Epoch: 76 [42240/60000 (70%)]\tLoss: 95.369644\n",
      "Train Epoch: 76 [43520/60000 (72%)]\tLoss: 96.254425\n",
      "Train Epoch: 76 [44800/60000 (75%)]\tLoss: 96.576942\n",
      "Train Epoch: 76 [46080/60000 (77%)]\tLoss: 96.501015\n",
      "Train Epoch: 76 [47360/60000 (79%)]\tLoss: 94.644684\n",
      "Train Epoch: 76 [48640/60000 (81%)]\tLoss: 100.565460\n",
      "Train Epoch: 76 [49920/60000 (83%)]\tLoss: 93.687798\n",
      "Train Epoch: 76 [51200/60000 (85%)]\tLoss: 94.314072\n",
      "Train Epoch: 76 [52480/60000 (87%)]\tLoss: 98.442856\n",
      "Train Epoch: 76 [53760/60000 (90%)]\tLoss: 98.890594\n",
      "Train Epoch: 76 [55040/60000 (92%)]\tLoss: 99.109818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 76 [56320/60000 (94%)]\tLoss: 97.017288\n",
      "Train Epoch: 76 [57600/60000 (96%)]\tLoss: 102.036835\n",
      "Train Epoch: 76 [58880/60000 (98%)]\tLoss: 93.450325\n",
      "====> Epoch: 76 Average loss: 96.4342\n",
      "====> Test set loss: 98.8332\n",
      "Train Epoch: 77 [0/60000 (0%)]\tLoss: 97.221489\n",
      "Train Epoch: 77 [1280/60000 (2%)]\tLoss: 100.378166\n",
      "Train Epoch: 77 [2560/60000 (4%)]\tLoss: 96.562180\n",
      "Train Epoch: 77 [3840/60000 (6%)]\tLoss: 95.962463\n",
      "Train Epoch: 77 [5120/60000 (9%)]\tLoss: 96.870659\n",
      "Train Epoch: 77 [6400/60000 (11%)]\tLoss: 98.190323\n",
      "Train Epoch: 77 [7680/60000 (13%)]\tLoss: 94.165268\n",
      "Train Epoch: 77 [8960/60000 (15%)]\tLoss: 94.248497\n",
      "Train Epoch: 77 [10240/60000 (17%)]\tLoss: 90.151047\n",
      "Train Epoch: 77 [11520/60000 (19%)]\tLoss: 97.388107\n",
      "Train Epoch: 77 [12800/60000 (21%)]\tLoss: 100.543877\n",
      "Train Epoch: 77 [14080/60000 (23%)]\tLoss: 96.511406\n",
      "Train Epoch: 77 [15360/60000 (26%)]\tLoss: 94.430489\n",
      "Train Epoch: 77 [16640/60000 (28%)]\tLoss: 94.464005\n",
      "Train Epoch: 77 [17920/60000 (30%)]\tLoss: 93.035355\n",
      "Train Epoch: 77 [19200/60000 (32%)]\tLoss: 96.087761\n",
      "Train Epoch: 77 [20480/60000 (34%)]\tLoss: 97.681900\n",
      "Train Epoch: 77 [21760/60000 (36%)]\tLoss: 93.329834\n",
      "Train Epoch: 77 [23040/60000 (38%)]\tLoss: 97.222595\n",
      "Train Epoch: 77 [24320/60000 (41%)]\tLoss: 97.420044\n",
      "Train Epoch: 77 [25600/60000 (43%)]\tLoss: 96.513153\n",
      "Train Epoch: 77 [26880/60000 (45%)]\tLoss: 96.676010\n",
      "Train Epoch: 77 [28160/60000 (47%)]\tLoss: 96.230927\n",
      "Train Epoch: 77 [29440/60000 (49%)]\tLoss: 99.107880\n",
      "Train Epoch: 77 [30720/60000 (51%)]\tLoss: 97.751740\n",
      "Train Epoch: 77 [32000/60000 (53%)]\tLoss: 97.589890\n",
      "Train Epoch: 77 [33280/60000 (55%)]\tLoss: 96.978340\n",
      "Train Epoch: 77 [34560/60000 (58%)]\tLoss: 97.766579\n",
      "Train Epoch: 77 [35840/60000 (60%)]\tLoss: 94.804779\n",
      "Train Epoch: 77 [37120/60000 (62%)]\tLoss: 95.070251\n",
      "Train Epoch: 77 [38400/60000 (64%)]\tLoss: 98.286461\n",
      "Train Epoch: 77 [39680/60000 (66%)]\tLoss: 96.846893\n",
      "Train Epoch: 77 [40960/60000 (68%)]\tLoss: 96.905289\n",
      "Train Epoch: 77 [42240/60000 (70%)]\tLoss: 96.369370\n",
      "Train Epoch: 77 [43520/60000 (72%)]\tLoss: 96.110527\n",
      "Train Epoch: 77 [44800/60000 (75%)]\tLoss: 96.921722\n",
      "Train Epoch: 77 [46080/60000 (77%)]\tLoss: 94.635864\n",
      "Train Epoch: 77 [47360/60000 (79%)]\tLoss: 99.153641\n",
      "Train Epoch: 77 [48640/60000 (81%)]\tLoss: 96.694412\n",
      "Train Epoch: 77 [49920/60000 (83%)]\tLoss: 94.587273\n",
      "Train Epoch: 77 [51200/60000 (85%)]\tLoss: 94.856941\n",
      "Train Epoch: 77 [52480/60000 (87%)]\tLoss: 95.469208\n",
      "Train Epoch: 77 [53760/60000 (90%)]\tLoss: 95.543335\n",
      "Train Epoch: 77 [55040/60000 (92%)]\tLoss: 92.987228\n",
      "Train Epoch: 77 [56320/60000 (94%)]\tLoss: 94.681496\n",
      "Train Epoch: 77 [57600/60000 (96%)]\tLoss: 93.794876\n",
      "Train Epoch: 77 [58880/60000 (98%)]\tLoss: 96.825684\n",
      "====> Epoch: 77 Average loss: 96.4777\n",
      "====> Test set loss: 98.9986\n",
      "Train Epoch: 78 [0/60000 (0%)]\tLoss: 96.022369\n",
      "Train Epoch: 78 [1280/60000 (2%)]\tLoss: 96.107315\n",
      "Train Epoch: 78 [2560/60000 (4%)]\tLoss: 96.081184\n",
      "Train Epoch: 78 [3840/60000 (6%)]\tLoss: 92.315834\n",
      "Train Epoch: 78 [5120/60000 (9%)]\tLoss: 97.373520\n",
      "Train Epoch: 78 [6400/60000 (11%)]\tLoss: 96.669739\n",
      "Train Epoch: 78 [7680/60000 (13%)]\tLoss: 91.166344\n",
      "Train Epoch: 78 [8960/60000 (15%)]\tLoss: 93.243996\n",
      "Train Epoch: 78 [10240/60000 (17%)]\tLoss: 94.187225\n",
      "Train Epoch: 78 [11520/60000 (19%)]\tLoss: 94.475235\n",
      "Train Epoch: 78 [12800/60000 (21%)]\tLoss: 96.807152\n",
      "Train Epoch: 78 [14080/60000 (23%)]\tLoss: 96.775894\n",
      "Train Epoch: 78 [15360/60000 (26%)]\tLoss: 93.050453\n",
      "Train Epoch: 78 [16640/60000 (28%)]\tLoss: 98.873337\n",
      "Train Epoch: 78 [17920/60000 (30%)]\tLoss: 97.346405\n",
      "Train Epoch: 78 [19200/60000 (32%)]\tLoss: 96.236115\n",
      "Train Epoch: 78 [20480/60000 (34%)]\tLoss: 95.095520\n",
      "Train Epoch: 78 [21760/60000 (36%)]\tLoss: 101.050598\n",
      "Train Epoch: 78 [23040/60000 (38%)]\tLoss: 95.924057\n",
      "Train Epoch: 78 [24320/60000 (41%)]\tLoss: 93.582092\n",
      "Train Epoch: 78 [25600/60000 (43%)]\tLoss: 99.301071\n",
      "Train Epoch: 78 [26880/60000 (45%)]\tLoss: 97.055046\n",
      "Train Epoch: 78 [28160/60000 (47%)]\tLoss: 95.776749\n",
      "Train Epoch: 78 [29440/60000 (49%)]\tLoss: 101.912003\n",
      "Train Epoch: 78 [30720/60000 (51%)]\tLoss: 95.899605\n",
      "Train Epoch: 78 [32000/60000 (53%)]\tLoss: 96.555428\n",
      "Train Epoch: 78 [33280/60000 (55%)]\tLoss: 95.202103\n",
      "Train Epoch: 78 [34560/60000 (58%)]\tLoss: 99.820389\n",
      "Train Epoch: 78 [35840/60000 (60%)]\tLoss: 95.833298\n",
      "Train Epoch: 78 [37120/60000 (62%)]\tLoss: 95.564308\n",
      "Train Epoch: 78 [38400/60000 (64%)]\tLoss: 93.638672\n",
      "Train Epoch: 78 [39680/60000 (66%)]\tLoss: 97.133690\n",
      "Train Epoch: 78 [40960/60000 (68%)]\tLoss: 94.300446\n",
      "Train Epoch: 78 [42240/60000 (70%)]\tLoss: 97.698288\n",
      "Train Epoch: 78 [43520/60000 (72%)]\tLoss: 95.823364\n",
      "Train Epoch: 78 [44800/60000 (75%)]\tLoss: 98.128448\n",
      "Train Epoch: 78 [46080/60000 (77%)]\tLoss: 97.731331\n",
      "Train Epoch: 78 [47360/60000 (79%)]\tLoss: 97.238541\n",
      "Train Epoch: 78 [48640/60000 (81%)]\tLoss: 98.707413\n",
      "Train Epoch: 78 [49920/60000 (83%)]\tLoss: 92.662079\n",
      "Train Epoch: 78 [51200/60000 (85%)]\tLoss: 95.660423\n",
      "Train Epoch: 78 [52480/60000 (87%)]\tLoss: 95.899445\n",
      "Train Epoch: 78 [53760/60000 (90%)]\tLoss: 98.077446\n",
      "Train Epoch: 78 [55040/60000 (92%)]\tLoss: 93.434738\n",
      "Train Epoch: 78 [56320/60000 (94%)]\tLoss: 94.119675\n",
      "Train Epoch: 78 [57600/60000 (96%)]\tLoss: 98.542587\n",
      "Train Epoch: 78 [58880/60000 (98%)]\tLoss: 95.620926\n",
      "====> Epoch: 78 Average loss: 96.3932\n",
      "====> Test set loss: 98.7170\n",
      "Train Epoch: 79 [0/60000 (0%)]\tLoss: 99.039406\n",
      "Train Epoch: 79 [1280/60000 (2%)]\tLoss: 95.422516\n",
      "Train Epoch: 79 [2560/60000 (4%)]\tLoss: 97.131630\n",
      "Train Epoch: 79 [3840/60000 (6%)]\tLoss: 97.735275\n",
      "Train Epoch: 79 [5120/60000 (9%)]\tLoss: 100.533661\n",
      "Train Epoch: 79 [6400/60000 (11%)]\tLoss: 97.291573\n",
      "Train Epoch: 79 [7680/60000 (13%)]\tLoss: 95.275589\n",
      "Train Epoch: 79 [8960/60000 (15%)]\tLoss: 95.028595\n",
      "Train Epoch: 79 [10240/60000 (17%)]\tLoss: 94.651230\n",
      "Train Epoch: 79 [11520/60000 (19%)]\tLoss: 95.565338\n",
      "Train Epoch: 79 [12800/60000 (21%)]\tLoss: 99.072525\n",
      "Train Epoch: 79 [14080/60000 (23%)]\tLoss: 100.303947\n",
      "Train Epoch: 79 [15360/60000 (26%)]\tLoss: 98.457169\n",
      "Train Epoch: 79 [16640/60000 (28%)]\tLoss: 93.086472\n",
      "Train Epoch: 79 [17920/60000 (30%)]\tLoss: 94.660294\n",
      "Train Epoch: 79 [19200/60000 (32%)]\tLoss: 99.471062\n",
      "Train Epoch: 79 [20480/60000 (34%)]\tLoss: 93.399460\n",
      "Train Epoch: 79 [21760/60000 (36%)]\tLoss: 97.993896\n",
      "Train Epoch: 79 [23040/60000 (38%)]\tLoss: 95.838623\n",
      "Train Epoch: 79 [24320/60000 (41%)]\tLoss: 95.296677\n",
      "Train Epoch: 79 [25600/60000 (43%)]\tLoss: 93.779076\n",
      "Train Epoch: 79 [26880/60000 (45%)]\tLoss: 97.633560\n",
      "Train Epoch: 79 [28160/60000 (47%)]\tLoss: 96.064705\n",
      "Train Epoch: 79 [29440/60000 (49%)]\tLoss: 96.978668\n",
      "Train Epoch: 79 [30720/60000 (51%)]\tLoss: 96.763382\n",
      "Train Epoch: 79 [32000/60000 (53%)]\tLoss: 95.155792\n",
      "Train Epoch: 79 [33280/60000 (55%)]\tLoss: 99.363335\n",
      "Train Epoch: 79 [34560/60000 (58%)]\tLoss: 97.384026\n",
      "Train Epoch: 79 [35840/60000 (60%)]\tLoss: 96.214203\n",
      "Train Epoch: 79 [37120/60000 (62%)]\tLoss: 96.221085\n",
      "Train Epoch: 79 [38400/60000 (64%)]\tLoss: 96.878395\n",
      "Train Epoch: 79 [39680/60000 (66%)]\tLoss: 100.094612\n",
      "Train Epoch: 79 [40960/60000 (68%)]\tLoss: 95.822029\n",
      "Train Epoch: 79 [42240/60000 (70%)]\tLoss: 97.375198\n",
      "Train Epoch: 79 [43520/60000 (72%)]\tLoss: 98.005173\n",
      "Train Epoch: 79 [44800/60000 (75%)]\tLoss: 96.731041\n",
      "Train Epoch: 79 [46080/60000 (77%)]\tLoss: 94.038620\n",
      "Train Epoch: 79 [47360/60000 (79%)]\tLoss: 96.934875\n",
      "Train Epoch: 79 [48640/60000 (81%)]\tLoss: 98.352722\n",
      "Train Epoch: 79 [49920/60000 (83%)]\tLoss: 99.168175\n",
      "Train Epoch: 79 [51200/60000 (85%)]\tLoss: 95.667725\n",
      "Train Epoch: 79 [52480/60000 (87%)]\tLoss: 94.662598\n",
      "Train Epoch: 79 [53760/60000 (90%)]\tLoss: 97.687225\n",
      "Train Epoch: 79 [55040/60000 (92%)]\tLoss: 94.158417\n",
      "Train Epoch: 79 [56320/60000 (94%)]\tLoss: 95.648148\n",
      "Train Epoch: 79 [57600/60000 (96%)]\tLoss: 94.134552\n",
      "Train Epoch: 79 [58880/60000 (98%)]\tLoss: 88.801239\n",
      "====> Epoch: 79 Average loss: 96.3936\n",
      "====> Test set loss: 98.9119\n",
      "Train Epoch: 80 [0/60000 (0%)]\tLoss: 96.093430\n",
      "Train Epoch: 80 [1280/60000 (2%)]\tLoss: 89.666290\n",
      "Train Epoch: 80 [2560/60000 (4%)]\tLoss: 98.168388\n",
      "Train Epoch: 80 [3840/60000 (6%)]\tLoss: 93.634720\n",
      "Train Epoch: 80 [5120/60000 (9%)]\tLoss: 97.029541\n",
      "Train Epoch: 80 [6400/60000 (11%)]\tLoss: 95.618805\n",
      "Train Epoch: 80 [7680/60000 (13%)]\tLoss: 96.135025\n",
      "Train Epoch: 80 [8960/60000 (15%)]\tLoss: 95.743912\n",
      "Train Epoch: 80 [10240/60000 (17%)]\tLoss: 96.901161\n",
      "Train Epoch: 80 [11520/60000 (19%)]\tLoss: 98.790215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 80 [12800/60000 (21%)]\tLoss: 100.610611\n",
      "Train Epoch: 80 [14080/60000 (23%)]\tLoss: 96.487587\n",
      "Train Epoch: 80 [15360/60000 (26%)]\tLoss: 94.698990\n",
      "Train Epoch: 80 [16640/60000 (28%)]\tLoss: 95.515793\n",
      "Train Epoch: 80 [17920/60000 (30%)]\tLoss: 93.409363\n",
      "Train Epoch: 80 [19200/60000 (32%)]\tLoss: 94.967331\n",
      "Train Epoch: 80 [20480/60000 (34%)]\tLoss: 96.412102\n",
      "Train Epoch: 80 [21760/60000 (36%)]\tLoss: 96.627625\n",
      "Train Epoch: 80 [23040/60000 (38%)]\tLoss: 99.044388\n",
      "Train Epoch: 80 [24320/60000 (41%)]\tLoss: 99.038689\n",
      "Train Epoch: 80 [25600/60000 (43%)]\tLoss: 93.707886\n",
      "Train Epoch: 80 [26880/60000 (45%)]\tLoss: 93.292336\n",
      "Train Epoch: 80 [28160/60000 (47%)]\tLoss: 93.351685\n",
      "Train Epoch: 80 [29440/60000 (49%)]\tLoss: 97.075691\n",
      "Train Epoch: 80 [30720/60000 (51%)]\tLoss: 95.774017\n",
      "Train Epoch: 80 [32000/60000 (53%)]\tLoss: 98.032791\n",
      "Train Epoch: 80 [33280/60000 (55%)]\tLoss: 96.897293\n",
      "Train Epoch: 80 [34560/60000 (58%)]\tLoss: 94.216164\n",
      "Train Epoch: 80 [35840/60000 (60%)]\tLoss: 98.811722\n",
      "Train Epoch: 80 [37120/60000 (62%)]\tLoss: 93.831497\n",
      "Train Epoch: 80 [38400/60000 (64%)]\tLoss: 97.486000\n",
      "Train Epoch: 80 [39680/60000 (66%)]\tLoss: 96.632294\n",
      "Train Epoch: 80 [40960/60000 (68%)]\tLoss: 98.558113\n",
      "Train Epoch: 80 [42240/60000 (70%)]\tLoss: 97.216621\n",
      "Train Epoch: 80 [43520/60000 (72%)]\tLoss: 96.609436\n",
      "Train Epoch: 80 [44800/60000 (75%)]\tLoss: 100.901436\n",
      "Train Epoch: 80 [46080/60000 (77%)]\tLoss: 100.137131\n",
      "Train Epoch: 80 [47360/60000 (79%)]\tLoss: 94.274612\n",
      "Train Epoch: 80 [48640/60000 (81%)]\tLoss: 92.907906\n",
      "Train Epoch: 80 [49920/60000 (83%)]\tLoss: 96.965775\n",
      "Train Epoch: 80 [51200/60000 (85%)]\tLoss: 96.170456\n",
      "Train Epoch: 80 [52480/60000 (87%)]\tLoss: 99.061584\n",
      "Train Epoch: 80 [53760/60000 (90%)]\tLoss: 91.166847\n",
      "Train Epoch: 80 [55040/60000 (92%)]\tLoss: 93.225853\n",
      "Train Epoch: 80 [56320/60000 (94%)]\tLoss: 96.116829\n",
      "Train Epoch: 80 [57600/60000 (96%)]\tLoss: 97.793434\n",
      "Train Epoch: 80 [58880/60000 (98%)]\tLoss: 96.066345\n",
      "====> Epoch: 80 Average loss: 96.4509\n",
      "====> Test set loss: 99.1197\n",
      "Train Epoch: 81 [0/60000 (0%)]\tLoss: 95.137634\n",
      "Train Epoch: 81 [1280/60000 (2%)]\tLoss: 95.053665\n",
      "Train Epoch: 81 [2560/60000 (4%)]\tLoss: 99.610123\n",
      "Train Epoch: 81 [3840/60000 (6%)]\tLoss: 101.593117\n",
      "Train Epoch: 81 [5120/60000 (9%)]\tLoss: 95.793419\n",
      "Train Epoch: 81 [6400/60000 (11%)]\tLoss: 99.082397\n",
      "Train Epoch: 81 [7680/60000 (13%)]\tLoss: 94.850098\n",
      "Train Epoch: 81 [8960/60000 (15%)]\tLoss: 92.853271\n",
      "Train Epoch: 81 [10240/60000 (17%)]\tLoss: 92.725479\n",
      "Train Epoch: 81 [11520/60000 (19%)]\tLoss: 97.224571\n",
      "Train Epoch: 81 [12800/60000 (21%)]\tLoss: 94.361938\n",
      "Train Epoch: 81 [14080/60000 (23%)]\tLoss: 102.608978\n",
      "Train Epoch: 81 [15360/60000 (26%)]\tLoss: 97.572113\n",
      "Train Epoch: 81 [16640/60000 (28%)]\tLoss: 97.381264\n",
      "Train Epoch: 81 [17920/60000 (30%)]\tLoss: 96.720139\n",
      "Train Epoch: 81 [19200/60000 (32%)]\tLoss: 95.763176\n",
      "Train Epoch: 81 [20480/60000 (34%)]\tLoss: 98.165382\n",
      "Train Epoch: 81 [21760/60000 (36%)]\tLoss: 94.155540\n",
      "Train Epoch: 81 [23040/60000 (38%)]\tLoss: 96.178566\n",
      "Train Epoch: 81 [24320/60000 (41%)]\tLoss: 96.796036\n",
      "Train Epoch: 81 [25600/60000 (43%)]\tLoss: 94.675240\n",
      "Train Epoch: 81 [26880/60000 (45%)]\tLoss: 98.031723\n",
      "Train Epoch: 81 [28160/60000 (47%)]\tLoss: 99.399628\n",
      "Train Epoch: 81 [29440/60000 (49%)]\tLoss: 95.968323\n",
      "Train Epoch: 81 [30720/60000 (51%)]\tLoss: 95.560478\n",
      "Train Epoch: 81 [32000/60000 (53%)]\tLoss: 98.950630\n",
      "Train Epoch: 81 [33280/60000 (55%)]\tLoss: 95.189545\n",
      "Train Epoch: 81 [34560/60000 (58%)]\tLoss: 96.454102\n",
      "Train Epoch: 81 [35840/60000 (60%)]\tLoss: 97.074371\n",
      "Train Epoch: 81 [37120/60000 (62%)]\tLoss: 97.617317\n",
      "Train Epoch: 81 [38400/60000 (64%)]\tLoss: 96.216019\n",
      "Train Epoch: 81 [39680/60000 (66%)]\tLoss: 98.431503\n",
      "Train Epoch: 81 [40960/60000 (68%)]\tLoss: 91.409195\n",
      "Train Epoch: 81 [42240/60000 (70%)]\tLoss: 93.822479\n",
      "Train Epoch: 81 [43520/60000 (72%)]\tLoss: 96.535782\n",
      "Train Epoch: 81 [44800/60000 (75%)]\tLoss: 95.476830\n",
      "Train Epoch: 81 [46080/60000 (77%)]\tLoss: 96.861443\n",
      "Train Epoch: 81 [47360/60000 (79%)]\tLoss: 94.337563\n",
      "Train Epoch: 81 [48640/60000 (81%)]\tLoss: 97.102463\n",
      "Train Epoch: 81 [49920/60000 (83%)]\tLoss: 94.785034\n",
      "Train Epoch: 81 [51200/60000 (85%)]\tLoss: 95.328430\n",
      "Train Epoch: 81 [52480/60000 (87%)]\tLoss: 98.069031\n",
      "Train Epoch: 81 [53760/60000 (90%)]\tLoss: 97.782013\n",
      "Train Epoch: 81 [55040/60000 (92%)]\tLoss: 96.621574\n",
      "Train Epoch: 81 [56320/60000 (94%)]\tLoss: 95.783546\n",
      "Train Epoch: 81 [57600/60000 (96%)]\tLoss: 99.277664\n",
      "Train Epoch: 81 [58880/60000 (98%)]\tLoss: 97.944229\n",
      "====> Epoch: 81 Average loss: 96.3944\n",
      "====> Test set loss: 98.7764\n",
      "Train Epoch: 82 [0/60000 (0%)]\tLoss: 95.700836\n",
      "Train Epoch: 82 [1280/60000 (2%)]\tLoss: 89.142181\n",
      "Train Epoch: 82 [2560/60000 (4%)]\tLoss: 98.947678\n",
      "Train Epoch: 82 [3840/60000 (6%)]\tLoss: 96.172523\n",
      "Train Epoch: 82 [5120/60000 (9%)]\tLoss: 97.805428\n",
      "Train Epoch: 82 [6400/60000 (11%)]\tLoss: 96.948380\n",
      "Train Epoch: 82 [7680/60000 (13%)]\tLoss: 100.310577\n",
      "Train Epoch: 82 [8960/60000 (15%)]\tLoss: 93.850388\n",
      "Train Epoch: 82 [10240/60000 (17%)]\tLoss: 95.550819\n",
      "Train Epoch: 82 [11520/60000 (19%)]\tLoss: 97.903366\n",
      "Train Epoch: 82 [12800/60000 (21%)]\tLoss: 97.834351\n",
      "Train Epoch: 82 [14080/60000 (23%)]\tLoss: 92.325974\n",
      "Train Epoch: 82 [15360/60000 (26%)]\tLoss: 96.766708\n",
      "Train Epoch: 82 [16640/60000 (28%)]\tLoss: 94.662613\n",
      "Train Epoch: 82 [17920/60000 (30%)]\tLoss: 91.427139\n",
      "Train Epoch: 82 [19200/60000 (32%)]\tLoss: 96.268906\n",
      "Train Epoch: 82 [20480/60000 (34%)]\tLoss: 97.283936\n",
      "Train Epoch: 82 [21760/60000 (36%)]\tLoss: 97.174667\n",
      "Train Epoch: 82 [23040/60000 (38%)]\tLoss: 94.394730\n",
      "Train Epoch: 82 [24320/60000 (41%)]\tLoss: 97.103279\n",
      "Train Epoch: 82 [25600/60000 (43%)]\tLoss: 98.863045\n",
      "Train Epoch: 82 [26880/60000 (45%)]\tLoss: 95.803688\n",
      "Train Epoch: 82 [28160/60000 (47%)]\tLoss: 96.501015\n",
      "Train Epoch: 82 [29440/60000 (49%)]\tLoss: 95.525589\n",
      "Train Epoch: 82 [30720/60000 (51%)]\tLoss: 94.307175\n",
      "Train Epoch: 82 [32000/60000 (53%)]\tLoss: 98.107788\n",
      "Train Epoch: 82 [33280/60000 (55%)]\tLoss: 100.768166\n",
      "Train Epoch: 82 [34560/60000 (58%)]\tLoss: 96.833984\n",
      "Train Epoch: 82 [35840/60000 (60%)]\tLoss: 97.162048\n",
      "Train Epoch: 82 [37120/60000 (62%)]\tLoss: 93.518211\n",
      "Train Epoch: 82 [38400/60000 (64%)]\tLoss: 94.766640\n",
      "Train Epoch: 82 [39680/60000 (66%)]\tLoss: 96.334824\n",
      "Train Epoch: 82 [40960/60000 (68%)]\tLoss: 94.959396\n",
      "Train Epoch: 82 [42240/60000 (70%)]\tLoss: 99.813766\n",
      "Train Epoch: 82 [43520/60000 (72%)]\tLoss: 91.137604\n",
      "Train Epoch: 82 [44800/60000 (75%)]\tLoss: 96.788910\n",
      "Train Epoch: 82 [46080/60000 (77%)]\tLoss: 97.027519\n",
      "Train Epoch: 82 [47360/60000 (79%)]\tLoss: 96.255844\n",
      "Train Epoch: 82 [48640/60000 (81%)]\tLoss: 94.912254\n",
      "Train Epoch: 82 [49920/60000 (83%)]\tLoss: 94.908676\n",
      "Train Epoch: 82 [51200/60000 (85%)]\tLoss: 95.397240\n",
      "Train Epoch: 82 [52480/60000 (87%)]\tLoss: 94.351013\n",
      "Train Epoch: 82 [53760/60000 (90%)]\tLoss: 96.300919\n",
      "Train Epoch: 82 [55040/60000 (92%)]\tLoss: 97.406403\n",
      "Train Epoch: 82 [56320/60000 (94%)]\tLoss: 95.077637\n",
      "Train Epoch: 82 [57600/60000 (96%)]\tLoss: 98.636040\n",
      "Train Epoch: 82 [58880/60000 (98%)]\tLoss: 97.905281\n",
      "====> Epoch: 82 Average loss: 96.3896\n",
      "====> Test set loss: 99.0254\n",
      "Train Epoch: 83 [0/60000 (0%)]\tLoss: 94.855042\n",
      "Train Epoch: 83 [1280/60000 (2%)]\tLoss: 96.457306\n",
      "Train Epoch: 83 [2560/60000 (4%)]\tLoss: 95.166740\n",
      "Train Epoch: 83 [3840/60000 (6%)]\tLoss: 96.598663\n",
      "Train Epoch: 83 [5120/60000 (9%)]\tLoss: 95.208572\n",
      "Train Epoch: 83 [6400/60000 (11%)]\tLoss: 97.001144\n",
      "Train Epoch: 83 [7680/60000 (13%)]\tLoss: 98.040558\n",
      "Train Epoch: 83 [8960/60000 (15%)]\tLoss: 99.359375\n",
      "Train Epoch: 83 [10240/60000 (17%)]\tLoss: 100.463165\n",
      "Train Epoch: 83 [11520/60000 (19%)]\tLoss: 97.215652\n",
      "Train Epoch: 83 [12800/60000 (21%)]\tLoss: 94.182205\n",
      "Train Epoch: 83 [14080/60000 (23%)]\tLoss: 95.022507\n",
      "Train Epoch: 83 [15360/60000 (26%)]\tLoss: 96.479813\n",
      "Train Epoch: 83 [16640/60000 (28%)]\tLoss: 93.604599\n",
      "Train Epoch: 83 [17920/60000 (30%)]\tLoss: 95.908684\n",
      "Train Epoch: 83 [19200/60000 (32%)]\tLoss: 100.531319\n",
      "Train Epoch: 83 [20480/60000 (34%)]\tLoss: 95.161514\n",
      "Train Epoch: 83 [21760/60000 (36%)]\tLoss: 94.743637\n",
      "Train Epoch: 83 [23040/60000 (38%)]\tLoss: 96.514587\n",
      "Train Epoch: 83 [24320/60000 (41%)]\tLoss: 98.384583\n",
      "Train Epoch: 83 [25600/60000 (43%)]\tLoss: 96.375854\n",
      "Train Epoch: 83 [26880/60000 (45%)]\tLoss: 97.585327\n",
      "Train Epoch: 83 [28160/60000 (47%)]\tLoss: 99.516373\n",
      "Train Epoch: 83 [29440/60000 (49%)]\tLoss: 95.489456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 83 [30720/60000 (51%)]\tLoss: 99.577652\n",
      "Train Epoch: 83 [32000/60000 (53%)]\tLoss: 96.573219\n",
      "Train Epoch: 83 [33280/60000 (55%)]\tLoss: 92.536194\n",
      "Train Epoch: 83 [34560/60000 (58%)]\tLoss: 94.066803\n",
      "Train Epoch: 83 [35840/60000 (60%)]\tLoss: 100.859940\n",
      "Train Epoch: 83 [37120/60000 (62%)]\tLoss: 93.399979\n",
      "Train Epoch: 83 [38400/60000 (64%)]\tLoss: 99.776848\n",
      "Train Epoch: 83 [39680/60000 (66%)]\tLoss: 95.544876\n",
      "Train Epoch: 83 [40960/60000 (68%)]\tLoss: 98.299774\n",
      "Train Epoch: 83 [42240/60000 (70%)]\tLoss: 96.911140\n",
      "Train Epoch: 83 [43520/60000 (72%)]\tLoss: 94.551025\n",
      "Train Epoch: 83 [44800/60000 (75%)]\tLoss: 93.016914\n",
      "Train Epoch: 83 [46080/60000 (77%)]\tLoss: 99.406105\n",
      "Train Epoch: 83 [47360/60000 (79%)]\tLoss: 97.042542\n",
      "Train Epoch: 83 [48640/60000 (81%)]\tLoss: 97.053329\n",
      "Train Epoch: 83 [49920/60000 (83%)]\tLoss: 97.356163\n",
      "Train Epoch: 83 [51200/60000 (85%)]\tLoss: 96.034187\n",
      "Train Epoch: 83 [52480/60000 (87%)]\tLoss: 97.117348\n",
      "Train Epoch: 83 [53760/60000 (90%)]\tLoss: 98.476791\n",
      "Train Epoch: 83 [55040/60000 (92%)]\tLoss: 96.795341\n",
      "Train Epoch: 83 [56320/60000 (94%)]\tLoss: 96.187637\n",
      "Train Epoch: 83 [57600/60000 (96%)]\tLoss: 95.272987\n",
      "Train Epoch: 83 [58880/60000 (98%)]\tLoss: 98.231979\n",
      "====> Epoch: 83 Average loss: 96.3773\n",
      "====> Test set loss: 98.7756\n",
      "Train Epoch: 84 [0/60000 (0%)]\tLoss: 100.311478\n",
      "Train Epoch: 84 [1280/60000 (2%)]\tLoss: 96.262367\n",
      "Train Epoch: 84 [2560/60000 (4%)]\tLoss: 95.915367\n",
      "Train Epoch: 84 [3840/60000 (6%)]\tLoss: 94.892242\n",
      "Train Epoch: 84 [5120/60000 (9%)]\tLoss: 98.648895\n",
      "Train Epoch: 84 [6400/60000 (11%)]\tLoss: 96.977386\n",
      "Train Epoch: 84 [7680/60000 (13%)]\tLoss: 98.873222\n",
      "Train Epoch: 84 [8960/60000 (15%)]\tLoss: 92.214088\n",
      "Train Epoch: 84 [10240/60000 (17%)]\tLoss: 95.219315\n",
      "Train Epoch: 84 [11520/60000 (19%)]\tLoss: 96.308365\n",
      "Train Epoch: 84 [12800/60000 (21%)]\tLoss: 93.805313\n",
      "Train Epoch: 84 [14080/60000 (23%)]\tLoss: 96.662231\n",
      "Train Epoch: 84 [15360/60000 (26%)]\tLoss: 93.227264\n",
      "Train Epoch: 84 [16640/60000 (28%)]\tLoss: 94.143051\n",
      "Train Epoch: 84 [17920/60000 (30%)]\tLoss: 97.020767\n",
      "Train Epoch: 84 [19200/60000 (32%)]\tLoss: 96.465225\n",
      "Train Epoch: 84 [20480/60000 (34%)]\tLoss: 99.161697\n",
      "Train Epoch: 84 [21760/60000 (36%)]\tLoss: 97.155716\n",
      "Train Epoch: 84 [23040/60000 (38%)]\tLoss: 93.594421\n",
      "Train Epoch: 84 [24320/60000 (41%)]\tLoss: 97.726280\n",
      "Train Epoch: 84 [25600/60000 (43%)]\tLoss: 98.824669\n",
      "Train Epoch: 84 [26880/60000 (45%)]\tLoss: 97.908661\n",
      "Train Epoch: 84 [28160/60000 (47%)]\tLoss: 97.195099\n",
      "Train Epoch: 84 [29440/60000 (49%)]\tLoss: 91.798523\n",
      "Train Epoch: 84 [30720/60000 (51%)]\tLoss: 96.234009\n",
      "Train Epoch: 84 [32000/60000 (53%)]\tLoss: 97.173958\n",
      "Train Epoch: 84 [33280/60000 (55%)]\tLoss: 94.438339\n",
      "Train Epoch: 84 [34560/60000 (58%)]\tLoss: 93.037117\n",
      "Train Epoch: 84 [35840/60000 (60%)]\tLoss: 94.232018\n",
      "Train Epoch: 84 [37120/60000 (62%)]\tLoss: 95.880363\n",
      "Train Epoch: 84 [38400/60000 (64%)]\tLoss: 96.970848\n",
      "Train Epoch: 84 [39680/60000 (66%)]\tLoss: 97.950798\n",
      "Train Epoch: 84 [40960/60000 (68%)]\tLoss: 96.323242\n",
      "Train Epoch: 84 [42240/60000 (70%)]\tLoss: 96.059059\n",
      "Train Epoch: 84 [43520/60000 (72%)]\tLoss: 95.658806\n",
      "Train Epoch: 84 [44800/60000 (75%)]\tLoss: 94.407600\n",
      "Train Epoch: 84 [46080/60000 (77%)]\tLoss: 96.913071\n",
      "Train Epoch: 84 [47360/60000 (79%)]\tLoss: 95.462906\n",
      "Train Epoch: 84 [48640/60000 (81%)]\tLoss: 92.696899\n",
      "Train Epoch: 84 [49920/60000 (83%)]\tLoss: 97.071167\n",
      "Train Epoch: 84 [51200/60000 (85%)]\tLoss: 96.248528\n",
      "Train Epoch: 84 [52480/60000 (87%)]\tLoss: 94.923828\n",
      "Train Epoch: 84 [53760/60000 (90%)]\tLoss: 95.940186\n",
      "Train Epoch: 84 [55040/60000 (92%)]\tLoss: 97.741272\n",
      "Train Epoch: 84 [56320/60000 (94%)]\tLoss: 99.586639\n",
      "Train Epoch: 84 [57600/60000 (96%)]\tLoss: 100.416656\n",
      "Train Epoch: 84 [58880/60000 (98%)]\tLoss: 97.018768\n",
      "====> Epoch: 84 Average loss: 96.3846\n",
      "====> Test set loss: 98.6631\n",
      "Train Epoch: 85 [0/60000 (0%)]\tLoss: 93.394051\n",
      "Train Epoch: 85 [1280/60000 (2%)]\tLoss: 95.847534\n",
      "Train Epoch: 85 [2560/60000 (4%)]\tLoss: 96.301102\n",
      "Train Epoch: 85 [3840/60000 (6%)]\tLoss: 98.966682\n",
      "Train Epoch: 85 [5120/60000 (9%)]\tLoss: 93.575211\n",
      "Train Epoch: 85 [6400/60000 (11%)]\tLoss: 98.808151\n",
      "Train Epoch: 85 [7680/60000 (13%)]\tLoss: 99.772446\n",
      "Train Epoch: 85 [8960/60000 (15%)]\tLoss: 97.122505\n",
      "Train Epoch: 85 [10240/60000 (17%)]\tLoss: 96.816315\n",
      "Train Epoch: 85 [11520/60000 (19%)]\tLoss: 97.727463\n",
      "Train Epoch: 85 [12800/60000 (21%)]\tLoss: 92.557716\n",
      "Train Epoch: 85 [14080/60000 (23%)]\tLoss: 98.702980\n",
      "Train Epoch: 85 [15360/60000 (26%)]\tLoss: 95.592163\n",
      "Train Epoch: 85 [16640/60000 (28%)]\tLoss: 94.883194\n",
      "Train Epoch: 85 [17920/60000 (30%)]\tLoss: 96.709946\n",
      "Train Epoch: 85 [19200/60000 (32%)]\tLoss: 98.701653\n",
      "Train Epoch: 85 [20480/60000 (34%)]\tLoss: 93.079414\n",
      "Train Epoch: 85 [21760/60000 (36%)]\tLoss: 95.172302\n",
      "Train Epoch: 85 [23040/60000 (38%)]\tLoss: 97.620956\n",
      "Train Epoch: 85 [24320/60000 (41%)]\tLoss: 95.266800\n",
      "Train Epoch: 85 [25600/60000 (43%)]\tLoss: 96.900871\n",
      "Train Epoch: 85 [26880/60000 (45%)]\tLoss: 95.546844\n",
      "Train Epoch: 85 [28160/60000 (47%)]\tLoss: 93.311180\n",
      "Train Epoch: 85 [29440/60000 (49%)]\tLoss: 96.571312\n",
      "Train Epoch: 85 [30720/60000 (51%)]\tLoss: 100.180984\n",
      "Train Epoch: 85 [32000/60000 (53%)]\tLoss: 95.917946\n",
      "Train Epoch: 85 [33280/60000 (55%)]\tLoss: 98.167534\n",
      "Train Epoch: 85 [34560/60000 (58%)]\tLoss: 100.262383\n",
      "Train Epoch: 85 [35840/60000 (60%)]\tLoss: 94.357742\n",
      "Train Epoch: 85 [37120/60000 (62%)]\tLoss: 96.050705\n",
      "Train Epoch: 85 [38400/60000 (64%)]\tLoss: 96.502266\n",
      "Train Epoch: 85 [39680/60000 (66%)]\tLoss: 94.606369\n",
      "Train Epoch: 85 [40960/60000 (68%)]\tLoss: 93.848419\n",
      "Train Epoch: 85 [42240/60000 (70%)]\tLoss: 96.889015\n",
      "Train Epoch: 85 [43520/60000 (72%)]\tLoss: 93.837708\n",
      "Train Epoch: 85 [44800/60000 (75%)]\tLoss: 95.187309\n",
      "Train Epoch: 85 [46080/60000 (77%)]\tLoss: 95.501350\n",
      "Train Epoch: 85 [47360/60000 (79%)]\tLoss: 96.996918\n",
      "Train Epoch: 85 [48640/60000 (81%)]\tLoss: 99.500618\n",
      "Train Epoch: 85 [49920/60000 (83%)]\tLoss: 95.786987\n",
      "Train Epoch: 85 [51200/60000 (85%)]\tLoss: 95.425812\n",
      "Train Epoch: 85 [52480/60000 (87%)]\tLoss: 97.084457\n",
      "Train Epoch: 85 [53760/60000 (90%)]\tLoss: 96.729149\n",
      "Train Epoch: 85 [55040/60000 (92%)]\tLoss: 96.131912\n",
      "Train Epoch: 85 [56320/60000 (94%)]\tLoss: 92.878128\n",
      "Train Epoch: 85 [57600/60000 (96%)]\tLoss: 96.717705\n",
      "Train Epoch: 85 [58880/60000 (98%)]\tLoss: 100.132339\n",
      "====> Epoch: 85 Average loss: 96.2871\n",
      "====> Test set loss: 98.9326\n",
      "Train Epoch: 86 [0/60000 (0%)]\tLoss: 93.404968\n",
      "Train Epoch: 86 [1280/60000 (2%)]\tLoss: 93.971169\n",
      "Train Epoch: 86 [2560/60000 (4%)]\tLoss: 93.404457\n",
      "Train Epoch: 86 [3840/60000 (6%)]\tLoss: 98.263161\n",
      "Train Epoch: 86 [5120/60000 (9%)]\tLoss: 92.932922\n",
      "Train Epoch: 86 [6400/60000 (11%)]\tLoss: 96.486160\n",
      "Train Epoch: 86 [7680/60000 (13%)]\tLoss: 97.620613\n",
      "Train Epoch: 86 [8960/60000 (15%)]\tLoss: 94.377579\n",
      "Train Epoch: 86 [10240/60000 (17%)]\tLoss: 97.632904\n",
      "Train Epoch: 86 [11520/60000 (19%)]\tLoss: 97.110794\n",
      "Train Epoch: 86 [12800/60000 (21%)]\tLoss: 98.636642\n",
      "Train Epoch: 86 [14080/60000 (23%)]\tLoss: 97.680122\n",
      "Train Epoch: 86 [15360/60000 (26%)]\tLoss: 94.925140\n",
      "Train Epoch: 86 [16640/60000 (28%)]\tLoss: 95.650131\n",
      "Train Epoch: 86 [17920/60000 (30%)]\tLoss: 94.601196\n",
      "Train Epoch: 86 [19200/60000 (32%)]\tLoss: 95.409073\n",
      "Train Epoch: 86 [20480/60000 (34%)]\tLoss: 94.038269\n",
      "Train Epoch: 86 [21760/60000 (36%)]\tLoss: 95.053162\n",
      "Train Epoch: 86 [23040/60000 (38%)]\tLoss: 96.909119\n",
      "Train Epoch: 86 [24320/60000 (41%)]\tLoss: 98.214371\n",
      "Train Epoch: 86 [25600/60000 (43%)]\tLoss: 98.117439\n",
      "Train Epoch: 86 [26880/60000 (45%)]\tLoss: 97.778656\n",
      "Train Epoch: 86 [28160/60000 (47%)]\tLoss: 93.156540\n",
      "Train Epoch: 86 [29440/60000 (49%)]\tLoss: 95.979752\n",
      "Train Epoch: 86 [30720/60000 (51%)]\tLoss: 92.940872\n",
      "Train Epoch: 86 [32000/60000 (53%)]\tLoss: 96.336563\n",
      "Train Epoch: 86 [33280/60000 (55%)]\tLoss: 96.284134\n",
      "Train Epoch: 86 [34560/60000 (58%)]\tLoss: 96.107574\n",
      "Train Epoch: 86 [35840/60000 (60%)]\tLoss: 94.230171\n",
      "Train Epoch: 86 [37120/60000 (62%)]\tLoss: 91.870758\n",
      "Train Epoch: 86 [38400/60000 (64%)]\tLoss: 96.883148\n",
      "Train Epoch: 86 [39680/60000 (66%)]\tLoss: 96.290604\n",
      "Train Epoch: 86 [40960/60000 (68%)]\tLoss: 97.084709\n",
      "Train Epoch: 86 [42240/60000 (70%)]\tLoss: 93.909180\n",
      "Train Epoch: 86 [43520/60000 (72%)]\tLoss: 97.223633\n",
      "Train Epoch: 86 [44800/60000 (75%)]\tLoss: 99.974869\n",
      "Train Epoch: 86 [46080/60000 (77%)]\tLoss: 94.912025\n",
      "Train Epoch: 86 [47360/60000 (79%)]\tLoss: 97.419670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 86 [48640/60000 (81%)]\tLoss: 95.128746\n",
      "Train Epoch: 86 [49920/60000 (83%)]\tLoss: 96.447449\n",
      "Train Epoch: 86 [51200/60000 (85%)]\tLoss: 95.500748\n",
      "Train Epoch: 86 [52480/60000 (87%)]\tLoss: 92.096588\n",
      "Train Epoch: 86 [53760/60000 (90%)]\tLoss: 98.316811\n",
      "Train Epoch: 86 [55040/60000 (92%)]\tLoss: 95.030930\n",
      "Train Epoch: 86 [56320/60000 (94%)]\tLoss: 97.113152\n",
      "Train Epoch: 86 [57600/60000 (96%)]\tLoss: 94.479889\n",
      "Train Epoch: 86 [58880/60000 (98%)]\tLoss: 97.287621\n",
      "====> Epoch: 86 Average loss: 96.3035\n",
      "====> Test set loss: 98.6203\n",
      "Train Epoch: 87 [0/60000 (0%)]\tLoss: 96.973053\n",
      "Train Epoch: 87 [1280/60000 (2%)]\tLoss: 94.739883\n",
      "Train Epoch: 87 [2560/60000 (4%)]\tLoss: 95.569305\n",
      "Train Epoch: 87 [3840/60000 (6%)]\tLoss: 96.359955\n",
      "Train Epoch: 87 [5120/60000 (9%)]\tLoss: 95.701263\n",
      "Train Epoch: 87 [6400/60000 (11%)]\tLoss: 96.542412\n",
      "Train Epoch: 87 [7680/60000 (13%)]\tLoss: 96.293411\n",
      "Train Epoch: 87 [8960/60000 (15%)]\tLoss: 97.358902\n",
      "Train Epoch: 87 [10240/60000 (17%)]\tLoss: 99.254242\n",
      "Train Epoch: 87 [11520/60000 (19%)]\tLoss: 93.383041\n",
      "Train Epoch: 87 [12800/60000 (21%)]\tLoss: 97.429222\n",
      "Train Epoch: 87 [14080/60000 (23%)]\tLoss: 95.547195\n",
      "Train Epoch: 87 [15360/60000 (26%)]\tLoss: 99.750534\n",
      "Train Epoch: 87 [16640/60000 (28%)]\tLoss: 98.199440\n",
      "Train Epoch: 87 [17920/60000 (30%)]\tLoss: 92.270386\n",
      "Train Epoch: 87 [19200/60000 (32%)]\tLoss: 95.357895\n",
      "Train Epoch: 87 [20480/60000 (34%)]\tLoss: 96.964600\n",
      "Train Epoch: 87 [21760/60000 (36%)]\tLoss: 96.077873\n",
      "Train Epoch: 87 [23040/60000 (38%)]\tLoss: 96.608688\n",
      "Train Epoch: 87 [24320/60000 (41%)]\tLoss: 95.956322\n",
      "Train Epoch: 87 [25600/60000 (43%)]\tLoss: 98.357864\n",
      "Train Epoch: 87 [26880/60000 (45%)]\tLoss: 98.237030\n",
      "Train Epoch: 87 [28160/60000 (47%)]\tLoss: 93.755363\n",
      "Train Epoch: 87 [29440/60000 (49%)]\tLoss: 96.743378\n",
      "Train Epoch: 87 [30720/60000 (51%)]\tLoss: 96.426842\n",
      "Train Epoch: 87 [32000/60000 (53%)]\tLoss: 98.288902\n",
      "Train Epoch: 87 [33280/60000 (55%)]\tLoss: 97.350555\n",
      "Train Epoch: 87 [34560/60000 (58%)]\tLoss: 92.963211\n",
      "Train Epoch: 87 [35840/60000 (60%)]\tLoss: 98.370522\n",
      "Train Epoch: 87 [37120/60000 (62%)]\tLoss: 95.081345\n",
      "Train Epoch: 87 [38400/60000 (64%)]\tLoss: 96.808578\n",
      "Train Epoch: 87 [39680/60000 (66%)]\tLoss: 96.671921\n",
      "Train Epoch: 87 [40960/60000 (68%)]\tLoss: 94.549141\n",
      "Train Epoch: 87 [42240/60000 (70%)]\tLoss: 97.634964\n",
      "Train Epoch: 87 [43520/60000 (72%)]\tLoss: 94.776184\n",
      "Train Epoch: 87 [44800/60000 (75%)]\tLoss: 95.461197\n",
      "Train Epoch: 87 [46080/60000 (77%)]\tLoss: 99.114594\n",
      "Train Epoch: 87 [47360/60000 (79%)]\tLoss: 98.469055\n",
      "Train Epoch: 87 [48640/60000 (81%)]\tLoss: 97.450050\n",
      "Train Epoch: 87 [49920/60000 (83%)]\tLoss: 95.738892\n",
      "Train Epoch: 87 [51200/60000 (85%)]\tLoss: 97.150543\n",
      "Train Epoch: 87 [52480/60000 (87%)]\tLoss: 98.743912\n",
      "Train Epoch: 87 [53760/60000 (90%)]\tLoss: 97.614449\n",
      "Train Epoch: 87 [55040/60000 (92%)]\tLoss: 99.764870\n",
      "Train Epoch: 87 [56320/60000 (94%)]\tLoss: 98.416107\n",
      "Train Epoch: 87 [57600/60000 (96%)]\tLoss: 97.144142\n",
      "Train Epoch: 87 [58880/60000 (98%)]\tLoss: 97.626396\n",
      "====> Epoch: 87 Average loss: 96.3398\n",
      "====> Test set loss: 98.8560\n",
      "Train Epoch: 88 [0/60000 (0%)]\tLoss: 94.881538\n",
      "Train Epoch: 88 [1280/60000 (2%)]\tLoss: 95.344254\n",
      "Train Epoch: 88 [2560/60000 (4%)]\tLoss: 93.931641\n",
      "Train Epoch: 88 [3840/60000 (6%)]\tLoss: 100.916054\n",
      "Train Epoch: 88 [5120/60000 (9%)]\tLoss: 98.626961\n",
      "Train Epoch: 88 [6400/60000 (11%)]\tLoss: 97.735489\n",
      "Train Epoch: 88 [7680/60000 (13%)]\tLoss: 98.546860\n",
      "Train Epoch: 88 [8960/60000 (15%)]\tLoss: 96.238144\n",
      "Train Epoch: 88 [10240/60000 (17%)]\tLoss: 97.466202\n",
      "Train Epoch: 88 [11520/60000 (19%)]\tLoss: 98.309555\n",
      "Train Epoch: 88 [12800/60000 (21%)]\tLoss: 99.832031\n",
      "Train Epoch: 88 [14080/60000 (23%)]\tLoss: 95.656006\n",
      "Train Epoch: 88 [15360/60000 (26%)]\tLoss: 95.072647\n",
      "Train Epoch: 88 [16640/60000 (28%)]\tLoss: 93.313293\n",
      "Train Epoch: 88 [17920/60000 (30%)]\tLoss: 97.120026\n",
      "Train Epoch: 88 [19200/60000 (32%)]\tLoss: 99.165306\n",
      "Train Epoch: 88 [20480/60000 (34%)]\tLoss: 97.051842\n",
      "Train Epoch: 88 [21760/60000 (36%)]\tLoss: 96.458649\n",
      "Train Epoch: 88 [23040/60000 (38%)]\tLoss: 99.008606\n",
      "Train Epoch: 88 [24320/60000 (41%)]\tLoss: 96.911842\n",
      "Train Epoch: 88 [25600/60000 (43%)]\tLoss: 93.540718\n",
      "Train Epoch: 88 [26880/60000 (45%)]\tLoss: 97.309219\n",
      "Train Epoch: 88 [28160/60000 (47%)]\tLoss: 95.553009\n",
      "Train Epoch: 88 [29440/60000 (49%)]\tLoss: 96.617622\n",
      "Train Epoch: 88 [30720/60000 (51%)]\tLoss: 99.006180\n",
      "Train Epoch: 88 [32000/60000 (53%)]\tLoss: 99.811523\n",
      "Train Epoch: 88 [33280/60000 (55%)]\tLoss: 100.380508\n",
      "Train Epoch: 88 [34560/60000 (58%)]\tLoss: 97.383438\n",
      "Train Epoch: 88 [35840/60000 (60%)]\tLoss: 96.344566\n",
      "Train Epoch: 88 [37120/60000 (62%)]\tLoss: 99.760597\n",
      "Train Epoch: 88 [38400/60000 (64%)]\tLoss: 93.397484\n",
      "Train Epoch: 88 [39680/60000 (66%)]\tLoss: 99.522621\n",
      "Train Epoch: 88 [40960/60000 (68%)]\tLoss: 98.823944\n",
      "Train Epoch: 88 [42240/60000 (70%)]\tLoss: 100.591339\n",
      "Train Epoch: 88 [43520/60000 (72%)]\tLoss: 96.691772\n",
      "Train Epoch: 88 [44800/60000 (75%)]\tLoss: 91.043114\n",
      "Train Epoch: 88 [46080/60000 (77%)]\tLoss: 93.851959\n",
      "Train Epoch: 88 [47360/60000 (79%)]\tLoss: 95.802238\n",
      "Train Epoch: 88 [48640/60000 (81%)]\tLoss: 95.623398\n",
      "Train Epoch: 88 [49920/60000 (83%)]\tLoss: 93.657608\n",
      "Train Epoch: 88 [51200/60000 (85%)]\tLoss: 97.628937\n",
      "Train Epoch: 88 [52480/60000 (87%)]\tLoss: 98.652596\n",
      "Train Epoch: 88 [53760/60000 (90%)]\tLoss: 96.863220\n",
      "Train Epoch: 88 [55040/60000 (92%)]\tLoss: 94.660500\n",
      "Train Epoch: 88 [56320/60000 (94%)]\tLoss: 97.031113\n",
      "Train Epoch: 88 [57600/60000 (96%)]\tLoss: 96.613144\n",
      "Train Epoch: 88 [58880/60000 (98%)]\tLoss: 97.854126\n",
      "====> Epoch: 88 Average loss: 96.2673\n",
      "====> Test set loss: 98.7237\n",
      "Train Epoch: 89 [0/60000 (0%)]\tLoss: 94.683899\n",
      "Train Epoch: 89 [1280/60000 (2%)]\tLoss: 94.588577\n",
      "Train Epoch: 89 [2560/60000 (4%)]\tLoss: 94.579468\n",
      "Train Epoch: 89 [3840/60000 (6%)]\tLoss: 98.027573\n",
      "Train Epoch: 89 [5120/60000 (9%)]\tLoss: 95.231758\n",
      "Train Epoch: 89 [6400/60000 (11%)]\tLoss: 97.613388\n",
      "Train Epoch: 89 [7680/60000 (13%)]\tLoss: 93.716103\n",
      "Train Epoch: 89 [8960/60000 (15%)]\tLoss: 99.493019\n",
      "Train Epoch: 89 [10240/60000 (17%)]\tLoss: 94.807907\n",
      "Train Epoch: 89 [11520/60000 (19%)]\tLoss: 94.014244\n",
      "Train Epoch: 89 [12800/60000 (21%)]\tLoss: 93.606834\n",
      "Train Epoch: 89 [14080/60000 (23%)]\tLoss: 96.350555\n",
      "Train Epoch: 89 [15360/60000 (26%)]\tLoss: 97.029106\n",
      "Train Epoch: 89 [16640/60000 (28%)]\tLoss: 96.311356\n",
      "Train Epoch: 89 [17920/60000 (30%)]\tLoss: 92.078522\n",
      "Train Epoch: 89 [19200/60000 (32%)]\tLoss: 94.831238\n",
      "Train Epoch: 89 [20480/60000 (34%)]\tLoss: 96.775551\n",
      "Train Epoch: 89 [21760/60000 (36%)]\tLoss: 95.259293\n",
      "Train Epoch: 89 [23040/60000 (38%)]\tLoss: 102.109367\n",
      "Train Epoch: 89 [24320/60000 (41%)]\tLoss: 95.820984\n",
      "Train Epoch: 89 [25600/60000 (43%)]\tLoss: 94.239075\n",
      "Train Epoch: 89 [26880/60000 (45%)]\tLoss: 98.490616\n",
      "Train Epoch: 89 [28160/60000 (47%)]\tLoss: 98.708389\n",
      "Train Epoch: 89 [29440/60000 (49%)]\tLoss: 97.905983\n",
      "Train Epoch: 89 [30720/60000 (51%)]\tLoss: 95.681412\n",
      "Train Epoch: 89 [32000/60000 (53%)]\tLoss: 95.370682\n",
      "Train Epoch: 89 [33280/60000 (55%)]\tLoss: 97.484100\n",
      "Train Epoch: 89 [34560/60000 (58%)]\tLoss: 95.444374\n",
      "Train Epoch: 89 [35840/60000 (60%)]\tLoss: 98.433281\n",
      "Train Epoch: 89 [37120/60000 (62%)]\tLoss: 97.220612\n",
      "Train Epoch: 89 [38400/60000 (64%)]\tLoss: 96.397537\n",
      "Train Epoch: 89 [39680/60000 (66%)]\tLoss: 98.406372\n",
      "Train Epoch: 89 [40960/60000 (68%)]\tLoss: 96.417305\n",
      "Train Epoch: 89 [42240/60000 (70%)]\tLoss: 95.104813\n",
      "Train Epoch: 89 [43520/60000 (72%)]\tLoss: 96.399597\n",
      "Train Epoch: 89 [44800/60000 (75%)]\tLoss: 97.402115\n",
      "Train Epoch: 89 [46080/60000 (77%)]\tLoss: 95.566650\n",
      "Train Epoch: 89 [47360/60000 (79%)]\tLoss: 97.145447\n",
      "Train Epoch: 89 [48640/60000 (81%)]\tLoss: 94.859947\n",
      "Train Epoch: 89 [49920/60000 (83%)]\tLoss: 91.880814\n",
      "Train Epoch: 89 [51200/60000 (85%)]\tLoss: 96.993958\n",
      "Train Epoch: 89 [52480/60000 (87%)]\tLoss: 95.454338\n",
      "Train Epoch: 89 [53760/60000 (90%)]\tLoss: 93.860458\n",
      "Train Epoch: 89 [55040/60000 (92%)]\tLoss: 96.460571\n",
      "Train Epoch: 89 [56320/60000 (94%)]\tLoss: 95.849380\n",
      "Train Epoch: 89 [57600/60000 (96%)]\tLoss: 95.101059\n",
      "Train Epoch: 89 [58880/60000 (98%)]\tLoss: 95.802765\n",
      "====> Epoch: 89 Average loss: 96.2757\n",
      "====> Test set loss: 98.8197\n",
      "Train Epoch: 90 [0/60000 (0%)]\tLoss: 95.649170\n",
      "Train Epoch: 90 [1280/60000 (2%)]\tLoss: 96.803024\n",
      "Train Epoch: 90 [2560/60000 (4%)]\tLoss: 97.984894\n",
      "Train Epoch: 90 [3840/60000 (6%)]\tLoss: 95.852173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 90 [5120/60000 (9%)]\tLoss: 100.211884\n",
      "Train Epoch: 90 [6400/60000 (11%)]\tLoss: 97.275856\n",
      "Train Epoch: 90 [7680/60000 (13%)]\tLoss: 96.035660\n",
      "Train Epoch: 90 [8960/60000 (15%)]\tLoss: 95.450104\n",
      "Train Epoch: 90 [10240/60000 (17%)]\tLoss: 95.076424\n",
      "Train Epoch: 90 [11520/60000 (19%)]\tLoss: 98.231903\n",
      "Train Epoch: 90 [12800/60000 (21%)]\tLoss: 95.536079\n",
      "Train Epoch: 90 [14080/60000 (23%)]\tLoss: 94.832703\n",
      "Train Epoch: 90 [15360/60000 (26%)]\tLoss: 96.424393\n",
      "Train Epoch: 90 [16640/60000 (28%)]\tLoss: 94.099838\n",
      "Train Epoch: 90 [17920/60000 (30%)]\tLoss: 95.334549\n",
      "Train Epoch: 90 [19200/60000 (32%)]\tLoss: 94.888199\n",
      "Train Epoch: 90 [20480/60000 (34%)]\tLoss: 97.927994\n",
      "Train Epoch: 90 [21760/60000 (36%)]\tLoss: 94.612671\n",
      "Train Epoch: 90 [23040/60000 (38%)]\tLoss: 95.302780\n",
      "Train Epoch: 90 [24320/60000 (41%)]\tLoss: 96.312798\n",
      "Train Epoch: 90 [25600/60000 (43%)]\tLoss: 98.737747\n",
      "Train Epoch: 90 [26880/60000 (45%)]\tLoss: 96.682144\n",
      "Train Epoch: 90 [28160/60000 (47%)]\tLoss: 95.118515\n",
      "Train Epoch: 90 [29440/60000 (49%)]\tLoss: 94.533005\n",
      "Train Epoch: 90 [30720/60000 (51%)]\tLoss: 96.090057\n",
      "Train Epoch: 90 [32000/60000 (53%)]\tLoss: 96.978050\n",
      "Train Epoch: 90 [33280/60000 (55%)]\tLoss: 97.301170\n",
      "Train Epoch: 90 [34560/60000 (58%)]\tLoss: 100.172485\n",
      "Train Epoch: 90 [35840/60000 (60%)]\tLoss: 101.099159\n",
      "Train Epoch: 90 [37120/60000 (62%)]\tLoss: 99.601540\n",
      "Train Epoch: 90 [38400/60000 (64%)]\tLoss: 95.409492\n",
      "Train Epoch: 90 [39680/60000 (66%)]\tLoss: 96.171898\n",
      "Train Epoch: 90 [40960/60000 (68%)]\tLoss: 95.048431\n",
      "Train Epoch: 90 [42240/60000 (70%)]\tLoss: 95.475922\n",
      "Train Epoch: 90 [43520/60000 (72%)]\tLoss: 96.296448\n",
      "Train Epoch: 90 [44800/60000 (75%)]\tLoss: 96.962563\n",
      "Train Epoch: 90 [46080/60000 (77%)]\tLoss: 97.754684\n",
      "Train Epoch: 90 [47360/60000 (79%)]\tLoss: 93.307419\n",
      "Train Epoch: 90 [48640/60000 (81%)]\tLoss: 95.146347\n",
      "Train Epoch: 90 [49920/60000 (83%)]\tLoss: 96.947968\n",
      "Train Epoch: 90 [51200/60000 (85%)]\tLoss: 96.623222\n",
      "Train Epoch: 90 [52480/60000 (87%)]\tLoss: 96.361221\n",
      "Train Epoch: 90 [53760/60000 (90%)]\tLoss: 91.396347\n",
      "Train Epoch: 90 [55040/60000 (92%)]\tLoss: 95.582474\n",
      "Train Epoch: 90 [56320/60000 (94%)]\tLoss: 94.422531\n",
      "Train Epoch: 90 [57600/60000 (96%)]\tLoss: 96.770706\n",
      "Train Epoch: 90 [58880/60000 (98%)]\tLoss: 99.667358\n",
      "====> Epoch: 90 Average loss: 96.2639\n",
      "====> Test set loss: 98.8094\n",
      "Train Epoch: 91 [0/60000 (0%)]\tLoss: 96.251205\n",
      "Train Epoch: 91 [1280/60000 (2%)]\tLoss: 96.298851\n",
      "Train Epoch: 91 [2560/60000 (4%)]\tLoss: 95.945915\n",
      "Train Epoch: 91 [3840/60000 (6%)]\tLoss: 95.822906\n",
      "Train Epoch: 91 [5120/60000 (9%)]\tLoss: 96.313995\n",
      "Train Epoch: 91 [6400/60000 (11%)]\tLoss: 99.306267\n",
      "Train Epoch: 91 [7680/60000 (13%)]\tLoss: 96.817413\n",
      "Train Epoch: 91 [8960/60000 (15%)]\tLoss: 95.452469\n",
      "Train Epoch: 91 [10240/60000 (17%)]\tLoss: 100.129623\n",
      "Train Epoch: 91 [11520/60000 (19%)]\tLoss: 97.431145\n",
      "Train Epoch: 91 [12800/60000 (21%)]\tLoss: 98.438759\n",
      "Train Epoch: 91 [14080/60000 (23%)]\tLoss: 100.107628\n",
      "Train Epoch: 91 [15360/60000 (26%)]\tLoss: 95.612808\n",
      "Train Epoch: 91 [16640/60000 (28%)]\tLoss: 95.987793\n",
      "Train Epoch: 91 [17920/60000 (30%)]\tLoss: 97.311432\n",
      "Train Epoch: 91 [19200/60000 (32%)]\tLoss: 95.342628\n",
      "Train Epoch: 91 [20480/60000 (34%)]\tLoss: 99.032402\n",
      "Train Epoch: 91 [21760/60000 (36%)]\tLoss: 97.893318\n",
      "Train Epoch: 91 [23040/60000 (38%)]\tLoss: 92.504517\n",
      "Train Epoch: 91 [24320/60000 (41%)]\tLoss: 100.753586\n",
      "Train Epoch: 91 [25600/60000 (43%)]\tLoss: 98.054329\n",
      "Train Epoch: 91 [26880/60000 (45%)]\tLoss: 96.840324\n",
      "Train Epoch: 91 [28160/60000 (47%)]\tLoss: 98.783913\n",
      "Train Epoch: 91 [29440/60000 (49%)]\tLoss: 96.641998\n",
      "Train Epoch: 91 [30720/60000 (51%)]\tLoss: 95.312798\n",
      "Train Epoch: 91 [32000/60000 (53%)]\tLoss: 97.405792\n",
      "Train Epoch: 91 [33280/60000 (55%)]\tLoss: 99.581276\n",
      "Train Epoch: 91 [34560/60000 (58%)]\tLoss: 95.377945\n",
      "Train Epoch: 91 [35840/60000 (60%)]\tLoss: 98.887405\n",
      "Train Epoch: 91 [37120/60000 (62%)]\tLoss: 97.240082\n",
      "Train Epoch: 91 [38400/60000 (64%)]\tLoss: 98.778763\n",
      "Train Epoch: 91 [39680/60000 (66%)]\tLoss: 98.055176\n",
      "Train Epoch: 91 [40960/60000 (68%)]\tLoss: 97.958054\n",
      "Train Epoch: 91 [42240/60000 (70%)]\tLoss: 96.711716\n",
      "Train Epoch: 91 [43520/60000 (72%)]\tLoss: 96.140060\n",
      "Train Epoch: 91 [44800/60000 (75%)]\tLoss: 93.552734\n",
      "Train Epoch: 91 [46080/60000 (77%)]\tLoss: 95.216003\n",
      "Train Epoch: 91 [47360/60000 (79%)]\tLoss: 97.838730\n",
      "Train Epoch: 91 [48640/60000 (81%)]\tLoss: 93.885384\n",
      "Train Epoch: 91 [49920/60000 (83%)]\tLoss: 97.633324\n",
      "Train Epoch: 91 [51200/60000 (85%)]\tLoss: 95.127571\n",
      "Train Epoch: 91 [52480/60000 (87%)]\tLoss: 94.133186\n",
      "Train Epoch: 91 [53760/60000 (90%)]\tLoss: 92.796150\n",
      "Train Epoch: 91 [55040/60000 (92%)]\tLoss: 91.118347\n",
      "Train Epoch: 91 [56320/60000 (94%)]\tLoss: 99.571320\n",
      "Train Epoch: 91 [57600/60000 (96%)]\tLoss: 97.032074\n",
      "Train Epoch: 91 [58880/60000 (98%)]\tLoss: 100.213951\n",
      "====> Epoch: 91 Average loss: 96.2731\n",
      "====> Test set loss: 98.9315\n",
      "Train Epoch: 92 [0/60000 (0%)]\tLoss: 95.667824\n",
      "Train Epoch: 92 [1280/60000 (2%)]\tLoss: 95.145630\n",
      "Train Epoch: 92 [2560/60000 (4%)]\tLoss: 95.025551\n",
      "Train Epoch: 92 [3840/60000 (6%)]\tLoss: 92.518463\n",
      "Train Epoch: 92 [5120/60000 (9%)]\tLoss: 96.913559\n",
      "Train Epoch: 92 [6400/60000 (11%)]\tLoss: 94.571915\n",
      "Train Epoch: 92 [7680/60000 (13%)]\tLoss: 99.838989\n",
      "Train Epoch: 92 [8960/60000 (15%)]\tLoss: 95.104980\n",
      "Train Epoch: 92 [10240/60000 (17%)]\tLoss: 97.098129\n",
      "Train Epoch: 92 [11520/60000 (19%)]\tLoss: 95.125435\n",
      "Train Epoch: 92 [12800/60000 (21%)]\tLoss: 95.303101\n",
      "Train Epoch: 92 [14080/60000 (23%)]\tLoss: 92.715210\n",
      "Train Epoch: 92 [15360/60000 (26%)]\tLoss: 98.645363\n",
      "Train Epoch: 92 [16640/60000 (28%)]\tLoss: 99.218437\n",
      "Train Epoch: 92 [17920/60000 (30%)]\tLoss: 97.692459\n",
      "Train Epoch: 92 [19200/60000 (32%)]\tLoss: 92.779953\n",
      "Train Epoch: 92 [20480/60000 (34%)]\tLoss: 93.222954\n",
      "Train Epoch: 92 [21760/60000 (36%)]\tLoss: 94.095268\n",
      "Train Epoch: 92 [23040/60000 (38%)]\tLoss: 97.920349\n",
      "Train Epoch: 92 [24320/60000 (41%)]\tLoss: 99.467712\n",
      "Train Epoch: 92 [25600/60000 (43%)]\tLoss: 100.151192\n",
      "Train Epoch: 92 [26880/60000 (45%)]\tLoss: 95.415161\n",
      "Train Epoch: 92 [28160/60000 (47%)]\tLoss: 95.991745\n",
      "Train Epoch: 92 [29440/60000 (49%)]\tLoss: 101.972275\n",
      "Train Epoch: 92 [30720/60000 (51%)]\tLoss: 94.145386\n",
      "Train Epoch: 92 [32000/60000 (53%)]\tLoss: 93.298439\n",
      "Train Epoch: 92 [33280/60000 (55%)]\tLoss: 93.203308\n",
      "Train Epoch: 92 [34560/60000 (58%)]\tLoss: 95.478149\n",
      "Train Epoch: 92 [35840/60000 (60%)]\tLoss: 97.358536\n",
      "Train Epoch: 92 [37120/60000 (62%)]\tLoss: 95.857147\n",
      "Train Epoch: 92 [38400/60000 (64%)]\tLoss: 97.546036\n",
      "Train Epoch: 92 [39680/60000 (66%)]\tLoss: 100.953941\n",
      "Train Epoch: 92 [40960/60000 (68%)]\tLoss: 94.344681\n",
      "Train Epoch: 92 [42240/60000 (70%)]\tLoss: 94.575500\n",
      "Train Epoch: 92 [43520/60000 (72%)]\tLoss: 95.194405\n",
      "Train Epoch: 92 [44800/60000 (75%)]\tLoss: 96.897217\n",
      "Train Epoch: 92 [46080/60000 (77%)]\tLoss: 97.851776\n",
      "Train Epoch: 92 [47360/60000 (79%)]\tLoss: 95.121712\n",
      "Train Epoch: 92 [48640/60000 (81%)]\tLoss: 94.453339\n",
      "Train Epoch: 92 [49920/60000 (83%)]\tLoss: 95.682388\n",
      "Train Epoch: 92 [51200/60000 (85%)]\tLoss: 97.650330\n",
      "Train Epoch: 92 [52480/60000 (87%)]\tLoss: 97.610855\n",
      "Train Epoch: 92 [53760/60000 (90%)]\tLoss: 99.583900\n",
      "Train Epoch: 92 [55040/60000 (92%)]\tLoss: 97.979340\n",
      "Train Epoch: 92 [56320/60000 (94%)]\tLoss: 97.929733\n",
      "Train Epoch: 92 [57600/60000 (96%)]\tLoss: 94.779144\n",
      "Train Epoch: 92 [58880/60000 (98%)]\tLoss: 97.418259\n",
      "====> Epoch: 92 Average loss: 96.2574\n",
      "====> Test set loss: 98.7571\n",
      "Train Epoch: 93 [0/60000 (0%)]\tLoss: 89.293915\n",
      "Train Epoch: 93 [1280/60000 (2%)]\tLoss: 98.515739\n",
      "Train Epoch: 93 [2560/60000 (4%)]\tLoss: 95.606094\n",
      "Train Epoch: 93 [3840/60000 (6%)]\tLoss: 97.920685\n",
      "Train Epoch: 93 [5120/60000 (9%)]\tLoss: 96.115425\n",
      "Train Epoch: 93 [6400/60000 (11%)]\tLoss: 97.665451\n",
      "Train Epoch: 93 [7680/60000 (13%)]\tLoss: 94.669876\n",
      "Train Epoch: 93 [8960/60000 (15%)]\tLoss: 98.775368\n",
      "Train Epoch: 93 [10240/60000 (17%)]\tLoss: 97.809944\n",
      "Train Epoch: 93 [11520/60000 (19%)]\tLoss: 96.831421\n",
      "Train Epoch: 93 [12800/60000 (21%)]\tLoss: 96.645294\n",
      "Train Epoch: 93 [14080/60000 (23%)]\tLoss: 95.674568\n",
      "Train Epoch: 93 [15360/60000 (26%)]\tLoss: 97.298744\n",
      "Train Epoch: 93 [16640/60000 (28%)]\tLoss: 95.341324\n",
      "Train Epoch: 93 [17920/60000 (30%)]\tLoss: 95.541023\n",
      "Train Epoch: 93 [19200/60000 (32%)]\tLoss: 99.877838\n",
      "Train Epoch: 93 [20480/60000 (34%)]\tLoss: 98.478119\n",
      "Train Epoch: 93 [21760/60000 (36%)]\tLoss: 99.130920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 93 [23040/60000 (38%)]\tLoss: 96.783020\n",
      "Train Epoch: 93 [24320/60000 (41%)]\tLoss: 95.051315\n",
      "Train Epoch: 93 [25600/60000 (43%)]\tLoss: 94.888321\n",
      "Train Epoch: 93 [26880/60000 (45%)]\tLoss: 96.326126\n",
      "Train Epoch: 93 [28160/60000 (47%)]\tLoss: 95.935974\n",
      "Train Epoch: 93 [29440/60000 (49%)]\tLoss: 96.902283\n",
      "Train Epoch: 93 [30720/60000 (51%)]\tLoss: 97.202347\n",
      "Train Epoch: 93 [32000/60000 (53%)]\tLoss: 98.839081\n",
      "Train Epoch: 93 [33280/60000 (55%)]\tLoss: 95.609665\n",
      "Train Epoch: 93 [34560/60000 (58%)]\tLoss: 97.674667\n",
      "Train Epoch: 93 [35840/60000 (60%)]\tLoss: 93.768860\n",
      "Train Epoch: 93 [37120/60000 (62%)]\tLoss: 97.478523\n",
      "Train Epoch: 93 [38400/60000 (64%)]\tLoss: 97.017410\n",
      "Train Epoch: 93 [39680/60000 (66%)]\tLoss: 98.045456\n",
      "Train Epoch: 93 [40960/60000 (68%)]\tLoss: 94.047165\n",
      "Train Epoch: 93 [42240/60000 (70%)]\tLoss: 96.344101\n",
      "Train Epoch: 93 [43520/60000 (72%)]\tLoss: 97.119278\n",
      "Train Epoch: 93 [44800/60000 (75%)]\tLoss: 94.582268\n",
      "Train Epoch: 93 [46080/60000 (77%)]\tLoss: 98.828484\n",
      "Train Epoch: 93 [47360/60000 (79%)]\tLoss: 96.319160\n",
      "Train Epoch: 93 [48640/60000 (81%)]\tLoss: 94.561432\n",
      "Train Epoch: 93 [49920/60000 (83%)]\tLoss: 97.034363\n",
      "Train Epoch: 93 [51200/60000 (85%)]\tLoss: 93.261612\n",
      "Train Epoch: 93 [52480/60000 (87%)]\tLoss: 94.642319\n",
      "Train Epoch: 93 [53760/60000 (90%)]\tLoss: 98.788948\n",
      "Train Epoch: 93 [55040/60000 (92%)]\tLoss: 95.145836\n",
      "Train Epoch: 93 [56320/60000 (94%)]\tLoss: 99.289810\n",
      "Train Epoch: 93 [57600/60000 (96%)]\tLoss: 98.988464\n",
      "Train Epoch: 93 [58880/60000 (98%)]\tLoss: 97.252380\n",
      "====> Epoch: 93 Average loss: 96.2568\n",
      "====> Test set loss: 98.6096\n",
      "Train Epoch: 94 [0/60000 (0%)]\tLoss: 92.608955\n",
      "Train Epoch: 94 [1280/60000 (2%)]\tLoss: 95.754623\n",
      "Train Epoch: 94 [2560/60000 (4%)]\tLoss: 96.803795\n",
      "Train Epoch: 94 [3840/60000 (6%)]\tLoss: 93.433578\n",
      "Train Epoch: 94 [5120/60000 (9%)]\tLoss: 94.506660\n",
      "Train Epoch: 94 [6400/60000 (11%)]\tLoss: 96.993599\n",
      "Train Epoch: 94 [7680/60000 (13%)]\tLoss: 97.681122\n",
      "Train Epoch: 94 [8960/60000 (15%)]\tLoss: 94.981995\n",
      "Train Epoch: 94 [10240/60000 (17%)]\tLoss: 95.638062\n",
      "Train Epoch: 94 [11520/60000 (19%)]\tLoss: 98.666367\n",
      "Train Epoch: 94 [12800/60000 (21%)]\tLoss: 98.108452\n",
      "Train Epoch: 94 [14080/60000 (23%)]\tLoss: 94.636978\n",
      "Train Epoch: 94 [15360/60000 (26%)]\tLoss: 94.940475\n",
      "Train Epoch: 94 [16640/60000 (28%)]\tLoss: 94.488205\n",
      "Train Epoch: 94 [17920/60000 (30%)]\tLoss: 93.132233\n",
      "Train Epoch: 94 [19200/60000 (32%)]\tLoss: 95.620758\n",
      "Train Epoch: 94 [20480/60000 (34%)]\tLoss: 98.351379\n",
      "Train Epoch: 94 [21760/60000 (36%)]\tLoss: 95.500168\n",
      "Train Epoch: 94 [23040/60000 (38%)]\tLoss: 95.854126\n",
      "Train Epoch: 94 [24320/60000 (41%)]\tLoss: 90.977859\n",
      "Train Epoch: 94 [25600/60000 (43%)]\tLoss: 96.323601\n",
      "Train Epoch: 94 [26880/60000 (45%)]\tLoss: 97.451874\n",
      "Train Epoch: 94 [28160/60000 (47%)]\tLoss: 96.797020\n",
      "Train Epoch: 94 [29440/60000 (49%)]\tLoss: 95.175720\n",
      "Train Epoch: 94 [30720/60000 (51%)]\tLoss: 95.676010\n",
      "Train Epoch: 94 [32000/60000 (53%)]\tLoss: 94.812836\n",
      "Train Epoch: 94 [33280/60000 (55%)]\tLoss: 93.992439\n",
      "Train Epoch: 94 [34560/60000 (58%)]\tLoss: 93.950745\n",
      "Train Epoch: 94 [35840/60000 (60%)]\tLoss: 96.042336\n",
      "Train Epoch: 94 [37120/60000 (62%)]\tLoss: 91.410797\n",
      "Train Epoch: 94 [38400/60000 (64%)]\tLoss: 96.327209\n",
      "Train Epoch: 94 [39680/60000 (66%)]\tLoss: 94.555145\n",
      "Train Epoch: 94 [40960/60000 (68%)]\tLoss: 94.967125\n",
      "Train Epoch: 94 [42240/60000 (70%)]\tLoss: 97.529465\n",
      "Train Epoch: 94 [43520/60000 (72%)]\tLoss: 95.979515\n",
      "Train Epoch: 94 [44800/60000 (75%)]\tLoss: 99.868271\n",
      "Train Epoch: 94 [46080/60000 (77%)]\tLoss: 96.393463\n",
      "Train Epoch: 94 [47360/60000 (79%)]\tLoss: 95.954651\n",
      "Train Epoch: 94 [48640/60000 (81%)]\tLoss: 95.542313\n",
      "Train Epoch: 94 [49920/60000 (83%)]\tLoss: 97.140869\n",
      "Train Epoch: 94 [51200/60000 (85%)]\tLoss: 96.530289\n",
      "Train Epoch: 94 [52480/60000 (87%)]\tLoss: 99.519165\n",
      "Train Epoch: 94 [53760/60000 (90%)]\tLoss: 96.786522\n",
      "Train Epoch: 94 [55040/60000 (92%)]\tLoss: 96.093414\n",
      "Train Epoch: 94 [56320/60000 (94%)]\tLoss: 97.306503\n",
      "Train Epoch: 94 [57600/60000 (96%)]\tLoss: 97.273010\n",
      "Train Epoch: 94 [58880/60000 (98%)]\tLoss: 100.397461\n",
      "====> Epoch: 94 Average loss: 96.2510\n",
      "====> Test set loss: 98.9084\n",
      "Train Epoch: 95 [0/60000 (0%)]\tLoss: 92.398643\n",
      "Train Epoch: 95 [1280/60000 (2%)]\tLoss: 99.497910\n",
      "Train Epoch: 95 [2560/60000 (4%)]\tLoss: 94.080711\n",
      "Train Epoch: 95 [3840/60000 (6%)]\tLoss: 95.769211\n",
      "Train Epoch: 95 [5120/60000 (9%)]\tLoss: 95.286522\n",
      "Train Epoch: 95 [6400/60000 (11%)]\tLoss: 94.712250\n",
      "Train Epoch: 95 [7680/60000 (13%)]\tLoss: 94.895584\n",
      "Train Epoch: 95 [8960/60000 (15%)]\tLoss: 98.214493\n",
      "Train Epoch: 95 [10240/60000 (17%)]\tLoss: 94.653900\n",
      "Train Epoch: 95 [11520/60000 (19%)]\tLoss: 90.924568\n",
      "Train Epoch: 95 [12800/60000 (21%)]\tLoss: 93.340057\n",
      "Train Epoch: 95 [14080/60000 (23%)]\tLoss: 95.681099\n",
      "Train Epoch: 95 [15360/60000 (26%)]\tLoss: 95.110260\n",
      "Train Epoch: 95 [16640/60000 (28%)]\tLoss: 99.916252\n",
      "Train Epoch: 95 [17920/60000 (30%)]\tLoss: 99.469971\n",
      "Train Epoch: 95 [19200/60000 (32%)]\tLoss: 95.499313\n",
      "Train Epoch: 95 [20480/60000 (34%)]\tLoss: 95.338387\n",
      "Train Epoch: 95 [21760/60000 (36%)]\tLoss: 93.949516\n",
      "Train Epoch: 95 [23040/60000 (38%)]\tLoss: 97.814453\n",
      "Train Epoch: 95 [24320/60000 (41%)]\tLoss: 91.446838\n",
      "Train Epoch: 95 [25600/60000 (43%)]\tLoss: 98.467133\n",
      "Train Epoch: 95 [26880/60000 (45%)]\tLoss: 96.430389\n",
      "Train Epoch: 95 [28160/60000 (47%)]\tLoss: 97.393051\n",
      "Train Epoch: 95 [29440/60000 (49%)]\tLoss: 99.397339\n",
      "Train Epoch: 95 [30720/60000 (51%)]\tLoss: 96.840736\n",
      "Train Epoch: 95 [32000/60000 (53%)]\tLoss: 97.349281\n",
      "Train Epoch: 95 [33280/60000 (55%)]\tLoss: 97.900894\n",
      "Train Epoch: 95 [34560/60000 (58%)]\tLoss: 95.429520\n",
      "Train Epoch: 95 [35840/60000 (60%)]\tLoss: 98.358093\n",
      "Train Epoch: 95 [37120/60000 (62%)]\tLoss: 97.746384\n",
      "Train Epoch: 95 [38400/60000 (64%)]\tLoss: 97.945595\n",
      "Train Epoch: 95 [39680/60000 (66%)]\tLoss: 98.230736\n",
      "Train Epoch: 95 [40960/60000 (68%)]\tLoss: 93.822495\n",
      "Train Epoch: 95 [42240/60000 (70%)]\tLoss: 99.062653\n",
      "Train Epoch: 95 [43520/60000 (72%)]\tLoss: 95.890839\n",
      "Train Epoch: 95 [44800/60000 (75%)]\tLoss: 100.416397\n",
      "Train Epoch: 95 [46080/60000 (77%)]\tLoss: 96.533508\n",
      "Train Epoch: 95 [47360/60000 (79%)]\tLoss: 98.963402\n",
      "Train Epoch: 95 [48640/60000 (81%)]\tLoss: 97.487411\n",
      "Train Epoch: 95 [49920/60000 (83%)]\tLoss: 100.085609\n",
      "Train Epoch: 95 [51200/60000 (85%)]\tLoss: 94.799080\n",
      "Train Epoch: 95 [52480/60000 (87%)]\tLoss: 95.559296\n",
      "Train Epoch: 95 [53760/60000 (90%)]\tLoss: 97.966553\n",
      "Train Epoch: 95 [55040/60000 (92%)]\tLoss: 95.757408\n",
      "Train Epoch: 95 [56320/60000 (94%)]\tLoss: 93.696861\n",
      "Train Epoch: 95 [57600/60000 (96%)]\tLoss: 95.431786\n",
      "Train Epoch: 95 [58880/60000 (98%)]\tLoss: 93.475510\n",
      "====> Epoch: 95 Average loss: 96.1741\n",
      "====> Test set loss: 98.7152\n",
      "Train Epoch: 96 [0/60000 (0%)]\tLoss: 98.146301\n",
      "Train Epoch: 96 [1280/60000 (2%)]\tLoss: 97.665215\n",
      "Train Epoch: 96 [2560/60000 (4%)]\tLoss: 94.889999\n",
      "Train Epoch: 96 [3840/60000 (6%)]\tLoss: 93.592903\n",
      "Train Epoch: 96 [5120/60000 (9%)]\tLoss: 95.322449\n",
      "Train Epoch: 96 [6400/60000 (11%)]\tLoss: 95.093071\n",
      "Train Epoch: 96 [7680/60000 (13%)]\tLoss: 97.125595\n",
      "Train Epoch: 96 [8960/60000 (15%)]\tLoss: 94.535858\n",
      "Train Epoch: 96 [10240/60000 (17%)]\tLoss: 100.794807\n",
      "Train Epoch: 96 [11520/60000 (19%)]\tLoss: 97.884338\n",
      "Train Epoch: 96 [12800/60000 (21%)]\tLoss: 99.267105\n",
      "Train Epoch: 96 [14080/60000 (23%)]\tLoss: 99.178833\n",
      "Train Epoch: 96 [15360/60000 (26%)]\tLoss: 93.722702\n",
      "Train Epoch: 96 [16640/60000 (28%)]\tLoss: 97.064102\n",
      "Train Epoch: 96 [17920/60000 (30%)]\tLoss: 99.703186\n",
      "Train Epoch: 96 [19200/60000 (32%)]\tLoss: 100.784309\n",
      "Train Epoch: 96 [20480/60000 (34%)]\tLoss: 95.958443\n",
      "Train Epoch: 96 [21760/60000 (36%)]\tLoss: 97.882339\n",
      "Train Epoch: 96 [23040/60000 (38%)]\tLoss: 95.318222\n",
      "Train Epoch: 96 [24320/60000 (41%)]\tLoss: 97.574104\n",
      "Train Epoch: 96 [25600/60000 (43%)]\tLoss: 95.688957\n",
      "Train Epoch: 96 [26880/60000 (45%)]\tLoss: 96.203781\n",
      "Train Epoch: 96 [28160/60000 (47%)]\tLoss: 96.669472\n",
      "Train Epoch: 96 [29440/60000 (49%)]\tLoss: 94.177773\n",
      "Train Epoch: 96 [30720/60000 (51%)]\tLoss: 96.848663\n",
      "Train Epoch: 96 [32000/60000 (53%)]\tLoss: 97.185944\n",
      "Train Epoch: 96 [33280/60000 (55%)]\tLoss: 98.299759\n",
      "Train Epoch: 96 [34560/60000 (58%)]\tLoss: 97.020836\n",
      "Train Epoch: 96 [35840/60000 (60%)]\tLoss: 95.229500\n",
      "Train Epoch: 96 [37120/60000 (62%)]\tLoss: 94.133652\n",
      "Train Epoch: 96 [38400/60000 (64%)]\tLoss: 98.401108\n",
      "Train Epoch: 96 [39680/60000 (66%)]\tLoss: 98.682777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 96 [40960/60000 (68%)]\tLoss: 93.790855\n",
      "Train Epoch: 96 [42240/60000 (70%)]\tLoss: 94.326340\n",
      "Train Epoch: 96 [43520/60000 (72%)]\tLoss: 97.245529\n",
      "Train Epoch: 96 [44800/60000 (75%)]\tLoss: 96.609779\n",
      "Train Epoch: 96 [46080/60000 (77%)]\tLoss: 94.990555\n",
      "Train Epoch: 96 [47360/60000 (79%)]\tLoss: 96.079933\n",
      "Train Epoch: 96 [48640/60000 (81%)]\tLoss: 94.085266\n",
      "Train Epoch: 96 [49920/60000 (83%)]\tLoss: 94.725983\n",
      "Train Epoch: 96 [51200/60000 (85%)]\tLoss: 97.035767\n",
      "Train Epoch: 96 [52480/60000 (87%)]\tLoss: 96.227760\n",
      "Train Epoch: 96 [53760/60000 (90%)]\tLoss: 94.771820\n",
      "Train Epoch: 96 [55040/60000 (92%)]\tLoss: 93.720596\n",
      "Train Epoch: 96 [56320/60000 (94%)]\tLoss: 96.185272\n",
      "Train Epoch: 96 [57600/60000 (96%)]\tLoss: 92.925285\n",
      "Train Epoch: 96 [58880/60000 (98%)]\tLoss: 99.001137\n",
      "====> Epoch: 96 Average loss: 96.1570\n",
      "====> Test set loss: 98.9481\n",
      "Train Epoch: 97 [0/60000 (0%)]\tLoss: 93.014694\n",
      "Train Epoch: 97 [1280/60000 (2%)]\tLoss: 97.830818\n",
      "Train Epoch: 97 [2560/60000 (4%)]\tLoss: 95.639175\n",
      "Train Epoch: 97 [3840/60000 (6%)]\tLoss: 95.010109\n",
      "Train Epoch: 97 [5120/60000 (9%)]\tLoss: 96.608551\n",
      "Train Epoch: 97 [6400/60000 (11%)]\tLoss: 97.614037\n",
      "Train Epoch: 97 [7680/60000 (13%)]\tLoss: 94.530151\n",
      "Train Epoch: 97 [8960/60000 (15%)]\tLoss: 96.777473\n",
      "Train Epoch: 97 [10240/60000 (17%)]\tLoss: 94.407425\n",
      "Train Epoch: 97 [11520/60000 (19%)]\tLoss: 98.470444\n",
      "Train Epoch: 97 [12800/60000 (21%)]\tLoss: 90.298248\n",
      "Train Epoch: 97 [14080/60000 (23%)]\tLoss: 96.181778\n",
      "Train Epoch: 97 [15360/60000 (26%)]\tLoss: 98.024094\n",
      "Train Epoch: 97 [16640/60000 (28%)]\tLoss: 95.427155\n",
      "Train Epoch: 97 [17920/60000 (30%)]\tLoss: 95.506134\n",
      "Train Epoch: 97 [19200/60000 (32%)]\tLoss: 96.732056\n",
      "Train Epoch: 97 [20480/60000 (34%)]\tLoss: 94.930634\n",
      "Train Epoch: 97 [21760/60000 (36%)]\tLoss: 97.148361\n",
      "Train Epoch: 97 [23040/60000 (38%)]\tLoss: 95.018105\n",
      "Train Epoch: 97 [24320/60000 (41%)]\tLoss: 96.299126\n",
      "Train Epoch: 97 [25600/60000 (43%)]\tLoss: 100.287460\n",
      "Train Epoch: 97 [26880/60000 (45%)]\tLoss: 95.583839\n",
      "Train Epoch: 97 [28160/60000 (47%)]\tLoss: 96.261383\n",
      "Train Epoch: 97 [29440/60000 (49%)]\tLoss: 96.884003\n",
      "Train Epoch: 97 [30720/60000 (51%)]\tLoss: 98.303596\n",
      "Train Epoch: 97 [32000/60000 (53%)]\tLoss: 101.340569\n",
      "Train Epoch: 97 [33280/60000 (55%)]\tLoss: 93.778336\n",
      "Train Epoch: 97 [34560/60000 (58%)]\tLoss: 97.667572\n",
      "Train Epoch: 97 [35840/60000 (60%)]\tLoss: 91.559776\n",
      "Train Epoch: 97 [37120/60000 (62%)]\tLoss: 94.034348\n",
      "Train Epoch: 97 [38400/60000 (64%)]\tLoss: 95.553116\n",
      "Train Epoch: 97 [39680/60000 (66%)]\tLoss: 96.591660\n",
      "Train Epoch: 97 [40960/60000 (68%)]\tLoss: 96.753586\n",
      "Train Epoch: 97 [42240/60000 (70%)]\tLoss: 100.743637\n",
      "Train Epoch: 97 [43520/60000 (72%)]\tLoss: 93.510300\n",
      "Train Epoch: 97 [44800/60000 (75%)]\tLoss: 95.814194\n",
      "Train Epoch: 97 [46080/60000 (77%)]\tLoss: 97.890182\n",
      "Train Epoch: 97 [47360/60000 (79%)]\tLoss: 95.642136\n",
      "Train Epoch: 97 [48640/60000 (81%)]\tLoss: 95.256058\n",
      "Train Epoch: 97 [49920/60000 (83%)]\tLoss: 92.477501\n",
      "Train Epoch: 97 [51200/60000 (85%)]\tLoss: 97.744743\n",
      "Train Epoch: 97 [52480/60000 (87%)]\tLoss: 97.379974\n",
      "Train Epoch: 97 [53760/60000 (90%)]\tLoss: 97.974243\n",
      "Train Epoch: 97 [55040/60000 (92%)]\tLoss: 98.433044\n",
      "Train Epoch: 97 [56320/60000 (94%)]\tLoss: 95.491669\n",
      "Train Epoch: 97 [57600/60000 (96%)]\tLoss: 98.034431\n",
      "Train Epoch: 97 [58880/60000 (98%)]\tLoss: 94.116966\n",
      "====> Epoch: 97 Average loss: 96.1762\n",
      "====> Test set loss: 98.9166\n",
      "Train Epoch: 98 [0/60000 (0%)]\tLoss: 95.307358\n",
      "Train Epoch: 98 [1280/60000 (2%)]\tLoss: 94.281639\n",
      "Train Epoch: 98 [2560/60000 (4%)]\tLoss: 93.236694\n",
      "Train Epoch: 98 [3840/60000 (6%)]\tLoss: 97.867088\n",
      "Train Epoch: 98 [5120/60000 (9%)]\tLoss: 95.921799\n",
      "Train Epoch: 98 [6400/60000 (11%)]\tLoss: 96.429077\n",
      "Train Epoch: 98 [7680/60000 (13%)]\tLoss: 93.887428\n",
      "Train Epoch: 98 [8960/60000 (15%)]\tLoss: 95.453415\n",
      "Train Epoch: 98 [10240/60000 (17%)]\tLoss: 96.305801\n",
      "Train Epoch: 98 [11520/60000 (19%)]\tLoss: 95.045837\n",
      "Train Epoch: 98 [12800/60000 (21%)]\tLoss: 93.867844\n",
      "Train Epoch: 98 [14080/60000 (23%)]\tLoss: 98.913315\n",
      "Train Epoch: 98 [15360/60000 (26%)]\tLoss: 98.375069\n",
      "Train Epoch: 98 [16640/60000 (28%)]\tLoss: 96.671082\n",
      "Train Epoch: 98 [17920/60000 (30%)]\tLoss: 98.526779\n",
      "Train Epoch: 98 [19200/60000 (32%)]\tLoss: 96.972771\n",
      "Train Epoch: 98 [20480/60000 (34%)]\tLoss: 98.501755\n",
      "Train Epoch: 98 [21760/60000 (36%)]\tLoss: 97.224686\n",
      "Train Epoch: 98 [23040/60000 (38%)]\tLoss: 99.084625\n",
      "Train Epoch: 98 [24320/60000 (41%)]\tLoss: 96.983856\n",
      "Train Epoch: 98 [25600/60000 (43%)]\tLoss: 90.341301\n",
      "Train Epoch: 98 [26880/60000 (45%)]\tLoss: 98.780029\n",
      "Train Epoch: 98 [28160/60000 (47%)]\tLoss: 95.626305\n",
      "Train Epoch: 98 [29440/60000 (49%)]\tLoss: 99.111664\n",
      "Train Epoch: 98 [30720/60000 (51%)]\tLoss: 99.908081\n",
      "Train Epoch: 98 [32000/60000 (53%)]\tLoss: 100.109138\n",
      "Train Epoch: 98 [33280/60000 (55%)]\tLoss: 95.017761\n",
      "Train Epoch: 98 [34560/60000 (58%)]\tLoss: 96.359627\n",
      "Train Epoch: 98 [35840/60000 (60%)]\tLoss: 94.239746\n",
      "Train Epoch: 98 [37120/60000 (62%)]\tLoss: 98.613564\n",
      "Train Epoch: 98 [38400/60000 (64%)]\tLoss: 94.063675\n",
      "Train Epoch: 98 [39680/60000 (66%)]\tLoss: 95.150139\n",
      "Train Epoch: 98 [40960/60000 (68%)]\tLoss: 94.220856\n",
      "Train Epoch: 98 [42240/60000 (70%)]\tLoss: 92.924332\n",
      "Train Epoch: 98 [43520/60000 (72%)]\tLoss: 95.775299\n",
      "Train Epoch: 98 [44800/60000 (75%)]\tLoss: 100.250824\n",
      "Train Epoch: 98 [46080/60000 (77%)]\tLoss: 98.121307\n",
      "Train Epoch: 98 [47360/60000 (79%)]\tLoss: 99.250893\n",
      "Train Epoch: 98 [48640/60000 (81%)]\tLoss: 94.623528\n",
      "Train Epoch: 98 [49920/60000 (83%)]\tLoss: 93.112556\n",
      "Train Epoch: 98 [51200/60000 (85%)]\tLoss: 95.046860\n",
      "Train Epoch: 98 [52480/60000 (87%)]\tLoss: 94.918144\n",
      "Train Epoch: 98 [53760/60000 (90%)]\tLoss: 97.488670\n",
      "Train Epoch: 98 [55040/60000 (92%)]\tLoss: 95.876099\n",
      "Train Epoch: 98 [56320/60000 (94%)]\tLoss: 94.506561\n",
      "Train Epoch: 98 [57600/60000 (96%)]\tLoss: 92.780678\n",
      "Train Epoch: 98 [58880/60000 (98%)]\tLoss: 94.675392\n",
      "====> Epoch: 98 Average loss: 96.1794\n",
      "====> Test set loss: 98.8211\n",
      "Train Epoch: 99 [0/60000 (0%)]\tLoss: 95.799957\n",
      "Train Epoch: 99 [1280/60000 (2%)]\tLoss: 96.754623\n",
      "Train Epoch: 99 [2560/60000 (4%)]\tLoss: 94.607849\n",
      "Train Epoch: 99 [3840/60000 (6%)]\tLoss: 95.144661\n",
      "Train Epoch: 99 [5120/60000 (9%)]\tLoss: 97.262276\n",
      "Train Epoch: 99 [6400/60000 (11%)]\tLoss: 95.884003\n",
      "Train Epoch: 99 [7680/60000 (13%)]\tLoss: 91.936295\n",
      "Train Epoch: 99 [8960/60000 (15%)]\tLoss: 95.977295\n",
      "Train Epoch: 99 [10240/60000 (17%)]\tLoss: 94.311073\n",
      "Train Epoch: 99 [11520/60000 (19%)]\tLoss: 93.870613\n",
      "Train Epoch: 99 [12800/60000 (21%)]\tLoss: 98.027359\n",
      "Train Epoch: 99 [14080/60000 (23%)]\tLoss: 94.282722\n",
      "Train Epoch: 99 [15360/60000 (26%)]\tLoss: 95.999344\n",
      "Train Epoch: 99 [16640/60000 (28%)]\tLoss: 97.365768\n",
      "Train Epoch: 99 [17920/60000 (30%)]\tLoss: 97.028366\n",
      "Train Epoch: 99 [19200/60000 (32%)]\tLoss: 98.470200\n",
      "Train Epoch: 99 [20480/60000 (34%)]\tLoss: 93.498550\n",
      "Train Epoch: 99 [21760/60000 (36%)]\tLoss: 98.658905\n",
      "Train Epoch: 99 [23040/60000 (38%)]\tLoss: 97.434525\n",
      "Train Epoch: 99 [24320/60000 (41%)]\tLoss: 97.207916\n",
      "Train Epoch: 99 [25600/60000 (43%)]\tLoss: 99.555290\n",
      "Train Epoch: 99 [26880/60000 (45%)]\tLoss: 95.032990\n",
      "Train Epoch: 99 [28160/60000 (47%)]\tLoss: 98.253510\n",
      "Train Epoch: 99 [29440/60000 (49%)]\tLoss: 93.816467\n",
      "Train Epoch: 99 [30720/60000 (51%)]\tLoss: 96.117935\n",
      "Train Epoch: 99 [32000/60000 (53%)]\tLoss: 95.891281\n",
      "Train Epoch: 99 [33280/60000 (55%)]\tLoss: 97.960571\n",
      "Train Epoch: 99 [34560/60000 (58%)]\tLoss: 96.145752\n",
      "Train Epoch: 99 [35840/60000 (60%)]\tLoss: 94.776176\n",
      "Train Epoch: 99 [37120/60000 (62%)]\tLoss: 96.520111\n",
      "Train Epoch: 99 [38400/60000 (64%)]\tLoss: 95.379829\n",
      "Train Epoch: 99 [39680/60000 (66%)]\tLoss: 91.298721\n",
      "Train Epoch: 99 [40960/60000 (68%)]\tLoss: 94.522781\n",
      "Train Epoch: 99 [42240/60000 (70%)]\tLoss: 97.026718\n",
      "Train Epoch: 99 [43520/60000 (72%)]\tLoss: 95.676926\n",
      "Train Epoch: 99 [44800/60000 (75%)]\tLoss: 95.084839\n",
      "Train Epoch: 99 [46080/60000 (77%)]\tLoss: 93.806984\n",
      "Train Epoch: 99 [47360/60000 (79%)]\tLoss: 96.794785\n",
      "Train Epoch: 99 [48640/60000 (81%)]\tLoss: 97.994179\n",
      "Train Epoch: 99 [49920/60000 (83%)]\tLoss: 94.768684\n",
      "Train Epoch: 99 [51200/60000 (85%)]\tLoss: 93.104218\n",
      "Train Epoch: 99 [52480/60000 (87%)]\tLoss: 99.097961\n",
      "Train Epoch: 99 [53760/60000 (90%)]\tLoss: 98.618645\n",
      "Train Epoch: 99 [55040/60000 (92%)]\tLoss: 95.803551\n",
      "Train Epoch: 99 [56320/60000 (94%)]\tLoss: 97.458031\n",
      "Train Epoch: 99 [57600/60000 (96%)]\tLoss: 95.391830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 99 [58880/60000 (98%)]\tLoss: 94.464935\n",
      "====> Epoch: 99 Average loss: 96.1748\n",
      "====> Test set loss: 98.8143\n",
      "Train Epoch: 100 [0/60000 (0%)]\tLoss: 99.117149\n",
      "Train Epoch: 100 [1280/60000 (2%)]\tLoss: 95.568810\n",
      "Train Epoch: 100 [2560/60000 (4%)]\tLoss: 94.677910\n",
      "Train Epoch: 100 [3840/60000 (6%)]\tLoss: 96.283142\n",
      "Train Epoch: 100 [5120/60000 (9%)]\tLoss: 93.642273\n",
      "Train Epoch: 100 [6400/60000 (11%)]\tLoss: 94.372238\n",
      "Train Epoch: 100 [7680/60000 (13%)]\tLoss: 93.804703\n",
      "Train Epoch: 100 [8960/60000 (15%)]\tLoss: 98.178871\n",
      "Train Epoch: 100 [10240/60000 (17%)]\tLoss: 97.421951\n",
      "Train Epoch: 100 [11520/60000 (19%)]\tLoss: 98.435837\n",
      "Train Epoch: 100 [12800/60000 (21%)]\tLoss: 95.516663\n",
      "Train Epoch: 100 [14080/60000 (23%)]\tLoss: 98.466820\n",
      "Train Epoch: 100 [15360/60000 (26%)]\tLoss: 94.063553\n",
      "Train Epoch: 100 [16640/60000 (28%)]\tLoss: 97.638641\n",
      "Train Epoch: 100 [17920/60000 (30%)]\tLoss: 94.968933\n",
      "Train Epoch: 100 [19200/60000 (32%)]\tLoss: 94.958710\n",
      "Train Epoch: 100 [20480/60000 (34%)]\tLoss: 96.703415\n",
      "Train Epoch: 100 [21760/60000 (36%)]\tLoss: 94.981934\n",
      "Train Epoch: 100 [23040/60000 (38%)]\tLoss: 96.025284\n",
      "Train Epoch: 100 [24320/60000 (41%)]\tLoss: 95.160706\n",
      "Train Epoch: 100 [25600/60000 (43%)]\tLoss: 92.962685\n",
      "Train Epoch: 100 [26880/60000 (45%)]\tLoss: 98.677216\n",
      "Train Epoch: 100 [28160/60000 (47%)]\tLoss: 99.071320\n",
      "Train Epoch: 100 [29440/60000 (49%)]\tLoss: 100.211372\n",
      "Train Epoch: 100 [30720/60000 (51%)]\tLoss: 95.989243\n",
      "Train Epoch: 100 [32000/60000 (53%)]\tLoss: 98.989120\n",
      "Train Epoch: 100 [33280/60000 (55%)]\tLoss: 98.866684\n",
      "Train Epoch: 100 [34560/60000 (58%)]\tLoss: 97.733604\n",
      "Train Epoch: 100 [35840/60000 (60%)]\tLoss: 97.890701\n",
      "Train Epoch: 100 [37120/60000 (62%)]\tLoss: 93.366959\n",
      "Train Epoch: 100 [38400/60000 (64%)]\tLoss: 97.385612\n",
      "Train Epoch: 100 [39680/60000 (66%)]\tLoss: 96.250351\n",
      "Train Epoch: 100 [40960/60000 (68%)]\tLoss: 97.661598\n",
      "Train Epoch: 100 [42240/60000 (70%)]\tLoss: 94.091576\n",
      "Train Epoch: 100 [43520/60000 (72%)]\tLoss: 99.247055\n",
      "Train Epoch: 100 [44800/60000 (75%)]\tLoss: 88.881363\n",
      "Train Epoch: 100 [46080/60000 (77%)]\tLoss: 97.841156\n",
      "Train Epoch: 100 [47360/60000 (79%)]\tLoss: 97.897995\n",
      "Train Epoch: 100 [48640/60000 (81%)]\tLoss: 100.215622\n",
      "Train Epoch: 100 [49920/60000 (83%)]\tLoss: 98.511688\n",
      "Train Epoch: 100 [51200/60000 (85%)]\tLoss: 99.336334\n",
      "Train Epoch: 100 [52480/60000 (87%)]\tLoss: 93.572723\n",
      "Train Epoch: 100 [53760/60000 (90%)]\tLoss: 93.363586\n",
      "Train Epoch: 100 [55040/60000 (92%)]\tLoss: 96.345108\n",
      "Train Epoch: 100 [56320/60000 (94%)]\tLoss: 99.051735\n",
      "Train Epoch: 100 [57600/60000 (96%)]\tLoss: 94.927567\n",
      "Train Epoch: 100 [58880/60000 (98%)]\tLoss: 99.073524\n",
      "====> Epoch: 100 Average loss: 96.1819\n",
      "====> Test set loss: 98.7963\n"
     ]
    }
   ],
   "source": [
    "## VAE TRAINING & TESTING\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_vae(epoch)\n",
    "    test_vae(epoch)\n",
    "    with torch.no_grad():\n",
    "        sample = torch.randn(64, nz).to(device)\n",
    "        sample = vae.decode(sample).cpu()\n",
    "        save_image(\n",
    "            sample.view(64, 1, 28, 28),\n",
    "            vae_results_directory + \"/sample_\" + str(epoch) + \".png\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe65c2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model_path = os.path.join(curr_dirname, \"vae.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5be857cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), vae_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52535086",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.load_state_dict(torch.load(vae_model_path))\n",
    "vae.eval()\n",
    "sum([param.nelement() for param in vae.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a0d0a91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_z = torch.randn(2, nz).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f8427f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_vae = vae.decode(sample_z).cpu()\n",
    "sample_gan = generator(sample_z).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc9cfefb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1736be760>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.imshow(transforms.functional.to_pil_image(sample_gan.view(2, 1, 28, 28)[0]),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ddae2d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b33d9af0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.imshow(transforms.functional.to_pil_image(sample_vae.view(2, 1, 28, 28)[0]),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8f5716b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_z1 = torch.randn(2, nz).to(device)\n",
    "sample_z2 = torch.randn(2, nz).to(device)\n",
    "sample_z3 = sample_z1 + sample_z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6db2286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_vae1 = vae.decode(sample_z1).cpu()\n",
    "sample_vae2 = vae.decode(sample_z2).cpu()\n",
    "sample_vae3 = vae.decode(sample_z3).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9523068f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b4928100>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.imshow(transforms.functional.to_pil_image(sample_vae1.view(2, 1, 28, 28)[0]),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b437b89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b499a370>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.imshow(transforms.functional.to_pil_image(sample_vae2.view(2, 1, 28, 28)[0]),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "aa49f9ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b4a00fd0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.imshow(transforms.functional.to_pil_image(sample_vae3.view(2, 1, 28, 28)[0]),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "5e6b20d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_z1 = torch.randn(2, nz).to(device)\n",
    "sample_z2 = torch.randn(2, nz).to(device)\n",
    "sample_z3 = sample_z1 + sample_z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0938dadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_gan1 = generator(sample_z1).cpu()\n",
    "sample_gan2 = generator(sample_z2).cpu()\n",
    "sample_gan3 = generator(sample_z3).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "78c459a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b5e754f0>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.imshow(transforms.functional.to_pil_image(sample_gan1.view(2, 1, 28, 28)[0]),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "323368b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b600ed90>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.imshow(transforms.functional.to_pil_image(sample_gan2.view(2, 1, 28, 28)[0]),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9f36695c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b6086610>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAie0lEQVR4nO3df2yV5f3/8ddpaY+ttKdCoT+ksIICk5+ZSkdUPioNUBMiyjJRF8EQCayYIXMajKLOJd0w/h7DZHNUNxFHIqhsYRGUErfCBooEYR3tUHC0RcGeUwq0pef6/kHovkd+yHXRc67T8nwkJ6Gn96v31bt3z4vTc/fdgDHGCACABEvxvQAAwMWJAgIAeEEBAQC8oIAAAF5QQAAALyggAIAXFBAAwAsKCADgRS/fC/imaDSqAwcOKCsrS4FAwPdyAACWjDFqbm5WYWGhUlLO/jwn6QrowIEDKioq8r0MAMAF2r9/vwYMGHDW9yddAWVlZXX+O97PgM7VzOcSjUa7eCXdU0ZGhnWmtbXVOsPxTry0tDTrTHt7exxWcjqXxwUmjl0Y22NujJExJubx/EziVkBLly7V008/rYaGBo0ZM0YvvfSSxo0b9625U59oIBCIewG5fnx+NHiSy3FIVAYXJpm/TpwPieda+t+Wi8tFCG+++aYWLlyoxx9/XB999JHGjBmjyZMn6+DBg/HYHQCgG4pLAT377LO67777dO+99+qqq67Syy+/rMzMTP3+97+Px+4AAN1QlxdQW1ubtm3bptLS0v/tJCVFpaWlqq6uPm371tZWRSKRmBsAoOfr8gL66quv1NHRoby8vJj78/Ly1NDQcNr2FRUVCoVCnTeugAOAi4P3X0RdtGiRwuFw523//v2+lwQASIAuvwouNzdXqampamxsjLm/sbFR+fn5p20fDAYVDAa7ehkAgCTX5c+A0tPTdfXVV2vDhg2d90WjUW3YsEHjx4/v6t0BALqpuPwe0MKFCzVz5kxdc801GjdunJ5//nm1tLTo3nvvjcfuAADdUFwK6I477tCXX36pxYsXq6GhQWPHjtW6detOuzABAHDxCpgkm1ERiUQUCoWUkpJi9du3LmN1Ojo6rDPJzuU4uI666d27t3XmyJEjTvuy5frb8kn27RAjkV/bZJ42kMxfI6lnjgqyPfeMMero6FA4HFZ2dvbZP+6FLgwAABcUEADACwoIAOAFBQQA8IICAgB4QQEBALyggAAAXlBAAAAvKCAAgBcUEADACwoIAOAFBQQA8CIu07C7QllZmdLS0s57+3feecd6H5mZmdYZSWpra7POnDhxwmlftlyHT7pI1GBRFy6DO6XkHlDr8rXtiUNZcVIiB8bedtttVtu3t7drzZo137odz4AAAF5QQAAALyggAIAXFBAAwAsKCADgBQUEAPCCAgIAeEEBAQC8oIAAAF5QQAAALyggAIAXFBAAwAsKCADgRcAk2djbSCSiUCikQCAQ92mvrhOTEzlx2lZqaqp1Zty4cU77qq6udsoBuHCJnHRuuy9jjKLRqMLhsLKzs8+6Hc+AAABeUEAAAC8oIACAFxQQAMALCggA4AUFBADwggICAHhBAQEAvKCAAABeUEAAAC8oIACAFxQQAMCLXr4XcDYpKSlWA/BcBoQmcqioy+DT3r17W2dcBg26DhV1GYYYDAatM8XFxdYZ10GNu3btSsi+kmwGsDcuxy4tLc0609bWZp2RpLFjx1pntm/f7rQvW67nkMsxt338OjWM9Fs/rvVKAADoAhQQAMALCggA4AUFBADwggICAHhBAQEAvKCAAABeUEAAAC8oIACAFxQQAMALCggA4AUFBADwImmHkUajUauheS4DCtvb260zkttATZdhiJFIxDrjIjU11SnnMgzx+PHj1pndu3dbZxIpUYNFXfbjOpQ1mbkch+uvv95pX1lZWdaZTz75xDqTyOG0Lvvq6OiIyz54BgQA8IICAgB40eUF9MQTTygQCMTchg8f3tW7AQB0c3F5DWjEiBFav379/3bSK2lfagIAeBKXZujVq5fy8/Pj8aEBAD1EXF4D2rNnjwoLCzV48GDdfffd2rdv31m3bW1tVSQSibkBAHq+Li+gkpISVVZWat26dVq2bJn27t2rG264Qc3NzWfcvqKiQqFQqPNWVFTU1UsCACShgInzBehNTU0aNGiQnn32Wc2ePfu097e2tqq1tbXz7UgkoqKios4LGM5XT/w9oET9bkAifw8oGo067Qs98/eAXNbn8ppySUmJdUZy+z2gdevWWWcS+XtAiWCMkTFG4XBY2dnZZ90u7lcH5OTkaOjQoaqtrT3j+4PBoNMDOgCge4v77wEdOXJEdXV1KigoiPeuAADdSJcX0IMPPqiqqip99tln+vvf/67bbrtNqampuvPOO7t6VwCAbqzLfwT3xRdf6M4779ShQ4fUr18/XX/99dq8ebP69evX1bsCAHRjXV5AK1eu7JKPY3sRgu2wvFP7cOF68UKycjl2UuJe4Ha5SML1c0pmyX5BgcuFQE8++aR15plnnrHOjB071jojSeFw2DrjchxcLlLqCZgFBwDwggICAHhBAQEAvKCAAABeUEAAAC8oIACAFxQQAMALCggA4AUFBADwggICAHhBAQEAvKCAAABexP0P0rmy/QuBKSn2Xer61zkzMzOtMy7DMY8fP26dSeTgTpe/4ugyqNElk56ebp2RpJaWFuvMiRMnnPaVCLm5uU65xYsXW2fuuece68wll1xinbnrrrusM67HYe7cudaZSy+91DrDMFIAABKIAgIAeEEBAQC8oIAAAF5QQAAALyggAIAXFBAAwAsKCADgBQUEAPCCAgIAeEEBAQC8oIAAAF5QQAAAL5J2GnavXr0UCATOe/tETiQ+evRowvZly3WytYuMjAzrzIwZM6wzK1eutM688MIL1hlJevrpp60zu3btctqXLZdJ51OnTnXa12233WadcZkS7zLx/dlnn7XOvPPOO9YZSaqvr7fOJPtka5fzyHbyvTHmvB6LeAYEAPCCAgIAeEEBAQC8oIAAAF5QQAAALyggAIAXFBAAwAsKCADgBQUEAPCCAgIAeEEBAQC8oIAAAF4EjO2UuTiLRCIKhUIKBAJWw0iTncugxkQNPU1Jcft/iMvAyqeeeso64zJgtayszDojuQ3H/Oqrr5z2lQiXXXaZU+6zzz6zzqSlpVlnKisrrTN//etfrTObNm2yzkhSU1OTdcblIdXlsS7JHrpjGGNkjFE4HFZ2dvZZt+MZEADACwoIAOAFBQQA8IICAgB4QQEBALyggAAAXlBAAAAvKCAAgBcUEADACwoIAOAFBQQA8IICAgB40cv3Ai4WiRos6iIjI8Mpd/PNN1tnXAaLugxyvemmm6wzkvT666875ZLVE0884ZTr1cv+oSEajVpnXL4v/vznP1tnXM47V8k+WNRl+LDL1/Z88AwIAOAFBQQA8MK6gDZt2qSpU6eqsLBQgUBAa9asiXm/MUaLFy9WQUGBMjIyVFpaqj179nTVegEAPYR1AbW0tGjMmDFaunTpGd+/ZMkSvfjii3r55Ze1ZcsWXXrppZo8ebLTH/oCAPRc1q80lpWVnfWvTRpj9Pzzz+vRRx/VrbfeKkl67bXXlJeXpzVr1mjGjBkXtloAQI/Rpa8B7d27Vw0NDSotLe28LxQKqaSkRNXV1WfMtLa2KhKJxNwAAD1flxZQQ0ODJCkvLy/m/ry8vM73fVNFRYVCoVDnraioqCuXBABIUt6vglu0aJHC4XDnbf/+/b6XBABIgC4toPz8fElSY2NjzP2NjY2d7/umYDCo7OzsmBsAoOfr0gIqLi5Wfn6+NmzY0HlfJBLRli1bNH78+K7cFQCgm7O+Cu7IkSOqra3tfHvv3r3avn27+vTpo4EDB2rBggX6xS9+oSuvvFLFxcV67LHHVFhYqGnTpnXlugEA3Zx1AW3dujVmztbChQslSTNnzlRlZaUeeughtbS0aM6cOWpqatL111+vdevW6ZJLLum6VQMAur2ASeQUvPMQiUQUCoUUCASchvrBnuvgzldeecU6883XB8/H7373O+vMH//4R+uMJLW1tVlnEvUtlJ6ebp05dOiQ075cBsC6HIfZs2dbZ/7whz9YZ1wfS4YOHWqd2b17t3XGZUCo63mXiPPVGCNjjMLh8Dlf1/d+FRwA4OJEAQEAvKCAAABeUEAAAC8oIACAFxQQAMALCggA4AUFBADwggICAHhBAQEAvKCAAABeUEAAAC8oIACAF9Z/jiFRLrvsMqsJsYcPH47jaroPlz/8d//99zvt6/LLL7fO5OXlWWeeeOIJ68zy5cutM5L71ORE+O1vf2udcZlq7aq5udk6s23btjis5HTRaNQp95///Mc6k5aWZp05ceJEQvYjuU18jxeeAQEAvKCAAABeUEAAAC8oIACAFxQQAMALCggA4AUFBADwggICAHhBAQEAvKCAAABeUEAAAC8oIACAF0k7jPTrr79O2sGQLusyxlhn+vbta5359NNPrTONjY3WGUlWw2JPcRm6+MILL1hnXIdPunxOvXv3ts5cc8011pnhw4dbZzo6Oqwzkts5vmTJEutMbW2tdcbleyk1NdU6I7kN/GxpabHOuHxOrkNFBw8ebJ1xGcp6PngGBADwggICAHhBAQEAvKCAAABeUEAAAC8oIACAFxQQAMALCggA4AUFBADwggICAHhBAQEAvKCAAABeJO0w0mTmMjjQRVNTk3UmOzvbOvPiiy9aZyQpNzfXOjNy5EjrzCeffGKdcR0+6WL+/PnWmdWrV1tngsGgdcZl+Ksk1dfXW2d27dplnWlvb7fOuOjVy+2h7qqrrrLO/POf/3TaV6K4DBbNzMy02t4Yc15DWXkGBADwggICAHhBAQEAvKCAAABeUEAAAC8oIACAFxQQAMALCggA4AUFBADwggICAHhBAQEAvKCAAABeJO0w0kAgoEAgcN7buwwIveSSS6wzknT8+HHrTCgUss4UFRVZZz799FPrzNdff22dkaR3333XOuMy+HT79u3WmYyMDOuMJN10003WmeHDh1tn1q5da51x+Zxch7Lu2LHDOnPo0CHrjMv3rctQ1o6ODuuMJH3++efWmbFjx1pnPv74Y+tMIh09etRq+/P9uvIMCADgBQUEAPDCuoA2bdqkqVOnqrCwUIFAQGvWrIl5/6xZszp/fHbqNmXKlK5aLwCgh7AuoJaWFo0ZM0ZLly496zZTpkxRfX195+2NN964oEUCAHoe64sQysrKVFZWds5tgsGg8vPznRcFAOj54vIa0MaNG9W/f38NGzZM8+bNO+fVMa2trYpEIjE3AEDP1+UFNGXKFL322mvasGGDfvWrX6mqqkplZWVnvQyyoqJCoVCo8+Zy6TEAoPvp8t8DmjFjRue/R40apdGjR2vIkCHauHGjJk6ceNr2ixYt0sKFCzvfjkQilBAAXATifhn24MGDlZubq9ra2jO+PxgMKjs7O+YGAOj54l5AX3zxhQ4dOqSCgoJ47woA0I1Y/wjuyJEjMc9m9u7dq+3bt6tPnz7q06ePnnzySU2fPl35+fmqq6vTQw89pCuuuEKTJ0/u0oUDALo36wLaunVrzLysU6/fzJw5U8uWLdOOHTv06quvqqmpSYWFhZo0aZKeeuopp/lNAICeK2BcpgHGUSQSUSgUsh5Gmp6ebr2vtrY264wk9eplf+2Gy2F2HaDY0/Tu3ds6M3DgQKd9rVq1yjqzfv1668y0adOsM01NTdYZ1+MwadIk60xNTY11pqWlxTpTWFhonVmyZIl1RpIWLFhgnWlsbHTaV09ijJExRuFw+Jyv6zMLDgDgBQUEAPCCAgIAeEEBAQC8oIAAAF5QQAAALyggAIAXFBAAwAsKCADgBQUEAPCCAgIAeEEBAQC8oIAAAF70mGnYLlw/vsvk7dbWVqd9we3rlJmZ6bQvl8nbhw8fts7ccsst1pnnnnvOOhMKhawzkrRlyxbrzA9/+EPrzJEjR6wzaWlp1hmXCfauUlLs/1/vMhXclcv3k21NMA0bAJDUKCAAgBcUEADACwoIAOAFBQQA8IICAgB4QQEBALyggAAAXlBAAAAvKCAAgBcUEADACwoIAOBF4ib0WUpJSbEamheNRq334TqMtKcNFnUZripJ7e3t1hmX2bcuXyeXtUlSc3Ozdeb222+3znR0dFhn/vvf/1pn9u/fb52RpBEjRlhnjh075rQvWydOnLDOjBo1ymlfdXV11plwOGydcRlg6vKYJ7l9D8YLz4AAAF5QQAAALyggAIAXFBAAwAsKCADgBQUEAPCCAgIAeEEBAQC8oIAAAF5QQAAALyggAIAXFBAAwIukHUZqOzDPZWCl6zA/F6mpqdaZUChkncnIyLDOuAy5TCSX4Ykuwz4l6dJLL7XO/OhHP7LODBgwwDpzzz33WGcqKyutM5I0b94860xOTo515vDhw9YZl+/1fv36WWckaefOndYZ1yHHtlwGmErS2LFjrTO7d++22t4Yo6NHj37rdjwDAgB4QQEBALyggAAAXlBAAAAvKCAAgBcUEADACwoIAOAFBQQA8IICAgB4QQEBALyggAAAXlBAAAAvAsZl0mMcRSIRhUIhpaamWg31c/k0XD91lyGAmZmZ1pm0tDTrTDgcts4kciirC5fhji5DWSW3gZpffvmldWbq1KnWmUceecQ6c+edd1pnJOnQoUPWmZaWFutMe3u7dcblfO3Vy23usstQW5fHFZfBwy7f665svweNMYpGowqHw8rOzj7rdjwDAgB4QQEBALywKqCKigpde+21ysrKUv/+/TVt2jTV1NTEbHP8+HGVl5erb9++6t27t6ZPn67GxsYuXTQAoPuzKqCqqiqVl5dr8+bNeu+999Te3q5JkybF/Oz3gQce0LvvvqtVq1apqqpKBw4c0O23397lCwcAdG9Wr8ytW7cu5u3Kykr1799f27Zt04QJExQOh/XKK69oxYoVuvnmmyVJy5cv13e/+11t3rxZ3//+97tu5QCAbu2CXgM6dRVGnz59JEnbtm1Te3u7SktLO7cZPny4Bg4cqOrq6jN+jNbWVkUikZgbAKDncy6gaDSqBQsW6LrrrtPIkSMlSQ0NDUpPTz/tUta8vDw1NDSc8eNUVFQoFAp13oqKilyXBADoRpwLqLy8XDt37tTKlSsvaAGLFi1SOBzuvO3fv/+CPh4AoHtw+u2s+fPna+3atdq0aZMGDBjQeX9+fr7a2trU1NQU8yyosbFR+fn5Z/xYwWBQwWDQZRkAgG7M6hmQMUbz58/X6tWr9f7776u4uDjm/VdffbXS0tK0YcOGzvtqamq0b98+jR8/vmtWDADoEayeAZWXl2vFihV6++23lZWV1fm6TigUUkZGhkKhkGbPnq2FCxeqT58+ys7O1v3336/x48dzBRwAIIZVAS1btkySdOONN8bcv3z5cs2aNUuS9NxzzyklJUXTp09Xa2urJk+erN/85jddslgAQM+RtMNIA4GA0xDKZOUyDPFcQ/zO5uuvv7bOJNkpcBqX8yA9Pd1pXy5XYQ4ZMsQ688wzz1hnfvCDH1hnvjmp5HylpqZaZ1yG9LoMI3VZm8tQ0URK9u9BW8YYGWMYRgoASE4UEADACwoIAOAFBQQA8IICAgB4QQEBALyggAAAXlBAAAAvKCAAgBcUEADACwoIAOAFBQQA8IICAgB44fQXUZORy8TkRE6gPXHihHWmqanJOtPTpupKbtOPR4wY4bQvl6/TFVdcYZ154YUXrDMuk61dz4doNGqdcfk6uazP5WuE5MQzIACAFxQQAMALCggA4AUFBADwggICAHhBAQEAvKCAAABeUEAAAC8oIACAFxQQAMALCggA4AUFBADwoscMI+2JQzhdBkL2ROnp6daZw4cPO+2rsLDQOlNdXW2d+fe//22dSSSXc6+trc06k+xDhBFfPAMCAHhBAQEAvKCAAABeUEAAAC8oIACAFxQQAMALCggA4AUFBADwggICAHhBAQEAvKCAAABeUEAAAC+SdhhpSkqK1aDCZB/c6TJ0MS0tzTrTt29f60xOTo51RnIbqDls2DDrTG1trXWmpKTEOiNJn3zyiXVm6NChTvuytXv3buvMsWPH4rCSM0tNTbXOnDhxIg4r6X5SUuyfCyT7Y9754BkQAMALCggA4AUFBADwggICAHhBAQEAvKCAAABeUEAAAC8oIACAFxQQAMALCggA4AUFBADwggICAHiRtMNIo9Go0wDPREjU4ECXQY1ffvmldaa+vt464+rVV1+1zvzlL3+xzjz22GPWGcltsGheXp51xuVz6ujosM64cjnHE7m+nsbl8cHla+S6r3jhGRAAwAsKCADghVUBVVRU6Nprr1VWVpb69++vadOmqaamJmabG2+8UYFAIOY2d+7cLl00AKD7syqgqqoqlZeXa/PmzXrvvffU3t6uSZMmqaWlJWa7++67T/X19Z23JUuWdOmiAQDdn9VFCOvWrYt5u7KyUv3799e2bds0YcKEzvszMzOVn5/fNSsEAPRIF/QaUDgcliT16dMn5v7XX39dubm5GjlypBYtWqSjR4+e9WO0trYqEonE3AAAPZ/zZdjRaFQLFizQddddp5EjR3bef9ddd2nQoEEqLCzUjh079PDDD6umpkZvvfXWGT9ORUWFnnzySddlAAC6KecCKi8v186dO/Xhhx/G3D9nzpzOf48aNUoFBQWaOHGi6urqNGTIkNM+zqJFi7Rw4cLOtyORiIqKilyXBQDoJpwKaP78+Vq7dq02bdqkAQMGnHPbkpISSVJtbe0ZCygYDCoYDLosAwDQjVkVkDFG999/v1avXq2NGzequLj4WzPbt2+XJBUUFDgtEADQM1kVUHl5uVasWKG3335bWVlZamhokCSFQiFlZGSorq5OK1as0C233KK+fftqx44deuCBBzRhwgSNHj06Lp8AAKB7siqgZcuWSTr5y6b/v+XLl2vWrFlKT0/X+vXr9fzzz6ulpUVFRUWaPn26Hn300S5bMACgZ7D+Edy5FBUVqaqq6oIWBAC4OCTtNGxbLhcytLa2Ou0rUdNkXfbzbf9J6EppaWnWmV//+tfWGZcf3+bk5FhnJKmtrc06s2rVKutMsk+OdjmPEnnuJYrLRP5EHYdETrW2/V43xpzX9xLDSAEAXlBAAAAvKCAAgBcUEADACwoIAOAFBQQA8IICAgB4QQEBALyggAAAXlBAAAAvKCAAgBcUEADAi4BJsgmCkUhEoVBIgUDAaRBgT5KammqdcRly6XqcE3XquKzP5dhJ0okTJ6wzyTywEvDBGCNjjMLhsLKzs8+6Hc+AAABeUEAAAC8oIACAFxQQAMALCggA4AUFBADwggICAHhBAQEAvKCAAABeUEAAAC8oIACAF718L+CbTs3IYlaW2zFI5HFL5q+R69oS9Tkl87EDLtT5Po4nXQE1Nzd3/vti/yZ1GYzpItmPs8v6otFoHFZyZsl+/ABfmpubFQqFzvr+pJuGHY1GdeDAAWVlZZ02ZTgSiaioqEj79+8/54TVno7jcBLH4SSOw0kch5OS4TgYY9Tc3KzCwkKlpJz9lZ6kewaUkpKiAQMGnHOb7Ozsi/oEO4XjcBLH4SSOw0kch5N8H4dzPfM5hYsQAABeUEAAAC+6VQEFg0E9/vjjCgaDvpfiFcfhJI7DSRyHkzgOJ3Wn45B0FyEAAC4O3eoZEACg56CAAABeUEAAAC8oIACAF92mgJYuXarvfOc7uuSSS1RSUqJ//OMfvpeUcE888YQCgUDMbfjw4b6XFXebNm3S1KlTVVhYqEAgoDVr1sS83xijxYsXq6CgQBkZGSotLdWePXv8LDaOvu04zJo167TzY8qUKX4WGycVFRW69tprlZWVpf79+2vatGmqqamJ2eb48eMqLy9X37591bt3b02fPl2NjY2eVhwf53McbrzxxtPOh7lz53pa8Zl1iwJ68803tXDhQj3++OP66KOPNGbMGE2ePFkHDx70vbSEGzFihOrr6ztvH374oe8lxV1LS4vGjBmjpUuXnvH9S5Ys0YsvvqiXX35ZW7Zs0aWXXqrJkyfr+PHjCV5pfH3bcZCkKVOmxJwfb7zxRgJXGH9VVVUqLy/X5s2b9d5776m9vV2TJk1SS0tL5zYPPPCA3n33Xa1atUpVVVU6cOCAbr/9do+r7nrncxwk6b777os5H5YsWeJpxWdhuoFx48aZ8vLyzrc7OjpMYWGhqaio8LiqxHv88cfNmDFjfC/DK0lm9erVnW9Ho1GTn59vnn766c77mpqaTDAYNG+88YaHFSbGN4+DMcbMnDnT3HrrrV7W48vBgweNJFNVVWWMOfm1T0tLM6tWrercZvfu3UaSqa6u9rXMuPvmcTDGmP/7v/8zP/nJT/wt6jwk/TOgtrY2bdu2TaWlpZ33paSkqLS0VNXV1R5X5seePXtUWFiowYMH6+6779a+fft8L8mrvXv3qqGhIeb8CIVCKikpuSjPj40bN6p///4aNmyY5s2bp0OHDvleUlyFw2FJUp8+fSRJ27ZtU3t7e8z5MHz4cA0cOLBHnw/fPA6nvP7668rNzdXIkSO1aNEiHT161MfyzirphpF+01dffaWOjg7l5eXF3J+Xl6d//etfnlblR0lJiSorKzVs2DDV19frySef1A033KCdO3cqKyvL9/K8aGhokKQznh+n3nexmDJlim6//XYVFxerrq5OjzzyiMrKylRdXa3U1FTfy+ty0WhUCxYs0HXXXaeRI0dKOnk+pKenKycnJ2bbnnw+nOk4SNJdd92lQYMGqbCwUDt27NDDDz+smpoavfXWWx5XGyvpCwj/U1ZW1vnv0aNHq6SkRIMGDdKf/vQnzZ492+PKkAxmzJjR+e9Ro0Zp9OjRGjJkiDZu3KiJEyd6XFl8lJeXa+fOnRfF66DncrbjMGfOnM5/jxo1SgUFBZo4caLq6uo0ZMiQRC/zjJL+R3C5ublKTU097SqWxsZG5efne1pVcsjJydHQoUNVW1vreynenDoHOD9ON3jwYOXm5vbI82P+/Plau3atPvjgg5g/35Kfn6+2tjY1NTXFbN9Tz4ezHYczKSkpkaSkOh+SvoDS09N19dVXa8OGDZ33RaNRbdiwQePHj/e4Mv+OHDmiuro6FRQU+F6KN8XFxcrPz485PyKRiLZs2XLRnx9ffPGFDh061KPOD2OM5s+fr9WrV+v9999XcXFxzPuvvvpqpaWlxZwPNTU12rdvX486H77tOJzJ9u3bJSm5zgffV0Gcj5UrV5pgMGgqKyvNrl27zJw5c0xOTo5paGjwvbSE+ulPf2o2btxo9u7da/72t7+Z0tJSk5ubaw4ePOh7aXHV3NxsPv74Y/Pxxx8bSebZZ581H3/8sfn888+NMcb88pe/NDk5Oebtt982O3bsMLfeeqspLi42x44d87zyrnWu49Dc3GwefPBBU11dbfbu3WvWr19vvve975krr7zSHD9+3PfSu8y8efNMKBQyGzduNPX19Z23o0ePdm4zd+5cM3DgQPP++++brVu3mvHjx5vx48d7XHXX+7bjUFtba37+85+brVu3mr1795q3337bDB482EyYMMHzymN1iwIyxpiXXnrJDBw40KSnp5tx48aZzZs3+15Swt1xxx2moKDApKenm8svv9zccccdpra21vey4u6DDz4wkk67zZw50xhz8lLsxx57zOTl5ZlgMGgmTpxoampq/C46Ds51HI4ePWomTZpk+vXrZ9LS0sygQYPMfffd1+P+k3amz1+SWb58eec2x44dMz/+8Y/NZZddZjIzM81tt91m6uvr/S06Dr7tOOzbt89MmDDB9OnTxwSDQXPFFVeYn/3sZyYcDvtd+Dfw5xgAAF4k/WtAAICeiQICAHhBAQEAvKCAAABeUEAAAC8oIACAFxQQAMALCggA4AUFBADwggICAHhBAQEAvKCAAABe/D9y4Lka6V1SYwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.imshow(transforms.functional.to_pil_image(sample_gan3.view(2, 1, 28, 28)[0]),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3392793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "5007be4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(gan_losses, columns =['g_loss', 'real_loss', 'fake_loss', 'd_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "857811ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.plot(df['g_loss'], label=\"Generator Loss\")\n",
    "matplotlib.pyplot.plot(df['d_loss'], label=\"Discriminator Loss\")\n",
    "matplotlib.pyplot.xlabel('epoch')\n",
    "matplotlib.pyplot.ylabel('loss')\n",
    "matplotlib.pyplot.legend(loc=\"upper left\")\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1ca492dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b9d6b730>]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.plot(df['d_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e3eddf2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.plot(vae_train_losses)\n",
    "matplotlib.pyplot.xlabel('epoch')\n",
    "matplotlib.pyplot.ylabel('loss')\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e24c0749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.plot(vae_test_losses)\n",
    "matplotlib.pyplot.xlabel('epoch')\n",
    "matplotlib.pyplot.ylabel('loss')\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115086d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
