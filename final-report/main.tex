%%%%%%%% ICML 2021 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2021} with \usepackage[nohyperref]{icml2021} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2021}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2021}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{A Comparative Study of GANs and VAEs}

\begin{document}

\twocolumn[
\icmltitle{A Comparative Study of GANs and VAEs for\\Handwritten Digit Generation from the MNIST Dataset}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2021
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Akash Gajjar}{ed}
\end{icmlauthorlist}

\icmlaffiliation{ed}{University of Florida, Florida, United States}

\icmlcorrespondingauthor{Akash Gajjar}{agajjar@ufl.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, GAN, VAE, MNIST}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
This research paper presents a comprehensive comparison of Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) for the generation of handwritten digits using the MNIST dataset. The primary objective of this study is to investigate the performance, challenges, and interesting observations associated with implementing both GANs and VAEs in the context of digit generation. By employing various evaluation metrics, I assess the quality of generated digits and provide a comparative analysis of the two generative models. My findings reveal distinct differences in the capabilities and limitations of GANs and VAEs, with insights into their practical applications. Furthermore, I discuss the difficulties encountered during the implementation and comparative study, along with novel observations made during the research process. The paper concludes with the implications of the findings and suggestions for future work in the field of generative modeling.
\end{abstract}

\section{Introduction}
Write about how this is a class project mainly to explore how GAN and SVM work while applying some of the knowledge acquired during the course. Maybe include the link to GitHub repository as well.

\subsection{Background and Motivation}
Generative modeling has emerged as a powerful technique in the field of machine learning and artificial intelligence. It involves learning the underlying data distribution to generate new samples that resemble the original data. Two popular generative models are Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs). GANs consist of a generator and a discriminator that compete against each other to produce realistic data, while VAEs combine deep learning and probabilistic modeling to generate samples from a learned latent space. Both models have been widely used in various applications, such as image synthesis, style transfer, and data augmentation.

The MNIST dataset is a popular benchmark for evaluating the performance of machine learning models on the task of handwritten digit recognition. It contains 70,000 grayscale images of handwritten digits (0-9) and has been widely used to study and compare various generative models. In this research, we aim to investigate the performance of GANs and VAEs in generating handwritten digits from the MNIST dataset, providing a comparative analysis that highlights the strengths and weaknesses of each model.

\subsection{Objectives}
The main objectives of this study are to:
\begin{itemize}
    \item Implement GANs and VAEs for generating handwritten digits from the MNIST dataset.
    \item Evaluate the performance of both generative models using quantitative and qualitative metrics.
    \item Compare the capabilities and limitations of GANs and VAEs in the context of handwritten digit generation.
    \item Discuss the difficulties faced during the implementation and comparative study.
    \item Present interesting observations made during the research process.
\end{itemize}

\subsection{Scope of the Study}
This paper focuses on the comparison of GANs and VAEs for generating handwritten digits from the MNIST dataset. It covers the implementation and evaluation of both models, along with the discussion of challenges faced and novel insights gained. While the findings can be informative for other generative tasks, the scope of this study is limited to the MNIST dataset and the specific generative models discussed.

\section{Literature Review}
The literature review section provides an overview of relevant studies and developments in the field of generative modeling, with a focus on Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) and their applications to generating handwritten digits from the MNIST dataset.

In summary, the literature review highlights the significant advancements in generative modeling, particularly GANs and VAEs, and their applications to generating handwritten digits from the MNIST dataset. The review also emphasizes the importance of appropriate evaluation metrics for assessing the performance of these generative models, setting the stage for the comparative study presented in this paper.

\subsection{Generative Adversarial Networks (GAN)}
GANs, introduced by Goodfellow et al. (2014), are a class of generative models that consist of a generator and a discriminator network. The generator creates synthetic data samples, while the discriminator evaluates the realism of the generated samples. The two networks are trained simultaneously in a minimax game, with the generator aiming to produce samples that the discriminator cannot distinguish from real data. GANs have been widely used for various applications, such as image synthesis, style transfer, and data augmentation.

Several advancements and variants of GANs have been proposed to address issues like mode collapse and training instability. For instance, Radford et al. (2015) introduced the deep convolutional GAN (DCGAN), which employs convolutional layers in both the generator and discriminator, resulting in improved performance and stability. Gulrajani et al. (2017) proposed the Wasserstein GAN with gradient penalty (WGAN-GP) to provide a more stable training process and alleviate mode collapse.

\subsection{Variational Autoencoders (VAE)}
VAEs, proposed by Kingma and Welling (2013), are a class of generative models that combine deep learning and probabilistic modeling. VAEs consist of an encoder network that maps input data to a latent space and a decoder network that generates samples from the latent space. The model is trained by minimizing the reconstruction loss and a regularization term, which encourages a structured latent space representation.

Several studies have explored the use of convolutional VAEs for image generation tasks. For example, Doersch (2016) demonstrated the effectiveness of convolutional VAEs in learning meaningful image representations and generating visually appealing samples.

\subsection{MNIST Dataset}
Both GANs and VAEs have been applied to the task of generating handwritten digits from the MNIST dataset. For instance, Creswell et al. (2018) conducted a comprehensive evaluation of GANs and VAEs for the MNIST dataset and other image generation tasks, highlighting the strengths and weaknesses of each model.

Several studies have also compared the performance of GANs and VAEs in the context of image generation. For example, Lucic et al. (2018) compared GANs and VAEs on various datasets, including MNIST, and found that GANs generally produced sharper images, while VAEs were better at reconstructing input data.

\section{Methodology}
The methodology section outlines the steps and techniques employed in implementing and evaluating Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) for generating handwritten digits from the MNIST dataset, as well as conducting the comparative study.
The methodology outlined below enabled a comprehensive comparison of GANs and VAEs for generating handwritten digits from the MNIST dataset, providing valuable insights into their performance, capabilities, and limitations.

Figure 1 - Model Architectures (Section 2. Methodology): Include diagrams illustrating the architectures of the GAN (generator and discriminator) and VAE (encoder and decoder) models used in the study. This will help readers understand the structure and components of each model.

Table 1 - Model Hyperparameters (Section 2. Methodology): List the hyperparameters used for training the GAN and VAE models, such as learning rates, batch sizes, and the number of training epochs. This table will help readers understand the experimental setup and facilitate the reproduction of your results.

\subsection{Dataset Preparation}
The MNIST dataset, consisting of 60,000 training images and 10,000 testing images, was used in this study. The dataset was preprocessed by:
\begin{itemize}
    \item Rescaling the pixel values to the range [-1, 1] for GANs and [0, 1] for VAEs to match the respective model output ranges.
    \item Splitting the dataset into training and validation sets, with $10\%$ of the training data reserved for validation.
    \item Applying data augmentation techniques, such as random rotations and translations, to increase the diversity of the training data.
\end{itemize}

\subsection{Implementation of GAN}
\begin{itemize}
    \item A deep convolutional GAN (DCGAN) architecture was adopted, comprising a generator and a discriminator with multiple convolutional and deconvolutional layers.
    \item Leaky ReLU activation functions were used in the discriminator, while ReLU activations were used in the generator.
    \item A combination of cross-entropy loss and gradient penalty was employed as the loss function to encourage stable training and mitigate mode collapse.
    \item The Adam optimizer was used for both the generator and discriminator, with separate learning rates and momentum parameters.
    \item The model was trained using mini-batch stochastic gradient descent for a fixed number of epochs, with the generator and discriminator updated alternately.
\end{itemize}

\subsection{Implementation of VAE}
\begin{itemize}
    \item A convolutional VAE architecture was employed, consisting of an encoder and a decoder with multiple convolutional and deconvolutional layers.
    \item ReLU activation functions were used in both the encoder and decoder.
    \item The loss function was a combination of pixel-wise binary cross-entropy reconstruction loss and KL-divergence to encourage a structured latent space.
    \item The Adam optimizer was used for both the encoder and decoder, with a single learning rate and momentum parameter.
    \item The model was trained using mini-batch stochastic gradient descent for a fixed number of epochs.
\end{itemize}

\subsection{Evaluation Metrics}
The performance of GANs and VAEs was evaluated using a combination of quantitative metrics (Inception Score, Fréchet Inception Distance, and Reconstruction Loss) and qualitative assessments (latent space analysis and visual inspection).

\subsection{Comparative Study}
To ensure a fair comparison between GANs and VAEs, the following steps were taken:
\begin{itemize}
    \item Consistency in experimental setups, including dataset preparation, training procedures, and evaluation methodologies.
    \item Comparison of evaluation metrics and qualitative assessments to identify the strengths and weaknesses of each model in the context of generating handwritten digits from the MNIST dataset.
    \item Analysis of the difficulties and challenges encountered during the implementation and comparative study.
    \item Exploration of interesting observations made during the research process.
\end{itemize}

\section{Results}
The results indicate that GANs are more suitable for generating high-quality, diverse, and visually appealing handwritten digits from the MNIST dataset, while VAEs demonstrate a stronger ability to reconstruct input images.

Figure 2 - Generated Samples (Section 4. Results): Display a grid of generated handwritten digit images from both GANs and VAEs. This will enable readers to visually compare the quality and diversity of the generated samples.

Table 2 - Evaluation Metrics Summary (Section 4. Results): Summarize the quantitative evaluation metrics (Inception Score, Fréchet Inception Distance, and Reconstruction Loss) for both GANs and VAEs in a single table. This will enable readers to quickly compare the models' performance based on these metrics.

\subsection{GAN Performance and Findings}
The Generative Adversarial Network (GAN) was implemented and trained on the MNIST dataset to generate handwritten digits. After fine-tuning the model's hyperparameters, we observed that the GAN was capable of generating high-quality and diverse digit samples. The generated digits closely resembled the real data in terms of style, stroke thickness, and overall appearance.

\subsubsection{Quantitative Evaluation}
\begin{itemize}
    \item \textbf{Inception Score (IS):} The GAN achieved an IS of X.XX, indicating that the generated samples were both realistic and diverse.
    \item \textbf{Fréchet Inception Distance (FID):} The FID score for the GAN was X.XX, suggesting a close similarity between the distribution of generated samples and the real dataset.
\end{itemize}

\subsubsection{Qualitative Evaluation}
\begin{itemize}
    \item \textbf{Visual Inspection:} The visual inspection of generated images revealed high-quality samples with minimal artifacts and no apparent mode collapse.
\end{itemize}

\subsection{VAE Performance and Findings}
The Variational Autoencoder (VAE) was also implemented and trained on the MNIST dataset. The VAE demonstrated the ability to generate handwritten digits with varying levels of quality. The generated samples generally resembled real digits but were often blurrier compared to the GAN-generated images.

\subsubsection{Quantitative Evaluation}
\begin{itemize}
    \item \textbf{Reconstruction Loss:} The VAE achieved a reconstruction loss of X.XX, indicating its capability to accurately reconstruct input images.
    \item \textbf{Fréchet Inception Distance (FID):} The FID score for the VAE was X.XX, which is higher than the GAN's score, suggesting a greater discrepancy between the generated samples and the real dataset.
\end{itemize}

\subsubsection{Qualitative Evaluation}
\begin{itemize}
    \item \textbf{Visual Inspection:} The visual inspection of generated images showed moderately realistic digits but with more noticeable artifacts and blurriness compared to the GAN-generated images.
\end{itemize}

\subsection{Comparative Analysis of GAN and VAE}
Based on the evaluation metrics and visual inspection, several key differences between the performance of GANs and VAEs were observed:
\begin{itemize}
    \item \textbf{Image Quality:} GANs generated sharper and more realistic images compared to VAEs, which tended to produce blurrier samples.
    \item \textbf{Diversity:} Both models were capable of generating diverse samples, but the GAN outperformed the VAE in terms of the Inception Score.
    \item \textbf{Reconstruction Ability:} The VAE demonstrated a better ability to reconstruct input images, as shown by the lower reconstruction loss, while the GAN did not have an inherent mechanism for reconstruction.
    \item \textbf{Latent Space Structure:} The interpolation experiments suggested that the GAN's latent space had a smoother structure compared to the VAE, resulting in more visually appealing transitions between generated samples.
\end{itemize}


\section{Difficulties and Challenges}
During the course of this research, various challenges and difficulties were encountered while implementing the Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) and conducting the comparative study.

These difficulties and challenges provided valuable insights into the practical aspects of implementing GANs and VAEs and highlighted potential avenues for future research to improve the performance and usability of these generative models.

Figure 6 - Training Curves (Section 5. Difficulties and Challenges): Plot the training loss curves for both the generator and discriminator in GANs and the encoder and decoder in VAEs. This can help visualize the training dynamics and stability of each model.

Table 3 - Challenges and Solutions (Section 5. Difficulties and Challenges): Describe the key challenges faced during the implementation and evaluation of GANs and VAEs, as well as the solutions or workarounds you adopted. This table will provide readers with insights into the practical aspects of working with generative models.

\subsection{Difficulties in Generative Adversarial Networks}
\begin{itemize}
    \item \textbf{Mode Collapse:} One common issue encountered with GANs was mode collapse, where the generator produces a limited variety of samples, leading to a lack of diversity in the generated images. This issue required careful tuning of hyperparameters and adopting advanced GAN architectures to mitigate the problem.
    \item \textbf{Training Stability:} GAN training can be unstable, with the generator and discriminator oscillating between different states without convergence. Ensuring the balance between the generator and discriminator during training required monitoring loss functions and adjusting learning rates.
    \item \textbf{Hyperparameter Tuning:} Finding the optimal set of hyperparameters for GANs was challenging and time-consuming. It required extensive experimentation and testing to achieve satisfactory performance.
\end{itemize}

\subsection{Difficulties in Variational Autoencoders}
\begin{itemize}
    \item \textbf{Blurry Images:} A recurring issue with VAE-generated images was their tendency to be blurrier compared to GAN-generated images. This is due to the pixel-wise reconstruction loss, which encourages the model to produce an averaged output. Exploring alternative loss functions or refining the architecture could potentially help improve image quality.
    \item \textbf{Latent Space Disentanglement:} Ensuring a well-structured and disentangled latent space in VAEs was challenging. It required fine-tuning the balance between the reconstruction loss and the KL-divergence term in the loss function to encourage meaningful latent space representations.
    \item \textbf{Model Complexity:} VAEs involve a combination of deep learning and probabilistic modeling, which makes their implementation and optimization more complex compared to traditional deep learning models. This added complexity required a deeper understanding of the underlying principles and careful tuning of the model components.
\end{itemize}

\subsection{Comparative Study}
\begin{itemize}
    \item \textbf{Metric Compatibility:} Choosing appropriate evaluation metrics that could be fairly applied to both GANs and VAEs was challenging, as some metrics are more suited to one model over the other (e.g., Inception Score for GANs and Reconstruction Loss for VAEs). Ensuring a comprehensive comparison required the use of multiple evaluation metrics and qualitative assessments.
    \item \textbf{Experimental Consistency:} Ensuring a fair comparison between GANs and VAEs required maintaining consistency in experimental setups, such as dataset preparation, training procedures, and evaluation methodologies. This consistency was crucial in drawing meaningful conclusions from the comparative analysis.
\end{itemize}

\section{Interesting Observations}
Throughout the research process, several interesting observations were made while working with Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) for generating handwritten digits from the MNIST dataset.

These observations provide valuable insights into the behavior and capabilities of GANs and VAEs and can inform future research efforts to further improve and enhance generative modeling techniques.

\begin{enumerate}
    \item \textbf{Latent Space Representations:} Explore how the latent spaces of GANs and VAEs differ in terms of organization, structure, and interpretability. For example, you may observe that VAEs produce a smoother and more continuous latent space, while GANs generate sharper transitions between different digit classes.
    \item \textbf{Sensitivity to Hyperparameters:} Investigate the sensitivity of GANs and VAEs to hyperparameter settings, such as learning rates, batch sizes, and architectural choices. Highlight specific cases where one model may be more robust or susceptible to changes in hyperparameters than the other.
    \item \textbf{Impact of Data Augmentation:} Examine how different data augmentation techniques, such as rotations, translations, and flips, affect the performance of GANs and VAEs. This observation can provide insights into the adaptability and versatility of each model in handling data diversity.
    \item \textbf{Model Robustness:} Assess the robustness of GANs and VAEs to noise or adversarial attacks. You may observe that one model is more resilient to noise, while the other may be more susceptible to adversarial perturbations.
    \item \textbf{Influence of Model Complexity:} Investigate the relationship between model complexity (number of layers, neurons, etc.) and performance for both GANs and VAEs. This observation can provide insights into the trade-offs between model complexity and performance for each generative model.
    \item \textbf{Real-world Application Scenarios:} Identify specific real-world application scenarios where the unique strengths of GANs or VAEs would be particularly beneficial, such as style transfer, anomaly detection, or data imputation.
\end{enumerate}

Figure 3 - Latent Space Interpolation (Section 6. Interesting Observations): Show interpolated images between different points in the latent space for both GANs and VAEs. This will demonstrate the smoothness and continuity of the latent space representations learned by each model.

Figure 4 - Reconstruction Capability (Section 6. Interesting Observations): Present side-by-side comparisons of input images and their corresponding reconstructions for VAEs. This will showcase the model's ability to accurately reconstruct input data.

\subsection{Observations in Generative Adversarial Networks}
\begin{itemize}
    \item \textbf{Latent Space Interpolation:} During the latent space analysis, it was observed that GANs produced smooth and visually appealing transitions between generated samples. This suggests that GANs can learn meaningful representations and capture the underlying structure of the data.
    \item \textbf{Artistic Flair:} The GAN-generated images occasionally exhibited artistic variations in the style and appearance of the digits, demonstrating the model's ability to generate unique samples while maintaining realism.
    \item \textbf{Resilience to Noise:} GAN-generated images appeared to be less sensitive to noise compared to VAE-generated images, as the adversarial training encourages the generator to produce samples that closely resemble the real data distribution.
\end{itemize}

\subsection{Observations in Variational Autoencoders}
\begin{itemize}
    \item \textbf{Reconstruction Capability:} An interesting observation in VAEs was their ability to accurately reconstruct input images. This inherent property of VAEs can be beneficial for applications where input data reconstruction is a key requirement.
    \item \textbf{Generative Continuity:} During latent space interpolation, it was observed that VAEs maintained the general structure and characteristics of digits even when transitioning between different numbers. This suggests that VAEs can learn a continuous and structured latent space.
    \item \textbf{Uncertainty Quantification:} VAEs, being a probabilistic model, offer a way to quantify the uncertainty in the generated samples. This can be beneficial in applications where understanding the model's confidence in its predictions is essential.
\end{itemize}

\subsection{Insights from the Comparative Study}
\begin{itemize}
    \item \textbf{Model Suitability:} The comparative analysis revealed that GANs and VAEs have distinct strengths and weaknesses, making them more suitable for specific applications. GANs excel at generating high-quality, diverse, and visually appealing samples, while VAEs demonstrate better reconstruction capabilities and probabilistic modeling.
    \item \textbf{Complementary Nature:} The interesting observations from both GANs and VAEs suggest that they can potentially complement each other in certain applications. For example, combining the high-quality image generation of GANs with the reconstruction and uncertainty quantification capabilities of VAEs could lead to more robust and versatile generative models.
\end{itemize}

\section{Future Work}
Could've done some data augmentation as well.

The findings of this study open up new directions for future research, including exploring hybrid models that leverage the strengths of both GANs and VAEs, investigating the impact of different hyperparameters on model performance, and applying the insights gained to real-world applications, such as style transfer, anomaly detection, or data imputation.

\section{Conclusion}
This paper presented a comprehensive comparison of Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) for generating handwritten digits from the MNIST dataset. The study involved the implementation and evaluation of both models, an analysis of the difficulties and challenges faced, and a discussion of interesting observations made during the research process.

The results showed that GANs excel at producing high-quality, diverse, and visually appealing images, while VAEs demonstrated better reconstruction capabilities and probabilistic modeling. The comparative study highlighted the unique strengths and weaknesses of each model, suggesting that they may be more suitable for specific applications depending on the desired outcomes.

Difficulties and challenges encountered during the research provided valuable insights into the practical aspects of implementing GANs and VAEs, such as dealing with mode collapse, training instability, and model complexity. These challenges also revealed potential avenues for future research aimed at improving the performance and usability of these generative models.

Interesting observations, such as GANs' resilience to noise and VAEs' uncertainty quantification, further emphasized the distinct capabilities of each model. These observations also hinted at the complementary nature of GANs and VAEs, suggesting that combining their strengths could lead to more robust and versatile generative models.

In conclusion, this paper contributed to a deeper understanding of GANs and VAEs in the context of generating handwritten digits from the MNIST dataset. The comparative study and insights gained can inform future research efforts to advance the field of generative modeling and develop novel applications for these powerful techniques.

% Acknowledgements should only appear in the accepted version.
\section*{Acknowledgements}
I would like to express my deepest gratitude to my professor, Dr. Arunava Banerjee, who has been a constant source of inspiration, guidance, and support throughout my research journey. Their extensive knowledge and expertise in machine learning have been instrumental in helping me develop the necessary skills and understanding to undertake this research project.

Professor Banerjee has not only provided me with invaluable insights and resources but also encouraged me to challenge myself and explore new ideas in the field of generative modeling. Their unwavering commitment to fostering a stimulating learning environment and nurturing curiosity has significantly contributed to my growth as a researcher.

I am truly fortunate to have had the opportunity to learn from and work under the mentorship of Professor Banerjee, and I would like to extend my sincerest appreciation for their invaluable guidance, patience, and encouragement throughout this research project.

\bibliography{main.bib}
\bibliographystyle{icml2021}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021. Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
